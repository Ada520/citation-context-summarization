<paper>
<cited id="B0">
<title id=" C04-1177.xml">automatic identification of infrequent word senses </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another possibility would be concentrate the selection process to senses with higher frequency, and filter out rare senses.
</prevsent>
<prevsent>this is implicitly done by systems which relyon hand-tagged training corpora, since rare senses often do not occur in the available data.
</prevsent>
</prevsection>
<citsent citstr=" W04-0837 ">
in this paper we use an unsupervised method to rank word senses from an inventory according to prevalence (mccarthy et al, 2004<papid> W04-0837 </papid>a), and utilise the ranking scores to identify senses which are rare.</citsent>
<aftsection>
<nextsent>we use wordnet for our inventory, since it is widely usedand freely available, but our method could in principle be used with another mrd (we comment on this in the conclusions).
</nextsent>
<nextsent>we report work with nounshere, and leave evaluation on other pos for the future.
</nextsent>
<nextsent>our approach exploits automatically acquired thesauruses which provide nearest neighbours?
</nextsent>
<nextsent>forgiven word entry.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B14">
<title id=" C04-1177.xml">automatic identification of infrequent word senses </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>21ffi  oqp i o=s oqs=o$tuv 5fiffifi *fi +  1   (1) where: *fifi 5fi#+w61 b x.yz o\[ oqs oqs=o]t g,h $*fiffifi 5fi#+$!
</prevsent>
<prevsent>fi#7fl  2.2 acquiring the automatic thesaurus.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
there are many alternative distributional similarity measures proposed in the literature, for this work we used the measure and thesaurus construction method described by lin (1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>for input we used grammatical relation data extracted using an automatic parser (briscoe and carroll, 2002).
</nextsent>
<nextsent>for each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.
</nextsent>
<nextsent>we could easily extend the set of relations in the future.
</nextsent>
<nextsent>a noun,  , is thus described by set of co-occurrence triples ^ !d_a` and associated frequencies, where dis grammatical relation and _ is possible cooccurrence with  in that relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B15">
<title id=" C04-1177.xml">automatic identification of infrequent word senses </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pan tel and lin (2002) are working with automatically constructed thesauruses and identifying senses directly from the nearest neighbours, where the gran ularity depends on the parameters of the clusteringprocess.
</prevsent>
<prevsent>in contrast we are using the nearest neighbours to indicate the frequency of the senses of the target word, using semantic similarity between the neighbours and the word senses listed in wordnet.
</prevsent>
</prevsection>
<citsent citstr=" J04-1003 ">
we do so here in order to identify the senses of the word which are rare in corpus data.lapata and brew (2004) <papid> J04-1003 </papid>have recently used syntactic evidence to produce prior distribution for verb senses and incorporate this in wsd system.the work presented here focusses on using prevalence ranking for word senses to identify andre move rare senses from generic resource such aswordnet.</citsent>
<aftsection>
<nextsent>we believe that this method will be useful for systems using such resource, which can incorporate prior distributions over word senses orwish to identify and remove rare word senses.
</nextsent>
<nextsent>systems requiring sense frequency distributions currently relyon available hand-tagged training data,and for wordnet the most extensive resource for all words is semcor.
</nextsent>
<nextsent>whilst semcor is extremely useful, it comprises only 250,000 words taken from subset of the brown corpus and novel.
</nextsent>
<nextsent>because of its size, and the zipfian distribution of words, there are many words which do not occur in this resource, for example embryo, fridge, pancake, wheelbarrow and many words which occur only once or twice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B22">
<title id=" C02-1079.xml">best analysis selection in inflectional languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we rather take figure of merit as measure bounding the true probabilities of the complete parses.
</prevsent>
<prevsent>s  hh np1  hh ap   hh hh adj and adj n1 np4  hh adj np4  hh n4 n2 ??
</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
selected trigrams: [adj,and,adj] [adj,n1,v] [n1,v,n4] [v,adj,n4] [adj,n4,n2] figure 2: lexical heads as n-grams elements.the standard methods of the best analysis selection (caraballo and charniak, 1998)<papid> J98-2004 </papid>usually use simple stochastic functions independent on the peculiarities of the underlying language.</citsent>
<aftsection>
<nextsent>this approach seems to work satisfactorily in case of analytical languages.
</nextsent>
<nextsent>on the other hand, the obstacles brought by the synthetical languages in relationship with those simple statistical techniques are indispensable.
</nextsent>
<nextsent>therefore, we try to improve the standardfoms taking into consideration specific features of free word order languages.
</nextsent>
<nextsent>the following text discusses the assets of three figures of merit that reflect selected phenomena of the czech language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B23">
<title id=" C02-1079.xml">best analysis selection in inflectional languages </title>
<section> figures of merit.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 rule-tied actions and 1 fom.
</prevsent>
<prevsent>a key question is then what the good candidates for foms are.
</prevsent>
</prevsection>
<citsent citstr=" H90-1053 ">
the use of probabilistic context-free grammars (pcfgs) involves simple cf rule probabilities to forma fom (chitrao and grishman, 1990; <papid> H90-1053 </papid>bo brow, 1991).<papid> H91-1042 </papid></citsent>
<aftsection>
<nextsent>the evaluation of the first fom is based on the mechanism of contextual actions built into the meta grammar conception (smrz?
</nextsent>
<nextsent>and horak, 2000).
</nextsent>
<nextsent>it distinguishes four kinds of contextual actions, tests or constraints: 1.
</nextsent>
<nextsent>rule-tied actions 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B24">
<title id=" C02-1079.xml">best analysis selection in inflectional languages </title>
<section> figures of merit.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 rule-tied actions and 1 fom.
</prevsent>
<prevsent>a key question is then what the good candidates for foms are.
</prevsent>
</prevsection>
<citsent citstr=" H91-1042 ">
the use of probabilistic context-free grammars (pcfgs) involves simple cf rule probabilities to forma fom (chitrao and grishman, 1990; <papid> H90-1053 </papid>bo brow, 1991).<papid> H91-1042 </papid></citsent>
<aftsection>
<nextsent>the evaluation of the first fom is based on the mechanism of contextual actions built into the meta grammar conception (smrz?
</nextsent>
<nextsent>and horak, 2000).
</nextsent>
<nextsent>it distinguishes four kinds of contextual actions, tests or constraints: 1.
</nextsent>
<nextsent>rule-tied actions 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B26">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase.
</prevsent>
<prevsent>the summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
the importance of learning to manipulate monolingual paraphrase relationships for applications like summarization, search, and dialog has been highlighted by number of recent efforts (barzilay &amp; mckeown 2001; <papid> P01-1008 </papid>shinyama et al 2002; lee &amp; barzilay 2003; lin &amp; pantel 2001).</citsent>
<aftsection>
<nextsent>while several different learning methods have been applied to this problem, all share need for large amounts of data in the form of pairs or sets of strings that are likely to exhibit lexical and/or structural paraphrase alternations.
</nextsent>
<nextsent>one approach1 1 an alternative approach involves identifying anchor points--pairs of words linked in known way--and collecting the strings that intervene.
</nextsent>
<nextsent>(shinyama, et al 2002; lin &amp; pantel 2001).
</nextsent>
<nextsent>since our interest is in that has been successfully used is edit distance, measure of similarity between strings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B27">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their goal is to extract sentential templates that can be used in high-precision generation of paraphrase alternations within limited domain.
</prevsent>
<prevsent>our goal here is rather different: our interest lies in constructing monolingual broad-domain corpus of pairwise aligned sentences.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
such data would be amenable to conventional statistical machine translation (smt) techniques (e.g., those discussed in och &amp; ney 2003).<papid> J03-1002 </papid>2 in what follows we compare two strategies for unsupervised construction of such corpus, one employing string similarity and the other associating sentences that may overlap very little at the string level.</citsent>
<aftsection>
<nextsent>we measure the relative utility of the two derived monolingual corpora in the context of word alignment techniques developed originally for bilingual text.
</nextsent>
<nextsent>we show that although the edit distance corpus is well-suited as training data for the alignment algorithms currently used in smt, it is an incomplete source of information about paraphrase relations, which exhibit many of the characteristics of comparable bilingual corpora or free translations.
</nextsent>
<nextsent>many of the more complex alternations that characterize monolingual paraphrase, such as large-scale lexical alternations and constituent reorderings, are not readily learning sentence level paraphrases, including major constituent reorganizations, we do not address this approach here.
</nextsent>
<nextsent>2 barzilay &amp; mckeown (2001) <papid> P01-1008 </papid>consider the possibility of using smt machinery, but reject the idea because of the noisy, comparable nature of their dataset.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B30">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> data/methodology.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude that paraphrase research would benefit by identifying richer data sources and developing appropriate learning techniques.
</prevsent>
<prevsent>our two paraphrase datasets are distilled from corpus of news articles gathered from thousands of news sources over an extended period.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
while the idea of exploiting multiple news reports for paraphrase acquisition is not new, previous efforts (for example, shinyama et al 2002; barzilay and lee 2003) <papid> N03-1003 </papid>have been restricted to at most two news sources.</citsent>
<aftsection>
<nextsent>our work represents what we believe to be the first attempt to exploit the explosion of news coverage on the web, where single event can generate scores or hundreds of different articles within brief period of time.
</nextsent>
<nextsent>some of these articles represent minor rewrites of an original ap or reuters story, while others represent truly distinct descriptions of the same basic facts.
</nextsent>
<nextsent>the massive redundancy of information conveyed with widely varying surface strings is resource begging to be exploited.
</nextsent>
<nextsent>figure 1 shows the flow of our data collection process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B33">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> levenshtein distance.  </section>
<citcontext>
<prevsection>
<prevsent>paraphrase data is of course monolingual, but otherwise the task is very similar to the mt alignment problem, posing the same issues with one-to-many, many-to-many, and one/many-to null word mappings.
</prevsent>
<prevsent>our priori assumption was that the lower the aer for corpus, the more likely it would be to yield learn able information about paraphrase alternations.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
we closely followed the evaluation standards established in melamed (2001) and och &amp; ney (2000),  <papid> P00-1056 </papid>ney (2003).</citsent>
<aftsection>
<nextsent>following och &amp; neys methodology, two annotators each created an initial annotation for each dataset, sub categorizing alignments as either sure (necessary) or possible (allowed, but not required).
</nextsent>
<nextsent>differences were then highlighted and the annotators were asked to review these cases.
</nextsent>
<nextsent>finally we combined the two annotations into single gold standard in the following manner: if both annotators agreed that an alignment should be sure, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as possible.
</nextsent>
<nextsent>to compute precision, recall, and alignment error rate (aer) for the twin datasets, we used exactly the formulae listed in och &amp; ney (2003).<papid> J03-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B36">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> levenshtein distance.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 data alignment.
</prevsent>
<prevsent>each corpus was used as input to the word alignment algorithms available in giza++ (och &amp; ney 2000).<papid> P00-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
giza++ is freely available implementation of ibm models 1-5 (brown et al 1993) <papid> J93-2003 </papid>and the hmm alignment (vogel et al 1996), <papid> C96-2141 </papid>along with various improvements and modifications motivated by experimentation by och &amp; ney (2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>giza++ accepts as input corpus of sentence pairs and produces as output viterbi alignment of that corpus as well as the parameters for the model that produced those alignments.
</nextsent>
<nextsent>while these models have proven effective at the word alignment task (mihalcea &amp; pedersen 2003), <papid> W03-0301 </papid>there are significant practical limitations in their output.</nextsent>
<nextsent>most fundamentally, all alignments have either zero or one connection to each target word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B37">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> levenshtein distance.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 data alignment.
</prevsent>
<prevsent>each corpus was used as input to the word alignment algorithms available in giza++ (och &amp; ney 2000).<papid> P00-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
giza++ is freely available implementation of ibm models 1-5 (brown et al 1993) <papid> J93-2003 </papid>and the hmm alignment (vogel et al 1996), <papid> C96-2141 </papid>along with various improvements and modifications motivated by experimentation by och &amp; ney (2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>giza++ accepts as input corpus of sentence pairs and produces as output viterbi alignment of that corpus as well as the parameters for the model that produced those alignments.
</nextsent>
<nextsent>while these models have proven effective at the word alignment task (mihalcea &amp; pedersen 2003), <papid> W03-0301 </papid>there are significant practical limitations in their output.</nextsent>
<nextsent>most fundamentally, all alignments have either zero or one connection to each target word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B39">
<title id=" C04-1051.xml">unsupervised construction of large paraphrase corpora exploiting massively parallel news sources </title>
<section> levenshtein distance.  </section>
<citcontext>
<prevsection>
<prevsent>giza++ is freely available implementation of ibm models 1-5 (brown et al 1993) <papid> J93-2003 </papid>and the hmm alignment (vogel et al 1996), <papid> C96-2141 </papid>along with various improvements and modifications motivated by experimentation by och &amp; ney (2000).<papid> P00-1056 </papid></prevsent>
<prevsent>giza++ accepts as input corpus of sentence pairs and produces as output viterbi alignment of that corpus as well as the parameters for the model that produced those alignments.</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
while these models have proven effective at the word alignment task (mihalcea &amp; pedersen 2003), <papid> W03-0301 </papid>there are significant practical limitations in their output.</citsent>
<aftsection>
<nextsent>most fundamentally, all alignments have either zero or one connection to each target word.
</nextsent>
<nextsent>hence they are unable to produce the many-to many alignments required to identify correspondences with idioms and other phrasal chunks.
</nextsent>
<nextsent>to mitigate this limitation on final mappings, we follow the approach of och (2000): we align once in the forward direction and again in the backward direction.
</nextsent>
<nextsent>these alignments can subsequently be recombined in variety of ways, 5 the formula for aer given here and in och &amp; ney (2003) <papid> J03-1002 </papid>is intended to compare an automatic alignment against gold standard alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B42">
<title id=" C08-1032.xml">integrating a unification based semantics in a large scale lexicalised tree adjoining grammar for french </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we focus on verb semantics andshow how factorisation can be used to support compact and principled encoding of the semantic information that needs to be associated with each of the verbal elementary trees.
</prevsent>
<prevsent>the factorisation is made possible by the use of xmg, high-level linguistic formalism designed to specify and compile computational grammars and in particular, grammars based on non-local trees or tree descriptions.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
whilst there exists large scale lfgs (lexical functional grammar) and hpsgs (head-drivenphrase structure grammar) equipped with compositional semantics (copestake et al, 2001; <papid> P01-1019 </papid>frank and van genabith, 2001), available tree adjoining grammars remain largely syntactic.</citsent>
<aftsection>
<nextsent>one reason for this is that there has been, upto recently, much debate about how best to combine tag with compositional semantics.
</nextsent>
<nextsent>should it be based on the derived or the derivation tree ? should feature-based ltag be used or should synchronous tag?
</nextsent>
<nextsent>many proposals have been put forward but only recently did sufficient consensus ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B43">
<title id=" C08-1032.xml">integrating a unification based semantics in a large scale lexicalised tree adjoining grammar for french </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another more practical reason for the absence of large scale tags integrating compositional semantics is the lack of available computational frameworks.
</prevsent>
<prevsent>up to recently, there has been no available grammar writing environment and parser that would support the integration of compositional semantics into tag.
</prevsent>
</prevsection>
<citsent citstr=" E03-1030 ">
one step in that direction is provided by the development of xmg(duchier et al, 2004), formalism which supports the specification of feature-based lt ags equipped with compositional semantics a` la (gardent and kallmeyer, 2003).<papid> E03-1030 </papid></citsent>
<aftsection>
<nextsent>in this paper, we report on the integration of unification-based semantics into feature-based ltag for french which consists of around 6 000 trees.
</nextsent>
<nextsent>this integration is specified using xmg and we show how this formalism can be used to support compact and principled encoding of the semantic information that needs to be associated with each of the 6 000 elementary trees.
</nextsent>
<nextsent>the article is structured as follows.
</nextsent>
<nextsent>we start (section 2) by presenting xmg and showing howit supports the specification of feature-based lt ags equipped with compositional semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B46">
<title id=" C08-1032.xml">integrating a unification based semantics in a large scale lexicalised tree adjoining grammar for french </title>
<section> the xmg formalism.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 node naming and identification.
</prevsent>
<prevsent>mechanisms in combining tree descriptions, the linguist often wants to identify nodes across descriptions.
</prevsent>
</prevsection>
<citsent citstr=" P06-2032 ">
one distinguishing feature of xmg it that it supports sophisticated treatment of node naming and node identification (gardent and parmentier, 2006).<papid> P06-2032 </papid>node naming.</citsent>
<aftsection>
<nextsent>in xmg, node names are by default local to class.
</nextsent>
<nextsent>however explicit import and export declarations can be used to make namesvisible?
</nextsent>
<nextsent>to children classes.
</nextsent>
<nextsent>an export declaration makes the exported name(s) visible to all children classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B50">
<title id=" C08-1032.xml">integrating a unification based semantics in a large scale lexicalised tree adjoining grammar for french </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>importantly, feature values can be assigned global names thereby allowing for the specification of constraints on features that are far apart from each other?
</prevsent>
<prevsent>within tree.in this paper, we have argued that these features of xmg are effective in supporting an encoding of an ftag with unification based compositional semantics which is principled, transparent and compact.
</prevsent>
</prevsection>
<citsent citstr=" P95-1011 ">
these features also markedly distinguish xmg from existing formalisms usedto encode tree based grammars such as the non monotonic encoding of tag proposed in (evans et al, 1995) (<papid> P95-1011 </papid>in contrast, xmg is fully monotonic)and the tree descriptions based approaches proposed in (candito, 1996; xia et al, 1998) where in particular, tree descriptions can only be conjoined (not disjoined) and where identification across tree fragments is restricted to nodes.more in general, we believe that expressive formalisms are necessary to allow not only for the quick development of symbolic tree based grammars but also for their comparison and for the factoring of several grammars be they different wrt to the language they handle (as for instance in the hpsg delphin or in the lfg pargram project)or in the semantics they integrate e.g., glue semantics as proposed in (frank and van genabith, 2001), lambda-based semantics as proposed in (gardent, 2007) or as shown here, unification based flat semantics.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B51">
<title id=" C04-1126.xml">information extraction from single and multiple sentences </title>
<section> event scope and representation.  </section>
<citcontext>
<prevsection>
<prevsent>the results and implications of this experiment are presented in section 4.
</prevsent>
<prevsent>some related work is discussed in section 5.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
the topic of the sixth muc (muc-6) was management succession events (grishman and sundheim, 1996).<papid> C96-1079 </papid></citsent>
<aftsection>
<nextsent>the muc-6 data has been commonly used to evaluate ie systems.
</nextsent>
<nextsent>thetest corpus consists of 100 wall street journal documents from the period january 1993to june 1994, 54 of which contained management succession events (sundheim, 1995).<papid> M95-1002 </papid></nextsent>
<nextsent>the format used to represent events in the muc-6 corpus is now described.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B52">
<title id=" C04-1126.xml">information extraction from single and multiple sentences </title>
<section> event scope and representation.  </section>
<citcontext>
<prevsection>
<prevsent>the topic of the sixth muc (muc-6) was management succession events (grishman and sundheim, 1996).<papid> C96-1079 </papid></prevsent>
<prevsent>the muc-6 data has been commonly used to evaluate ie systems.</prevsent>
</prevsection>
<citsent citstr=" M95-1002 ">
thetest corpus consists of 100 wall street journal documents from the period january 1993to june 1994, 54 of which contained management succession events (sundheim, 1995).<papid> M95-1002 </papid></citsent>
<aftsection>
<nextsent>the format used to represent events in the muc-6 corpus is now described.
</nextsent>
<nextsent>2.1 muc representation.
</nextsent>
<nextsent>events in the muc-6 evaluation data are recorded in nested template structure.
</nextsent>
<nextsent>this format is useful for representing complex events which have more than one participant, forex ample, when one executive leaves post to be replaced by another.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B57">
<title id=" C04-1126.xml">information extraction from single and multiple sentences </title>
<section> event scope and representation.  </section>
<citcontext>
<prevsection>
<prevsent>the minimum possible set of components which can form an event are (1) person in, (2) person out or (3) both post and org.
</prevsent>
<prevsent>therefore sentence must contain certain amount of information to be listed as an event in this data set: the name of an organisation and post participating in management succession event or the name of person changing position and the direction of that change.soderland created this data from the muc 6 evaluation texts without using any of the existing annotations.
</prevsent>
</prevsection>
<citsent citstr=" M95-1011 ">
the texts were firstpre-processing using the university of massachusetts badger syntactic analyser (fisher et al, 1995) <papid> M95-1011 </papid>to identify syntactic clauses and the named entities relevant to the management succession task: people, posts and organisations.</citsent>
<aftsection>
<nextsent>each sentence containing relevant entities was examined and succession events manually identified.
</nextsent>
<nextsent>1the representation has been simplified slightly for clarity.
</nextsent>
<nextsent>this format is more practical for machine learning research since the entities which participate in the event are marked directly in the text.
</nextsent>
<nextsent>the learning task is simplified by the fact that the information which describes the event is contained within single sentence and so the feature space used by learning algorithm can be safely limited to items within that context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B61">
<title id=" C04-1126.xml">information extraction from single and multiple sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the events listed in soderlands data require minimal amount of information to be contained within sentence for it to be marked as containing information about management succession event.
</prevsent>
<prevsent>although it is difficult to see howany less information could be viewed as representing even part of management succession event.
</prevsent>
</prevsection>
<citsent citstr=" C02-1165 ">
huttunen et al (2002) <papid> C02-1165 </papid>found that there is variation between the complexity of ie tasks depending upon how the event descriptions are spread through the text and the ways in which they are encoded linguistically.</citsent>
<aftsection>
<nextsent>the analysis presented here is consistent with their finding as it has full match partial match nomatch total % type 112 / 112 100 / 108 0 / 56 212 / 276 76.8% person 112 / 112 100 / 108 0 / 56 212 / 276 76.8% org 112 / 112 6 / 108 0 / 53 118 / 273 43.2% post 111 / 111 74 / 108 0 / 50 185 / 269 68.8% total 447 / 447 280 / 432 0 / 215 727 / 1094 66.5% table 2: matches between muc and soderland data at field level been observed that the muc texts are often written in such as way that the name of the organisation in the event is in different partof the text to the rest of the organisation description and the entire event can only be constructed by resolving anaphoric expressions in the text.
</nextsent>
<nextsent>the choice over which information about events should be extracted could have an effect on the difficulty of the ie task.
</nextsent>
<nextsent>it seems that the majority of events are not fully described within single sentence, at least for one of the most commonly used ie evaluation sets.
</nextsent>
<nextsent>only around 40% of events in the original muc dataset were fully expressed within the soderland dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B62">
<title id=" C02-2019.xml">morphological analysis of the spontaneous speech corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither dictionary nor training corpus.
</prevsent>
<prevsent>two statistical approaches have been applied to this problem.
</prevsent>
</prevsection>
<citsent citstr=" C96-2202 ">
one is to find unknown words from corpora and put them into dictionary (e.g., (mori and nagao, 1996)), <papid> C96-2202 </papid>and the other is to estimate model that can identify unknown words correctly (e.g., (kashioka et al, 1997; nagata, 1999)).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>uchimoto et al used both approaches.
</nextsent>
<nextsent>they proposed morphological analysis method based on maximum entropy (m.e.) model (uchimoto et al, 2001).<papid> W01-0512 </papid></nextsent>
<nextsent>we used their method to tag spontaneous speech cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B63">
<title id=" C02-2019.xml">morphological analysis of the spontaneous speech corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither dictionary nor training corpus.
</prevsent>
<prevsent>two statistical approaches have been applied to this problem.
</prevsent>
</prevsection>
<citsent citstr=" P99-1036 ">
one is to find unknown words from corpora and put them into dictionary (e.g., (mori and nagao, 1996)), <papid> C96-2202 </papid>and the other is to estimate model that can identify unknown words correctly (e.g., (kashioka et al, 1997; nagata, 1999)).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>uchimoto et al used both approaches.
</nextsent>
<nextsent>they proposed morphological analysis method based on maximum entropy (m.e.) model (uchimoto et al, 2001).<papid> W01-0512 </papid></nextsent>
<nextsent>we used their method to tag spontaneous speech cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B64">
<title id=" C02-2019.xml">morphological analysis of the spontaneous speech corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one is to find unknown words from corpora and put them into dictionary (e.g., (mori and nagao, 1996)), <papid> C96-2202 </papid>and the other is to estimate model that can identify unknown words correctly (e.g., (kashioka et al, 1997; nagata, 1999)).<papid> P99-1036 </papid></prevsent>
<prevsent>uchimoto et al used both approaches.</prevsent>
</prevsection>
<citsent citstr=" W01-0512 ">
they proposed morphological analysis method based on maximum entropy (m.e.) model (uchimoto et al, 2001).<papid> W01-0512 </papid></citsent>
<aftsection>
<nextsent>we used their method to tag spontaneous speech corpus.
</nextsent>
<nextsent>their method uses model that can not only consult dictionary but can also identify unknown words by learning certain characteristics.
</nextsent>
<nextsent>to learn these characteristics, we focused on such information as whether or not string is found in dictionary and what types of characters are used in string.
</nextsent>
<nextsent>the model estimates how likely string is to be morpheme.this model is independent of the domain of cor pora; in this paper we demonstrate that this is true by applying our model to the spontaneous speech corpus, corpus of spontaneous japanese (csj) (maekawa et al, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B65">
<title id=" C02-2019.xml">morphological analysis of the spontaneous speech corpus </title>
<section> a morpheme model </section>
<citcontext>
<prevsection>
<prevsent>&amp; = 1 0 : otherwise.
</prevsent>
<prevsent>(1)here has(h,x)?
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
is binary function that returns true if the history has feature x. in our experiments, we focused on such information as whether or not string is found in dictionary, the length of the string, what types of characters are used in the string, and what part-of-speech the adjacent morpheme is. given set of features and some training data, the m.e. estimation process produces model, which is represented as follows (berger et al, 1996; <papid> J96-1002 </papid>ristad, 1997; ristad, 1998): (f |h) = ? ? i (h,f) z ?</citsent>
<aftsection>
<nextsent>(h) (2) ?
</nextsent>
<nextsent>(h) = ? ? ? i (h,f) .
</nextsent>
<nextsent>(3)we define model which estimates the likelihood that given string is morpheme and has the grammatical attribute i(1 ? ? n) as morpheme model.
</nextsent>
<nextsent>this model is represented by eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B66">
<title id=" C04-1054.xml">medical wordnet a new methodology for the construction and validation of information resources for consumer health </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>words and the representation of their senses, stored in lexical databases, can be linked for this purpose to specific occurrences in corpora.
</prevsent>
<prevsent>currently, several resources are being built in the spirit of this methodology.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
examples are framenet (baker, et al , 1998), (<papid> P98-1013 </papid>baker, et al , 2003) and penn proposition bank (kingsbury and palmer, 2002), both of which focus on word usage in general, rather than on domain-specific contexts.</citsent>
<aftsection>
<nextsent>in contrast to our own project, neither of the mentioned resources attempts to build corpus in systematic way that is designed to ensure adequate coverage of some given domain.
</nextsent>
<nextsent>furthermore, neither project is concerned with the questions of factuality or validation of statements.
</nextsent>
<nextsent>another project with goals similar to those of mfn is the cyc (short for encyclopedia) knowledge base, collection of hundreds of thousands of statements, mostly about the external world, such as: the earth is round, albany is the capital of new york.
</nextsent>
<nextsent>(lenat, 1995), (guha, et al , 1990) these statements, which were entered over many years by cyc employees, are parcelled out into separate micro-theories devoted to different domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B67">
<title id=" C04-1054.xml">medical wordnet a new methodology for the construction and validation of information resources for consumer health </title>
<section> wordnet.  </section>
<citcontext>
<prevsection>
<prevsent>(http://www.nlm.nih.gov/medlineplus) our work differs in number of ways from cyc: (i) we focus on one single (albeit very large) domain; (ii) cyc does not store english sentences but rather ? in keeping with its goal of being language-unspecific ? statements couched in the symbolism of modified first-order logic; (iii) cyc incorporates folk beliefs and expert knowledge indiscriminately, and its separate micro-theories are not designed to be consistent either with each other or with the body of established science; (iv) only reduced part of cyc is publicly available.
</prevsent>
<prevsent>wordnet 2.0 is large electronic lexical database of english that has found wide acceptance in areas as diverse as artificial intelligence, natural language processing, and psychology.
</prevsent>
</prevsection>
<citsent citstr=" A97-1055 ">
(agirre and martinez, 2000), (al-halimi and kazman, 1998), (artale, et al , 1997), (basili et al , 1997), (berwick, et al , 1990), (burg and van de riet 1998), (cucchiarelli and velardi 1997), (<papid> A97-1055 </papid>fellbaum 1990), (gonzalo, et al , 1998), (<papid> W98-0705 </papid>harabagiu and moldovan, 1996) its coverage, which is comparable to that of collegiate dictionary, extends over some 130,000 word forms.</citsent>
<aftsection>
<nextsent>the most common application is in information technology, where it is used for information retrieval, document classification, question-answer systems, language generation, and machine translation.
</nextsent>
<nextsent>wordnet was originally conceived as full-scale model of human semantic organization, and its design was guided by early experiments in artificial intelligence.
</nextsent>
<nextsent>(collins and quill ian, 1969) wordnet was quickly embraced by the nlp community, development that has guided its subsequent growth and design, and wordnet is now widely recognized as the lexical database of choice for nlp.
</nextsent>
<nextsent>the appeal of wordnets design is reflected in the fact that wordnets have been, and continue to be, built in dozens of languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B68">
<title id=" C04-1054.xml">medical wordnet a new methodology for the construction and validation of information resources for consumer health </title>
<section> wordnet.  </section>
<citcontext>
<prevsection>
<prevsent>(http://www.nlm.nih.gov/medlineplus) our work differs in number of ways from cyc: (i) we focus on one single (albeit very large) domain; (ii) cyc does not store english sentences but rather ? in keeping with its goal of being language-unspecific ? statements couched in the symbolism of modified first-order logic; (iii) cyc incorporates folk beliefs and expert knowledge indiscriminately, and its separate micro-theories are not designed to be consistent either with each other or with the body of established science; (iv) only reduced part of cyc is publicly available.
</prevsent>
<prevsent>wordnet 2.0 is large electronic lexical database of english that has found wide acceptance in areas as diverse as artificial intelligence, natural language processing, and psychology.
</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
(agirre and martinez, 2000), (al-halimi and kazman, 1998), (artale, et al , 1997), (basili et al , 1997), (berwick, et al , 1990), (burg and van de riet 1998), (cucchiarelli and velardi 1997), (<papid> A97-1055 </papid>fellbaum 1990), (gonzalo, et al , 1998), (<papid> W98-0705 </papid>harabagiu and moldovan, 1996) its coverage, which is comparable to that of collegiate dictionary, extends over some 130,000 word forms.</citsent>
<aftsection>
<nextsent>the most common application is in information technology, where it is used for information retrieval, document classification, question-answer systems, language generation, and machine translation.
</nextsent>
<nextsent>wordnet was originally conceived as full-scale model of human semantic organization, and its design was guided by early experiments in artificial intelligence.
</nextsent>
<nextsent>(collins and quill ian, 1969) wordnet was quickly embraced by the nlp community, development that has guided its subsequent growth and design, and wordnet is now widely recognized as the lexical database of choice for nlp.
</nextsent>
<nextsent>the appeal of wordnets design is reflected in the fact that wordnets have been, and continue to be, built in dozens of languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B69">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the textual demand analysis proposed in this paper shares some properties with phrase-level sa, the detection of sentiments and evaluations expressed in phrases, rather than document-level sa, the classification of documents in terms of goodness of reputation.
</prevsent>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></nextsent>
<nextsent>however, the extraction of the contents of writers?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B70">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the textual demand analysis proposed in this paper shares some properties with phrase-level sa, the detection of sentiments and evaluations expressed in phrases, rather than document-level sa, the classification of documents in terms of goodness of reputation.
</prevsent>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></nextsent>
<nextsent>however, the extraction of the contents of writers?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B71">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the textual demand analysis proposed in this paper shares some properties with phrase-level sa, the detection of sentiments and evaluations expressed in phrases, rather than document-level sa, the classification of documents in terms of goodness of reputation.
</prevsent>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
</prevsection>
<citsent citstr=" H05-1044 ">
the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></nextsent>
<nextsent>however, the extraction of the contents of writers?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B72">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the textual demand analysis proposed in this paper shares some properties with phrase-level sa, the detection of sentiments and evaluations expressed in phrases, rather than document-level sa, the classification of documents in terms of goodness of reputation.
</prevsent>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
</prevsection>
<citsent citstr=" W06-1642 ">
the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></nextsent>
<nextsent>however, the extraction of the contents of writers?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B73">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
<prevsent>the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1134 ">
as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></citsent>
<aftsection>
<nextsent>however, the extraction of the contents of writers?
</nextsent>
<nextsent>demands which this paper handles is less studied while this type of information is very valuable for commercial applications.for the tasks of information extraction andre lation extraction, bootstrapping approaches have been proven successful (yangarber, 2003; <papid> P03-1044 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></nextsent>
<nextsent>the pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively.the work by inui et al (2003) <papid> W03-1607 </papid>handles semantics of type similar to ours.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B74">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted, and nasukawa and yi (2003) clarified the need for deep syntactic analysis for the phrase-level sa.
</prevsent>
<prevsent>the acquisition of clues is key technology inthese research efforts, as seen in learning methods for document-level sa (hatzivassiloglou and mckeown, 1997; <papid> P97-1023 </papid>turney, 2002) <papid> P02-1053 </papid>and for phrase level sa (wilson et al, 2005; <papid> H05-1044 </papid>kanayama and nasukawa, 2006).<papid> W06-1642 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1113 ">
as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></citsent>
<aftsection>
<nextsent>however, the extraction of the contents of writers?
</nextsent>
<nextsent>demands which this paper handles is less studied while this type of information is very valuable for commercial applications.for the tasks of information extraction andre lation extraction, bootstrapping approaches have been proven successful (yangarber, 2003; <papid> P03-1044 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></nextsent>
<nextsent>the pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively.the work by inui et al (2003) <papid> W03-1607 </papid>handles semantics of type similar to ours.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B75">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></prevsent>
<prevsent>however, the extraction of the contents of writers?</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
demands which this paper handles is less studied while this type of information is very valuable for commercial applications.for the tasks of information extraction andre lation extraction, bootstrapping approaches have been proven successful (yangarber, 2003; <papid> P03-1044 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></citsent>
<aftsection>
<nextsent>the pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively.the work by inui et al (2003) <papid> W03-1607 </papid>handles semantics of type similar to ours.</nextsent>
<nextsent>they aimed to detect the requests in the responses to open-ended questionnaires, seeking direct requests such as ?... ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B76">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as well as the sentiment expressions leading to evaluations, there are many semantic aspects to be extracted from documents which contain writers opinions, such as subjectivity (wiebe and mihalcea, 2006), <papid> P06-1134 </papid>comparative sentences (jindal and liu, 2006), or predictive expressions (kim and hovy, 2007).<papid> D07-1113 </papid></prevsent>
<prevsent>however, the extraction of the contents of writers?</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
demands which this paper handles is less studied while this type of information is very valuable for commercial applications.for the tasks of information extraction andre lation extraction, bootstrapping approaches have been proven successful (yangarber, 2003; <papid> P03-1044 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></citsent>
<aftsection>
<nextsent>the pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively.the work by inui et al (2003) <papid> W03-1607 </papid>handles semantics of type similar to ours.</nextsent>
<nextsent>they aimed to detect the requests in the responses to open-ended questionnaires, seeking direct requests such as ?... ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B77">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, the extraction of the contents of writers?
</prevsent>
<prevsent>demands which this paper handles is less studied while this type of information is very valuable for commercial applications.for the tasks of information extraction andre lation extraction, bootstrapping approaches have been proven successful (yangarber, 2003; <papid> P03-1044 </papid>pantel and pennacchiotti, 2006).<papid> P06-1015 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1607 ">
the pattern induction method in this paper exploits their ideas, but their application to the demand detection is not trivial, because some instances of demands are previously unknown and do not appear frequently, so they have to be abstracted effectively.the work by inui et al (2003) <papid> W03-1607 </papid>handles semantics of type similar to ours.</citsent>
<aftsection>
<nextsent>they aimed to detect the requests in the responses to open-ended questionnaires, seeking direct requests such as ?... ?
</nextsent>
<nextsent>(?[i] would like you to ...?)
</nextsent>
<nextsent>and other forms which can be paraphrased as direct requests.
</nextsent>
<nextsent>they classified sentences into requests or non-requests, where their source documents were responses to questionnaires, and where more than 60% of the utterances could be regarded as requests of some sort.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B78">
<title id=" C08-1052.xml">textual demand analysis detection of users wants and needs from opinions </title>
<section> baseline method of textual demand.  </section>
<citcontext>
<prevsection>
<prevsent>4 apparent character variations like ?]?
</prevsent>
<prevsent>and ??,and alternative forms of particles were aggregated in the parsing process.
</prevsent>
</prevsection>
<citsent citstr=" C04-1071 ">
5 total of 95 auxiliary patterns which kanayama et al (2004) <papid> C04-1071 </papid>used for the sentiment analysis.</citsent>
<aftsection>
<nextsent>412 table 2: the list of augmented demand patterns dp . ? -] (i want n?), n?-#-y (i hope n?), ? -#- 6-!
</nextsent>
<nextsent>(please [give] n?), n?-#-6 (i wish ? ), ? -#--4  (please do n?), ?-#-^ (i ask [you] n?), n?
</nextsent>
<nextsent>-!- (n ? should be), ? -#--p (please do n?)table 3: the result with the minimum set of demand patterns dp 1 and the larger set dp . patterns precision recall dp 1 94% (17/18) 30% (17/56) dp 78% (18/22) 32% (18/56) since only one demand pattern was used.
</nextsent>
<nextsent>to make the recall higher, we created another set of demand patterns dp qlisted in table 2, which are generally used as clues for the request expressions inthe analysis of responses to open-ended questionnaires.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B80">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these corpora are useful for training and evaluating sentence extraction systems.
</prevsent>
<prevsent>however, it is costly to create these corpora.
</prevsent>
</prevsection>
<citsent citstr=" W03-0507 ">
figure 1 shows an example of summary sentence sand original sentences from tsc-2 (text summarization challenge 2) multiple document summarization data (okumura et al, 2003).<papid> W03-0507 </papid></citsent>
<aftsection>
<nextsent>from this example, we can see many-to-many correspondences.
</nextsent>
<nextsent>for instance, summary sentence (a) consists of part of source sentence (a).
</nextsent>
<nextsent>summary sentence (b) consists of parts of source sentences (a), (b), and (c).
</nextsent>
<nextsent>it is clear that the correspondence among the sentences is very complex.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B81">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>banko et al (1999) proposed method based on sentence similarity using bag-of-words (bow)representation.
</prevsent>
<prevsent>for each sentence in the given abstract, the corresponding source sentence is determined by combing the similarity score and heuristicrules.
</prevsent>
</prevsection>
<citsent citstr=" P03-1005 ">
however, it is known that bag-of-words representation is not optimal for short texts like single sentences (suzuki et al, 2003).<papid> P03-1005 </papid></citsent>
<aftsection>
<nextsent>marcu (1999) regards sentence as set ofunits?
</nextsent>
<nextsent>that correspond to clauses and defines similarity between units based on bow representation.
</nextsent>
<nextsent>next, the best source sentences are extracted in terms of unit?
</nextsent>
<nextsent>similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B84">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, these three methods tend to be strongly influenced by word order.
</prevsent>
<prevsent>when the summary sentence andthe source sentences disagree in terms of word order, the methods fail to work well.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
the supervised learning-based method called sim finder was proposed by hatzivassiloglou et al (hatzivassiloglou et al, 1999; <papid> W99-0625 </papid>hatzivassiloglou et al., 2001).</citsent>
<aftsection>
<nextsent>they translate sentence into feature vector based on word counts and proper nouns, and so on, and then sentence pairs are classified into similar?
</nextsent>
<nextsent>or not.
</nextsent>
<nextsent>their approach is effective whena lot of training data is available.
</nextsent>
<nextsent>however, the human cost of making this training data cannot be disregarded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B85">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> an alignment method based on syntax.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 similarity metrics.
</prevsent>
<prevsent>we need similarity metric to rank dtp similarity.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
the following cosine measure (hearst, 1997) <papid> J97-1003 </papid>is used in many nlp tasks.</citsent>
<aftsection>
<nextsent>simcos 3i???
</nextsent>
<nextsent>l n6 ???
</nextsent>
<nextsent>l? ? ?
</nextsent>
<nextsent>a? j? ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B86">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> an alignment method based on syntax.  </section>
<citcontext>
<prevsection>
<prevsent>note that syntactic and semantic information is lost in the bow representation.
</prevsent>
<prevsent>in order to solve this problem, we use similarity measures based on word co-occurrences.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
as an example of it application, n-gram co-occurrence is used for evaluating machine translations (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>string sub sequence kernel (ssk) (lodhi et al, 2002) and word sequence kernel(wsk) (cancedda et al, 2003) are extensions of gram-based measures used for text categorization.
</nextsent>
<nextsent>in this paper, we compare wsk to its extension, the extended string sub sequence kernel (esk).first, we describe wsk.
</nextsent>
<nextsent>wsk receives two sequences of words as input and maps each of them into high-dimensional vector space.
</nextsent>
<nextsent>wsks value is just the inner product of the two vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B90">
<title id=" C04-1064.xml">dependency based sentence alignment for multiple document summarization </title>
<section> evaluation settings.  </section>
<citcontext>
<prevsection>
<prevsent>each summary sentence is compared with all source sentences and the top 7 sentences that have similarity score over certain threshold value 8 are aligned.
</prevsent>
<prevsent>dtp-based method this method was described in section 3.2.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
in orderto obtain dtps, we used the japanese morphological analyzer chasen and the dependency structure analyzer cabocha (kudo and matsumoto, 2002).<papid> W02-2016 </papid></citsent>
<aftsection>
<nextsent>4.2.1 similarity measures we utilized the following similarity measures.
</nextsent>
<nextsent>bow bowis defined by equation (1).
</nextsent>
<nextsent>here, we use only nouns and verbs.
</nextsent>
<nextsent>n-gram this is simple extension of bow.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B94">
<title id=" C08-1068.xml">hindi urdu machine transliteration using finite state transducers </title>
<section> humt.  </section>
<citcontext>
<prevsection>
<prevsent>a person using the persio-arabic script cannot understand the devanagari script and vice versa.
</prevsent>
<prevsent>the same is true for hindi and urdu which are varieties or dialects of the same language, called hindus tani by platts (1909).
</prevsent>
</prevsection>
<citsent citstr=" P06-1143 ">
pmt (punjabi machine transliteration) (malik, 2006) <papid> P06-1143 </papid>was first effort to bridge this scriptural divide between the two scripts of punjabi namely shahmukhi (a derivation of perio-arabic script) and gurmukhi (a derivation of landa, shardha and takri, old indian scripts).</citsent>
<aftsection>
<nextsent>humt is logical extension of pmt.
</nextsent>
<nextsent>our humt system is generic and flexible such that it will be extendable to handle similar cases like kashmiri, punjabi, sindhi, etc. humt is also special type of machine transliteration like pmt.
</nextsent>
<nextsent>a brief account of hindi and urdu is first given for unacquainted readers.
</nextsent>
<nextsent>2.1 hindi.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B95">
<title id=" C08-1068.xml">hindi urdu machine transliteration using finite state transducers </title>
<section> analysis of scripts and uit mappings.  </section>
<citcontext>
<prevsection>
<prevsent>[?], etc. called maatraas.
</prevsent>
<prevsent>when vowel comes at the start of word or syllable, the independent form is used; otherwise the dependent form is used (kellogg, 1872; montaut, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W04-1613 ">
urdu contains 10 vowels and 7 of them have nasalized forms (hussain, 2004; <papid> W04-1613 </papid>khan, 1997).</citsent>
<aftsection>
<nextsent>urdu vowels are represented using four long vowels (alef madda (?), alef (?), vav (?)
</nextsent>
<nextsent>and choti yeh (?)) and three short vowels (arabic fatha ? zabar -?, arabic damma ? pesh -?
</nextsent>
<nextsent>and arabic kasra ? zer -?).
</nextsent>
<nextsent>vowel representation is context sensitive in urdu.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B96">
<title id=" C08-1068.xml">hindi urdu machine transliteration using finite state transducers </title>
<section> humt rules.  </section>
<citcontext>
<prevsection>
<prevsent>-  b, ? -  p, ? -  z, ? -  [d _? _? h]]; read regex [?
</prevsent>
<prevsent>-  ?@?, ? -  a, ? -  || .#.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
_ ] ? figure 1: sample xfst code finite-state transducers are robust and time and space efficient (mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>they are logical choice for hindi-urdu transliteration via uit as this problem could also be seen as string matching and producing an analysis string as an output like finite-state morphological analysis.
</nextsent>
<nextsent>5.2 contextual humt rules.
</nextsent>
<nextsent>uit mappings need to be accompanied by necessary contextual humt rules for correct hindi to urdu transliteration and vice versa.
</nextsent>
<nextsent>for example, vav (?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B99">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper proposes to integrate them using hidden markov random fields and demonstrates its effectiveness through experiments.
</prevsent>
<prevsent>word clustering is technique of grouping similar words together, and it is important for various nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
applications of word clustering include language modeling (brown et al, 1992), <papid> J92-4003 </papid>text classification (baker and mccallum, 1998), thesaurus construction (lin, 1998) <papid> P98-2127 </papid>and so on.</citsent>
<aftsection>
<nextsent>furthermore,recent studies revealed that word clustering is useful for semi-supervised learning in nlp (miller et al., 2004; <papid> N04-1043 </papid>li and mccallum, 2005; kazama and torisawa, 2008; <papid> P08-1047 </papid>koo et al, 2008).<papid> P08-1068 </papid></nextsent>
<nextsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B100">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper proposes to integrate them using hidden markov random fields and demonstrates its effectiveness through experiments.
</prevsent>
<prevsent>word clustering is technique of grouping similar words together, and it is important for various nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
applications of word clustering include language modeling (brown et al, 1992), <papid> J92-4003 </papid>text classification (baker and mccallum, 1998), thesaurus construction (lin, 1998) <papid> P98-2127 </papid>and so on.</citsent>
<aftsection>
<nextsent>furthermore,recent studies revealed that word clustering is useful for semi-supervised learning in nlp (miller et al., 2004; <papid> N04-1043 </papid>li and mccallum, 2005; kazama and torisawa, 2008; <papid> P08-1047 </papid>koo et al, 2008).<papid> P08-1068 </papid></nextsent>
<nextsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B101">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word clustering is technique of grouping similar words together, and it is important for various nlp systems.
</prevsent>
<prevsent>applications of word clustering include language modeling (brown et al, 1992), <papid> J92-4003 </papid>text classification (baker and mccallum, 1998), thesaurus construction (lin, 1998) <papid> P98-2127 </papid>and so on.</prevsent>
</prevsection>
<citsent citstr=" N04-1043 ">
furthermore,recent studies revealed that word clustering is useful for semi-supervised learning in nlp (miller et al., 2004; <papid> N04-1043 </papid>li and mccallum, 2005; kazama and torisawa, 2008; <papid> P08-1047 </papid>koo et al, 2008).<papid> P08-1068 </papid></citsent>
<aftsection>
<nextsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.
</nextsent>
<nextsent>it is founded on the hypothesis that similar words tend to appear in similar contexts (harris, 1968).
</nextsent>
<nextsent>based on this idea, some studies proposed probabilistic models for word clustering (pereira et al, 1993; <papid> P93-1024 </papid>li and abe, 1998; <papid> P98-2124 </papid>rooth ? 2008.</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B102">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word clustering is technique of grouping similar words together, and it is important for various nlp systems.
</prevsent>
<prevsent>applications of word clustering include language modeling (brown et al, 1992), <papid> J92-4003 </papid>text classification (baker and mccallum, 1998), thesaurus construction (lin, 1998) <papid> P98-2127 </papid>and so on.</prevsent>
</prevsection>
<citsent citstr=" P08-1047 ">
furthermore,recent studies revealed that word clustering is useful for semi-supervised learning in nlp (miller et al., 2004; <papid> N04-1043 </papid>li and mccallum, 2005; kazama and torisawa, 2008; <papid> P08-1047 </papid>koo et al, 2008).<papid> P08-1068 </papid></citsent>
<aftsection>
<nextsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.
</nextsent>
<nextsent>it is founded on the hypothesis that similar words tend to appear in similar contexts (harris, 1968).
</nextsent>
<nextsent>based on this idea, some studies proposed probabilistic models for word clustering (pereira et al, 1993; <papid> P93-1024 </papid>li and abe, 1998; <papid> P98-2124 </papid>rooth ? 2008.</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B103">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word clustering is technique of grouping similar words together, and it is important for various nlp systems.
</prevsent>
<prevsent>applications of word clustering include language modeling (brown et al, 1992), <papid> J92-4003 </papid>text classification (baker and mccallum, 1998), thesaurus construction (lin, 1998) <papid> P98-2127 </papid>and so on.</prevsent>
</prevsection>
<citsent citstr=" P08-1068 ">
furthermore,recent studies revealed that word clustering is useful for semi-supervised learning in nlp (miller et al., 2004; <papid> N04-1043 </papid>li and mccallum, 2005; kazama and torisawa, 2008; <papid> P08-1047 </papid>koo et al, 2008).<papid> P08-1068 </papid></citsent>
<aftsection>
<nextsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.
</nextsent>
<nextsent>it is founded on the hypothesis that similar words tend to appear in similar contexts (harris, 1968).
</nextsent>
<nextsent>based on this idea, some studies proposed probabilistic models for word clustering (pereira et al, 1993; <papid> P93-1024 </papid>li and abe, 1998; <papid> P98-2124 </papid>rooth ? 2008.</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B104">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.
</prevsent>
<prevsent>it is founded on the hypothesis that similar words tend to appear in similar contexts (harris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
based on this idea, some studies proposed probabilistic models for word clustering (pereira et al, 1993; <papid> P93-1024 </papid>li and abe, 1998; <papid> P98-2124 </papid>rooth ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>et al, 1999; torisawa, 2002).<papid> C02-1120 </papid></nextsent>
<nextsent>others proposed distributional similarity measures between words (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; <papid> P98-2127 </papid>lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B105">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a well-known approach to grouping similar words is to use distribution of contexts in which target words appear.
</prevsent>
<prevsent>it is founded on the hypothesis that similar words tend to appear in similar contexts (harris, 1968).
</prevsent>
</prevsection>
<citsent citstr=" P98-2124 ">
based on this idea, some studies proposed probabilistic models for word clustering (pereira et al, 1993; <papid> P93-1024 </papid>li and abe, 1998; <papid> P98-2124 </papid>rooth ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>et al, 1999; torisawa, 2002).<papid> C02-1120 </papid></nextsent>
<nextsent>others proposed distributional similarity measures between words (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; <papid> P98-2127 </papid>lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B106">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" C02-1120 ">
et al, 1999; torisawa, 2002).<papid> C02-1120 </papid></citsent>
<aftsection>
<nextsent>others proposed distributional similarity measures between words (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; <papid> P98-2127 </papid>lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).</nextsent>
<nextsent>once such similarity is defined, it is trivial to perform clustering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B107">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>et al, 1999; torisawa, 2002).<papid> C02-1120 </papid></prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
others proposed distributional similarity measures between words (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; <papid> P98-2127 </papid>lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).</citsent>
<aftsection>
<nextsent>once such similarity is defined, it is trivial to perform clustering.
</nextsent>
<nextsent>on the other hand, some researchers utilizedco-occurrence for word clustering.
</nextsent>
<nextsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</nextsent>
<nextsent>considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B109">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>et al, 1999; torisawa, 2002).<papid> C02-1120 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
others proposed distributional similarity measures between words (hindle, 1990; <papid> P90-1034 </papid>lin, 1998; <papid> P98-2127 </papid>lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).</citsent>
<aftsection>
<nextsent>once such similarity is defined, it is trivial to perform clustering.
</nextsent>
<nextsent>on the other hand, some researchers utilizedco-occurrence for word clustering.
</nextsent>
<nextsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</nextsent>
<nextsent>considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B110">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, some researchers utilizedco-occurrence for word clustering.
</prevsent>
<prevsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></citsent>
<aftsection>
<nextsent>in addition tothe classical window-based technique, some studies investigated the use of lexico-syntactic patterns (e.g., or y) to get more accurate co-occurrence statistics (chilovski and pantel, 2004; bollegala et al., 2007).<papid> N07-1043 </papid></nextsent>
<nextsent>these two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.consider to cluster set of words based on the distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B111">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, some researchers utilizedco-occurrence for word clustering.
</prevsent>
<prevsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></citsent>
<aftsection>
<nextsent>in addition tothe classical window-based technique, some studies investigated the use of lexico-syntactic patterns (e.g., or y) to get more accurate co-occurrence statistics (chilovski and pantel, 2004; bollegala et al., 2007).<papid> N07-1043 </papid></nextsent>
<nextsent>these two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.consider to cluster set of words based on the distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B112">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, some researchers utilizedco-occurrence for word clustering.
</prevsent>
<prevsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</prevsent>
</prevsection>
<citsent citstr=" W06-1664 ">
considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></citsent>
<aftsection>
<nextsent>in addition tothe classical window-based technique, some studies investigated the use of lexico-syntactic patterns (e.g., or y) to get more accurate co-occurrence statistics (chilovski and pantel, 2004; bollegala et al., 2007).<papid> N07-1043 </papid></nextsent>
<nextsent>these two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.consider to cluster set of words based on the distributional similarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B113">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the idea behind it is that similar words tend to co-occur in certain patterns.
</prevsent>
<prevsent>considerable efforts have been devoted to measure word similarity based on cooccurrence frequency of two words in window (church and hanks, 1989; <papid> P89-1010 </papid>turney, 2001; terra and clarke, 2003; <papid> N03-1032 </papid>matsuo et al, 2006).<papid> W06-1664 </papid></prevsent>
</prevsection>
<citsent citstr=" N07-1043 ">
in addition tothe classical window-based technique, some studies investigated the use of lexico-syntactic patterns (e.g., or y) to get more accurate co-occurrence statistics (chilovski and pantel, 2004; bollegala et al., 2007).<papid> N07-1043 </papid></citsent>
<aftsection>
<nextsent>these two approaches are complementary with each other, because they are founded on different hypotheses and utilize different corpus statistics.consider to cluster set of words based on the distributional similarity.
</nextsent>
<nextsent>it is likely that some words are difficult to cluster due to the data sparseness or some other problems, while we can still expect that those words are correctly classified using patterns.this consideration leads us to combine distributional and pattern-based word clustering.
</nextsent>
<nextsent>in this paper we propose to combine them using mixture models based on hidden markov random fields.
</nextsent>
<nextsent>this model was originally proposed by (basu etal., 2004) for semi-supervised clustering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B117">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the first and second row shows the number of distinct words (and word pairs) used for the distributional and pattern-based word clustering respectively.
</prevsent>
<prevsent>three k-means algorithms using different distributional similarity or dissimilarity mea sures: cosine, ?-skew divergence (lee, 1999)<papid> P99-1004 </papid>4, and lins similarity (lin, 1998).<papid> P98-2127 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
the cbc algorithm (lin and pantel, 2002; <papid> C02-1144 </papid>pantel and lin, 2002).</citsent>
<aftsection>
<nextsent>5.3 evaluation procedure.
</nextsent>
<nextsent>all the nouns in the dataset were clustered by the proposed and baseline systems.5 for the mixture models and k-means, the number of clusters was set to 1,000.
</nextsent>
<nextsent>the parameter ? was set to 100.
</nextsent>
<nextsent>the result was assessed by precision and recall using the test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B118">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the parameter ? was set to 100.
</prevsent>
<prevsent>the result was assessed by precision and recall using the test data.
</prevsent>
</prevsection>
<citsent citstr=" P98-1012 ">
the precision and recall were computed by the b-cubed algorithm as follows (bagga and baldwin, 1998).<papid> P98-1012 </papid></citsent>
<aftsection>
<nextsent>for each noun i in the test data, precision and recall are defined as precision = |s ? i | |s | recall = |s ? i | | i | where iis the system generated cluster containing i and i is the gold standard cluster containing i . the precision and recall are defined as an av-.
</nextsent>
<nextsent>erage of precision and recall for all the nouns inthe test data respectively.
</nextsent>
<nextsent>the result of soft clustering models cannot be directly evaluated by the precision and recall.
</nextsent>
<nextsent>in such cases, each noun is assigned to the cluster that maximizes p(z|n).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B123">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>one problem that we did not examine is how to determine optimal number of clusters.
</prevsent>
<prevsent>in the experiment, the number was decided with trial-and error through our initial experiment.
</prevsent>
</prevsection>
<citsent citstr=" N06-4007 ">
we leave itas our future work to test methods of automatically determining the cluster number (pedersen and kulkarni, 2006; <papid> N06-4007 </papid>blei and jordan, 2006).</citsent>
<aftsection>
<nextsent>as far as we know, the distributional and pattern based word clustering have been discussed independently (e.g., (pazienza et al, 2006)).
</nextsent>
<nextsent>one of the most relevant work is (bollegala et al, 2007),<papid> N07-1043 </papid>which proposed to integrate various patterns in order to measure semantic similarity between words.although they extensively discussed the use of patterns, they did not address the distributional ap proach.</nextsent>
<nextsent>mirkin (2006) pointed out the importance of integrating distributional similarity and lexico syntactic patterns, and showed how to combine the two approaches for textual entailment acquisition.although their work inspired our research, we discussed word clustering, which is related to but different from entailment acquisition.lin (2003) also proposed to use both distributional similarity and lexico-syntactic patterns for finding synonyms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B125">
<title id=" C08-1051.xml">using hidden markov random fields to combine distributional and pattern based word clustering </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as we have already discussed, the integration of such patterns can also be formalized using similar probabilistic model to ours.a variety of studies discussed determining polarity of words.
</prevsent>
<prevsent>because this problem is ternary (positive, negative, and neutral) classification ofwords, it can be seen as one kind of word clustering.
</prevsent>
</prevsection>
<citsent citstr=" P05-1017 ">
the literature suggested two methods of determining polarity, and they are analogous to the distributional and co-occurrence-based approaches in word clustering (takamura et al, 2005; <papid> P05-1017 </papid>hi gashiyama et al, 2008).</citsent>
<aftsection>
<nextsent>we consider it is also promising to integrate them for polarity determination.
</nextsent>
<nextsent>the distributional and pattern-based word clustering have long been discussed separately despite the potentiality for their integration.
</nextsent>
<nextsent>in this paper,we provided probabilistic framework for combining the two approaches, and demonstrated that the clustering result is significantly improved.
</nextsent>
<nextsent>our important future work is to extend current framework so as to incorporate patterns for dissimilar words using cannot-link constraints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B126">
<title id=" C08-1080.xml">computer aided correction and extension of a syntactic wide coverage lexicon </title>
<section> parsing originally non-parsable.  </section>
<citcontext>
<prevsection>
<prevsent>2, in order to generate lexical corrections, we first need to get as close as possible to the set of parses that the grammar would have allowed with an error-free lexicon.
</prevsent>
<prevsent>we achieve this goal by replacing in the associated sentences every suspicious forms with special underspecified forms called wildcards.the simplest way would be to use totally underspecified wildcards.
</prevsent>
</prevsection>
<citsent citstr=" E03-1041 ">
indeed, this would have the benefit to cover all kinds of conflicts and thus, it would notably increase the parsing coverage.however, as observed by (fouvry, 2003), <papid> E03-1041 </papid>it introduces an unnecessary ambiguity which usually leads to severe over generation of parses or to no parses at all because of time or memory shortage.</citsent>
<aftsection>
<nextsent>in metaphorical way, we said that we wanted the grammar to tell us what lexical information it would have accepted for the suspicious form.
</nextsent>
<nextsent>well, by introducing totally underspecified wild card, either the grammar has so many things to say that it is hard to know what to listen to, or it has so many things to think about that it stutters and does not say anything at all.therefore, we refined the wild card by intro duc 635ing some data.
</nextsent>
<nextsent>for technical, linguistic and read ability reasons, we added pos information.
</nextsent>
<nextsent>when facing pos defect, we need the parser to explore other grammar rules than those already visited during the failed parses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B127">
<title id=" C08-1080.xml">computer aided correction and extension of a syntactic wide coverage lexicon </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>the idea of pre filtering the sentences (sec.
</prevsent>
<prevsent>3.2) to improve the error mining performance has never been applied so far.
</prevsent>
</prevsection>
<citsent citstr=" P98-1014 ">
the wild cards generation started to be refined with barg and walther (1998).<papid> P98-1014 </papid></citsent>
<aftsection>
<nextsent>since then,the wild cards are partially underspecified andre strained to open class pos.
</nextsent>
<nextsent>in yi and kordoni (2006), the authors use an elegant technique based on an entropy classifier to select the most adequate wildcards.
</nextsent>
<nextsent>the way to rank the corrections is usually basedon trained tool (van de cruys, 2006; yi and kordoni, 2006), such as an entropy classifier.
</nextsent>
<nextsent>surprisingly, the evaluation of hypotheses on various sentences for same suspicious form in order to discriminate the irrelevant ones has never been considered so far.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B128">
<title id=" C08-1080.xml">computer aided correction and extension of a syntactic wide coverage lexicon </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>9.1 practical context.
</prevsent>
<prevsent>the lexicon we are improving is called the lefff.
</prevsent>
</prevsection>
<citsent citstr=" P06-1042 ">
2 this wide-coverage morphological and syntactic french lexicon has been built partly automatically (sagot et al, 2006) <papid> P06-1042 </papid>and is under constant de velopment.</citsent>
<aftsection>
<nextsent>at the time these lines are written, it contains more than 520 000 entries.
</nextsent>
<nextsent>the less data an entry has, the more specified it is. we used two parsers based on two different grammars in order to improve the quality of our corrections.?
</nextsent>
<nextsent>the frmg (french meta-grammar) grammar is generated in an hybrid tag/tig form from more abstract meta-grammar with highly factor ized trees (thomasset and villemonte de la clergerie, 2005).
</nextsent>
<nextsent>the sxlfg-fr grammar (boullier andsagot, 2006), is an efficient deep non probabilistic lfg grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B129">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results show that the proposed method is valuable for both systems.
</prevsent>
<prevsent>we are exploring methods to boost the translation quality of corpus-based machine translation (mt) systems for speech translation.
</prevsent>
</prevsection>
<citsent citstr=" W03-0318 ">
among them, the technique of splitting an input sentence and translating the split sentences appears promising (doi and sumita, 2003).<papid> W03-0318 </papid></citsent>
<aftsection>
<nextsent>an mt system sometimes fails to translate aninput correctly.
</nextsent>
<nextsent>such failure occurs particularly when an input is long.
</nextsent>
<nextsent>in such case, by splitting the input, translation may be successfully performed for each portion.
</nextsent>
<nextsent>particularly in dialogue, sentences tend not to have complicated nested structures, and many long sentences can besplit into mutually independent portions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B130">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and try it on please?.
</prevsent>
<prevsent>in this case, translating the three portions and arranging the results in the same order give us the translation of the input sentence.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
in previous research on splitting sentences, many methods have been based on word-sequence characteristics like n-gram (lavie et al, 1996; berger et al, 1996; <papid> J96-1002 </papid>nakajima and yamamoto, 2001; gupta et al, 2002).</citsent>
<aftsection>
<nextsent>some research effort shave achieved high performance in recall and precision against correct splitting positions.
</nextsent>
<nextsent>despite such high performance, from the view point of translation, mt systems are not always able to translate the split sentences well.
</nextsent>
<nextsent>in order to supplement sentence splitting basedon word-sequence characteristics, this paper introduces another measure of sentence similarity.
</nextsent>
<nextsent>in our splitting method, we generate candidates for splitting positions based on n-grams, and select the best combination of positions by measuring sentence similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B131">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> splitting method.  </section>
<citcontext>
<prevsection>
<prevsent>substitutions are permitted only between content words of the same part of speech.
</prevsent>
<prevsent>substitution is considered as the semantic distance between two substituted words, described as sem, which is defined using thesaurus and ranges from 0 to 1.
</prevsent>
</prevsection>
<citsent citstr=" P91-1024 ">
semis the division of (the level of the least common abstraction in the thesaurus of twowords) by (the height of the thesaurus) according to equation (3) (sumita and iida, 1991).<papid> P91-1024 </papid></citsent>
<aftsection>
<nextsent>sim 0 (s 1 , 2 ) = 1 ? + + 2 ? sem s 1 + s 2 (2) sem = n (3) using sim 0, the similarity of sentence splitting to corpus is defined as simin equation (4).
</nextsent>
<nextsent>in this equation, is sentence-splitting and is given corpus that is set of sentences.
</nextsent>
<nextsent>sim is mean similarity of sub-sentences against the corpus weighted with the length of each sub sentence.
</nextsent>
<nextsent>the similarity of sentence including sub-sentence to corpus is the greatest similarity between the sentence and sentence in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B132">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> experimental conditions.  </section>
<citcontext>
<prevsection>
<prevsent>one of the systems was hierarchical phrase alignment-based translator (hpat) (imamura, 2002), whose unit of translation expression is phrase.
</prevsent>
<prevsent>hpat translates an input sentence by combining phrases.
</prevsent>
</prevsection>
<citsent citstr=" P98-1070 ">
the hpat system is equipped with another sentence splitting method based on parsing trees (furuse et al, 1998).<papid> P98-1070 </papid></citsent>
<aftsection>
<nextsent>the other system was dp-match driven transducer (d3) (sumita, 2001), <papid> W01-1401 </papid>whose unit of expression is sentence.</nextsent>
<nextsent>for both systems, translation knowledge is automatically acquired from parallel corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B133">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> experimental conditions.  </section>
<citcontext>
<prevsection>
<prevsent>hpat translates an input sentence by combining phrases.
</prevsent>
<prevsent>the hpat system is equipped with another sentence splitting method based on parsing trees (furuse et al, 1998).<papid> P98-1070 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-1401 ">
the other system was dp-match driven transducer (d3) (sumita, 2001), <papid> W01-1401 </papid>whose unit of expression is sentence.</citsent>
<aftsection>
<nextsent>for both systems, translation knowledge is automatically acquired from parallel corpus.
</nextsent>
<nextsent>3.2 linguistic resources.
</nextsent>
<nextsent>we used japanese-and-english parallel corpora, i.e., basic travel expression corpus (btec)and bilingual travel conversation corpus of spoken language (sldb) for training, and english sentences in machine-translation-aided bilingual dialogues (mad) for test set (takezawa and kikui, 2003).
</nextsent>
<nextsent>btec is collection of japanese sentences and their english translations usually found in phrase-books for foreign tourists.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B134">
<title id=" C04-1017.xml">splitting input sentence for machine translation using language model with sentence similarity </title>
<section> experimental conditions.  </section>
<citcontext>
<prevsection>
<prevsent>we compared translation quality under the conditions of with or without splitting.
</prevsent>
<prevsent>to evaluate translation quality, we used objective measures and subjective measure as follows.
</prevsent>
</prevsection>
<citsent citstr=" W02-1021 ">
the objective measures used were the bleu score (papineni et al, 2001), the nist score (dod dington, 2002) and multi-reference word error rate (mwer) (ueffing et al, 2002).<papid> W02-1021 </papid></citsent>
<aftsection>
<nextsent>they were calculated with the test set.
</nextsent>
<nextsent>both bleu and nist compare the system output translation with set of reference translations of the same source text by finding sequences of words in the reference translations that match those in the system output translation.
</nextsent>
<nextsent>therefore, achieving higher scores by these measures means that the translation result scan be regarded as being more adequate translations.
</nextsent>
<nextsent>mwer indicates the error rate based on the edit-distance between the system output and the reference translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B135">
<title id=" C04-1151.xml">multilevel bootstrapping for extracting parallel sentences from a quasi comparable corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure1.
</prevsent>
<prevsent>multi-level bootstrapping for parallel sentence extraction extraction of matching bilingual segments from non-parallel data has remained challenging task after almost decade.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
previously, the author and other researchers had suggested that bi-lexical information based on context can still be used to find correspondences between passages, sentences, or words, in non-parallel, comparable texts of the same topic (fung and mckeown 1995, rapp 1995, <papid> P95-1050 </papid>grefenstette 1998, fung and lo 1998, <papid> P98-1069 </papid>kikui 1999).<papid> W99-0905 </papid></citsent>
<aftsection>
<nextsent>more recent works on parallel sentence extraction from comparable data align documents first, before extracting sentences from the aligned documents (munteanu and marcu, 2002, zhao and vogel, 2002).
</nextsent>
<nextsent>both work used translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus.
</nextsent>
<nextsent>in zhao and vogel (2002), the comparable corpus consists of chinese and english versions of new stories from the xinhua newsagency.
</nextsent>
<nextsent>munteanu and marcu (2002) used unaligned segments from the french-english hansard corpus and finds parallel sentences among them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B137">
<title id=" C04-1151.xml">multilevel bootstrapping for extracting parallel sentences from a quasi comparable corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure1.
</prevsent>
<prevsent>multi-level bootstrapping for parallel sentence extraction extraction of matching bilingual segments from non-parallel data has remained challenging task after almost decade.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
previously, the author and other researchers had suggested that bi-lexical information based on context can still be used to find correspondences between passages, sentences, or words, in non-parallel, comparable texts of the same topic (fung and mckeown 1995, rapp 1995, <papid> P95-1050 </papid>grefenstette 1998, fung and lo 1998, <papid> P98-1069 </papid>kikui 1999).<papid> W99-0905 </papid></citsent>
<aftsection>
<nextsent>more recent works on parallel sentence extraction from comparable data align documents first, before extracting sentences from the aligned documents (munteanu and marcu, 2002, zhao and vogel, 2002).
</nextsent>
<nextsent>both work used translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus.
</nextsent>
<nextsent>in zhao and vogel (2002), the comparable corpus consists of chinese and english versions of new stories from the xinhua newsagency.
</nextsent>
<nextsent>munteanu and marcu (2002) used unaligned segments from the french-english hansard corpus and finds parallel sentences among them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B139">
<title id=" C04-1151.xml">multilevel bootstrapping for extracting parallel sentences from a quasi comparable corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure1.
</prevsent>
<prevsent>multi-level bootstrapping for parallel sentence extraction extraction of matching bilingual segments from non-parallel data has remained challenging task after almost decade.
</prevsent>
</prevsection>
<citsent citstr=" W99-0905 ">
previously, the author and other researchers had suggested that bi-lexical information based on context can still be used to find correspondences between passages, sentences, or words, in non-parallel, comparable texts of the same topic (fung and mckeown 1995, rapp 1995, <papid> P95-1050 </papid>grefenstette 1998, fung and lo 1998, <papid> P98-1069 </papid>kikui 1999).<papid> W99-0905 </papid></citsent>
<aftsection>
<nextsent>more recent works on parallel sentence extraction from comparable data align documents first, before extracting sentences from the aligned documents (munteanu and marcu, 2002, zhao and vogel, 2002).
</nextsent>
<nextsent>both work used translation model trained from parallel corpus and adaptively extract more parallel sentences and bilingual lexicon in the comparable corpus.
</nextsent>
<nextsent>in zhao and vogel (2002), the comparable corpus consists of chinese and english versions of new stories from the xinhua newsagency.
</nextsent>
<nextsent>munteanu and marcu (2002) used unaligned segments from the french-english hansard corpus and finds parallel sentences among them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B146">
<title id=" C04-1151.xml">multilevel bootstrapping for extracting parallel sentences from a quasi comparable corpus </title>
<section> occurrence frequencies of bilingual word.  </section>
<citcontext>
<prevsection>
<prevsent>the documents are word segmented with the language data consortium (ldc) chinese english dictionary 2.0.
</prevsent>
<prevsent>the chinese document is then glossed using all the dictionary entries.
</prevsent>
</prevsection>
<citsent citstr=" P99-1043 ">
multiple translations of chinese word is disambiguated by looking at the context of the sentences this word appears in (fung et al, 1999).<papid> P99-1043 </papid></citsent>
<aftsection>
<nextsent>both the glossed chinese document and the english document are then represented in word vectors, with term weighting.
</nextsent>
<nextsent>we evaluated different combinations of term weighting of each word in the corpus: term freuency (tf), inverse document frequency (idf), tf.idf, the product of function of tf and idf.
</nextsent>
<nextsent>the documents?
</nextsent>
<nextsent>here are sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B147">
<title id=" C04-1151.xml">multilevel bootstrapping for extracting parallel sentences from a quasi comparable corpus </title>
<section> occurrence frequencies of bilingual word.  </section>
<citcontext>
<prevsection>
<prevsent>hence, we need to refine bi-lexicon by learning new word translations from the intermediate output of parallel sentences extraction.
</prevsent>
<prevsent>in this work, we focus on learning translations for name entities since these are the words most likely missing in our baseline lexicon.
</prevsent>
</prevsection>
<citsent citstr=" N04-4010 ">
the chinese name entities are extracted with the system described in (zhai et al 2004).<papid> N04-4010 </papid></citsent>
<aftsection>
<nextsent>new bilingual word pairs are learned from the extracted sentence pairs based on (fung and lo 98) as follows: 1.
</nextsent>
<nextsent>extract new chinese name entities (zhai et. al 2004); <papid> N04-4010 </papid>2.</nextsent>
<nextsent>for each new chinese name entity:.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B149">
<title id=" C08-1075.xml">robust similarity measures for named entities matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2which is essential in the web people search task.
</prevsent>
<prevsent>there are different ways to tackle the problem of ne matching: the first and certainly most reliable one consists in studying the specific features of the data, and then use any available tool to design specialized method for the matching task.
</prevsent>
</prevsection>
<citsent citstr=" N06-1060 ">
this approach will generally take advantage of language-specific (e.g. in (freeman et al , 2006)) <papid> N06-1060 </papid>and domain-specific knowledge, of any external resources (e.g. database, names dictionaries, etc.),and of any information about the entities to process, e.g. their type (person name, organization, etc.), or internal structure (e.g. in (prager et al , 2007)).</citsent>
<aftsection>
<nextsent>in such an in-depth approach, supervised learning is helpful: it has been used for example in database context3 in (bilenko et al , 2003), butthis approach requires labeled data which is usually costly.
</nextsent>
<nextsent>all those data specific appproaches would necessitate some sort of human expertise.
</nextsent>
<nextsent>the second approach is the robust one: we propose here to try to match any kind of ne, extracted from real world?
</nextsent>
<nextsent>(potentially noisy) sources, without any kind of prior knowledge4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B150">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> feature-based log-linear models.  </section>
<citcontext>
<prevsection>
<prevsent>the conversion from to in fig.
</prevsent>
<prevsent>1 is the machine translation process.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
according to the statistical machine translation formalism (brown et al, 1993), <papid> J93-2003 </papid>the translation process is to search for the best sentence e?</citsent>
<aftsection>
<nextsent>such that e?
</nextsent>
<nextsent>= arg max p (e|j) = arg max p (j |e)p (e)where (j |e) is translation model characterizing the correspondence between and ; (e), the english language model probability.
</nextsent>
<nextsent>in the ibm model 4, the translation modelp (j |e) is further decomposed into four sub models: ? lexicon model ? t(j|e): probability of word in the japanese language being translated into word in the english language.
</nextsent>
<nextsent>1hereafter, j1 is called the single-best hypothesis of speech recognition; jn1 , the -best hypotheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B151">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> feature-based log-linear models.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments 81 part-of-speech tags and 5 gram pos language model were used.
</prevsent>
<prevsent>length model (l|e, j): is the length (number of words) of translated english sentence.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
jump weight: jump width for adjacent cepts in model 4 (marcu and wong, 2002).<papid> W02-1018 </papid></citsent>
<aftsection>
<nextsent>example matching score: the translated english sentence is matched with phrase translation examples.
</nextsent>
<nextsent>a score is derived based on the count of matches (watanabe and sumita, 2003).
</nextsent>
<nextsent>dynamic example matching score: similar to the example matching score but phrases were extracted dynamically from sentence examples (watanabe and sumita, 2003).altogether, we used m(=12) different features.
</nextsent>
<nextsent>in section 3, we review powells algorithm (press et al, 2000) as our tool to optimize model parameters, m1 , based on different objective translation metrics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B152">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> parameter optimization based.  </section>
<citcontext>
<prevsection>
<prevsent>d(e? ,r) is translation distortion?
</prevsent>
<prevsent>or an objective translation assessment.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the following four metrics were used specifically in this study: ? bleu (papineni et al, 2002): <papid> P02-1040 </papid>weighted geometric mean of the n-gram matches between test and reference sentences multiplied by brevity penalty that penalizes short translation sentences.</citsent>
<aftsection>
<nextsent>nist : an arithmetic mean of the n-grammatches between test and reference sentences multiplied by length factor which again penalizes short translation sentences.?
</nextsent>
<nextsent>mwer (niessen et al, 2000): multiple reference word error rate, which computes theedit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences.?
</nextsent>
<nextsent>mper: multiple reference position independent word error rate, which computes the edit distance without considering the word order.the bleu score and nist score are calculated by the tool downloadable 2.
</nextsent>
<nextsent>because the objective function in the model(eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B153">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> parameter optimization based.  </section>
<citcontext>
<prevsection>
<prevsent>because the objective function in the model(eq.
</prevsent>
<prevsent>3) is not smoothed function, we used powells search method to find solution.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the powells algorithm used in this work is similar as the one from (press et al, 2000) but we modified the line optimization codes, subroutine of powells algorithm, with reference to (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>finding global optimum is usually difficult in high dimensional vector space.
</nextsent>
<nextsent>to make sure that we had found good local optimum,we restarted the algorithm by using various ini tializations and used the best local optimum as the final solution.
</nextsent>
<nextsent>4.1 corpus &amp; system.
</nextsent>
<nextsent>the data used in this study was the basic travel expression corpus (btec) (kikui et al, 2003), consisting of commonly used sentences listed in travel guidebooks and tour conversations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B155">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the acoustic hmms were tri phone models with 2,100 state sin total, using 25 dimensional, short-time spectrum features.
</prevsent>
<prevsent>in the first and second pass of decoding, multiclass word bigram of lexicon of 37,000 words plus 10,000 compound words was used.
</prevsent>
</prevsection>
<citsent citstr=" W02-1021 ">
a word trigram was used in rescoring the results.the machine translation system is graph based decoder (ueffing et al, 2002).<papid> W02-1021 </papid></citsent>
<aftsection>
<nextsent>the first pass of the decoder generates word-graph, acompact representation of alternative translation candidates, using beam search based onthe scores of the lexicon and language models.
</nextsent>
<nextsent>in the second pass an a* search traverses the graph.
</nextsent>
<nextsent>the edges of the word-graph, orthe phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model.
</nextsent>
<nextsent>the phrase translations extracted from the viterbi alignments of the training corpus also constitute the edges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B156">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences (watanabe and sumita, 2003).
</prevsent>
<prevsent>the decoder used the ibm model 4 with trigram language model and 5-gram part-of-speech language model.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the training of ibm model 4 was implemented by the giza++ package (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>4.2 model training.
</nextsent>
<nextsent>in order to quantify translation improvement by features from speech recognition and machine translation respectively, we built four log-linear models by adding features successively.
</nextsent>
<nextsent>the four models are: ? standard translation model(stm): only features from the ibm model 4 (m=5) described in section 2 were used in the loglinear models.
</nextsent>
<nextsent>we did not perform parameter optimization on this model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B158">
<title id=" C04-1168.xml">a unified approach in speechtospeech translation integrating features of speech recognition and machine translation </title>
<section> discussions.  </section>
<citcontext>
<prevsection>
<prevsent>as regards to integrating speech recognition with translation, coupling structure (ney,1999) was proposed as speech translation infrastructure that multiplies acoustic probabilities with translation probabilities in one-stepdecoding procedure.
</prevsent>
<prevsent>but no experimental results have been given on whether and how this coupling structure improved speech translation.
</prevsent>
</prevsection>
<citsent citstr=" W02-0706 ">
(casacuberta et al, 2002) <papid> W02-0706 </papid>used finite-statetransducer where scores from acoustic information sources and lexicon translation models were integrated together.</citsent>
<aftsection>
<nextsent>word pairs of source and target languages were tied in the decoding graph.
</nextsent>
<nextsent>however, this method was only tested for pair of similar languages, i.e., spanish to english.
</nextsent>
<nextsent>for translating between languages of different families where the syntactic structure scan be quite different, like japanese and english, rigid tying of word pair still remains to be shown its effectiveness for translation.our approach is rather general, easy to implement and flexible to expand.
</nextsent>
<nextsent>in the experiments we incorporated features from acoustic models and language models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B161">
<title id=" C08-1037.xml">an algorithm for adverbial aspect shift </title>
<section> the state of the art.  </section>
<citcontext>
<prevsection>
<prevsent>i am going to argue that by following this general line of thought too strictly, linguistic research so far failed to answer the question about the source of the semantic flexibility of temporal adverbials (compare (gruender, 2008)).
</prevsent>
<prevsent>the claim to be made in this section isthat the problems one can point out come about inevitably and for principle reasons.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
and that they should be seen as the result of an attempt to take good metaphor too literally.moens and steedman (1988) <papid> J88-2003 </papid>conceived temporal adverbials as functions which coerce?</citsent>
<aftsection>
<nextsent>their inputs to the appropriate type, by loose [sic!]analogy with type-coercion in programming lan guages?.
</nextsent>
<nextsent>under this perspective, aspect ual shift is triggered by conflict between the aspect ual type of the situation to be modified and the aspectualconstraint set by the temporal preposition heading the modifier.
</nextsent>
<nextsent>1 coercion operators, then, are thought to adapt the verbal input on the level of model-theoretical interpretation by mapping one sort of situation onto another.
</nextsent>
<nextsent>the underlying model is commonly supposed to include situation sas first-order objects of the four basic types previously mentioned, i. e., states, activities, accomplishments and achievements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B162">
<title id=" C02-1065.xml">measuring the similarity between compound nouns in different languages using non parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual natural language processing such as machine translation (mt) and cross-lingual information retrieval (clir) is becoming more important.
</prevsent>
<prevsent>when we read or write documents in foreign language, we need more knowledge than what is provided in an ordinary dictionary, such as terminology, words relevant to current affairs,etc. such expressions can be made up of multiple words, and there are almost infinite possible variations.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
therefore, so it is quite difficult to add them and their translations to dictionary.many approaches have tried to acquire translation equivalents automatically from parallel corpora (dagan and itai, 1994; <papid> J94-4003 </papid>fung, 1995).<papid> P95-1032 </papid></citsent>
<aftsection>
<nextsent>in parallel corpora, effective features that have obvious correlations between these corpora can be used ? e.g., similarity of position and frequency of words.however, we cannot always get enough parallel corpora to extract the desired information.we propose method of measuring the similarity to acquire compound noun translations by corpus information, which is not restricted to parallel corpora.
</nextsent>
<nextsent>co-occurrence information is obtained as context where target nouns appear from the corpora.
</nextsent>
<nextsent>in specific domain (e.g., financial news), target word and its translation are often used in similar context.
</nextsent>
<nextsent>for example, in financial newspaper, price competition may appear with products (electric appliances, clothes, and foods), stores and companies more often than with nations and public facilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B163">
<title id=" C02-1065.xml">measuring the similarity between compound nouns in different languages using non parallel corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cross-lingual natural language processing such as machine translation (mt) and cross-lingual information retrieval (clir) is becoming more important.
</prevsent>
<prevsent>when we read or write documents in foreign language, we need more knowledge than what is provided in an ordinary dictionary, such as terminology, words relevant to current affairs,etc. such expressions can be made up of multiple words, and there are almost infinite possible variations.
</prevsent>
</prevsection>
<citsent citstr=" P95-1032 ">
therefore, so it is quite difficult to add them and their translations to dictionary.many approaches have tried to acquire translation equivalents automatically from parallel corpora (dagan and itai, 1994; <papid> J94-4003 </papid>fung, 1995).<papid> P95-1032 </papid></citsent>
<aftsection>
<nextsent>in parallel corpora, effective features that have obvious correlations between these corpora can be used ? e.g., similarity of position and frequency of words.however, we cannot always get enough parallel corpora to extract the desired information.we propose method of measuring the similarity to acquire compound noun translations by corpus information, which is not restricted to parallel corpora.
</nextsent>
<nextsent>co-occurrence information is obtained as context where target nouns appear from the corpora.
</nextsent>
<nextsent>in specific domain (e.g., financial news), target word and its translation are often used in similar context.
</nextsent>
<nextsent>for example, in financial newspaper, price competition may appear with products (electric appliances, clothes, and foods), stores and companies more often than with nations and public facilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B164">
<title id=" C02-1065.xml">measuring the similarity between compound nouns in different languages using non parallel corpora </title>
<section> extraction of translations from.  </section>
<citcontext>
<prevsection>
<prevsent>fung assumed that co-occurring words of translation equivalents are similar, and compared distributions of the co-occurring words to acquire chinese-english translations from comparable corpora (fung, 1997).
</prevsent>
<prevsent>this method generates co-occurring words vectors for target words, and judges the pair of words whose similarity is high to be translation equivalents.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
rapp made german and english association word vectors and calculated the similarity of these vectors to find translations (rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>k.tanakaand iwasaki (1996) <papid> C96-2098 </papid>also assumed the resemblance between co-occurring words in source language and those in target language, and performed experiments to find irrelevant translations intentionally added to dictionary.</nextsent>
<nextsent>in fact, finding translation equivalents from non-parallel corpora is very difficult problem,so it is not practical to acquire all kinds of translations in the corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B165">
<title id=" C02-1065.xml">measuring the similarity between compound nouns in different languages using non parallel corpora </title>
<section> extraction of translations from.  </section>
<citcontext>
<prevsection>
<prevsent>this method generates co-occurring words vectors for target words, and judges the pair of words whose similarity is high to be translation equivalents.
</prevsent>
<prevsent>rapp made german and english association word vectors and calculated the similarity of these vectors to find translations (rapp, 1999).<papid> P99-1067 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
k.tanakaand iwasaki (1996) <papid> C96-2098 </papid>also assumed the resemblance between co-occurring words in source language and those in target language, and performed experiments to find irrelevant translations intentionally added to dictionary.</citsent>
<aftsection>
<nextsent>in fact, finding translation equivalents from non-parallel corpora is very difficult problem,so it is not practical to acquire all kinds of translations in the corpora.
</nextsent>
<nextsent>most technical terms are composed of known words, and we must collect these words to translate them correctly because new terms can be infinitely created by combining several words.
</nextsent>
<nextsent>we focus on translations of compound nouns here.
</nextsent>
<nextsent>first, we collect the translation candidates of target compound, and then measure the similarity between them to choose an appropriate candidate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B166">
<title id=" C02-1065.xml">measuring the similarity between compound nouns in different languages using non parallel corpora </title>
<section> context representation.  </section>
<citcontext>
<prevsection>
<prevsent>bank : [374: enterprise/company],[494: embankment] store : [374: enterprise/company] hotel : [437: lodging facilities],[374: enterprise/company] 4.3 context vector.
</prevsent>
<prevsent>a simple representation of context is set of co-occurring words for target word.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
as the strength of relevance between target compound noun and its co-occurring word r, the feature value of r, w(t, r) is defined by the log likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>1 as follows.</citsent>
<aftsection>
<nextsent>w(t, r) = { l(t, r) : f(t, r) 6= 0 0 : f(t, r) = 0 (1) l(t, r) = ? i,j1,2 kij log kijn cirj = k11 log k11n c1r1 + k12 log k12n c1r2 + k21 log k21n c2r1 + k22 log k22n c2r2 (2) k11 = f(t, r) k12 = f(t)?
</nextsent>
<nextsent>k11 k21 = f(r)?
</nextsent>
<nextsent>k11 k22 = ? k11 ? k12 ? k21 (3) c1 = k11 + k12 c2 = k21 + k22 r1 = k11 + k21 r2 = k12 + k22where f(t) and f(r) are frequencies of compound noun and co-occurring word r, respectively.
</nextsent>
<nextsent>f(t, r) is the co-occurring frequency between and r, and is the total frequencies of all words in corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B168">
<title id=" C02-1010.xml">structure alignment using bilingual chunking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results with english- chinese structure alignment show that our model can produce 90% in precision for chunking, and 87% in precision for chunk alignment.
</prevsent>
<prevsent>1 related works.
</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
most of the previous works conduct structure alignment with complex, hierarchical structures, such as phrase structures (e.g., kaji, kida &amp; morimoto, 1992), or dependency structures (e.g., matsumoto et al  1993; <papid> P93-1004 </papid>grishman, 1994; mey-ers, yanharber &amp; grishman 1996; watanabe, kurohashi &amp; aramaki 2000).</citsent>
<aftsection>
<nextsent>however, the mis matching between complex structures across languages and the poor parsing accuracy of the parser will hinder structure alignment algorithms from working out high accuracy results.
</nextsent>
<nextsent>a straightforward strategy for structure align-ment is parse-to-parse matching, which regards the parsing and alignment as two separate and successive procedures.
</nextsent>
<nextsent>first, parsing is conduct-ed on each language, respectively.
</nextsent>
<nextsent>then the correspondent structures in different languages are aligned (e.g., kaji, kida &amp; morimoto 1992; matsumoto et al  1993; <papid> P93-1004 </papid>grishman 1994; meyers, yanharber &amp; grishman 1996; watanabe, kurohashi &amp; aramaki 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B172">
<title id=" C02-1010.xml">structure alignment using bilingual chunking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, automatic parse-to-parse matching has some weaknesses as described in wu (2000).
</prevsent>
<prevsent>for example, grammar inconsistency exists across languages; and it is hard to handle multiple alignment choices.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
to deal with the difficulties in parse-to-parse matching, wu (1997) <papid> J97-3002 </papid>utilizes inversion transduction grammar (itg) for bilingual parsing.</citsent>
<aftsection>
<nextsent>bilingual parsing approach looks upon the parsing and alignment as single procedure which simultaneously encodes both the parsing and transferring information.
</nextsent>
<nextsent>it is, however, difficult to write broad coverage bilingual grammar?
</nextsent>
<nextsent>for bilingual parsing.
</nextsent>
<nextsent>chunking 2.1 principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B174">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>3 previous work.
</prevsent>
<prevsent>we review here previous work, mainly focusing on time complexity.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
in english as well as in japanese,dependency analysis has been studied (e.g., (laf ferty et al, 1992; collins, 1996; <papid> P96-1025 </papid>eisner, 1996)).<papid> C96-1058 </papid></citsent>
<aftsection>
<nextsent>the parsing algorithms in their papers require
</nextsent>
<nextsent> time where  is the number of words.2 in dependency analysis of japanese it is very common to use probabilities of dependencies between each two bunsetsus in sentence.
</nextsent>
<nextsent>haruno et al (1998) <papid> P98-1083 </papid>used decision trees to estimate the dependency probabilities.</nextsent>
<nextsent>fujio and matsumoto (1998) applied modified version of collins?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B175">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>3 previous work.
</prevsent>
<prevsent>we review here previous work, mainly focusing on time complexity.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
in english as well as in japanese,dependency analysis has been studied (e.g., (laf ferty et al, 1992; collins, 1996; <papid> P96-1025 </papid>eisner, 1996)).<papid> C96-1058 </papid></citsent>
<aftsection>
<nextsent>the parsing algorithms in their papers require
</nextsent>
<nextsent> time where  is the number of words.2 in dependency analysis of japanese it is very common to use probabilities of dependencies between each two bunsetsus in sentence.
</nextsent>
<nextsent>haruno et al (1998) <papid> P98-1083 </papid>used decision trees to estimate the dependency probabilities.</nextsent>
<nextsent>fujio and matsumoto (1998) applied modified version of collins?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B176">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>the parsing algorithms in their papers require
</prevsent>
<prevsent> time where  is the number of words.2 in dependency analysis of japanese it is very common to use probabilities of dependencies between each two bunsetsus in sentence.
</prevsent>
</prevsection>
<citsent citstr=" P98-1083 ">
haruno et al (1998) <papid> P98-1083 </papid>used decision trees to estimate the dependency probabilities.</citsent>
<aftsection>
<nextsent>fujio and matsumoto (1998) applied modified version of collins?
</nextsent>
<nextsent>model (collins, 1996) <papid> P96-1025 </papid>to japanese dependency analysis.</nextsent>
<nextsent>both haruno et al, and fujio and matsumoto used the cyk algorithm, which requires</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B178">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>both haruno et al, and fujio and matsumoto used the cyk algorithm, which requires
</prevsent>
<prevsent> time, where  is sentence length, i.e., the number of bunsetsus.
</prevsent>
</prevsection>
<citsent citstr=" C00-2109 ">
sekine et al (2000) <papid> C00-2109 </papid>used maximum entropy (me) modeling for dependency probabilities and proposed backward beam search to findthe best parse.</citsent>
<aftsection>
<nextsent>this beam search algorithm requires
</nextsent>
<nextsent> time.
</nextsent>
<nextsent>kudo and matsumoto (2000) <papid> W00-1303 </papid>also used the same backward beam search together with svms rather than me. there are few statistical methods that do not use dependency probabilities of each two bunsetsus.2nivre (2003) proposes deterministic algorithm for projective dependency parsing, the running time of which is linear.</nextsent>
<nextsent>the algorithm has been evaluated on swedish text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B179">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>this beam search algorithm requires
</prevsent>
<prevsent> time.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
kudo and matsumoto (2000) <papid> W00-1303 </papid>also used the same backward beam search together with svms rather than me. there are few statistical methods that do not use dependency probabilities of each two bunsetsus.2nivre (2003) proposes deterministic algorithm for projective dependency parsing, the running time of which is linear.</citsent>
<aftsection>
<nextsent>the algorithm has been evaluated on swedish text.
</nextsent>
<nextsent>ken-ga kanojo-ni ano hon-wo age-ta.
</nextsent>
<nextsent>ken-subj to her that book-acc gave.
</nextsent>
<nextsent>id 0 1 2 3 4head 4 4 3 4 figure 3: sample sentencesekine (2000) <papid> C00-2110 </papid>observed that 98.7% of the head locations are covered by five candidates in sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B180">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>ken-ga kanojo-ni ano hon-wo age-ta.
</prevsent>
<prevsent>ken-subj to her that book-acc gave.
</prevsent>
</prevsection>
<citsent citstr=" C00-2110 ">
id 0 1 2 3 4head 4 4 3 4 figure 3: sample sentencesekine (2000) <papid> C00-2110 </papid>observed that 98.7% of the head locations are covered by five candidates in sentence.</citsent>
<aftsection>
<nextsent>maruyama and ogino (maruyama and ogino, 1992)also observed similar phenomena.
</nextsent>
<nextsent>based on this observation, sekine (2000) <papid> C00-2110 </papid>proposed an efficient analysis algorithm using deterministic finite state transducers.</nextsent>
<nextsent>this algorithm, in which the limited number of bunsetsus are considered in order to avoid exhaustive search, takes</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B186">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>this algorithm, in which the limited number of bunsetsus are considered in order to avoid exhaustive search, takes
</prevsent>
<prevsent> time.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
however, his parser achieved an accuracy of 77.97% on the kyoto university corpus, which is considerably lower than the state-of-the-art accuracy around 89%.another interesting method that does not use dependency probabilities between each two bunsetsu sis the cascaded chunking model by kudo and matsumoto (2002) <papid> W02-2016 </papid>based on the idea in (abney, 1991; ratnaparkhi, 1997).<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>they used the model with svms and achieved an accuracy of 89.29%, which is the best result on the kyoto university corpus.although the number of dependencies that are estimated in parsing are significantly fewer than that either in cyk or the backward beam search, the upper bound of time complexity is still
</nextsent>
<nextsent>.thus, it is still an open question as to how we analyze dependencies for japanese in linear time with state-of-the-art accuracy.
</nextsent>
<nextsent>the algorithm described below will be an answer to this question.
</nextsent>
<nextsent>4.1 algorithm to parse sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B187">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> analyze dependencies between these bunset-.  </section>
<citcontext>
<prevsection>
<prevsent>this algorithm, in which the limited number of bunsetsus are considered in order to avoid exhaustive search, takes
</prevsent>
<prevsent> time.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
however, his parser achieved an accuracy of 77.97% on the kyoto university corpus, which is considerably lower than the state-of-the-art accuracy around 89%.another interesting method that does not use dependency probabilities between each two bunsetsu sis the cascaded chunking model by kudo and matsumoto (2002) <papid> W02-2016 </papid>based on the idea in (abney, 1991; ratnaparkhi, 1997).<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>they used the model with svms and achieved an accuracy of 89.29%, which is the best result on the kyoto university corpus.although the number of dependencies that are estimated in parsing are significantly fewer than that either in cyk or the backward beam search, the upper bound of time complexity is still
</nextsent>
<nextsent>.thus, it is still an open question as to how we analyze dependencies for japanese in linear time with state-of-the-art accuracy.
</nextsent>
<nextsent>the algorithm described below will be an answer to this question.
</nextsent>
<nextsent>4.1 algorithm to parse sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B192">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> models for estimating dependency.  </section>
<citcontext>
<prevsection>
<prevsent>excellent performances have been reported for many classification tasks.
</prevsent>
<prevsent>please see (vapnik, 1995) for formal descriptions of svms.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
3kudo and matsumoto (2002) <papid> W02-2016 </papid>give more comprehensive comparison with the probabilistic models as used in (uchimoto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>at estimate dependency() in figure 1, we encode an example with features described below.
</nextsent>
<nextsent>then wegive it to the svm and receive the estimated decision as to whether bunsetsu modifies the other.
</nextsent>
<nextsent>5.1 standard features.
</nextsent>
<nextsent>by the standard features?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B202">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> models for estimating dependency.  </section>
<citcontext>
<prevsection>
<prevsent>5.4 features for conjunctive structures.
</prevsent>
<prevsent>detecting conjunctive structures is one of hard tasks in parsing long sentences correctly.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
kurohashi andnagao (1994) <papid> J94-4001 </papid>proposed method to detect conjunctive structures by calculating similarity scores between two sequences of bunsetsus.</citsent>
<aftsection>
<nextsent>so far few attempts have been made to explore features for detecting conjunctive structures.
</nextsent>
<nextsent>as afirst step we tried two preliminary features for con junctive structures.
</nextsent>
<nextsent>if the current modifier bunsetsu is distinctive key bunsetsu (kurohashi and nagao, 1994, <papid> J94-4001 </papid>page 510), these features are triggered.</nextsent>
<nextsent>oneis feature which is activated when modifier bunsetsu is distinctive key bunsetsu.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B214">
<title id=" C04-1002.xml">linear time dependency analysis for japanese </title>
<section> experimental results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the misclassification cost is set to 0.0056.classifiers in our experiments have about forty thousand support vectors.
</prevsent>
<prevsent>therefore, for every decision of dependency also huge computation of dot products is required.
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
fortunately, solutions to this problem have already been given by kudo and matsumoto (2003).<papid> P03-1004 </papid></citsent>
<aftsection>
<nextsent>they proposed methods to convert polynomial kernel with higher degrees to simple linear kernel and reported new classifier with the converted kernel was about 30 to 300 times faster than the original one while keeping the accuracy.
</nextsent>
<nextsent>by applying their methods to our parser, its processing time would be enough practical.
</nextsent>
<nextsent>in order to roughly estimate the improved speed of our parser, we built parser with linear kernel and ran it on the same test set.
</nextsent>
<nextsent>figure 5 shows the observed time of the parser with linear kernel using the same machine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B226">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1: predicate-argument structure of sample sentence.
</prevsent>
<prevsent>argument labels are in propbank-style.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></nextsent>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B227">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B228">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B229">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B230">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B231">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-2008 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B232">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-2009 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B233">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>argument labels are in propbank-style.
</prevsent>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-4037 ">
since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></citsent>
<aftsection>
<nextsent>large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</nextsent>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B235">
<title id=" C04-1186.xml">semantic role labeling using dependency trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic role labeling based on predicate argument structure was first explored in detail by (gildea and jurafsky, 2002).<papid> J02-3001 </papid></prevsent>
<prevsent>since then several variants of the basic approach have been introduced using different features and different classifiers based on various machine learning techniques (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and hockenmaier, 2003; <papid> W03-1008 </papid>surdeanu et. al., 2003; <papid> P03-1002 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>fleischman and hovy, 2003; <papid> N03-2008 </papid>hacioglu and ward, 2003; <papid> N03-2009 </papid>thompson et. al., 2003; pradhan et. al., 2003b; hacioglu, 2004).<papid> N04-4037 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
large semantically annotated databases, like framenet (baker et.al, 1998) <papid> P98-1013 </papid>and propbank (kingsbury and palmer, 2002) have been used to train and test the classifiers.</citsent>
<aftsection>
<nextsent>most of those approaches can be divided into one of the following three broad classes with respect to the type of tokens classified; namely, constituent-by constituent (c-by-c), phrase-by-phrase (p-by-p) and word-by-word (w-by-w) semantic role labelers.
</nextsent>
<nextsent>in c-by-c semantic role labeling, the syntactic tree representation of sentence is linear ized into sequence of its syntactic constituents (non terminals).
</nextsent>
<nextsent>then each constituent is classified into one of several semantic roles using number of features derived from the sentence structure or linguistic context defined for the constituent token.
</nextsent>
<nextsent>in the p-by-p and w-by-w methods (hacioglu, 2004; <papid> N04-4037 </papid>hacioglu and ward, 2003) <papid> N03-2009 </papid>the problem is formulated as chunking task and the features are derived for each base phrase and word, respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B245">
<title id=" C04-1110.xml">semantic similarity applied to spoken dialogue summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results show that our system outperforms lead, random and tf*idf baselines.
</prevsent>
<prevsent>research in automatic text summarization began inthe late 1950s and has been receiving more attention again over the last decade.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
the maturity of this research area is indicated by recent large-scale evaluation efforts (radev et al, 2003).<papid> P03-1048 </papid></citsent>
<aftsection>
<nextsent>in comparison, speech summarization is rather new research area which emerged only few years ago.
</nextsent>
<nextsent>however, the demand for speech summarization is growing because of the increasing availability of (digitally encoded) speech databases (e.g. spoken news, political speeches).
</nextsent>
<nextsent>our research is concerned with the development of system for automatically generating summaries of conversational speech.
</nextsent>
<nextsent>as potential application we envision the automatic generation of meeting minutes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B246">
<title id=" C04-1110.xml">semantic similarity applied to spoken dialogue summarization </title>
<section> text, speech and dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>this work was based either oncorpus-based, statistical methods or on knowledge based techniques (for an overview over both strands of research see mani &amp; maybury (1999)).
</prevsent>
<prevsent>recent advances in text summarization are mostly dueto statistical techniques with some additional us age of linguistic knowledge, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
(marcu, 2000;teufel &amp; moens, 2002), <papid> J02-4002 </papid>which can be applied to unrestricted input.</citsent>
<aftsection>
<nextsent>research on speech summarization focused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.).
</nextsent>
<nextsent>the methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation.
</nextsent>
<nextsent>difficulties arise because speech recognition systems are not perfect.
</nextsent>
<nextsent>therefore, spoken dialogue summarization systems have to deal with errors in the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B247">
<title id=" C04-1110.xml">semantic similarity applied to spoken dialogue summarization </title>
<section> text, speech and dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, spoken dialogue summarization systems have to deal with errors in the input.
</prevsent>
<prevsent>there are no sentence boundaries in spoken language either.
</prevsent>
</prevsection>
<citsent citstr=" P00-1040 ">
work on spoken dialogue summarization is still in its infancy (reithinger et al, 2000; <papid> P00-1040 </papid>zechner, 2002).<papid> J02-4003 </papid></citsent>
<aftsection>
<nextsent>multiparty dialogue is much more difficult to process than written text.
</nextsent>
<nextsent>in addition to the difficulties speech summarization has to face, spoken dialogue contains whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. also, the information to be summarized may be contributed by different speakers (e.g. in question answer pairs).
</nextsent>
<nextsent>finally, the language used in spoken dialogue differs from language used in texts.
</nextsent>
<nextsent>be cause discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B248">
<title id=" C04-1110.xml">semantic similarity applied to spoken dialogue summarization </title>
<section> text, speech and dialogue.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, spoken dialogue summarization systems have to deal with errors in the input.
</prevsent>
<prevsent>there are no sentence boundaries in spoken language either.
</prevsent>
</prevsection>
<citsent citstr=" J02-4003 ">
work on spoken dialogue summarization is still in its infancy (reithinger et al, 2000; <papid> P00-1040 </papid>zechner, 2002).<papid> J02-4003 </papid></citsent>
<aftsection>
<nextsent>multiparty dialogue is much more difficult to process than written text.
</nextsent>
<nextsent>in addition to the difficulties speech summarization has to face, spoken dialogue contains whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. also, the information to be summarized may be contributed by different speakers (e.g. in question answer pairs).
</nextsent>
<nextsent>finally, the language used in spoken dialogue differs from language used in texts.
</nextsent>
<nextsent>be cause discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B250">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>like other problems in natural language processing, knowledge acquisition is crucial for translation selection.
</prevsent>
<prevsent>therefore, many researchers have endeavored to extract knowledge from existing resources.as masses of language resources become available, statistical methods have been attempted for translation selection and shown practical results.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
some of the approaches have used bilingual corpus as knowledge source based on the idea of brown et al (1990), <papid> J90-2002 </papid>but they are not preferred in general since bilingual corpus ishard to come by.</citsent>
<aftsection>
<nextsent>though some latest approaches have exploited word co-occurrence that is extracted from monolingual corpus in target language (dagan and itai, 1994; <papid> J94-4003 </papid>prescher et al, 2000; <papid> C00-2094 </papid>koehn and knight, 2001), <papid> W01-0504 </papid>those methods often fail in selecting appropriate words because they do not consider sense ambiguity of target word.</nextsent>
<nextsent>in bilingual dictionary, senses of word are classi ed into several sense divisions and also its translations are grouped by each sense division.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B251">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, many researchers have endeavored to extract knowledge from existing resources.as masses of language resources become available, statistical methods have been attempted for translation selection and shown practical results.
</prevsent>
<prevsent>some of the approaches have used bilingual corpus as knowledge source based on the idea of brown et al (1990), <papid> J90-2002 </papid>but they are not preferred in general since bilingual corpus ishard to come by.</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
though some latest approaches have exploited word co-occurrence that is extracted from monolingual corpus in target language (dagan and itai, 1994; <papid> J94-4003 </papid>prescher et al, 2000; <papid> C00-2094 </papid>koehn and knight, 2001), <papid> W01-0504 </papid>those methods often fail in selecting appropriate words because they do not consider sense ambiguity of target word.</citsent>
<aftsection>
<nextsent>in bilingual dictionary, senses of word are classi ed into several sense divisions and also its translations are grouped by each sense division.
</nextsent>
<nextsent>therefore, when one looks up the translation of word in dictionary, she/he ought to resolve the sense of word in source language sentence, and then choose target word among translations corresponding to the sense.
</nextsent>
<nextsent>in this paper, the fact that word has many sense sand each sense can be mapped into many target words (lee et al, 1999) is referred to as the `word-to-sense and sense-to-word  relationship, based on which we propose hybrid method for translation selection.
</nextsent>
<nextsent>in our method, translation selection is takenas the combined problem of sense disambiguation of source language word and selection of target language word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B252">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, many researchers have endeavored to extract knowledge from existing resources.as masses of language resources become available, statistical methods have been attempted for translation selection and shown practical results.
</prevsent>
<prevsent>some of the approaches have used bilingual corpus as knowledge source based on the idea of brown et al (1990), <papid> J90-2002 </papid>but they are not preferred in general since bilingual corpus ishard to come by.</prevsent>
</prevsection>
<citsent citstr=" C00-2094 ">
though some latest approaches have exploited word co-occurrence that is extracted from monolingual corpus in target language (dagan and itai, 1994; <papid> J94-4003 </papid>prescher et al, 2000; <papid> C00-2094 </papid>koehn and knight, 2001), <papid> W01-0504 </papid>those methods often fail in selecting appropriate words because they do not consider sense ambiguity of target word.</citsent>
<aftsection>
<nextsent>in bilingual dictionary, senses of word are classi ed into several sense divisions and also its translations are grouped by each sense division.
</nextsent>
<nextsent>therefore, when one looks up the translation of word in dictionary, she/he ought to resolve the sense of word in source language sentence, and then choose target word among translations corresponding to the sense.
</nextsent>
<nextsent>in this paper, the fact that word has many sense sand each sense can be mapped into many target words (lee et al, 1999) is referred to as the `word-to-sense and sense-to-word  relationship, based on which we propose hybrid method for translation selection.
</nextsent>
<nextsent>in our method, translation selection is takenas the combined problem of sense disambiguation of source language word and selection of target language word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B253">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, many researchers have endeavored to extract knowledge from existing resources.as masses of language resources become available, statistical methods have been attempted for translation selection and shown practical results.
</prevsent>
<prevsent>some of the approaches have used bilingual corpus as knowledge source based on the idea of brown et al (1990), <papid> J90-2002 </papid>but they are not preferred in general since bilingual corpus ishard to come by.</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
though some latest approaches have exploited word co-occurrence that is extracted from monolingual corpus in target language (dagan and itai, 1994; <papid> J94-4003 </papid>prescher et al, 2000; <papid> C00-2094 </papid>koehn and knight, 2001), <papid> W01-0504 </papid>those methods often fail in selecting appropriate words because they do not consider sense ambiguity of target word.</citsent>
<aftsection>
<nextsent>in bilingual dictionary, senses of word are classi ed into several sense divisions and also its translations are grouped by each sense division.
</nextsent>
<nextsent>therefore, when one looks up the translation of word in dictionary, she/he ought to resolve the sense of word in source language sentence, and then choose target word among translations corresponding to the sense.
</nextsent>
<nextsent>in this paper, the fact that word has many sense sand each sense can be mapped into many target words (lee et al, 1999) is referred to as the `word-to-sense and sense-to-word  relationship, based on which we propose hybrid method for translation selection.
</nextsent>
<nextsent>in our method, translation selection is takenas the combined problem of sense disambiguation of source language word and selection of target language word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B254">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> calculation of each measure.  </section>
<citcontext>
<prevsection>
<prevsent>spf(s ) = w 2snt max d 2def k sim(w ; d ) (1) + w 2snt max e 2ex k sim(w ; e )sense preference is obtained by calculating similarity between words in snt and words in de and ex.
</prevsent>
<prevsent>for all words in an input sentence (w 2snt), we sum up the maximum similarity between i and all clue words (i.e. d and e ).
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
to get similarity between words (sim(w ; j )), we use wordnet and the metric proposed in rigau et al (1997).<papid> P97-1007 </papid></citsent>
<aftsection>
<nextsent>senses in dictionary are generally ordered by frequency.
</nextsent>
<nextsent>to re ect distribution of senses in common text, we use weight factor (s ) that is inversely proportional to the order of sense k in dictionary.
</nextsent>
<nextsent>then, we calculate normalized sense preference to combine sense preference and sense probability.
</nextsent>
<nextsent>spf (s ) = (s )
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B255">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> calculation of each measure.  </section>
<citcontext>
<prevsection>
<prevsent>in an element (s ;m; c) of (s ), j is word that co-occurs with i in an input sentences, is syntactic relation 2 between i and j , and is the number of translations of j . provided.
</prevsent>
<prevsent>that the set of translations of sense k is k and member of k is k iq, the frequency of cooccurring k iq and jp with syntactic relation is denoted as f(t iq ; jp; c), which is extracted from target language corpora.
</prevsent>
</prevsection>
<citsent citstr=" W99-0707 ">
3 therefore, 2we guess 37 syntactic relations in english sentences based on verb pattern information in the dictionary and the results of the memory based shallow parser (daelemans et al, 1999): <papid> W99-0707 </papid>subj-verb, obj-verb, modifier-noun, adverb-modifiee, etc. 3 when `jeobsi  has 4 translations (`plate , `dish ,`saucer , `platter ), (kkae-da) has member (jeob n(t iq) in the equation (4) represents how frequently k iq co-occurs with translations of j . by.</citsent>
<aftsection>
<nextsent>summing up n(t iq ) for all target words in k , we obtain sense probability of k . 3.3 word probability.
</nextsent>
<nextsent>word probability represents how frequently target word in sense division co-occurs in corpus with translations of other words within the input sentence.
</nextsent>
<nextsent>we denote word probability as wp(t iq ) that is probability of selecting k iq from k . using n(t. iq ) in the equation (4), we calculate word probability as follows: wp(t iq ) = p^ word (t iq ) = n(t iq ) x n(t ix ) (6) 3.4 translation preference.
</nextsent>
<nextsent>to select target word among all translationsof source word, we compute translation preference (tpf) by merging sense preference, sense probability and word probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B257">
<title id=" C02-1046.xml">translation selection through source word sense disambiguation and target word selection </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we randomly chose 945 sentences as an evaluation set, in which 3,081 words in english sentences satisfy the condition.
</prevsent>
<prevsent>among them, 1,653 are nouns, 990 are verbs, 322 are adjectives, and 116 are adverbs.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
we used brill tagger (brill, 1995) <papid> J95-4004 </papid>and memory-based shallow parser (daelemans etal., 1999) <papid> W99-0707 </papid>to analyze english sentences.</citsent>
<aftsection>
<nextsent>to analyze korean sentences, we used pos tagger for korean (kim and kim, 1996).
</nextsent>
<nextsent>first, we evaluated the accuracies of sense preference (spf) and sense probability (sp).
</nextsent>
<nextsent>if any target word of the sense with the highest value is included in an aligned target language sentence, we regarded it as correct result.
</nextsent>
<nextsent>the accuracy of sense preference is shown in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B259">
<title id=" C02-1140.xml">bringing the dictionary to the user the foks system </title>
<section> data preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>our system is intended to handle strings both in the form they appear in texts (as combination of the three japanese orthographies) and as they are read (with the reading expressed in hiragana).
</prevsent>
<prevsent>given area ding input, the system needs to establish relationship between the reading and one or more dictionary entries, and rate the plausibility of each entry being realized with the entered reading.
</prevsent>
</prevsection>
<citsent citstr=" C00-1050 ">
in sense this problem is analogous to kanakanji conversion (see, e.g., ichimura et al (2000) <papid> C00-1050 </papid>and takahashi et al (1996)), <papid> C96-2206 </papid>in that we seek to determine ranked listing of kanji strings that could correspond to the input kana string.</citsent>
<aftsection>
<nextsent>there is one major difference, however.
</nextsent>
<nextsent>kanakanji conversion systems correctly identify word boundaries.
</nextsent>
<nextsent>2http://www.rikai.com are designed for native speakers of japanese and assuch expect accurate input.
</nextsent>
<nextsent>in cases when the correct or standardized reading is not available, kanji characters have to be converted one by one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B260">
<title id=" C02-1140.xml">bringing the dictionary to the user the foks system </title>
<section> data preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>our system is intended to handle strings both in the form they appear in texts (as combination of the three japanese orthographies) and as they are read (with the reading expressed in hiragana).
</prevsent>
<prevsent>given area ding input, the system needs to establish relationship between the reading and one or more dictionary entries, and rate the plausibility of each entry being realized with the entered reading.
</prevsent>
</prevsection>
<citsent citstr=" C96-2206 ">
in sense this problem is analogous to kanakanji conversion (see, e.g., ichimura et al (2000) <papid> C00-1050 </papid>and takahashi et al (1996)), <papid> C96-2206 </papid>in that we seek to determine ranked listing of kanji strings that could correspond to the input kana string.</citsent>
<aftsection>
<nextsent>there is one major difference, however.
</nextsent>
<nextsent>kanakanji conversion systems correctly identify word boundaries.
</nextsent>
<nextsent>2http://www.rikai.com are designed for native speakers of japanese and assuch expect accurate input.
</nextsent>
<nextsent>in cases when the correct or standardized reading is not available, kanji characters have to be converted one by one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B261">
<title id=" C02-1140.xml">bringing the dictionary to the user the foks system </title>
<section> data preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>this can be painstaking process due to the large number of characters taking on identical readings, resulting in large lists of characters for the user to choose from.
</prevsent>
<prevsent>our system, on the other hand, does not assume 100% accurate knowledge of readings, but instead expects readings to be predictably derived from the source kanji.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
what we do assume is that the user is able to determine word boundaries, which is in reality non-trivial task due to japanese being non segmenting (see kurohashi et al (1994) and nagata (1994), <papid> C94-1032 </papid>among others, for details of automatic segmentation methods).</citsent>
<aftsection>
<nextsent>in sense, the problem of word segmentation is distinct from the dictionary look-up task, so we do not tackle it in this paper.
</nextsent>
<nextsent>to be able to infer how kanji characters can be read, we first determine all possible readings kanji character can take based on automatically-derivedalignment data.
</nextsent>
<nextsent>then, we machine learn phonological rules governing the formation of compound kanjistrings.
</nextsent>
<nextsent>given this information we are able to generate set of readings for each dictionary entry that might be perceived as correct by learner possessing some, potentially partial, knowledge of the character readings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B262">
<title id=" C02-1140.xml">bringing the dictionary to the user the foks system </title>
<section> data preprocessing.  </section>
<citcontext>
<prevsection>
<prevsent>then, we machine learn phonological rules governing the formation of compound kanjistrings.
</prevsent>
<prevsent>given this information we are able to generate set of readings for each dictionary entry that might be perceived as correct by learner possessing some, potentially partial, knowledge of the character readings.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
our generative method is analogous to that successfully applied by knight and graehl (1998) <papid> J98-4003 </papid>to the related problem of japanese (back) transliteration.</citsent>
<aftsection>
<nextsent>2.2 generating and grading readings.
</nextsent>
<nextsent>in order to generate set of plausible readings we first extract all dictionary entries containing kanji, and for each entry perform the following steps: 1.
</nextsent>
<nextsent>segment the kanji string into minimal morpho-.
</nextsent>
<nextsent>phonemic units3 and align each resulting unit with the corresponding reading.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B263">
<title id=" C04-1170.xml">lexicalisation strategies in cooperative question answering systems </title>
<section> motivations.  </section>
<citcontext>
<prevsection>
<prevsent>the response involves evaluating village proximity and sorting responses, e.g. by increasing distance from mege`ve.
</prevsent>
<prevsent>the first part of the response there are no bungalows in meg`eve is the direct response, which corrects the user false pre supposition, while the remainder of the response reflects the cooperative know-how of the respon der.
</prevsent>
</prevsection>
<citsent citstr=" W04-0202 ">
cooperative know-how involves several forms of responses that include relaxations, intensional calculus, expression of restrictions, of warnings and conditional responses (benamara et al  2004).<papid> W04-0202 </papid>this simple example shows that, if direct response cannot be found, several forms of knowledge and reasoning schemas need to be used and that thenl form of the response requires adequate and subtle lexicalisations, often directed by the reasoning schemas.</citsent>
<aftsection>
<nextsent>lexicalisation is the operation that associates word or an expression to concept.
</nextsent>
<nextsent>it is major parameter in response production (reiter et dale, 1997), (a good synthesis can be found in (cahill, 1999)).
</nextsent>
<nextsent>lexicalisation is often decomposed into two different stages: lexical choice, which occurs during content determination, where the lexical term chosen, which may still be underspecified, is dependent on reasoning procedures, the knowledge base contents, and grammatical constraints; and lexical variation which is the choice of particular word or form among possible synonyms or paraphrases.
</nextsent>
<nextsent>lexical variation occurs in the surface realizer, andmay have some pragmatic connotations, for example an implicit evaluation via the language level chosen (e.g. argotic entails low evaluation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B273">
<title id=" C04-1191.xml">inferring parts of speech for lexical mappings via the cyc kb </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in terms of absolute accuracy it might seem that the system based on opencyc is doing nearly as well as the system based on full cyc.
</prevsent>
<prevsent>this is somewhat misleading, since the distribution of parts of speech is simpler in opencyc, as shown by the lower entropy value (jurafsky and martin, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
there has not been much work in the automatic determination of the preferred lexicalization part of speech, outside of work related to part-of-speech tagging (brill, 1995), <papid> J95-4004 </papid>which concentrates on the dataset characteristics opencyc cyc instances 2607 30676 classes 2 2 entropy 0.76 0.90 accuracy figures opencyc cyc baseline 78.3 68.6 just-headwords 87.5 89.3 just-suffixes 78.3 71.9 just-corpus 78.2 68.6 just-terms 87.4 90.5 combination 88.4 93.0 table 4: mass-count classification over cyclexical mappings.</citsent>
<aftsection>
<nextsent>instances is size of the training data.
</nextsent>
<nextsent>classes is the number of choices.
</nextsent>
<nextsent>entropy characterizes distribution uniformity.
</nextsent>
<nextsent>baseline uses more frequent case.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B274">
<title id=" C04-1191.xml">inferring parts of speech for lexical mappings via the cyc kb </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>unknown words are handled basically via rules that change the default assignment to another based on the suffixes of the unknown word.
</prevsent>
<prevsent>pedersen and chen (1995) discuss an approach to inferring the grammatical categories of unknown words using constraint solving over the properties of the known words.
</prevsent>
</prevsection>
<citsent citstr=" A00-1024 ">
toole (2000) <papid> A00-1024 </papid>applies decision trees to similar problem, distinguishing common nouns, pronouns, and various types of names, using framework analogous to that commonly applied in named-entity recognition.</citsent>
<aftsection>
<nextsent>in work closer to ours, woods (2000) <papid> A00-1030 </papid>describe san approach to this problem using manually constructed rules incorporating syntactic, morphological, and semantic tests (via an ontology).</nextsent>
<nextsent>for example, patterns targeting specific stems are applied provided that the root meets certain semantic constraints.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B275">
<title id=" C04-1191.xml">inferring parts of speech for lexical mappings via the cyc kb </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pedersen and chen (1995) discuss an approach to inferring the grammatical categories of unknown words using constraint solving over the properties of the known words.
</prevsent>
<prevsent>toole (2000) <papid> A00-1024 </papid>applies decision trees to similar problem, distinguishing common nouns, pronouns, and various types of names, using framework analogous to that commonly applied in named-entity recognition.</prevsent>
</prevsection>
<citsent citstr=" A00-1030 ">
in work closer to ours, woods (2000) <papid> A00-1030 </papid>describe san approach to this problem using manually constructed rules incorporating syntactic, morphological, and semantic tests (via an ontology).</citsent>
<aftsection>
<nextsent>for example, patterns targeting specific stems are applied provided that the root meets certain semantic constraints.
</nextsent>
<nextsent>there has been clustering-based workin part-of-speech induction, but these tend to target idiosyncratic classes, such as capitalized words and words ending in ?-ed?
</nextsent>
<nextsent>(clark, 2003).<papid> E03-1009 </papid></nextsent>
<nextsent>the special case of classifying the mass-count distinction has received some attention.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B276">
<title id=" C04-1191.xml">inferring parts of speech for lexical mappings via the cyc kb </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, patterns targeting specific stems are applied provided that the root meets certain semantic constraints.
</prevsent>
<prevsent>there has been clustering-based workin part-of-speech induction, but these tend to target idiosyncratic classes, such as capitalized words and words ending in ?-ed?
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
(clark, 2003).<papid> E03-1009 </papid></citsent>
<aftsection>
<nextsent>the special case of classifying the mass-count distinction has received some attention.
</nextsent>
<nextsent>bond andvatikiotis-bateson (2002) infer five types of count ability distinctions using nt&ts; japanese to english transfer dictionary, including the categories strongly countable, weakly countable, and plural only.
</nextsent>
<nextsent>the count ability assigned to particular semantic category is based on the most common case associated with the english words mapping into the category.
</nextsent>
<nextsent>our earlier work (ohara et al,2003) just used semantic features as well but accounted for inheritance of types, achieving 89.5% with baseline of 68.2%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B277">
<title id=" C04-1191.xml">inferring parts of speech for lexical mappings via the cyc kb </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>schwartz (2002) uses the five nt&t; count ability distinctions when tagging word occurrences in corpus (i.e., word tokens), based primarily on clues provided by determiners.
</prevsent>
<prevsent>results are given in terms of agreement rather than accuracy; compared to nt&ts; dictionary there isabout 90% agreement for the fully or strong count able types and about 40% agreement for the weakly countable or uncountable types, with half of the tokens left untagged for countability.
</prevsent>
</prevsection>
<citsent citstr=" P03-1059 ">
baldwin and bond (2003) <papid> P03-1059 </papid>apply sophisticated preprocessing to derive variety of count ability clues, such as grammatical number of modifiers, co-occurrence of specific types of determiners and pronouns, and specific types of prepositions.</citsent>
<aftsection>
<nextsent>they achieve 94.6% accuracy using four categories of count ability, including two categories for types of plural-only nouns.
</nextsent>
<nextsent>since multiple assignments are allowed, negative agreement is considered as well as positive.
</nextsent>
<nextsent>when restricted to just count versus mass nouns, the accuracy is 89.9% (personal communication).
</nextsent>
<nextsent>note that, as with schwartz, the task is different from ours and that of bond and vatikiotis-bateson: we assign count ability to word/concept pairs instead of just to words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B278">
<title id=" C08-1076.xml">modeling the structure and dynamics of the consonant inventories a complex network approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to identify the nature of these interactions one has to understand the growth dynamics of these inventories.
</prevsent>
<prevsent>the theories of complex networks provide number of growth models that have proved to be extremely successful in explaining the evolutionary dynamics of various social (newman, 2001; ramasco et al ., 2004), biological (jeong et al , 2000) and other natural systems.
</prevsent>
</prevsection>
<citsent citstr=" P06-2017 ">
the basic framework for the current study develops around two such complex networks namely, the phoneme-language network or planet (choudhury et al , 2006) <papid> P06-2017 </papid>and its one mode projection, the phoneme-phoneme network or phonet (mukherjee et al 2007<papid> P07-1014 </papid>a).</citsent>
<aftsection>
<nextsent>we begin by analyzing some of the structural properties (sec.
</nextsent>
<nextsent>2) of the networks and observe that the consonant nodes in both planet and phonet follow power law-like degree distribution.
</nextsent>
<nextsent>moreover, phonet 601 is characterized by high clustering coefficient, property that has been found to be prevalent inmany other social networks (newman, 2001; ra masco et al , 2004).
</nextsent>
<nextsent>we propose four synthesis models for planet(sec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B282">
<title id=" C08-1076.xml">modeling the structure and dynamics of the consonant inventories a complex network approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to identify the nature of these interactions one has to understand the growth dynamics of these inventories.
</prevsent>
<prevsent>the theories of complex networks provide number of growth models that have proved to be extremely successful in explaining the evolutionary dynamics of various social (newman, 2001; ramasco et al ., 2004), biological (jeong et al , 2000) and other natural systems.
</prevsent>
</prevsection>
<citsent citstr=" P07-1014 ">
the basic framework for the current study develops around two such complex networks namely, the phoneme-language network or planet (choudhury et al , 2006) <papid> P06-2017 </papid>and its one mode projection, the phoneme-phoneme network or phonet (mukherjee et al 2007<papid> P07-1014 </papid>a).</citsent>
<aftsection>
<nextsent>we begin by analyzing some of the structural properties (sec.
</nextsent>
<nextsent>2) of the networks and observe that the consonant nodes in both planet and phonet follow power law-like degree distribution.
</nextsent>
<nextsent>moreover, phonet 601 is characterized by high clustering coefficient, property that has been found to be prevalent inmany other social networks (newman, 2001; ra masco et al , 2004).
</nextsent>
<nextsent>we propose four synthesis models for planet(sec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B305">
<title id=" C08-1073.xml">applying discourse analysis and data mining methods to spoken osce assessments </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>levow et al (1999) propose an architecture to automatically assess language proficiency.
</prevsent>
<prevsent>in their paper, they propose an architecture that employs data mining methods, but do not build classifiers over their spoken data to test this proposal.
</prevsent>
</prevsection>
<citsent citstr=" P01-1014 ">
a closely related line of research is on the automatic classification of discourse elements to assess the quality of written genre (burstein et al., 2001).<papid> P01-1014 </papid></citsent>
<aftsection>
<nextsent>like this work, it focuses on extracting features from the discourse as whole.
</nextsent>
<nextsent>but unlike this study, the authors extract high level features, such as rhetorical structure, of written discourse.
</nextsent>
<nextsent>the study we present in this paper is rather unique in its approach to language assessment.
</nextsent>
<nextsent>the data is taken from transcribed recordings of examinations from students enrolled in bridging course at box hill hospital in melbourne,australia.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B307">
<title id=" C02-1141.xml">a complete integrated nlg system using ai and nlu tools </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gephox is presented in section 2.
</prevsent>
<prevsent>as this paper intends to present complete nlg system, there is no room for explaining each module in detail.
</prevsent>
</prevsection>
<citsent citstr=" W01-0803 ">
we refer the reader to (elghali, 2001) for the content determination module, to (danlos et al, 2001) <papid> W01-0803 </papid>for the document structuring module and to danlos (1998), and to danlos (2000) for the lexicalized tactical component.</citsent>
<aftsection>
<nextsent>these goal p, : n(d 6= n0 ? q, : n(r   ? = ? + r)) 1.
</nextsent>
<nextsent>intros.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>elim 4 well founded.n. 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B311">
<title id=" C04-1116.xml">term aggregation mining synonymous expressions using personal stylistic variations </title>
<section> mwave.  </section>
<citcontext>
<prevsection>
<prevsent>5      screw 6 mark 7                     cheque 8 diskette 9 check mark 10 boot author rank candidate 1 batt 2 form 3 protector 4 diskette.
</prevsent>
<prevsent>6                     adapter 7 mouse 8                     cheque 9 check mark 10 process table 4: noise candidates from each authors corpus word.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
the words we want to aggregate for text analysis are not rigorous synonyms, but the roleis the same, so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words (hindle, 1990; <papid> P90-1034 </papid>strzalkowski, 1992).<papid> H92-1040 </papid></citsent>
<aftsection>
<nextsent>on the other hand, window-based techniques are not suit able for our data, because the documents are written by several authors who have variety of different writing styles (e.g. selecting different prepositions and articles).
</nextsent>
<nextsent>therefore we consider only syntactic features: dependency pairs, which consist of nouns, verbs, and their relationships.
</nextsent>
<nextsent>a dependency pair is written as (noun, verb(with its relationship)) as in the following examples.
</nextsent>
<nextsent>(customer, boot) (customer, shut off) (tp, shut off) the symbol  means the noun modifies the verb,and  means the verb modifies the noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B312">
<title id=" C04-1116.xml">term aggregation mining synonymous expressions using personal stylistic variations </title>
<section> mwave.  </section>
<citcontext>
<prevsection>
<prevsent>5      screw 6 mark 7                     cheque 8 diskette 9 check mark 10 boot author rank candidate 1 batt 2 form 3 protector 4 diskette.
</prevsent>
<prevsent>6                     adapter 7 mouse 8                     cheque 9 check mark 10 process table 4: noise candidates from each authors corpus word.
</prevsent>
</prevsection>
<citsent citstr=" H92-1040 ">
the words we want to aggregate for text analysis are not rigorous synonyms, but the roleis the same, so we have to consider the syntactic relation based on the assumptions that words with the same role tend to modify or be modified by similar words (hindle, 1990; <papid> P90-1034 </papid>strzalkowski, 1992).<papid> H92-1040 </papid></citsent>
<aftsection>
<nextsent>on the other hand, window-based techniques are not suit able for our data, because the documents are written by several authors who have variety of different writing styles (e.g. selecting different prepositions and articles).
</nextsent>
<nextsent>therefore we consider only syntactic features: dependency pairs, which consist of nouns, verbs, and their relationships.
</nextsent>
<nextsent>a dependency pair is written as (noun, verb(with its relationship)) as in the following examples.
</nextsent>
<nextsent>(customer, boot) (customer, shut off) (tp, shut off) the symbol  means the noun modifies the verb,and  means the verb modifies the noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B314">
<title id=" C04-1116.xml">term aggregation mining synonymous expressions using personal stylistic variations </title>
<section> mwave.  </section>
<citcontext>
<prevsection>
<prevsent>5 related work.
</prevsent>
<prevsent>there have been many approachs to automatic detection of similar words from text.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
our method is similar to (hindle, 1990), (<papid> P90-1034 </papid>lin, 1998), <papid> P98-2127 </papid>and(gasperin, 2001) in the use of dependency relationships as the word features.</citsent>
<aftsection>
<nextsent>another approach used the words?
</nextsent>
<nextsent>distribution to cluster the words (pereira, 1993), and inoue (inoue, 1991) <papid> P91-1026 </papid>also used the word distributional information in the japanese-englishword pairs to resolve the polysemous word prob lem.</nextsent>
<nextsent>wu (wu, 2003) shows one approach to collect synonymous collocation by using translation infor mation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B315">
<title id=" C04-1116.xml">term aggregation mining synonymous expressions using personal stylistic variations </title>
<section> mwave.  </section>
<citcontext>
<prevsection>
<prevsent>our method is similar to (hindle, 1990), (<papid> P90-1034 </papid>lin, 1998), <papid> P98-2127 </papid>and(gasperin, 2001) in the use of dependency relationships as the word features.</prevsent>
<prevsent>another approach used the words?</prevsent>
</prevsection>
<citsent citstr=" P91-1026 ">
distribution to cluster the words (pereira, 1993), and inoue (inoue, 1991) <papid> P91-1026 </papid>also used the word distributional information in the japanese-englishword pairs to resolve the polysemous word prob lem.</citsent>
<aftsection>
<nextsent>wu (wu, 2003) shows one approach to collect synonymous collocation by using translation information.
</nextsent>
<nextsent>this time we considered only synonymous expression terms, but the phrasal synonymous expression should be the target of aggregation in text analysis.not only synonymous expressions, but abbreviation is one of the most important issues in term aggregation.
</nextsent>
<nextsent>youngja (youngja, 2001) proposed amethod for finding abbreviations and their definitions, using the pattern-based rules which were generated automatically and/or manually.
</nextsent>
<nextsent>to re-evaluate the baseline synonym extraction system, we used the authors?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B316">
<title id=" C08-1096.xml">event frame extraction based on a gene regulation corpus </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>event frame extraction is different to event instance extraction (or template filling).
</prevsent>
<prevsent>our event frames are destined for incorporation in the boot strep bio lexicon to support identification of relevant event instances and discovery of event instance participants by nlp systems.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
there are several well-established, large-scale repositories of semantic frames for general language, e.g., verbnet (kipper-schuler, 2005), propbank (palmer et al, 2005) <papid> J05-1004 </papid>and framenet (rupenhoffer et al 2006).</citsent>
<aftsection>
<nextsent>these all aim to characterise verb behaviour in terms of the semantic arguments with which verbs occur but differ in how they represent semantic arguments and groupings of verbs.
</nextsent>
<nextsent>in verbnet, the semantic roles of arguments come from frame-independent roles, e.g. agent, patient, location and instrument.
</nextsent>
<nextsent>in contrast, propbank and framenet use mixture of role types: some are common amongst number of frames; others are specific to particular frames.
</nextsent>
<nextsent>whilst framenet and verbnet differ in their treatment of semantic roles, they both specify 761 semantic frames that correspond to groups of verbs with similar behaviour.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B317">
<title id=" C08-1096.xml">event frame extraction based on a gene regulation corpus </title>
<section> annotated corpus.  </section>
<citcontext>
<prevsection>
<prevsent>to aid semantic event frame extraction, we need corpus annotated with event-level information.
</prevsent>
<prevsent>several already exist for biology.
</prevsent>
</prevsection>
<citsent citstr=" W06-0602 ">
some target extraction of propbank-style frames (e.g. chou et al (2006), <papid> W06-0602 </papid>kulick et al (2004)).<papid> W04-3111 </papid></citsent>
<aftsection>
<nextsent>the corpus produced by kim et al (2008) uses frame independent roles.
</nextsent>
<nextsent>however, only few semantic argument types are annotated.
</nextsent>
<nextsent>the target of our event frame extraction is set of semantic frames which specify all potential arguments of gene regulation events.
</nextsent>
<nextsent>for this purpose, we had to produce our own annotated corpus, using larger set of event-independent semantic roles than kim et al (2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B318">
<title id=" C08-1096.xml">event frame extraction based on a gene regulation corpus </title>
<section> annotated corpus.  </section>
<citcontext>
<prevsection>
<prevsent>to aid semantic event frame extraction, we need corpus annotated with event-level information.
</prevsent>
<prevsent>several already exist for biology.
</prevsent>
</prevsection>
<citsent citstr=" W04-3111 ">
some target extraction of propbank-style frames (e.g. chou et al (2006), <papid> W06-0602 </papid>kulick et al (2004)).<papid> W04-3111 </papid></citsent>
<aftsection>
<nextsent>the corpus produced by kim et al (2008) uses frame independent roles.
</nextsent>
<nextsent>however, only few semantic argument types are annotated.
</nextsent>
<nextsent>the target of our event frame extraction is set of semantic frames which specify all potential arguments of gene regulation events.
</nextsent>
<nextsent>for this purpose, we had to produce our own annotated corpus, using larger set of event-independent semantic roles than kim et al (2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B319">
<title id=" C08-1096.xml">event frame extraction based on a gene regulation corpus </title>
<section> annotated corpus.  </section>
<citcontext>
<prevsection>
<prevsent>in conjunction with annotation guidelines, the chunks were used to help ensure consistency of annotated semantic arguments.
</prevsent>
<prevsent>for example, the guidelines state that semantic arguments should normally consist of complete (and preferably single) syntactic chunks.
</prevsent>
</prevsection>
<citsent citstr=" N03-4009 ">
the annotation was performed using customised version of word freak (morton and lacivita, 2003), <papid> N03-4009 </papid>java based linguistic annotation tool.</citsent>
<aftsection>
<nextsent>3.4 corpus statistics.
</nextsent>
<nextsent>the corpus is divided into 2 parts, i.e. 1) 597 abstracts, each annotated by single annotator, containing total of 3612 events, 2) 80 pairs of double-annotated documents, allowing checking of inter-annotator agreement and consistency, and containing 1158 distinct events.
</nextsent>
<nextsent>in the corpus, 277 distinct verbs were annotated as denoting gene regulation events, of which 73 were annotated 10 times or more.
</nextsent>
<nextsent>in addition, annotation has identified 135 relevant nominal ised verbs, of which 22 were annotated 10 times or more.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B320">
<title id=" C08-1074.xml">random restarts in minimum error rate training for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we compare several ways of performing random restarts with mert.
</prevsent>
<prevsent>we find that all of our random restart methods out perform mert without random restarts,and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
och (2003) <papid> P03-1021 </papid>introduced minimum error rate training (mert) for optimizing feature weights in statistical machine translation (smt) models, and demonstrated that it produced higher translation quality scores than maximizing the conditional likelihood of maximum entropy model using the same features.</citsent>
<aftsection>
<nextsent>ochs method performs series of one-dimensional optimizations of the feature weight vector, using an innovative line search that takes advantage of special properties of the mapping from sets of feature weights to the resulting translation quality measurement.
</nextsent>
<nextsent>ochs line search is guaranteed to find global optimum, whereas more general line search methods are guaranteed only to find local optimum.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B321">
<title id=" C08-1074.xml">random restarts in minimum error rate training for statistical machine translation </title>
<section> optimization with random restarts.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 preliminary evaluation.
</prevsent>
<prevsent>in our first experiments, we compared variant of ochs mert procedure with and without random restarts as described above.
</prevsent>
</prevsection>
<citsent citstr=" W06-3114 ">
for our training and test data we used the english-french subset of the europarl corpus provided for the shared task (koehn and monz, 2006) <papid> W06-3114 </papid>at the statistical machine translation workshop held in conjunction with the 2006 hlt-naacl conference.</citsent>
<aftsection>
<nextsent>we built stan-.
</nextsent>
<nextsent>dard baseline phrasal smt system, as described by koehn et al (2003), <papid> N03-1017 </papid>for translating from english to french (e-to-f), using the word alignments and french target language model provided by the workshop organizers.we trained model with the standard eight fea tures: e-to-f and f-to-e phrase translation log 1since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to higher local optimum.</nextsent>
<nextsent>586 probabilities, e-to-f and f-to-e phrase translation lexical scores, french language model log probabilities, phrase pair count, french word count, and distortion score.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B322">
<title id=" C08-1074.xml">random restarts in minimum error rate training for statistical machine translation </title>
<section> optimization with random restarts.  </section>
<citcontext>
<prevsection>
<prevsent>for our training and test data we used the english-french subset of the europarl corpus provided for the shared task (koehn and monz, 2006) <papid> W06-3114 </papid>at the statistical machine translation workshop held in conjunction with the 2006 hlt-naacl conference.</prevsent>
<prevsent>we built stan-.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
dard baseline phrasal smt system, as described by koehn et al (2003), <papid> N03-1017 </papid>for translating from english to french (e-to-f), using the word alignments and french target language model provided by the workshop organizers.we trained model with the standard eight fea tures: e-to-f and f-to-e phrase translation log 1since additional hypotheses have been added, initiating an optimization search from this point on the new set of hypotheses will often lead to higher local optimum.</citsent>
<aftsection>
<nextsent>586 probabilities, e-to-f and f-to-e phrase translation lexical scores, french language model log probabilities, phrase pair count, french word count, and distortion score.
</nextsent>
<nextsent>feature weight optimization was performed on the designated 2000-sentence-pair development set, and the resulting feature weights were evaluated on the designated 2000-sentence pair development test set, using the bleu-4 metric with one reference translation per sentence.
</nextsent>
<nextsent>at each decoding iteration we generated the 100-best translation hypotheses found by our phrasal smt decoder.
</nextsent>
<nextsent>to generate the initial 100-best list, we applied the policy of setting the weights for features we expected to be positively correlated with bleu to 1, the weights for features we expected to be negatively correlated with bleu to 1, and the remaining weights to 0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B323">
<title id=" C04-1088.xml">linguistic correlates of style authorship classification with deep linguistic analysis features </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>baayen et al (1996) also have pointed out the discriminatory role of infrequent syntactic patterns.
</prevsent>
<prevsent>what we need, then, is more sophisticated thresholding technique to restrict the feature vector size.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we have begun experimenting with log likelihood ratio (dunning 1993) <papid> J93-1003 </papid>as thresholding technique.</citsent>
<aftsection>
<nextsent>to assess at least anecdotally whether our results hold in different domain, we also tested on sentences from speeches of george bush jr. and bill clinton (2231 sentences from the former, 2433 sentences from the latter).
</nextsent>
<nextsent>using document samples with 5 sentences each, 10-fold cross validation and frequency cutoff of 5, we achieved 87.63% classification accuracy using all features, and 83.00% accuracy using only shallow features (function word frequencies and pos trigrams).
</nextsent>
<nextsent>additional experiments with similar methodology are under way for stylistic classification task based on unedited versus highly edited documents within the technical domain.
</nextsent>
<nextsent>we have shown that the use of deep linguistic analysis features in authorship attribution can yield significant reduction in error rate over the use of shallow linguistic features such as function word frequencies and part of speech trigrams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B324">
<title id=" C04-1137.xml">identification of conf usable drug names a new approach and evaluation methodology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(24th ed., 2003).
</prevsent>
<prevsent>the phonetic transcription of the two names, [znks] and [zntk], reveals their sound-alike similarity that is not apparent in their orthographic form.
</prevsent>
</prevsection>
<citsent citstr=" A00-2038 ">
for the detection of sound-alike confusion pairs, we apply the aline phonetic aligner (kondrak, 2000), <papid> A00-2038 </papid>which estimates the similarity between two phonetically-transcribed words.</citsent>
<aftsection>
<nextsent>we demonstrate that aline outperforms orthographic approaches on test set containing sound-alike confusion pairs.the next section describes several commonly used measures of word similarity.
</nextsent>
<nextsent>after this, we present two new methods for identifying look-alike and sound-alike drug names.
</nextsent>
<nextsent>we then compare the effectiveness of various measures using our recall based evaluation methodology on u.s. pharmacopeial gold standard and on another test set containing sound-alike confusion pairs.
</nextsent>
<nextsent>we conclude with discussion of our experimental results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B325">
<title id=" C04-1137.xml">identification of conf usable drug names a new approach and evaluation methodology </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>string-edit distance (wagner and fischer, 1974) (edit) (also known as levenshtein distance) counts up the number of steps it takes to transform one string into another, where the cost of substitution is the same as the cost of insertion or deletion.
</prevsent>
<prevsent>a normalized edit distance (ned) is calculated by dividing the total edit cost by the length of the longer string.
</prevsent>
</prevsection>
<citsent citstr=" J99-1003 ">
the longest common sub sequence ratio (melamed, 1999) (<papid> J99-1003 </papid>lcsr) is computed by dividing measure zantac/ zantac/ xanax/ xanax contac contac edit 3 2 4 ned 0.500 0.333 0.667 lcsr 0.500 0.667 0.333 bigram 0.222 0.600 0.000 trigram-2b 0.000 0.333 0.000 soundex 3 1 3 editex 5 2 7 aline 9.542 9.333 8.958 bi-sim 0.417 0.583 0.250 tri-sim 0.333 0.500 0.167 prefix 0.000 0.000 0.000 table 2: examples of values returned by various measures.</citsent>
<aftsection>
<nextsent>the length of the longest common sub sequence by the length of the longer string.
</nextsent>
<nextsent>lcsr is closely related to normalized edit distance.
</nextsent>
<nextsent>if the costof substitution is at least twice the cost of inser tion/deletion and the strings are of equal length, lcsr is equivalent to the normalized edit distance.
</nextsent>
<nextsent>in   -gram measures, the number of   -grams thatare shared by two strings is doubled and then divided by the total number of   -grams in each string:
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B327">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography.
</prevsent>
<prevsent>several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
(brown et al , 1993; <papid> J93-2003 </papid>vogelet al , 1996; <papid> C96-2141 </papid>garca-varea et al , 2002; ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999; tufis andbarbu, 2002; melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>word alignment usually includes links between so-calledmulti-word units (mwus) in cases where lexical items cannot be split into separated words with appropriate translations in another language.
</nextsent>
<nextsent>see for example the alignment between an english sentence and swedish sentence illustrated in figure 1.
</nextsent>
<nextsent>there are mwus in both languages aligned to corresponding translations in the other language.
</nextsent>
<nextsent>the swedish compoundmittplatsen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B328">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography.
</prevsent>
<prevsent>several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
(brown et al , 1993; <papid> J93-2003 </papid>vogelet al , 1996; <papid> C96-2141 </papid>garca-varea et al , 2002; ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999; tufis andbarbu, 2002; melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>word alignment usually includes links between so-calledmulti-word units (mwus) in cases where lexical items cannot be split into separated words with appropriate translations in another language.
</nextsent>
<nextsent>see for example the alignment between an english sentence and swedish sentence illustrated in figure 1.
</nextsent>
<nextsent>there are mwus in both languages aligned to corresponding translations in the other language.
</nextsent>
<nextsent>the swedish compoundmittplatsen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B329">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography.
</prevsent>
<prevsent>several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-1004 ">
(brown et al , 1993; <papid> J93-2003 </papid>vogelet al , 1996; <papid> C96-2141 </papid>garca-varea et al , 2002; ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999; tufis andbarbu, 2002; melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>word alignment usually includes links between so-calledmulti-word units (mwus) in cases where lexical items cannot be split into separated words with appropriate translations in another language.
</nextsent>
<nextsent>see for example the alignment between an english sentence and swedish sentence illustrated in figure 1.
</nextsent>
<nextsent>there are mwus in both languages aligned to corresponding translations in the other language.
</nextsent>
<nextsent>the swedish compoundmittplatsen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B330">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typical applications that make use of word alignment techniques are machine translation and multi-lingual lexicography.
</prevsent>
<prevsent>several approaches have been proposed for the automatic alignment of words and phrases using statistical techniques and alignment heuristics, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
(brown et al , 1993; <papid> J93-2003 </papid>vogelet al , 1996; <papid> C96-2141 </papid>garca-varea et al , 2002; ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999; tufis andbarbu, 2002; melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>word alignment usually includes links between so-calledmulti-word units (mwus) in cases where lexical items cannot be split into separated words with appropriate translations in another language.
</nextsent>
<nextsent>see for example the alignment between an english sentence and swedish sentence illustrated in figure 1.
</nextsent>
<nextsent>there are mwus in both languages aligned to corresponding translations in the other language.
</nextsent>
<nextsent>the swedish compoundmittplatsen?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B331">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which corresponds tothe swedish expression det gor mig inte sa my cket?
</prevsent>
<prevsent>there is no proper way of connecting single words with each other in order to express this relation.
</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
in some approaches such relations are constructed in form of an exhaustive set of links between all word pairs included in both expressions (melamed, 1998; mihalcea and pedersen, 2003).<papid> W03-0301 </papid></citsent>
<aftsection>
<nextsent>in other approaches complex expressions are identified in pre-processing step in order to handle them as complex units in the same manner as single words in alignment (smadja et al , 1996; <papid> J96-1001 </papid>ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999).</nextsent>
<nextsent>the one-to-one word linking approach seems to be very limited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B332">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is no proper way of connecting single words with each other in order to express this relation.
</prevsent>
<prevsent>in some approaches such relations are constructed in form of an exhaustive set of links between all word pairs included in both expressions (melamed, 1998; mihalcea and pedersen, 2003).<papid> W03-0301 </papid></prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
in other approaches complex expressions are identified in pre-processing step in order to handle them as complex units in the same manner as single words in alignment (smadja et al , 1996; <papid> J96-1001 </papid>ahrenberg et al , 1998; <papid> P98-1004 </papid>tiedemann, 1999).</citsent>
<aftsection>
<nextsent>the one-to-one word linking approach seems to be very limited.
</nextsent>
<nextsent>however, single word link scan be combined in order to describe links between multi-word units as illustrated in figure 1.
</nextsent>
<nextsent>in this paper we investigate different align-.
</nextsent>
<nextsent>ment strategies using this approach1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B334">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> word alignment with clues.  </section>
<citcontext>
<prevsection>
<prevsent>for this we apply clue alignment introduced in the next section.
</prevsent>
<prevsent>the clue alignment approach has been presented in (tiedemann, 2003).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
alignment clues represent probabilistic indications of associa 1a similar study on statistical alignment models is included in (och and ney, 2003).<papid> J03-1002 </papid>tions between lexical items collected from different sources.</citsent>
<aftsection>
<nextsent>declarative clues can be taken from linguistic resources such as bilingual dictionaries.
</nextsent>
<nextsent>they may also include pre-definedrelations between lexical items based on certain features such as parts of speech.
</nextsent>
<nextsent>estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. thedice coefficient (smadja et al , 1996)), <papid> J96-1001 </papid>statistical alignment models (e.g. ibm models from statistical machine translation (brown et al , 1993)), <papid> J93-2003 </papid>or string similarity measures (e.g. the longest common sub-sequence ratio (melamed,1995)).<papid> W95-0115 </papid></nextsent>
<nextsent>they can also be learned from previously aligned training data using linguistic and contextual features associated with aligned items.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B338">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> word alignment with clues.  </section>
<citcontext>
<prevsection>
<prevsent>declarative clues can be taken from linguistic resources such as bilingual dictionaries.
</prevsent>
<prevsent>they may also include pre-definedrelations between lexical items based on certain features such as parts of speech.
</prevsent>
</prevsection>
<citsent citstr=" W95-0115 ">
estimated clues are derived from the parallel data using, for example, measures of co-occurrence (e.g. thedice coefficient (smadja et al , 1996)), <papid> J96-1001 </papid>statistical alignment models (e.g. ibm models from statistical machine translation (brown et al , 1993)), <papid> J93-2003 </papid>or string similarity measures (e.g. the longest common sub-sequence ratio (melamed,1995)).<papid> W95-0115 </papid></citsent>
<aftsection>
<nextsent>they can also be learned from previously aligned training data using linguistic and contextual features associated with aligned items.
</nextsent>
<nextsent>relations between certain word classes with respect to the translational association of words belonging to these classes is one example of such clues that can be learned from aligned training data.
</nextsent>
<nextsent>in our experiments, for example, we will use clues that indicate relations between lexical items based on their part-of-speech tags and their positions in the sentence relative toeach other.
</nextsent>
<nextsent>they are learned from automatically word-aligned training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B339">
<title id=" C04-1031.xml">word to word alignment strategies </title>
<section> alignment strategies.  </section>
<citcontext>
<prevsection>
<prevsent>= l??
</prevsent>
<prevsent>the union and the intersection of links do not produce satisfactory results as seen in the example.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
another alignment strategy is refined combination of link sets (lr = {ld ? li} ? {lr1 , ..., r }) as suggested by (och and ney, 2000<papid> P00-1056 </papid>b).</citsent>
<aftsection>
<nextsent>in this approach, the intersection of links is iteratively extended by additional linkslrr which pass one of the following two con straints: ? new link is accepted if both items in the link are not yet al gned.
</nextsent>
<nextsent>mapped on two-dimensional bitext space,the new link is either vertically or horizontally adjacent to an existing link and thenew link does not cause any link to be adjacent to other links in both dimensions (hor izontally and vertically).
</nextsent>
<nextsent>applying this approach to the example, we get: lr = ? ?????
</nextsent>
<nextsent>no ingen is visar very sars kilt very mycket one ingen patient talamod ? ?????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B360">
<title id=" C02-1163.xml">machine translation by interaction between paraphraser and transfer </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is also significant that this models paraphraser can be employed not only for mt but also formost natural language processing (nlp) applications.
</prevsent>
<prevsent>this is possible because both the input and output of paraphraser is the same natural language.
</prevsent>
</prevsection>
<citsent citstr=" C02-1056 ">
we have been building the sand glass mtsystem for the japanese-chinese, chinese japanese language pairs (yamamoto et al, 2001; zhang and yamamoto, 2002).<papid> C02-1056 </papid></citsent>
<aftsection>
<nextsent>we have already constructed prototype for japanesechinese.
</nextsent>
<nextsent>in this paper, we report the core concepts of this prototype and discuss issues of both our principle and our implementation.
</nextsent>
<nextsent>figure 1 shows our paradigm for translationmodel.
</nextsent>
<nextsent>in the conventional mt model, the process load and the information used to deal with conventional paradigm sand glass paradigm translation process load maximum load in bilingual transfer process maximum load in monolingual process by paraphrasing figure 1: comparison of the two mt paradigm sit are maximized in the transfer module; how ever, we propose that they be minimized in the transfer in consideration of language portability and task portability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B361">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the literature, feature-based methods have dominated the research in semantic relation extraction.
</prevsent>
<prevsent>featured-based methods achieve promising performance and competitive efficiency by transforming relation example into set of syntactic and semantic features, such as lexical knowledge, entity-related information, syntactic parse trees and deep semantic information.
</prevsent>
</prevsection>
<citsent citstr=" P05-1053 ">
how ever, detailed research (zhou et al, 2005) <papid> P05-1053 </papid>shows that its difficult to extract new effective features to further improve the extraction accuracy.</citsent>
<aftsection>
<nextsent>therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.
</nextsent>
<nextsent>from prior work (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>bunescu and mooney, 2005) <papid> H05-1091 </papid>to current research (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al., 2007), <papid> D07-1076 </papid>kernel methods have been showing more and more potential in relation extraction.</nextsent>
<nextsent>the key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B363">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, detailed research (zhou et al, 2005) <papid> P05-1053 </papid>shows that its difficult to extract new effective features to further improve the extraction accuracy.</prevsent>
<prevsent>therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
from prior work (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>bunescu and mooney, 2005) <papid> H05-1091 </papid>to current research (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al., 2007), <papid> D07-1076 </papid>kernel methods have been showing more and more potential in relation extraction.</citsent>
<aftsection>
<nextsent>the key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.
</nextsent>
<nextsent>while kernel methods using the dependency tree (culotta and sorensen, 2004) <papid> P04-1054 </papid>and the shortest dependency path (bunescu and mooney, 2005) <papid> H05-1091 </papid>suffer from low recall performance, convolution tree kernels (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al, 2007) <papid> D07-1076 </papid>over syntactic parse trees achieve comparable or even better performance than feature-based methods.</nextsent>
<nextsent>however, there still exist two problems regarding currently widely used tree spans.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B364">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, detailed research (zhou et al, 2005) <papid> P05-1053 </papid>shows that its difficult to extract new effective features to further improve the extraction accuracy.</prevsent>
<prevsent>therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
from prior work (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>bunescu and mooney, 2005) <papid> H05-1091 </papid>to current research (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al., 2007), <papid> D07-1076 </papid>kernel methods have been showing more and more potential in relation extraction.</citsent>
<aftsection>
<nextsent>the key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.
</nextsent>
<nextsent>while kernel methods using the dependency tree (culotta and sorensen, 2004) <papid> P04-1054 </papid>and the shortest dependency path (bunescu and mooney, 2005) <papid> H05-1091 </papid>suffer from low recall performance, convolution tree kernels (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al, 2007) <papid> D07-1076 </papid>over syntactic parse trees achieve comparable or even better performance than feature-based methods.</nextsent>
<nextsent>however, there still exist two problems regarding currently widely used tree spans.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B367">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, detailed research (zhou et al, 2005) <papid> P05-1053 </papid>shows that its difficult to extract new effective features to further improve the extraction accuracy.</prevsent>
<prevsent>therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.</prevsent>
</prevsection>
<citsent citstr=" P06-1104 ">
from prior work (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>bunescu and mooney, 2005) <papid> H05-1091 </papid>to current research (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al., 2007), <papid> D07-1076 </papid>kernel methods have been showing more and more potential in relation extraction.</citsent>
<aftsection>
<nextsent>the key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.
</nextsent>
<nextsent>while kernel methods using the dependency tree (culotta and sorensen, 2004) <papid> P04-1054 </papid>and the shortest dependency path (bunescu and mooney, 2005) <papid> H05-1091 </papid>suffer from low recall performance, convolution tree kernels (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al, 2007) <papid> D07-1076 </papid>over syntactic parse trees achieve comparable or even better performance than feature-based methods.</nextsent>
<nextsent>however, there still exist two problems regarding currently widely used tree spans.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B371">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, detailed research (zhou et al, 2005) <papid> P05-1053 </papid>shows that its difficult to extract new effective features to further improve the extraction accuracy.</prevsent>
<prevsent>therefore, researchers turn to kernel-based methods, which avoids the burden of feature engineering through computing the similarity of two discrete objects (e.g. parse trees) directly.</prevsent>
</prevsection>
<citsent citstr=" D07-1076 ">
from prior work (zelenko et al, 2003; culotta and sorensen, 2004; <papid> P04-1054 </papid>bunescu and mooney, 2005) <papid> H05-1091 </papid>to current research (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al., 2007), <papid> D07-1076 </papid>kernel methods have been showing more and more potential in relation extraction.</citsent>
<aftsection>
<nextsent>the key problem for kernel methods on relation extraction is how to represent and capture the structured syntactic information inherent in relation instances.
</nextsent>
<nextsent>while kernel methods using the dependency tree (culotta and sorensen, 2004) <papid> P04-1054 </papid>and the shortest dependency path (bunescu and mooney, 2005) <papid> H05-1091 </papid>suffer from low recall performance, convolution tree kernels (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al, 2007) <papid> D07-1076 </papid>over syntactic parse trees achieve comparable or even better performance than feature-based methods.</nextsent>
<nextsent>however, there still exist two problems regarding currently widely used tree spans.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B408">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, the cs-spt only recovers part of contextual information and may contain noisy information as much as spt.
</prevsent>
<prevsent>in order to fully utilize the advantages of fea ture-based methods and kernel-based methods, researchers turn to composite kernel methods.
</prevsent>
</prevsection>
<citsent citstr=" P05-1052 ">
zhao and grishman (2005) <papid> P05-1052 </papid>define several fea ture-based composite kernels to capture diverse linguistic knowledge and achieve the f-measure of 70.4 on the 7 relation types in the ace rdc 2004 corpus.</citsent>
<aftsection>
<nextsent>zhang et al (2006) <papid> P06-1104 </papid>design composite kernel consisting of an entity linear kernel and standard ctk, obtaining the f-measure of 72.1 on the 7 relation types in the ace rdc 2004 corpus.</nextsent>
<nextsent>zhou et al (2007) <papid> D07-1076 </papid>describe composite kernel to integrate context-sensitive ctk and state-of-the-art linear kernel.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B438">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> dynamic syntactic parse tree.  </section>
<citcontext>
<prevsection>
<prevsent>then the path nodes and their head children are kept while any other nodes are removed from the tree.
</prevsent>
<prevsent>eventually we arrive at tree called dynamic syntactic parse tree (dspt), which is dynamically determined by constituent dependencies and only contains necessary information as expected.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
there exist considerable number of constituent dependencies in cfg as described by collins (2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>however, since our task is to extract the relationship between two named entities, our focus is on how to condense noun-phrases (nps) and other useful constituents for relation extraction.
</nextsent>
<nextsent>therefore constituent dependencies can be classified according to constituent types of the cfg rules: 699 (1) modification within base-nps: base-nps mean that they do not directly dominate an npthemselves, unless the dominated np is possessive np.
</nextsent>
<nextsent>the noun phrase right above the entity headword, whose mention type is nominal or name, can be categorized into this type.
</nextsent>
<nextsent>in this case, the entity headword is also the headword of the noun phrase, thus all the constituents before the headword are dependent on the headword, and may be removed from the parse tree, while the headword and the constituents right after the headword remain unchanged.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B452">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> entity-related semantic tree.  </section>
<citcontext>
<prevsection>
<prevsent>however, detailed evaluation (qian et al., 2007) indicates that the upst achieves the best performance when the feature nodes are attached under the top node.
</prevsent>
<prevsent>hence, we also attach three kinds of entity-related semantic trees (i.e. bof, fpt and ept) under the top node of the dspt right after its original children.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
thereafter, we employ the standard ctk (collins and duffy, 2001) to compute the similarity between two upsts, since this ctk and its variations are successfully applied in syntactic parsing, semantic role labeling (moschitti, 2004) <papid> P04-1043 </papid>and relation extraction (zhang et al, 2006; <papid> P06-1104 </papid>zhou et al, 2007) <papid> D07-1076 </papid>as well.</citsent>
<aftsection>
<nextsent>this section will evaluate the effectiveness of the dspt and the contribution of entity-related semantic information through experiments.
</nextsent>
<nextsent>5.1 experimental setting.
</nextsent>
<nextsent>for evaluation, we use the ace rdc 2004 corpus as the benchmark data.
</nextsent>
<nextsent>this dataset contains 451 documents and 5702 relation instances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B463">
<title id=" C08-1088.xml">exploiting constituent dependencies for tree kernel based semantic relation extraction </title>
<section> experimentation.  </section>
<citcontext>
<prevsection>
<prevsent>it defines 7 entity types, 7 major relation types and 23 subtypes.
</prevsent>
<prevsent>for comparison with previous work, evaluation is done on 347 (nwire/bnews) documents and 4307 relation instances using 5-fold cross-validation.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
here, the corpus is parsed using charniaks parser (charniak, 2001) <papid> P01-1017 </papid>and relation instances are generated by ite rating over all pairs of entity mentions occurring in the same sentence with given true?</citsent>
<aftsection>
<nextsent>mentions and coreferential information.
</nextsent>
<nextsent>in our experiment ations, svm light (joachims, 1998) with the tree kernel function (moschitti, 2004) <papid> P04-1043 </papid>2 is selected as our classifier.</nextsent>
<nextsent>for efficiency, we apply the one vs. others strategy, which builds classifiers so as to separate one class from all others.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B493">
<title id=" C08-1017.xml">latent morpho semantic analysis multilingual information retrieval with character ngrams and mutual information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a number of developments in recent years have brought that goal more within reach.
</prevsent>
<prevsent>one of the factors that severely hampered early attempts at machine translation, for example, was the lack of available computing power.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however, moores law, the driving force of change in computing since then, has opened the way for recent progress in the field, such as statistical machine translation (smt) (koehn et al 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>even more closely related to the topic of the present paper, implementations of the singular value decomposition (svd) (which is at the heart of lsa), and related algorithms such as parafac2 (harshman 1972), have become both more widely available and more powerful.
</nextsent>
<nextsent>svd, for example, is available in both commercial off-the-shelf packages and at least one open source implementation designed to run on parallel cluster (heroux et al. 2005).
</nextsent>
<nextsent>despite these advances, there are (as yet) not fully surmounted obstacles to working with certain language pairs, particularly when the languages are not closely related.
</nextsent>
<nextsent>this is demonstrated in chew and abdelali (2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B494">
<title id=" C08-1017.xml">latent morpho semantic analysis multilingual information retrieval with character ngrams and mutual information </title>
<section> possible solutions.  </section>
<citcontext>
<prevsection>
<prevsent>stemming has been shown to improve ir, in particular for morphologically complex languages (recent examples, including with arabic, are lavie et al 2004 and abdou et al. 2005).
</prevsent>
<prevsent>we are not aware, however, of any previous results that show unequivocally that stemming is beneficial specifically in clir.
</prevsent>
</prevsection>
<citsent citstr=" W02-0506 ">
chew and abdelali (2008) examine the use of light stemmer for arabic (darwish 2002), <papid> W02-0506 </papid>and while this does result in small overall increase in overall precision, there is paradoxically no increase for arabic.</citsent>
<aftsection>
<nextsent>the problem may be that the approach for arabic needs to be matched by similar approach for other languages in the parallel corpus.
</nextsent>
<nextsent>however, since stem mers are usually tailored to particular languages ? and may even be unavailable for some languages ? use of existing stem mers may not always be an option.
</nextsent>
<nextsent>another more obviously language independent approach is to replace terms with character n-grams3.
</nextsent>
<nextsent>this is feasible for more or less any language, regardless of script.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B495">
<title id=" C08-1017.xml">latent morpho semantic analysis multilingual information retrieval with character ngrams and mutual information </title>
<section> subject the n-gram-by-document array to svd.  </section>
<citcontext>
<prevsection>
<prevsent>would presumably receive high weighting based on its frequency elsewhere in the corpus.
</prevsent>
<prevsent>an alternative is to select the tokenization which maximizes mutual information (mi).
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
brown et al (1992) <papid> J92-4003 </papid>describe one application of mi to identify word collocations; kashioka et al (1998) <papid> P98-1108 </papid>describe another, based on mi of character n-grams, for morphological analysis of japa nese.</citsent>
<aftsection>
<nextsent>the pointwise mi of pair s1 and s2 as adjacent symbols is mi = log p(s1 s2) ? log p(s1) ? log p(s2) (3) if s1 follows s2 less often than expected on the basis of their independent frequencies, then mi is negative; otherwise, it is positive.
</nextsent>
<nextsent>in our application, we want to consider all candidate tokenizations, sum mi for each candidate, and rule out all but one candidate.
</nextsent>
<nextsent>a to 132 ken ization is candidate if it exhaustively parses the entire string and has no overlapping tokens.
</nextsent>
<nextsent>thus, for comingle?, co+mingle, coming+le, co mingle, c+o+m+i+n+g+l+e, etc., are some of the candidates, but comi+ngl and com+mingle are not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B496">
<title id=" C08-1017.xml">latent morpho semantic analysis multilingual information retrieval with character ngrams and mutual information </title>
<section> subject the n-gram-by-document array to svd.  </section>
<citcontext>
<prevsection>
<prevsent>would presumably receive high weighting based on its frequency elsewhere in the corpus.
</prevsent>
<prevsent>an alternative is to select the tokenization which maximizes mutual information (mi).
</prevsent>
</prevsection>
<citsent citstr=" P98-1108 ">
brown et al (1992) <papid> J92-4003 </papid>describe one application of mi to identify word collocations; kashioka et al (1998) <papid> P98-1108 </papid>describe another, based on mi of character n-grams, for morphological analysis of japa nese.</citsent>
<aftsection>
<nextsent>the pointwise mi of pair s1 and s2 as adjacent symbols is mi = log p(s1 s2) ? log p(s1) ? log p(s2) (3) if s1 follows s2 less often than expected on the basis of their independent frequencies, then mi is negative; otherwise, it is positive.
</nextsent>
<nextsent>in our application, we want to consider all candidate tokenizations, sum mi for each candidate, and rule out all but one candidate.
</nextsent>
<nextsent>a to 132 ken ization is candidate if it exhaustively parses the entire string and has no overlapping tokens.
</nextsent>
<nextsent>thus, for comingle?, co+mingle, coming+le, co mingle, c+o+m+i+n+g+l+e, etc., are some of the candidates, but comi+ngl and com+mingle are not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B497">
<title id=" C08-1017.xml">latent morpho semantic analysis multilingual information retrieval with character ngrams and mutual information </title>
<section> subject the n-gram-by-document array to svd.  </section>
<citcontext>
<prevsection>
<prevsent>word form tokenization ]^p_`abnodp`e_c ]^p_`abno dp`e_c ]^p_`abnodl`l_c ]^p_`abno dl`l_c ]^p_`abnodl`_c ]^p_`abno dl`_c ]^p_`abnodlf_c ]^p_`abno dlf_c table 5.
</prevsent>
<prevsent>examples of mi-based tokenization information?
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
measure used in goldsmith (2001: <papid> J01-2001 </papid>172).</citsent>
<aftsection>
<nextsent>we do not directly test the accuracy of these tokenizations.
</nextsent>
<nextsent>rather, measures of clir precision (described in section 4) indirectly validate our morphological tokenizations.
</nextsent>
<nextsent>4 testing framework.
</nextsent>
<nextsent>to assess our results on basis comparable with previous work, we used the same training and test data as used in chew et al (2007) and chew and abdelali (2008).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B498">
<title id=" C04-1028.xml">generalizing dimensionality in combinatory categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, these signs are monolithic structures which permit information to be freely shared across all dimensions: any given dimension can place restrictions on another.
</prevsent>
<prevsent>forex ample, variables resolved during the construction of the logical form can block syntactic analysis.
</prevsent>
</prevsection>
<citsent citstr=" J93-4001 ">
this provides clean, unified formal system for dealing with the different levels, but it also can adversely affect the complexity of parsing grammars written in these frameworks (maxwell and kaplan, 1993).<papid> J93-4001 </papid>we thus find two competing perspectives on communication between levels in sign.</citsent>
<aftsection>
<nextsent>in this paper, we propose generalization of linguistic signs forcombinatory categorial grammar (ccg) (steed man, 2000b).
</nextsent>
<nextsent>this generalization enables different levels of linguistic information to be represented but limits their interaction in resource-bounded manner, following white (2004).
</nextsent>
<nextsent>this provides clean separation of the levels and allows them to be designed and utilized in more modular fashion.
</nextsent>
<nextsent>most importantly, it allows us to retain the parsing complexity of ccg while gaining the representational advantages of the hpsg and lfg paradigms.to illustrate the approach, we use it to model various aspects of the realization of information structure, an inherent aspect of the (linguistic) meaning of an utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B499">
<title id=" C04-1028.xml">generalizing dimensionality in combinatory categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>speakers use information structure to present some parts of that meaning as depending on the preceding discourse context and others as affecting the context by adding new content.languages may realize information structure using different, often interacting means, such as word order, prosody, (marked) syntactic constructions, or morphological marking (vallduv??
</prevsent>
<prevsent>and engdahl,1996; kruijff, 2002).
</prevsent>
</prevsection>
<citsent citstr=" E95-1034 ">
the literature presents various proposals for how information structure can be captured in categorial grammar (steedman, 2000a; hoffman, 1995; <papid> E95-1034 </papid>kruijff, 2001).</citsent>
<aftsection>
<nextsent>here, we model the essential aspects of these accounts in more per spicuous manner by using our generalized signs.the main outcomes of the proposal are three fold: (1) ccg gains more flexible and general kind of sign; (2) these signs contain multiple levels that interact in modular fashion and are built viaccg derivations without increasing parsing complexity; and (3) we use these signs to simplify previous ccgs accounts of the effects of word order and prosody on information structure.
</nextsent>
<nextsent>in this section, we give an overview of syntactic combination and semantic construction in ccg.
</nextsent>
<nextsent>we use ccgs multi-modal extension (baldridge and kruijff, 2003), which enriches the inventory of slash types.
</nextsent>
<nextsent>this formalization renders constraints on rules unnecessary and supports universal set of rules for all grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B500">
<title id=" C04-1028.xml">generalizing dimensionality in combinatory categorial grammar </title>
<section> combinatory categorial grammar.  </section>
<citcontext>
<prevsection>
<prevsent>by using the @ operator, hierarchical terms such as (2) can be flattened to an equivalent conjunction of fixed-size elementary predications (eps): (3) @eprove ? @etensepast ? @eactm ? @epatc ? @mmarcel ? @ccomp.
</prevsent>
<prevsent>2.3 semantic construction.
</prevsent>
</prevsection>
<citsent citstr=" P02-1041 ">
baldridge and kruijff (2002) <papid> P02-1041 </papid>show how hlds representations can be built via ccg derivations.white (2004) improves hlds construction by operating on flattened representations such as (3) and using simple semantic index feature in the syntax.</citsent>
<aftsection>
<nextsent>we adopt this latter approach, described below.
</nextsent>
<nextsent>eps are paired with syntactic categories in the lexicon as shown in (4)?(6) below.
</nextsent>
<nextsent>each atomic category has an index feature, shown as subscript,which makes nominal available for capturing syntactically induced dependencies.
</nextsent>
<nextsent>(4) prove ` (se\npx)/npy : @eprove ? @etensepast ? @eactx ? @epaty (5) marcel ` npm : @mmarcel (6) completeness ` npc : @ccompleteness applications of the combinatory rules co-indexthe appropriate nominals via unification on the categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B503">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> a local aligment kernel </section>
<citcontext>
<prevsection>
<prevsent>in contrast, edit string distances treat the entire sequences and, by comparing them, calculate the minimal number of the transformation operations converting sequence into sequencey.
</prevsent>
<prevsent>examples of string edit distances are leven shtein, needleman-wunsch and smith-waterman metrics.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
levenshtein distance has been used in natural language processing field as component in the variety of tasks, including semantic role labeling (tjong kim sang et al, 2005), construction of the paraphrase corpora (dolan et al, 2004), <papid> C04-1051 </papid>evaluation of machine translation output (leusch et al, 2003), and others.</citsent>
<aftsection>
<nextsent>smith-waterman distance is mostly used in the biological domain, there are,however, some applications of modified smith waterman distance to the text data as well (monge and elkan, 1996), (cohen et al, 2003).
</nextsent>
<nextsent>hmm based measures present probabilistic extensions of edit distances.
</nextsent>
<nextsent>according to the definition of la kernel, two strings (sequences) are considered similar if they have many local alignments with high scores (saigo et al, 2006).
</nextsent>
<nextsent>given two sequences = 1 2 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B504">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> a local aligment kernel </section>
<citcontext>
<prevsection>
<prevsent>such evidence can be provided by the distributional similarity metrics.
</prevsent>
<prevsent>there are number of measures proposed over the years, including such metrics as cosine, dice coefficient, and jaccard distance.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
distributional similarity measures have been extensively studied in (lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).<papid> C04-1146 </papid>we have chosen the following metrics: dice, co sine and l2 (euclidean) whose definitions are given in table 1.</citsent>
<aftsection>
<nextsent>here, i and j denote two words and stands for context.
</nextsent>
<nextsent>similarly to (lee, 1999), <papid> P99-1004 </papid>we use un smoothed relative frequencies to derive probability estimates . in the definition of the dice coefficient, (x ) = {c : (c|x )   0}.we are mainly interested in the symmetric measures (d(x , j ) = d(y , i)) because of symmetric positive semi-definite matrix required by kernel methods.</nextsent>
<nextsent>consequently, such measures as theskew divergence were excluded from the consideration (lee, 1999).<papid> P99-1004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B505">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> a local aligment kernel </section>
<citcontext>
<prevsection>
<prevsent>such evidence can be provided by the distributional similarity metrics.
</prevsent>
<prevsent>there are number of measures proposed over the years, including such metrics as cosine, dice coefficient, and jaccard distance.
</prevsent>
</prevsection>
<citsent citstr=" C04-1146 ">
distributional similarity measures have been extensively studied in (lee, 1999; <papid> P99-1004 </papid>weeds et al, 2004).<papid> C04-1146 </papid>we have chosen the following metrics: dice, co sine and l2 (euclidean) whose definitions are given in table 1.</citsent>
<aftsection>
<nextsent>here, i and j denote two words and stands for context.
</nextsent>
<nextsent>similarly to (lee, 1999), <papid> P99-1004 </papid>we use un smoothed relative frequencies to derive probability estimates . in the definition of the dice coefficient, (x ) = {c : (c|x )   0}.we are mainly interested in the symmetric measures (d(x , j ) = d(y , i)) because of symmetric positive semi-definite matrix required by kernel methods.</nextsent>
<nextsent>consequently, such measures as theskew divergence were excluded from the consideration (lee, 1999).<papid> P99-1004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B508">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>y to contain words (x iw ) and syntactic functions accompanied by direction (x /w ).
</prevsent>
<prevsent>then, ?
</prevsent>
</prevsection>
<citsent citstr=" H05-1091 ">
(x , j ) = 8           : d(x , j ) i , j 1 i , j /w &amp; i = j 0 i , j /w &amp; i 6= j 0 i &amp; j /w 0 i /w &amp; j (6)baseline to test how well local alignment kernels perform compared to the kernels proposed in the past, we implemented method described in (bunescu and mooney, 2005) <papid> H05-1091 </papid>as baseline.</citsent>
<aftsection>
<nextsent>here,similarly to our approach, the shortest path between relation arguments is extracted and kernel between two sequences (paths) and is computed as follows: k(x, y) = { 0 6= ? i=1 f(x , i ) = (7) in eq.
</nextsent>
<nextsent>7, f(x , i) is the number of common features shared by i and i . bunescu and.
</nextsent>
<nextsent>mooney (2005) use several features such as word(protesters), part of speech tag (nns), generalized part of speech tag (noun), and entity type(e.g., person ) if applicable.
</nextsent>
<nextsent>in addition, direction feature (?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B511">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>method pr,% r,% 1 ,% lak-dice 75.56 79.72 77.56 lak-cosine 76.4 80.66 78.13 lak-l2 77.56 79.31 78.42 baseline 32.04 75.63 45.00 table 2: results on the bc-ppi data setat first glance, the la kernel based on the distributional similarity measures that we selected provides similar performance.
</prevsent>
<prevsent>we can notice that the l2 metric seems to be the best performing measure.
</prevsent>
</prevsection>
<citsent citstr=" E06-1051 ">
on the bc-ppi data, the method based on the l2 measure outperforms the methods based on diceand on cosine but the differences are not signifi cant.on the lll dataset, the la method using distributional similarity measures significantly outperforms both baselines and also yields better results than an approach based on shallow linguistic information (giuliano et al, 2006).<papid> E06-1051 </papid></citsent>
<aftsection>
<nextsent>giuliano et al (2006) <papid> E06-1051 </papid>use no syntactic information.</nextsent>
<nextsent>recent work reported in (fundel, 2007) also uses dependency information but in contrast to our method, it servesas representation on which extraction rules are de fined.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B529">
<title id=" C08-1053.xml">a local alignment kernel in the context of nlp </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most work done for relation extraction considers binary relations in sentential context (mcdonald, 2005).
</prevsent>
<prevsent>current techniques for relation extraction include hand-written patterns (sekimizu et al, 1998), kernel methods (zelenko et al, 2003), pattern induction methods (snow et al., 2005), and finite-state automata (pustejovsky et al, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
kernel methods have become very popular in natural language processing in general and 422 for learning relations, in particular (culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>there are many kernels defined for the text data.
</nextsent>
<nextsent>for instance, string kernels are special kernels which consider inner products ofall sub sequences from the given sequences of elements (lodhi et al, 2002).
</nextsent>
<nextsent>they can be further extended to syllable kernels which proved to perform well for text categorization (saunders et al, 2002).
</nextsent>
<nextsent>for relation learning, zelenko et al(2003) use shallow parsing in conjunction with contiguous and non-contiguous kernels to learn relations.bunescu et al(2006) define several kernels to accomplish the same task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B530">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will discuss some of our experiments based on automatically extracted multilingual lexicons.
</prevsent>
<prevsent>most modern approaches to automatic extraction of translation equivalents relyon statistical techniques and roughly fall into two categories.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
the hypotheses-testing methods such as gale and church (1991), <papid> H91-1026 </papid>smadja et al (1996), <papid> J96-1001 </papid>tied mann (1998), ahrenberg (2000), melamed (2001) etc. use generative device that produces list of translation equivalence candidates (tecs), extracted from corresponding segments of the parallel texts (translation units-tu), each of them being subject to an independence statistical test.</citsent>
<aftsection>
<nextsent>the tecs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (teps).
</nextsent>
<nextsent>the teps are extracted independently one of another and therefore the process might be characterized as local maximization (greedy) one.
</nextsent>
<nextsent>the estimating approach such as brown et al (1993), <papid> J93-2003 </papid>kay and rscheisen (1993), <papid> J93-1006 </papid>kupiec (1993), <papid> P93-1003 </papid>hiemstra (1997) etc. is based on building from data statistical bitext model, the parameters of which are to be estimated according to given set of assumptions.</nextsent>
<nextsent>the bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B532">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will discuss some of our experiments based on automatically extracted multilingual lexicons.
</prevsent>
<prevsent>most modern approaches to automatic extraction of translation equivalents relyon statistical techniques and roughly fall into two categories.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
the hypotheses-testing methods such as gale and church (1991), <papid> H91-1026 </papid>smadja et al (1996), <papid> J96-1001 </papid>tied mann (1998), ahrenberg (2000), melamed (2001) etc. use generative device that produces list of translation equivalence candidates (tecs), extracted from corresponding segments of the parallel texts (translation units-tu), each of them being subject to an independence statistical test.</citsent>
<aftsection>
<nextsent>the tecs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (teps).
</nextsent>
<nextsent>the teps are extracted independently one of another and therefore the process might be characterized as local maximization (greedy) one.
</nextsent>
<nextsent>the estimating approach such as brown et al (1993), <papid> J93-2003 </papid>kay and rscheisen (1993), <papid> J93-1006 </papid>kupiec (1993), <papid> P93-1003 </papid>hiemstra (1997) etc. is based on building from data statistical bitext model, the parameters of which are to be estimated according to given set of assumptions.</nextsent>
<nextsent>the bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B533">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the tecs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (teps).
</prevsent>
<prevsent>the teps are extracted independently one of another and therefore the process might be characterized as local maximization (greedy) one.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the estimating approach such as brown et al (1993), <papid> J93-2003 </papid>kay and rscheisen (1993), <papid> J93-1006 </papid>kupiec (1993), <papid> P93-1003 </papid>hiemstra (1997) etc. is based on building from data statistical bitext model, the parameters of which are to be estimated according to given set of assumptions.</citsent>
<aftsection>
<nextsent>the bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments).
</nextsent>
<nextsent>there are pros and cons for each type of approach, some of them discussed in hiemstra (1997).
</nextsent>
<nextsent>our translation equivalents extraction process may be characterized as hypotheses testing?
</nextsent>
<nextsent>approach and does not need pre-existing bilingual lexicon for the considered languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B534">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the tecs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (teps).
</prevsent>
<prevsent>the teps are extracted independently one of another and therefore the process might be characterized as local maximization (greedy) one.
</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
the estimating approach such as brown et al (1993), <papid> J93-2003 </papid>kay and rscheisen (1993), <papid> J93-1006 </papid>kupiec (1993), <papid> P93-1003 </papid>hiemstra (1997) etc. is based on building from data statistical bitext model, the parameters of which are to be estimated according to given set of assumptions.</citsent>
<aftsection>
<nextsent>the bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments).
</nextsent>
<nextsent>there are pros and cons for each type of approach, some of them discussed in hiemstra (1997).
</nextsent>
<nextsent>our translation equivalents extraction process may be characterized as hypotheses testing?
</nextsent>
<nextsent>approach and does not need pre-existing bilingual lexicon for the considered languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B536">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the tecs that show an association measure higher than expected under the independence assumption are assumed to be translation-equivalence pairs (teps).
</prevsent>
<prevsent>the teps are extracted independently one of another and therefore the process might be characterized as local maximization (greedy) one.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
the estimating approach such as brown et al (1993), <papid> J93-2003 </papid>kay and rscheisen (1993), <papid> J93-1006 </papid>kupiec (1993), <papid> P93-1003 </papid>hiemstra (1997) etc. is based on building from data statistical bitext model, the parameters of which are to be estimated according to given set of assumptions.</citsent>
<aftsection>
<nextsent>the bitext model allows for global maximization of the translation equivalence relation, considering not individual translation equivalents but sets of translation equivalents (sometimes called assignments).
</nextsent>
<nextsent>there are pros and cons for each type of approach, some of them discussed in hiemstra (1997).
</nextsent>
<nextsent>our translation equivalents extraction process may be characterized as hypotheses testing?
</nextsent>
<nextsent>approach and does not need pre-existing bilingual lexicon for the considered languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B541">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>none of these hopotheses is true in general, but the situations where they are not observed are rare enough so that ignoring the exceptions would not produce significant number of errors and would not lose too many useful translations.
</prevsent>
<prevsent>the assumptions we made were the following: ? lexical token in one half of the translation unit (tu) corresponds to at most one non-empty lexical unit in the other half of the tu; this is the 1:1 mapping assumption which underlines the work of many other researchers (ahrenberg et al (2000), brew and mckelvie (1996), hiemstra (1996), kay and rscheisen (1993), <papid> J93-1006 </papid>tied mann (1998), melamed (2001) etc); ? polysemous lexical token, if used several times in the same tu, is used with the same meaning; this assumption is explicitly used by gale and church (1991), <papid> H91-1026 </papid>melamed (2001) and implicitly by all the previously mentioned authors; ? lexical token in one part of tu can be aligned to lexical token in the other part of the tu only if the two tokens have compatible types (part-of-speech); in most cases, compatibility reduces to the same pos, but it is also possible to define other compatibility mappings (e.g. participles or gerunds in english are quite often translated as adjectives or nouns in romanian and vice-versa); ? although the word order is not an in variant of translation, it is not random either (ahrenberg et al (2000)); when two or more candidate translation pairs are equally scored, the one containing tokens which are closer in relative position are preferred.</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
the proper extraction of translation equivalents requires special pre-processing: ? sentence alignment; we used slightly modified version of char align described by gale and church (1993) . ? <papid> J93-1004 </papid>tokenization; the segmenter we used (mtseg, developed by p. di cristo for the multext project: http://www.lpl.univ-aix.fr/projects/multext/ mtseg/), may process multiword expressions as single lexical tokens.</citsent>
<aftsection>
<nextsent>the segmenter comes with tokenization resources for several western european languages, further enhanced in the multext-east project (dimitrova et al (1998), <papid> P98-1050 </papid>erjavec et al(1998), tufis et al(1998)) with corresponding resources for bulgarian, czech, estonian, hungarian, romanian and slovene.</nextsent>
<nextsent>tagging and lemmatization; we used tiered tagging with combined language models approach (tufis (1999), tufis (2000)) based on brantss tnt tagger.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B542">
<title id=" C02-1002.xml">a cheap and fast way to build useful translation lexicons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the assumptions we made were the following: ? lexical token in one half of the translation unit (tu) corresponds to at most one non-empty lexical unit in the other half of the tu; this is the 1:1 mapping assumption which underlines the work of many other researchers (ahrenberg et al (2000), brew and mckelvie (1996), hiemstra (1996), kay and rscheisen (1993), <papid> J93-1006 </papid>tied mann (1998), melamed (2001) etc); ? polysemous lexical token, if used several times in the same tu, is used with the same meaning; this assumption is explicitly used by gale and church (1991), <papid> H91-1026 </papid>melamed (2001) and implicitly by all the previously mentioned authors; ? lexical token in one part of tu can be aligned to lexical token in the other part of the tu only if the two tokens have compatible types (part-of-speech); in most cases, compatibility reduces to the same pos, but it is also possible to define other compatibility mappings (e.g. participles or gerunds in english are quite often translated as adjectives or nouns in romanian and vice-versa); ? although the word order is not an in variant of translation, it is not random either (ahrenberg et al (2000)); when two or more candidate translation pairs are equally scored, the one containing tokens which are closer in relative position are preferred.</prevsent>
<prevsent>the proper extraction of translation equivalents requires special pre-processing: ? sentence alignment; we used slightly modified version of char align described by gale and church (1993) . ? <papid> J93-1004 </papid>tokenization; the segmenter we used (mtseg, developed by p. di cristo for the multext project: http://www.lpl.univ-aix.fr/projects/multext/ mtseg/), may process multiword expressions as single lexical tokens.</prevsent>
</prevsection>
<citsent citstr=" P98-1050 ">
the segmenter comes with tokenization resources for several western european languages, further enhanced in the multext-east project (dimitrova et al (1998), <papid> P98-1050 </papid>erjavec et al(1998), tufis et al(1998)) with corresponding resources for bulgarian, czech, estonian, hungarian, romanian and slovene.</citsent>
<aftsection>
<nextsent>tagging and lemmatization; we used tiered tagging with combined language models approach (tufis (1999), tufis (2000)) based on brantss tnt tagger.
</nextsent>
<nextsent>after the sentence alignment, tagging and lemmatization, the first step is to compute list of translation equivalence candidates (tecl).
</nextsent>
<nextsent>this list contains several sub-lists, one for each pos considered in the extraction procedure.
</nextsent>
<nextsent>each pos-specific sub-list contains several pairs of tokens  tokens tokent  of the corresponding pos that appeared in the same tus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B544">
<title id=" C02-1113.xml">natural language and inference in a computer game </title>
<section> architecture.  </section>
<citcontext>
<prevsection>
<prevsent>there are two separate knowledge bases, which share set of common def initions: one represents the true state of the world in world model, the other keeps track of what the player knows about the world.
</prevsent>
<prevsent>solid arrows indicate the general flow of information, dashed arrows indicate access to the knowledge bases.
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
the users input is first parsed using an efficient parser for dependency grammar (duchier and debusmann, 2001).<papid> P01-1024 </papid></citsent>
<aftsection>
<nextsent>next, referring expressions are resolved to individuals in the game world.
</nextsent>
<nextsent>the result is ground term or sequence of ground terms that indicates the action(s) the user wants to take.
</nextsent>
<nextsent>the actions module looks up these actions in database (where they are specified in strips-like format), checks whether the actions preconditions are met in the world, and, if yes, updates the world state with the effects of the action.
</nextsent>
<nextsent>the action can also specify effects on the users knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B545">
<title id=" C02-1113.xml">natural language and inference in a computer game </title>
<section> architecture.  </section>
<citcontext>
<prevsection>
<prevsent>this information is further enriched by the content determination module; for example,this module computes detailed descriptions of objects the player wants to look at.
</prevsent>
<prevsent>the reference generation module translates the internal namesof individuals into descriptions that can be verbalized.
</prevsent>
</prevsection>
<citsent citstr=" P02-1003 ">
in the last step, an efficient realization module (koller and striegnitz, 2002) <papid> P02-1003 </papid>builds the output sentences according to tag grammar.</citsent>
<aftsection>
<nextsent>the player knowledge is updated after reference generation when the content of the games response, including the new information carried e.g. by indefinite nps, is fully established.if an error occurs at any stage, e.g. because precondition of the action fails, an error message specifying the reasons for the failure is generated byusing the normal generation track (content determination, reference generation, realization) of the game.
</nextsent>
<nextsent>the system is implemented in the programming language mozart (mozart consortium, 1999) and provides an interface to the dl reasoning system racer (haarslev and moller, 2001), which is used for main ting and accessing the knowledge bases.
</nextsent>
<nextsent>now we will look at the way that the state of the world is represented in the game, which will be important in the language processing modules described in sections 4 and 5.
</nextsent>
<nextsent>we will first give short overview of description logic (dl) and the theorem prover we use and then discuss some aspects of the world model in more detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B546">
<title id=" C02-1113.xml">natural language and inference in a computer game </title>
<section> referring expressions.  </section>
<citcontext>
<prevsection>
<prevsent>we use racers retrieval functionality to extract this information from the knowledge base.
</prevsent>
<prevsent>to refer to an object that the player already has encountered, we try to construct definite description that, given the player knowledge, uniquely identifies this object.
</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
for this purpose we use variant of dale and reiters (1995) incremental algorithm, extended to deal with relations between objects (dale and haddock, 1991).<papid> E91-1028 </papid></citsent>
<aftsection>
<nextsent>the properties of the target referent are looked at in some predefined order (e.g. first its type, then its color, its location, parts it may have, . . .).
</nextsent>
<nextsent>a property is added to the description if at least one other object (a distrac tor) is excluded from it because it doesnt share this property.
</nextsent>
<nextsent>this is done until the description uniquely identifies the target referent.the algorithm uses racers reasoning and retrieval functionality to access the relevant information about the context, which included e.g. computing the properties of the target referent and finding the distracting instances.
</nextsent>
<nextsent>assuming we want to refer to entity a1 in the a-box in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B547">
<title id=" C02-1113.xml">natural language and inference in a computer game </title>
<section> ambiguity resolution.  </section>
<citcontext>
<prevsection>
<prevsent>we try to resolve them, too, by taking referential information into account.
</prevsent>
<prevsent>in the simplest case, the referring expressions insome of the syntactic readings have no possible referent in the player a-box at all.
</prevsent>
</prevsection>
<citsent citstr=" P01-1061 ">
if this happens, we filter these readings out and only continue with the others (schuler, 2001).<papid> P01-1061 </papid></citsent>
<aftsection>
<nextsent>for example, the sentence unlock the toolbox with the key is ambiguous.
</nextsent>
<nextsent>in scenario where there is toolbox and key, but the key is not attached to the toolbox, resolution fails forone of the analyses and thereby resolves the syntactic ambiguity.
</nextsent>
<nextsent>if more than one syntactic reading survives this first test, we perform the same computations as above to filter out possible referents which are eitherunsalient or violate the players knowledge.
</nextsent>
<nextsent>sometimes, only one syntactic reading will have referent in this narrower sense; in this case, we are done.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B548">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, practitioners of the tex ting language excel in devising acronyms which condense, sometimes in radical way, multi-word units: this is for instance the case with afair, which stands for as far as recall.
</prevsent>
<prevsent>as result, from natural language processing (nlp) point of view, these messages contain an abnormally high rate ofout-of-vocabulary forms, and the ambiguity of existing word forms is aggravated, two factors that contribute to degrade the performance of natural language processing tools.
</prevsent>
</prevsection>
<citsent citstr=" P06-2005 ">
recovering normalized orthography seems thus to be necessary preprocessing step for many real-world nlp applications, such as text-to-speech, translation, or text mining applications (filtering, routing, information retrieval, etc).these short messages have so far received relatively little attention from the nlp community 2 : see, for english, (aw et al, 2006; <papid> P06-2005 </papid>choudhury et al., 2007), which both address the problem with statistical learning techniques, and, for french, (guimier de neef et al, 2007), which details complete pipe-line of hand-crafted, symbolic, modules.</citsent>
<aftsection>
<nextsent>in fact, the problem of normalizing sms shares lot of commonalities with other nlp applications, and can be addressed from severalviewpoints.
</nextsent>
<nextsent>the first, maybe the most natural angle, is to make an analogy with the spelling correction problem.
</nextsent>
<nextsent>this problem has been extensively studied in the past and variety of statistical approaches are readily available, most notably the noisy channel?
</nextsent>
<nextsent>approach (see eg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B550">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this problem has been extensively studied in the past and variety of statistical approaches are readily available, most notably the noisy channel?
</prevsent>
<prevsent>approach (see eg.
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
(church and gale, 1991; brill and moore, 2000; <papid> P00-1037 </papid>toutanova and moore, 2002)).<papid> P02-1019 </papid></citsent>
<aftsection>
<nextsent>an alternative metaphor is the translation metaphor: under this view, the normalization task is accomplished by taking the sms 2 couple of on-line sms-to-english translation systems are accessible on the internet, see notably http://www.
</nextsent>
<nextsent>transl8it.com/ and http://www.lingo2word.
</nextsent>
<nextsent>com/; netspeak?
</nextsent>
<nextsent>dictionaries, again for english, also abound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B552">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this problem has been extensively studied in the past and variety of statistical approaches are readily available, most notably the noisy channel?
</prevsent>
<prevsent>approach (see eg.
</prevsent>
</prevsection>
<citsent citstr=" P02-1019 ">
(church and gale, 1991; brill and moore, 2000; <papid> P00-1037 </papid>toutanova and moore, 2002)).<papid> P02-1019 </papid></citsent>
<aftsection>
<nextsent>an alternative metaphor is the translation metaphor: under this view, the normalization task is accomplished by taking the sms 2 couple of on-line sms-to-english translation systems are accessible on the internet, see notably http://www.
</nextsent>
<nextsent>transl8it.com/ and http://www.lingo2word.
</nextsent>
<nextsent>com/; netspeak?
</nextsent>
<nextsent>dictionaries, again for english, also abound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B562">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> two normalization systems.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the mt-like system.
</prevsent>
<prevsent>our first normalization system is entirely based onopen-source, public domain packages for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
giza++ (och and ney,2003) <papid> J03-1002 </papid>is used to induce, based on statistical principles (brown et al, 1990), <papid> J90-2002 </papid>an automatic word alignment of sms tokens with their normalized coun terparts; moses (koehn et al, 2007) <papid> P07-2045 </papid>is used to learn the various parameters of the phrase-based model, to optimize the weight combination and to perform the translation using multi-stack searchalgorithm; the sri language model toolkit (stolcke, 2002) is finally used to estimate statistical language models.</citsent>
<aftsection>
<nextsent>for this system, the training set has been split in learning set 3 (approximately 25000 messages) and development set (about 11700 messages), which is used to tune parameters.
</nextsent>
<nextsent>as suggested in the previous sections, we have constrained both systems to consider only monotonic?
</nextsent>
<nextsent>alignments between the source and the target languages.
</nextsent>
<nextsent>3.2 the asr-like system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B563">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> two normalization systems.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the mt-like system.
</prevsent>
<prevsent>our first normalization system is entirely based onopen-source, public domain packages for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
giza++ (och and ney,2003) <papid> J03-1002 </papid>is used to induce, based on statistical principles (brown et al, 1990), <papid> J90-2002 </papid>an automatic word alignment of sms tokens with their normalized coun terparts; moses (koehn et al, 2007) <papid> P07-2045 </papid>is used to learn the various parameters of the phrase-based model, to optimize the weight combination and to perform the translation using multi-stack searchalgorithm; the sri language model toolkit (stolcke, 2002) is finally used to estimate statistical language models.</citsent>
<aftsection>
<nextsent>for this system, the training set has been split in learning set 3 (approximately 25000 messages) and development set (about 11700 messages), which is used to tune parameters.
</nextsent>
<nextsent>as suggested in the previous sections, we have constrained both systems to consider only monotonic?
</nextsent>
<nextsent>alignments between the source and the target languages.
</nextsent>
<nextsent>3.2 the asr-like system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B564">
<title id=" C08-1056.xml">normalizing sms are two metaphors better than one  </title>
<section> two normalization systems.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 the mt-like system.
</prevsent>
<prevsent>our first normalization system is entirely based onopen-source, public domain packages for statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
giza++ (och and ney,2003) <papid> J03-1002 </papid>is used to induce, based on statistical principles (brown et al, 1990), <papid> J90-2002 </papid>an automatic word alignment of sms tokens with their normalized coun terparts; moses (koehn et al, 2007) <papid> P07-2045 </papid>is used to learn the various parameters of the phrase-based model, to optimize the weight combination and to perform the translation using multi-stack searchalgorithm; the sri language model toolkit (stolcke, 2002) is finally used to estimate statistical language models.</citsent>
<aftsection>
<nextsent>for this system, the training set has been split in learning set 3 (approximately 25000 messages) and development set (about 11700 messages), which is used to tune parameters.
</nextsent>
<nextsent>as suggested in the previous sections, we have constrained both systems to consider only monotonic?
</nextsent>
<nextsent>alignments between the source and the target languages.
</nextsent>
<nextsent>3.2 the asr-like system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B571">
<title id=" C04-1087.xml">enhancing automatic term recognition through recognition of variation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in any case, they do not change?
</prevsent>
<prevsent>the meaning as they refer to the same concept.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
daille et al (1996) and jacquemin (1999),  <papid> P99-1044 </papid>and jacquemin (2001) further identified types of variation that modified the meaning of terms.</citsent>
<aftsection>
<nextsent>although many authors mention the problems related to term variation, few have dealt with linking the corresponding term variants.
</nextsent>
<nextsent>also, the recognition of variants is typically performed as separate operation, and not as part of atr.
</nextsent>
<nextsent>the simplest technique to handle some types of term variation (e.g. morphological) is based on stemming: if two term forms share stemmed representation, they are considered as mutual variants (jacquemin and tzoukermann, 1999; ananiadou et al, 2000).
</nextsent>
<nextsent>however, stemming may result in ambiguous denot ations related to over stemming?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B572">
<title id=" C04-1011.xml">kullbackleibler distance between probabilistic context free grammars and probabilistic finite automata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as fas cannot describe structure as such, it is impractical to write the automata required for such applications by hand, and even difficult to derive them automatically by training.
</prevsent>
<prevsent>for this reason, the used fas are often derived from cfgs, by means of some form of approximation.
</prevsent>
</prevsection>
<citsent citstr=" J00-1003 ">
an overview of different methods of approximating cfgs by fas, along with an experimental comparison, was given by (nederhof, 2000).<papid> J00-1003 </papid></citsent>
<aftsection>
<nextsent>the next step is to assign probabilities to the transitions of the approximating fa, as the application outlined above requires qualitative distinction between hypotheses rather than the purely boolean distinction of language member ship.
</nextsent>
<nextsent>under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic cfg (pcfg), as shown for the special case of n-grams by (rimon and herz, 1991; stolcke and segal, 1994), <papid> P94-1011 </papid>or by training of the fa on corpus generated by the pcfg (jurafsky et al, 1994).</nextsent>
<nextsent>see also (mohri and nederhof, 2001) for discussion of related ideas.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B573">
<title id=" C04-1011.xml">kullbackleibler distance between probabilistic context free grammars and probabilistic finite automata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an overview of different methods of approximating cfgs by fas, along with an experimental comparison, was given by (nederhof, 2000).<papid> J00-1003 </papid></prevsent>
<prevsent>the next step is to assign probabilities to the transitions of the approximating fa, as the application outlined above requires qualitative distinction between hypotheses rather than the purely boolean distinction of language member ship.</prevsent>
</prevsection>
<citsent citstr=" P94-1011 ">
under certain circumstances, this may be done by carrying over the probabilities from an input probabilistic cfg (pcfg), as shown for the special case of n-grams by (rimon and herz, 1991; stolcke and segal, 1994), <papid> P94-1011 </papid>or by training of the fa on corpus generated by the pcfg (jurafsky et al, 1994).</citsent>
<aftsection>
<nextsent>see also (mohri and nederhof, 2001) for discussion of related ideas.
</nextsent>
<nextsent>an obvious question to ask is then how well the resulting pfa approximates the input pcfg, possibly for different methods of determining an fa and different ways of attaching probabilities to the transitions.
</nextsent>
<nextsent>until now, any direct way of measuring the distance between pcfg and pfa has been lacking.
</nextsent>
<nextsent>as we will argue in this paper, the natural distance measure between probability distributions, the kullback-leibler (kl) distance, is difficult tocompute.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B574">
<title id=" C04-1011.xml">kullbackleibler distance between probabilistic context free grammars and probabilistic finite automata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the constraint of determinism is not problem in practice, as any fa can bedeterminized, and fas derived by approximation algorithms are normally determinized (and minimized).
</prevsent>
<prevsent>as second possible application, we now look more closely into the matter of determinizationof finite-state models.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
not all pfas can be de termini zed, as discussed by (mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>this is unfortunate, as deterministic (p)fas process input with time and space costs independent of the size of the automaton, whereas these costs are linear in the size of the automaton in the non deterministic case, which may be too high for some real-time applications.
</nextsent>
<nextsent>instead of distribution-preserving determinization, we may therefore approximate non deterministic pfa by deterministic pfa whose probability distribution is close to, but not necessarily identical to, that of the first pfa.
</nextsent>
<nextsent>again, an important question is how close the two models are to each other.
</nextsent>
<nextsent>it was argued before by (juang and rabiner, 1985; falkhausen et al, 1995; viholaet al, 2002) that the kl distance between finite state models is difficult to compute in general.the theory developed in this paper shows how ever that the cross-entropy between the input pfa and the approximating deterministic pfa can be expressed in closed form, relying on the fact that pfa can be seen as special case ofa pcfg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B575">
<title id=" C04-1187.xml">web based list question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this unsatisfactory performance exposes the limitation of using only traditional information retrieval and natural language processing techniques to find an exhaustive set of factoid answers as compared to only one.
</prevsent>
<prevsent>trec-12 run tag avg f1 lccmains03 0.396 nusmml03r2 0.319 mitcsail03c 0.134 isi03a 0.118 bbn2003b 0.097 average 0.213 table 1: trec-12 top 5 performers (voorhees, 2003) in contrast to the traditional techniques, the web is used extensively in systems to rally round factoid questions.
</prevsent>
</prevsection>
<citsent citstr=" P02-1054 ">
qa researchers have explored variety of uses of the web, ranging from surface pattern mining (ravichandran et al, 2002), query formulation (yang et al, 2003), answer validation (magnini et al., 2002), <papid> P02-1054 </papid>to directly finding answers on the web by data redundancy analysis (brill et al, 2001).</citsent>
<aftsection>
<nextsent>these systems demonstrated that with the help of the web they could generally boost baseline performance by 25%-30% (lin 2002).<papid> C02-1026 </papid></nextsent>
<nextsent>the well-known redundancy-based approach identifies the factoid answer as an n-gram appearing most frequently on the web (brill et al 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B576">
<title id=" C04-1187.xml">web based list question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>trec-12 run tag avg f1 lccmains03 0.396 nusmml03r2 0.319 mitcsail03c 0.134 isi03a 0.118 bbn2003b 0.097 average 0.213 table 1: trec-12 top 5 performers (voorhees, 2003) in contrast to the traditional techniques, the web is used extensively in systems to rally round factoid questions.
</prevsent>
<prevsent>qa researchers have explored variety of uses of the web, ranging from surface pattern mining (ravichandran et al, 2002), query formulation (yang et al, 2003), answer validation (magnini et al., 2002), <papid> P02-1054 </papid>to directly finding answers on the web by data redundancy analysis (brill et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" C02-1026 ">
these systems demonstrated that with the help of the web they could generally boost baseline performance by 25%-30% (lin 2002).<papid> C02-1026 </papid></citsent>
<aftsection>
<nextsent>the well-known redundancy-based approach identifies the factoid answer as an n-gram appearing most frequently on the web (brill et al 2001).
</nextsent>
<nextsent>this idea works well on factoid questions because factoid questions require only one instance and web documents contains large number of repeated information about possible answers.
</nextsent>
<nextsent>however, when dealing with list questions, we need to find all distinct instances and hence we cannot ignore the less frequent answer candidates.
</nextsent>
<nextsent>the redundancy-based approach fails to spot novel or unexpectedly valuable information in lower ranked web pages with few occurrences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B577">
<title id=" C04-1175.xml">combining prediction by partial matching and logistic regression for thai word segmentation </title>
<section> syllable segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>syllable segmentation can be viewed as the problem of inserting spaces between pairs of characters in the text.
</prevsent>
<prevsent>thai language consists of 66 distinct characters.
</prevsent>
</prevsection>
<citsent citstr=" J00-3004 ">
treating each character individually as in (teahan et al, 2000) <papid> J00-3004 </papid>requires large amount of training data in order to calculate all the probabilities in the tables, as well as large amount of table space and time to lookup data from the tables.</citsent>
<aftsection>
<nextsent>we reduce the amount of training data required by partitioning the characters into 16 types, as shown in table 1.
</nextsent>
<nextsent>as side effect of the character classification, the algorithm can handle syllables not present in the training data.
</nextsent>
<nextsent>each character is represented by its respective type symbol.
</nextsent>
<nextsent>for instance  ???*????*??*???*?????*???????*???*???  is represented as:  de*zdps*mu*hlt*asthg*ahsutss* aor*fst .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B578">
<title id=" C08-1039.xml">homotopybased semi supervised hidden markov models for sequence labeling </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>freez error on citation test (300l5000u) viterbi decoding sms decoding mle (b) figure 2: em ? error rates while increasing the allocation from 0 to 1 by the step size 0.025.
</prevsent>
<prevsent>to segment the document into fields, and to label each field.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
in our experiments we use the bibliographic citation dataset described in (peng and mccallum, 2004) <papid> N04-1042 </papid>see fig.</citsent>
<aftsection>
<nextsent>1 for an example of the input and expected label output for this task).
</nextsent>
<nextsent>this dataset has 500 annotated citations with 13 fields; 5000 unannotated citations were added to it later by (grenager et al, 2005).<papid> P05-1046 </papid></nextsent>
<nextsent>the annotated data is split into 300-document training set, a100-document development (dev) set, and 100 document test set7.we use first order hmm with the size of hidden states equal to the number of fields (equal to 13).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B580">
<title id=" C08-1039.xml">homotopybased semi supervised hidden markov models for sequence labeling </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in our experiments we use the bibliographic citation dataset described in (peng and mccallum, 2004) <papid> N04-1042 </papid>see fig.</prevsent>
<prevsent>1 for an example of the input and expected label output for this task).</prevsent>
</prevsection>
<citsent citstr=" P05-1046 ">
this dataset has 500 annotated citations with 13 fields; 5000 unannotated citations were added to it later by (grenager et al, 2005).<papid> P05-1046 </papid></citsent>
<aftsection>
<nextsent>the annotated data is split into 300-document training set, a100-document development (dev) set, and 100 document test set7.we use first order hmm with the size of hidden states equal to the number of fields (equal to 13).
</nextsent>
<nextsent>we freeze the transition probabilities to what has been observed in the labeled data and only learn the emission probabilities.
</nextsent>
<nextsent>the transition probabilities are kept frozen due to the nature of this task in which the transition information can be learned with very little labeled data, e.g. first start with author?
</nextsent>
<nextsent>then move to title?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B581">
<title id=" C08-1039.xml">homotopybased semi supervised hidden markov models for sequence labeling </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>here em performs hill climbing on the likelihood surface, and arguably there sulting (locally optimal) model may not reflect thequality of the globally optimal mle.
</prevsent>
<prevsent>but we conjecture that even the mle model(s) which globally maximize the likelihood may suffer from the problem of the size imbalance between labeled and unlabeled data, since what matters is the influence of unlabeled data on the likelihood.
</prevsent>
</prevsection>
<citsent citstr=" P07-1036 ">
(chang et. al., 2007) <papid> P07-1036 </papid>also report on using hard-em on thesedatasets9 in which the performance degrades compared to the purely supervised model.</citsent>
<aftsection>
<nextsent>5.2 choosing ? in homotopy-based hmm.
</nextsent>
<nextsent>we analyze different criteria in picking the best value of ? based on inspection of the continuation path.
</nextsent>
<nextsent>the following criteria are considered: ? monotone: the first iteration in which themonotonicity of the path is changed, or equivalently the first iteration in which the determinant of ??
</nextsent>
<nextsent>em 1 (?)i in algorithm 1 becomes zero (corduneanu and jaakkola, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B587">
<title id=" C02-1052.xml">using an ontology to determine english count ability </title>
<section> linguistic background.  </section>
<citcontext>
<prevsection>
<prevsent>these are not, however directly linked to full ontology.
</prevsent>
<prevsent>therefore there is no direct connection between being an animal and being countable.
</prevsent>
</prevsection>
<citsent citstr=" C94-1002 ">
bond et al (1994) <papid> C94-1002 </papid>suggested division of count ability into five major types, based on allan (1980)s noun count ability preferences(ncps).</citsent>
<aftsection>
<nextsent>nouns which rarely undergo conversion are marked as either fully countable, uncountable or plural only.
</nextsent>
<nextsent>nouns that are non-specified are marked as either strongly countable (for count nouns that can be converted to mass, such as cake) or weakly countable (for mass nouns that are readily convertible to count, such as beer).
</nextsent>
<nextsent>conversion is triggered by surrounding context.
</nextsent>
<nextsent>noun phrases headed by uncountable nouns can be converted to countable noun phrases by generating clas sifiers: one piece of equipment , as described in bond and ikehara (1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B589">
<title id=" C02-1052.xml">using an ontology to determine english count ability </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>intuition, as there are no large sense-tagged corpora to train from.
</prevsent>
<prevsent>in fact, almost all english nouns can be used in uncountable environments, for example, if they are given the ground interpretation.
</prevsent>
</prevsection>
<citsent citstr=" J99-4002 ">
the only exception is classifiers such as piece or bit ,which refer to quanta, and thus have no uncountable interpretation.language users are sensitive to relative frequencies of variant forms and senses of lexical items (briscoe and copestake, 1999, <papid> J99-4002 </papid>p511).</citsent>
<aftsection>
<nextsent>the division into fully, strongly, weakly 1we ignore the two subclasses in this paper: collective nouns are treated as fully countable and semi-countable as uncountable.
</nextsent>
<nextsent>index usagi sense 1 ? ?
</nextsent>
<nextsent>english translation rabbit part of speech noun noun count ability pref.
</nextsent>
<nextsent>strongly countable default number singular semantic classes [ common noun animal, meat ] ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B590">
<title id=" C08-1011.xml">a concept centered approach to noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these relational possibilities are evident at linguistic level in the syntagmatic patterns that connect nouns to the verbal actions that act upon, or are facilitated by, these nouns.
</prevsent>
<prevsent>we present model of noun-compound interpretation that first learns the relational possibilities for individual nouns from corpora, and which then uses these to hypothesize about the most likely relationship that underpins noun compound.
</prevsent>
</prevsection>
<citsent citstr=" W96-0309 ">
noun compounds hide remarkable depth of conceptual machinery behind simple syntactic form, noun-noun, and thus pose considerable problem for the computational processing of language (johnston and busa, 1996).<papid> W96-0309 </papid></citsent>
<aftsection>
<nextsent>it is not just that compounds are commonplace in language, or that their interpretation requires synthesis of lexical, semantic, and pragmatic information sources (finin, 1980); compounds provide highly compressed picture of the workings of concept combination, so there are as many ways of interpreting noun compound as there are ways of combining the underlying concepts (gagn?, 2002).
</nextsent>
<nextsent>linguists have thus attempted to ? 2008.
</nextsent>
<nextsent>licensed under the creative commons attribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B591">
<title id=" C08-1011.xml">a concept centered approach to noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since these noun-pairs must allow an audience to reconstruct the decompressed proposition, there must be some systematic means by which the missing relation can easily be inferred.
</prevsent>
<prevsent>this framing of the problem as search for missing relation suggests two broad strategies for the interpretation of compounds.
</prevsent>
</prevsection>
<citsent citstr=" P06-2064 ">
in the first, the top-down strategy, we assume that there are only so many ways of combining two concepts; by enumerating these ways, we can view the problem of interpretation as problem of classification, in which compounds are placed into separate classes that each correspond to single manner of concept connection (kim and baldwin, 2006), (<papid> P06-2064 </papid>nastase and szpakowicz, 2003).</citsent>
<aftsection>
<nextsent>this strategy explicitly shaped the semeval task on classifying semantic relations between nominals (girju et al, 2007) and so is employed by all of the systems that participated in that task.
</nextsent>
<nextsent>in the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relation ally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (nakov, 2006), (turney, 2006<papid> P06-1040 </papid>a).</nextsent>
<nextsent>in this paper we describe an approach that employs the bottom-up strategy with an open- rather than closed-inventory of inter-concept relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B592">
<title id=" C08-1011.xml">a concept centered approach to noun compound interpretation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the first, the top-down strategy, we assume that there are only so many ways of combining two concepts; by enumerating these ways, we can view the problem of interpretation as problem of classification, in which compounds are placed into separate classes that each correspond to single manner of concept connection (kim and baldwin, 2006), (<papid> P06-2064 </papid>nastase and szpakowicz, 2003).</prevsent>
<prevsent>this strategy explicitly shaped the semeval task on classifying semantic relations between nominals (girju et al, 2007) and so is employed by all of the systems that participated in that task.</prevsent>
</prevsection>
<citsent citstr=" P06-1040 ">
in the second, the bottom-up strategy, we assume that it is futile to try and enumerate the many ways in which concepts can relation ally combine, but look instead to large corpora to discover the ways in which different word combinations are explicitly framed by language (nakov, 2006), (turney, 2006<papid> P06-1040 </papid>a).</citsent>
<aftsection>
<nextsent>in this paper we describe an approach that employs the bottom-up strategy with an open- rather than closed-inventory of inter-concept relations.
</nextsent>
<nextsent>these relations are acquired from the analysis of large corpora, such as the web it corpus of google n-grams (brants and franz, 2006).
</nextsent>
<nextsent>we argue that an understanding of noun compounds requires an understanding of lexicalized concept combination, which in turn requires an understanding of how lexical concepts can be used and connected to others.
</nextsent>
<nextsent>as such, we do not use corpora as means of 81 characterizing noun-compounds themselves, but as means of characterizing the action possibilities of the individual nouns that can participate in compound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B593">
<title id=" C08-1011.xml">a concept centered approach to noun compound interpretation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we present an evaluation of this approach in section 5 and conclude the paper with some final remarks in section 6.
</prevsent>
<prevsent>machine-learning and example-based approaches to noun-compounds generally favor the top-down strategy for defining relations, since it allows training data and exemplars/cases to be labeled using fixed inventory of relational classes.
</prevsent>
</prevsection>
<citsent citstr=" W07-2083 ">
as noted earlier, this strategy is characteristic of the systems that participated in the semeval task on classifying semantic relations between nominals (girju et al, 2007), such as butnariu and veale (2007).<papid> W07-2083 </papid></citsent>
<aftsection>
<nextsent>though the inventory is fixed in size, it can be defined using varying levels of abstraction; for instance, nastase and szpakowicz (2003) use an inventory of 35 relations, 5 of which are top level relations with the remaining 30 at the lower level.
</nextsent>
<nextsent>the top down strategy pre-dates these computational approaches, and is key aspect of the foundational work of levi (1978) and of subsequent work by gagn?
</nextsent>
<nextsent>and shoben (1997), both of whom posit small set of semantic relations as underpinning all noun compounds.
</nextsent>
<nextsent>more recently, kim and baldwin (2005) use fixed inventory of semantic relations to annotate case-base of examples; new compounds are understood by determining their lexical similarity to the closest annotated compounds in the case base.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B605">
<title id=" C08-1011.xml">a concept centered approach to noun compound interpretation </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we have presented model of noun compounding that places nouns and their specific linguistic relational possibilities at the centre of processing.
</prevsent>
<prevsent>when one considers that linguistic relational possibilities capture aspects of noun meaning such as purpose, constitution and agency, their realization here can be viewed as 87 generalized and lexicalized aspect of qualia structure in the sense of pustejovsky (1995) and johnston and busa (1996).<papid> W96-0309 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1112 ">
indeed, the n-gram patterns used to extract these relational possibilities from corpora are not unlike the patterns used by cimiano and wenderoth (2007) <papid> P07-1112 </papid>to harvest qualia structures from the web.</citsent>
<aftsection>
<nextsent>we conclude from the empirical observation that the hybrid model outperforms the web-based model (albeit slimly) in experiment 2 while both perform equally well in experiment 1, is that the modifier and head are of comparable performance when paraphrasing the interpretations of noun compounds.
</nextsent>
<nextsent>recall that the web-validation approach (model-1) generates interpretations from either the modifier or the head, while the matching-relational possibilities and hybrid models require both to contribute equally.
</nextsent>
<nextsent>necessary extensions to the approach include the acquisition of more relational possibilities of greater linguistic complexity, the ability to organize relational possibilities hierarchically according to their underlying semantic meanings, and the ability to recognize an implication structure among different but related relational possibilities.
</nextsent>
<nextsent>acknowledgement we would like to thank preslav nakov for providing us the data used in the second experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B606">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical mt systems are based on stochastic mapping between lexical items, assuming the underlying semantic transfer is hidden.
</prevsent>
<prevsent>transfer systems use explicit lexical, syntactic and semantic transfer rules.
</prevsent>
</prevsection>
<citsent citstr=" J03-2001 ">
consequently, cognitive scientists and computational linguists alike have been interested in the study of semantic mapping between languages (ploux and ji, 2003, <papid> J03-2001 </papid>dorr et al , 2002, ngai et al , 2002, <papid> C02-1162 </papid>boas 2002, palmer and wu, 1995).</citsent>
<aftsection>
<nextsent>we propose to automatically construct bilingual lexical semantic network with word sense and semantic role mapping between english and chinese, simulating the concept lexicon?, suggested by cognitive scientists, of bilingual person.
</nextsent>
<nextsent>figure 1.
</nextsent>
<nextsent>biframenet lexicon and example sentence induction the linguists-defined ontologies?-framenet (baker et al , 1998), <papid> P98-1013 </papid>hownet (dong and dong, 2000), and bilingual dictionaries are the basis for the induction of the mapping.</nextsent>
<nextsent>we automatically estimate the semantic transfer likelihoods between english framenet lexical entries and the chinese word senses in hownet, and align those frames and lexical pairs with high likelihood values.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B607">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical mt systems are based on stochastic mapping between lexical items, assuming the underlying semantic transfer is hidden.
</prevsent>
<prevsent>transfer systems use explicit lexical, syntactic and semantic transfer rules.
</prevsent>
</prevsection>
<citsent citstr=" C02-1162 ">
consequently, cognitive scientists and computational linguists alike have been interested in the study of semantic mapping between languages (ploux and ji, 2003, <papid> J03-2001 </papid>dorr et al , 2002, ngai et al , 2002, <papid> C02-1162 </papid>boas 2002, palmer and wu, 1995).</citsent>
<aftsection>
<nextsent>we propose to automatically construct bilingual lexical semantic network with word sense and semantic role mapping between english and chinese, simulating the concept lexicon?, suggested by cognitive scientists, of bilingual person.
</nextsent>
<nextsent>figure 1.
</nextsent>
<nextsent>biframenet lexicon and example sentence induction the linguists-defined ontologies?-framenet (baker et al , 1998), <papid> P98-1013 </papid>hownet (dong and dong, 2000), and bilingual dictionaries are the basis for the induction of the mapping.</nextsent>
<nextsent>we automatically estimate the semantic transfer likelihoods between english framenet lexical entries and the chinese word senses in hownet, and align those frames and lexical pairs with high likelihood values.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B608">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose to automatically construct bilingual lexical semantic network with word sense and semantic role mapping between english and chinese, simulating the concept lexicon?, suggested by cognitive scientists, of bilingual person.
</prevsent>
<prevsent>figure 1.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
biframenet lexicon and example sentence induction the linguists-defined ontologies?-framenet (baker et al , 1998), <papid> P98-1013 </papid>hownet (dong and dong, 2000), and bilingual dictionaries are the basis for the induction of the mapping.</citsent>
<aftsection>
<nextsent>we automatically estimate the semantic transfer likelihoods between english framenet lexical entries and the chinese word senses in hownet, and align those frames and lexical pairs with high likelihood values.
</nextsent>
<nextsent>in addition, we propose to induce chinese example sentences automatically to match english annotated sentences provided in the framenet.
</nextsent>
<nextsent>the biframenet thus induced provides an additional resource for machine-aided or machine translation systems.
</nextsent>
<nextsent>it can also serve as reference to be compared to cognitive studies of the translation process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B612">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> cross-lingual induction of example.  </section>
<citcontext>
<prevsection>
<prevsent>framenet is collection of over 100-million words of samples of written and spoken language from wide range of sources, including british and american english.
</prevsent>
<prevsent>all the example sentences are chosen by linguists for their representative-ness of particular semantic roles, grammatical functions, and phrase type.
</prevsent>
</prevsection>
<citsent citstr=" W03-1007 ">
the current framenet contains on average 30 annotated example sentences per predicate, which is still inadequate for automatic semantic parsing systems (fleischman et al , 2003).<papid> W03-1007 </papid></citsent>
<aftsection>
<nextsent>each framenet example sentence contains predicate.
</nextsent>
<nextsent>the semantic roles of the related frame elements are manually labeled.
</nextsent>
<nextsent>the syntactic phrase type (e.g. np, pp) and their grammatical function (e.g. external argument, object argument) are also labeled.
</nextsent>
<nextsent>an example annotated sentence containing the predicate beat.v?, in the cause_harm?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B613">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> cross-lingual induction of example.  </section>
<citcontext>
<prevsection>
<prevsent>iii) mine chinese sentences from monolingual corpus that are syntactically similar to the english example sentence, and induce semantic roles from the syntactic transfer function between english and chinese.
</prevsent>
<prevsent>this is the approach we take.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
inspired by previous work on syntax-driven semantic parsing (gildea and jurafsky, 2002; <papid> J02-3001 </papid>fleischman et al , 2003), <papid> W03-1007 </papid>and syntax-based machine translation (wu, 1997; <papid> J97-3002 </papid>cuerzan and yarowsky, 2002), we postulate that syntactically similar sentences with the same predicate also share similar semantic roles.</citsent>
<aftsection>
<nextsent>in this paper, we present our first experiments on inducing semantic roles based on shallow syntactic information.
</nextsent>
<nextsent>we mine chinese example sentences from naturally occurring monolingual corpus, and rank them by their syntactic similarity to our english example sentences.
</nextsent>
<nextsent>a dynamic programming algorithm then annotates the aligned syntactic units with the same semantic roles.
</nextsent>
<nextsent>the example chinese sentences are not translations of the english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B615">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> cross-lingual induction of example.  </section>
<citcontext>
<prevsection>
<prevsent>iii) mine chinese sentences from monolingual corpus that are syntactically similar to the english example sentence, and induce semantic roles from the syntactic transfer function between english and chinese.
</prevsent>
<prevsent>this is the approach we take.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
inspired by previous work on syntax-driven semantic parsing (gildea and jurafsky, 2002; <papid> J02-3001 </papid>fleischman et al , 2003), <papid> W03-1007 </papid>and syntax-based machine translation (wu, 1997; <papid> J97-3002 </papid>cuerzan and yarowsky, 2002), we postulate that syntactically similar sentences with the same predicate also share similar semantic roles.</citsent>
<aftsection>
<nextsent>in this paper, we present our first experiments on inducing semantic roles based on shallow syntactic information.
</nextsent>
<nextsent>we mine chinese example sentences from naturally occurring monolingual corpus, and rank them by their syntactic similarity to our english example sentences.
</nextsent>
<nextsent>a dynamic programming algorithm then annotates the aligned syntactic units with the same semantic roles.
</nextsent>
<nextsent>the example chinese sentences are not translations of the english sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B617">
<title id=" C04-1134.xml">biframenet bilingual frame semantics resource construction by cross lingual induction </title>
<section> cross-lingual induction of example.  </section>
<citcontext>
<prevsection>
<prevsent>also output the final alignment score normalized by the path length.
</prevsent>
<prevsent>we estimate the syntactic pos transfer probabilities from the hk news corpus.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we use two state-of-the-art pos taggersa maximum entropy based english pos tagger (ratnaparkhi, 1996), <papid> W96-0213 </papid>and an hmm based chinese pos tagger.2 we perform two sets of experiments: (1) for each example english sentence in the cause_harm?</citsent>
<aftsection>
<nextsent>frame from framenet, we extract corresponding chinese sentence annotated with the same semantic roles; (2) rank all the chinese sentences that have been aligned to the english sentences by alignment score.
</nextsent>
<nextsent>the highest ranking chinese sentences are used for the biframenet.
</nextsent>
<nextsent>table 6 shows that the average annotation accuracy of all top chinese sentence candidates for each english example sentence is 68%.
</nextsent>
<nextsent>table 7 shows that the annotation accuracy of the top 100 chinese example sentences, sorted by dp score, is 71.8%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B618">
<title id=" C04-1173.xml">word sense disambiguation using a dictionary for sense similarity measure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second kind of approach tries to exploit the lexical knowledge that is represented in dictionaries or thesaurus, with various results from its inception up to now (lesk, 1986; banerjee and pedersen, 2003).
</prevsent>
<prevsent>in all cases, distance between words or word senses is used as way to findthe right sense in given context.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
dictionary based approaches usually relyon comparison of the set of words used in sense definitions and 1a good introduction is (ide and vronis, 1998), <papid> J98-1001 </papid>or (manning and schtze, 1999), chap.</citsent>
<aftsection>
<nextsent>7.
</nextsent>
<nextsent>in the context to disambiguate2.
</nextsent>
<nextsent>this paper presents an algorithm which uses dictionary as network of lexical items (cf.sections 2 and 3) to compute semantic similarity measure between words and word senses.
</nextsent>
<nextsent>it takes into account the whole topology of the dictionary instead of just the entry of target words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B619">
<title id=" C04-1173.xml">word sense disambiguation using a dictionary for sense similarity measure </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>as means of comparison, (patwardhan etal., 2003) applies various measures of semantic proximity (due to number of authors), using the wordnet hierarchy, to the task of word sense disambiguation of few selected words with results ranging from 0.2 to 0.4 with respect to sense definition given in wordnet (the average of senses for each entry giving random score of about 0.2).
</prevsent>
<prevsent>our method already gives similar results on the fine polysemy task (which has an even harder random baseline) when using both nouns and verbs as nodes, and does not focus on selected targets.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
a method not evaluated by (patwardhan etal., 2003) and using another semantic relatedness measure ( conceptual density ) is (agirreand rigau, 1996).<papid> C96-1005 </papid></citsent>
<aftsection>
<nextsent>it is also based on distance within the wordnet hierarchy.
</nextsent>
<nextsent>they used variable context size for the task and present results only for the best size (thus being not fully unsupervised method).
</nextsent>
<nextsent>their random baseline is around 30%, and their precision is 43% for 80% attempted disambiguations.another study of disambiguation using semantic similarity derived from wordnet is (abney and light, 1999); <papid> W99-0901 </papid>it sees the task as hidden markov model, whose parameters are estimated from corpus data, so this is mixed model more than purely dictionary-based model.</nextsent>
<nextsent>with baseline of 0.285, they reach score of 0.423.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B620">
<title id=" C04-1173.xml">word sense disambiguation using a dictionary for sense similarity measure </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>it is also based on distance within the wordnet hierarchy.
</prevsent>
<prevsent>they used variable context size for the task and present results only for the best size (thus being not fully unsupervised method).
</prevsent>
</prevsection>
<citsent citstr=" W99-0901 ">
their random baseline is around 30%, and their precision is 43% for 80% attempted disambiguations.another study of disambiguation using semantic similarity derived from wordnet is (abney and light, 1999); <papid> W99-0901 </papid>it sees the task as hidden markov model, whose parameters are estimated from corpus data, so this is mixed model more than purely dictionary-based model.</citsent>
<aftsection>
<nextsent>with baseline of 0.285, they reach score of 0.423.
</nextsent>
<nextsent>again, the method we used is much simpler, for comparable or better results.besides, by using all connections simultaneously between words in the context to disambiguate and the rest of the lexicon, this method avoids the combinatorial explosion of methods purely based on similarity measure, where every potential sense of every meaningful wordin the context must be considered (unless every word sense of words other than the target isknown beforehand, which is not very realistic assumption), so that only local optimization can be achieved.
</nextsent>
<nextsent>in our case disambiguating lot of different words appearing in the same context may result in poorer results than with only few words, but it will not take longer.
</nextsent>
<nextsent>the only downside is heavy memory usage, as with any dictionary-based method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B622">
<title id=" C08-1040.xml">tracking the dynamic evolution of participants salience in a discussion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several methods have been proposed for identifying the most central nodes in network.
</prevsent>
<prevsent>degree centrality, closeness, and between ness (newman, 2003) are among the most known methods for measuring centrality of nodes in network.
</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
eigen vector centrality is another powerful method that that has been applied to several types of networks.for example it has been used to measure centrality in hyper linked web pages networks (brinand page, 1998; kleinberg, 1998), lexical networks (erkan and radev, 2004; mihalcea and ta rau, 2004; <papid> W04-3252 </papid>kurland and lee, 2005; kurland and lee, 2006), and semantic networks (mihalcea et al ., 2004).<papid> C04-1162 </papid>the interest of applying natural language processing techniques in the area of political science has been recently increasing.</citsent>
<aftsection>
<nextsent>(quinn et al , 2006) introduce multinomial mixture model to cluster political speeches into topics or related categories.
</nextsent>
<nextsent>in (porter et al , 2005), network analysis of the members and committees of the us house of representatives is performed.the authors prove that there are connections linking some political positions to certain committees.this suggests that there are factors affecting committee membership and that they are not determined at random.
</nextsent>
<nextsent>in (thomas et al , 2006), the authors try to automatically classify speeches, fromthe us congress debates, as supporting or opposing given topic by taking advantage of the voting records of the speakers.
</nextsent>
<nextsent>(fader et al , 2007) <papid> D07-1069 </papid>introduce mavenrank , which is method based on lexical centrality that identifies the most influential members of the us senate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B623">
<title id=" C08-1040.xml">tracking the dynamic evolution of participants salience in a discussion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several methods have been proposed for identifying the most central nodes in network.
</prevsent>
<prevsent>degree centrality, closeness, and between ness (newman, 2003) are among the most known methods for measuring centrality of nodes in network.
</prevsent>
</prevsection>
<citsent citstr=" C04-1162 ">
eigen vector centrality is another powerful method that that has been applied to several types of networks.for example it has been used to measure centrality in hyper linked web pages networks (brinand page, 1998; kleinberg, 1998), lexical networks (erkan and radev, 2004; mihalcea and ta rau, 2004; <papid> W04-3252 </papid>kurland and lee, 2005; kurland and lee, 2006), and semantic networks (mihalcea et al ., 2004).<papid> C04-1162 </papid>the interest of applying natural language processing techniques in the area of political science has been recently increasing.</citsent>
<aftsection>
<nextsent>(quinn et al , 2006) introduce multinomial mixture model to cluster political speeches into topics or related categories.
</nextsent>
<nextsent>in (porter et al , 2005), network analysis of the members and committees of the us house of representatives is performed.the authors prove that there are connections linking some political positions to certain committees.this suggests that there are factors affecting committee membership and that they are not determined at random.
</nextsent>
<nextsent>in (thomas et al , 2006), the authors try to automatically classify speeches, fromthe us congress debates, as supporting or opposing given topic by taking advantage of the voting records of the speakers.
</nextsent>
<nextsent>(fader et al , 2007) <papid> D07-1069 </papid>introduce mavenrank , which is method based on lexical centrality that identifies the most influential members of the us senate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B624">
<title id=" C08-1040.xml">tracking the dynamic evolution of participants salience in a discussion </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in (porter et al , 2005), network analysis of the members and committees of the us house of representatives is performed.the authors prove that there are connections linking some political positions to certain committees.this suggests that there are factors affecting committee membership and that they are not determined at random.
</prevsent>
<prevsent>in (thomas et al , 2006), the authors try to automatically classify speeches, fromthe us congress debates, as supporting or opposing given topic by taking advantage of the voting records of the speakers.
</prevsent>
</prevsection>
<citsent citstr=" D07-1069 ">
(fader et al , 2007) <papid> D07-1069 </papid>introduce mavenrank , which is method based on lexical centrality that identifies the most influential members of the us senate.</citsent>
<aftsection>
<nextsent>it computes single salience score for each speaker that is constant over time.
</nextsent>
<nextsent>in this paper, we introduce new method for tracking the evolution of the salience of participants in discussion over time.
</nextsent>
<nextsent>our method is based on the ones described in (erkan and radev, 2004; mihalcea and tarau, 2004; <papid> W04-3252 </papid>fader et al ,2007), <papid> D07-1069 </papid>the objective of this paper is to dynamically rank speakers or participants in discussion.</nextsent>
<nextsent>the proposed method is dynamic in the sense that the computed importance varies over time.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B630">
<title id=" C08-1040.xml">tracking the dynamic evolution of participants salience in a discussion </title>
<section> speaker centrality.  </section>
<citcontext>
<prevsection>
<prevsent>the tf-idf cosine similarity measure is compute das the cosine of the angle between the tf-idf vectors.
</prevsent>
<prevsent>it is defined as follows: wu,v tf (w) tf (w) idf(w) 2 ? wu (tf (w) idf(w)) 2 ? wv (tf (w) idf(w)) 2 , (2) the choice of tf-idf scores to measure speech similarity is an arbitrary choice.
</prevsent>
</prevsection>
<citsent citstr=" N06-1061 ">
some other possible similarity measures are edit distance, language models (kurland and lee, 2005), or generation probabilities (erkan, 2006).<papid> N06-1061 </papid></citsent>
<aftsection>
<nextsent>the recursive definition of the score of any speech in the speeches network is given by p(s) = ? tadj[s] p(t) deg(t) (3) where deg(t) is the degree of node t, and adj[s] is the set of all speeches adjacent to in the network.
</nextsent>
<nextsent>this can be rewritten in matrix notation as: = pb (4) where = (p(s 1 ), p(s 2 ), . . .
</nextsent>
<nextsent>, p(s n)) and thema trix is the row normalized similarity matrix of the graph b(i, j) = s(i, j) ? s(i, k) (5) where s(i, j) = sim(s , j ).
</nextsent>
<nextsent>equation (4) shows that the vector of salience scores is the left eigen vector of with eigenvalue 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B631">
<title id=" C04-1043.xml">unificational combinatory categorial grammar combining information structure and discourse representations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there are twomain reasons for this: (1) formalisations of information structure often use variants of higher order logic to characterise its semantic impact(krifka, 1993; kruijff-korbayova, 1998; steedman, 2000), which limits the use of inference in practice (blackburn and bos, 2003); and (2)the effect of information structure on the compositional semantics of an utterance is rarely worked out in enough detail useful for computational implementation.
</prevsent>
<prevsent>on the other hand,exploring information structure in spoken dialogue systems is becoming realistic now because of the recent advances made in text-to-speech synthesisers and automated speech recognisers?
</prevsent>
</prevsection>
<citsent citstr=" C88-1018 ">
hence there is growing need for computational implementations of information structure in grammar formalisms.in this paper we present unificational com bina tory categorial grammar (uccg), which integrates aspects of combinatory categorial grammar (steedman, 2000), unification categorial grammar (zeevat, 1988; calder et al, 1988), <papid> C88-1018 </papid>and discourse representation theory(kamp and reyle, 1993).</citsent>
<aftsection>
<nextsent>it offers compositional analysis of information structure, semantics compatible with first-order logic, and computational implementation for fragment of english, using unification in combining grammatical categories.
</nextsent>
<nextsent>as we will show, this makesuccg easy to implement, and allows us to integrate prosodic information in the semantics in transparent and systematic way.
</nextsent>
<nextsent>although based on first-order logic, we claim that uccghas enough expressive power to model information structure such that it has the potential to improve speech generation with context appropriate intonation in spoken dialogue systems.
</nextsent>
<nextsent>categorial grammars (cg) (wood, 2000) are lexicalised theories of grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B633">
<title id=" C02-1150.xml">learning question classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we learn hierarchical classifier that is guided by layered semantic hierarchy of answer types, and eventually classifies questions into fine grained classes.
</prevsent>
<prevsent>we show accurate results on large collection of free-form questions used in trec 10.
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
open-domain question answering (lehnert, 1986; harabagiu et al, 2001; light et al, 2001) and story comprehension (hirschman et al, 1999) <papid> P99-1042 </papid>have become important directions in natural language pro cessing.</citsent>
<aftsection>
<nextsent>question answering is retrieval task more challenging than common search engine tasks be cause its purpose is to find an accurate and concise answer to question rather than relevant document.
</nextsent>
<nextsent>the difficulty is more acute in tasks such as story comprehension in which the target text is less likely to overlap with the text in the questions.
</nextsent>
<nextsent>for this reason, advanced natural language techniques rather than simple key term extraction are needed.one of the important stages in this process is analyzing the question to degree that allows determining the type?
</nextsent>
<nextsent>of the sought after answer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B634">
<title id=" C02-1150.xml">learning question classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the trec competition (voorhees, 2000), participants are requested to build system which, given set of english questions, can automatically extract answers (a short phrase) of no more than 50 bytes from a5-gigabyte document library.
</prevsent>
<prevsent>participants have re research supported by nsf grants iis-9801638 and itr iis 0085836 and an onr muri award.
</prevsent>
</prevsection>
<citsent citstr=" H01-1069 ">
alized that locating an answer accurately hinges on first filtering out wide range of candidates (hovy et al, 2001; <papid> H01-1069 </papid>ittycheriah et al, 2001) based on some categorization of answer types.</citsent>
<aftsection>
<nextsent>this work develops machine learning approach to question classification (qc) (harabagiu et al, 2001; hermjakob, 2001).<papid> W01-1203 </papid></nextsent>
<nextsent>our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answeringprocess.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B636">
<title id=" C02-1150.xml">learning question classifiers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>participants have re research supported by nsf grants iis-9801638 and itr iis 0085836 and an onr muri award.
</prevsent>
<prevsent>alized that locating an answer accurately hinges on first filtering out wide range of candidates (hovy et al, 2001; <papid> H01-1069 </papid>ittycheriah et al, 2001) based on some categorization of answer types.</prevsent>
</prevsection>
<citsent citstr=" W01-1203 ">
this work develops machine learning approach to question classification (qc) (harabagiu et al, 2001; hermjakob, 2001).<papid> W01-1203 </papid></citsent>
<aftsection>
<nextsent>our goal is to categorize questions into different semantic classes that impose constraints on potential answers, so that they can be utilized in later stages of the question answeringprocess.
</nextsent>
<nextsent>for example, when considering the question q: what canadian city has the largest popula tion?, the hope is to classify this question as having answer type city, implying that only candidate answers that are cities need consideration.based on the snow learning architecture, we develop hierarchical classifier that is guided by layered semantic hierarchy of answer types and is able to classify questions into fine-grained classes.
</nextsent>
<nextsent>we suggest that it is useful to consider this classification task as multi-label classification and find that it is possible to achieve good classification results(over 90%) despite the fact that the number of different labels used is fairly large, 50.
</nextsent>
<nextsent>we observe that local features are not sufficient to support this accuracy, and that inducing semantic features is crucial for good performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B639">
<title id=" C02-1150.xml">learning question classifiers </title>
<section> what is the ph scale?.  </section>
<citcontext>
<prevsection>
<prevsent>among the 6 primitive feature types, pos tags, chunks and head chunks are syntactic features while named entities and semantically related words are semantic features.
</prevsent>
<prevsent>pos tags are extracted using snow-based pos tagger (even-zohar and roth, 2001).
</prevsent>
</prevsection>
<citsent citstr=" W01-0706 ">
chunks are extracted using previously learned classifier (punyakanok and roth, 2001; li and roth, 2001).<papid> W01-0706 </papid></citsent>
<aftsection>
<nextsent>the named entity classifier isalso learned and makes use of the same technology developed for the chunker (roth et al, 2002).the related word?
</nextsent>
<nextsent>sensors were constructed semi automatically.
</nextsent>
<nextsent>most question classes have semantically related word list.
</nextsent>
<nextsent>features will be extracted for this class ifa word in question belongs to the list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B641">
<title id=" C04-1099.xml">query translation by text categorization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the two main problems are: ? it can be difficult for users to think in terms of controlled vocabulary, therefore theuse of these systems -like most boolean supported engines- is often performed by professionals rather than general users; ? this retrieval method ignores the free-text portions of documents during indexing.
</prevsent>
<prevsent>1.1 translation-based approach.
</prevsent>
</prevsection>
<citsent citstr=" P99-1027 ">
a second approach to multilingual interrogation is to use existing machine translation (mt) systems to automatically translate the queries (davis, 1998), or even the entire textual database (oard and hackett, 1998) (mccarley, 1999) <papid> P99-1027 </papid>from one language to another, thereby transforming the clir problem into monolingual information retrieval (mlir) problem.</citsent>
<aftsection>
<nextsent>this kind of method would be satisfactory if current mt systems did not make errors.
</nextsent>
<nextsent>acer tain amount of syntactic error can be accepted without perturbing results of information retrieval systems, but mt errors in translating concepts can prevent relevant documents, indexed on the missing concepts, from being found.
</nextsent>
<nextsent>for example, if the word traitement in french is translated by processing instead of prescription, the retrieval process would yield wrong results.
</nextsent>
<nextsent>this drawback is limited in mt systems that use huge transfer lexicons of noun phrases by taking advantage of frequent collocations to help disambiguation, but in any collection of text, ambiguous nouns will still appear as isolated nouns phrases untouched by this approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B642">
<title id=" C04-1099.xml">query translation by text categorization </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>vector space classifier.
</prevsent>
<prevsent>the vector space module is based on general ir engine with tf.idf 6 weighting schema.
</prevsent>
</prevsection>
<citsent citstr=" C02-1109 ">
the engine uses list of 544 stop words.as for setting the weighting factors, we ob 6we use the smart representation for expressing statistical weighting factors: formal description can be found in (ruch, 2002).<papid> C02-1109 </papid></citsent>
<aftsection>
<nextsent>served that cosine normalization was especially effective for our task.
</nextsent>
<nextsent>this is not surprising, considering the fact that cosine normalization performs well when documents have similar length (singhal et al, 1996).
</nextsent>
<nextsent>as for the respective performance of each basic classifiers, table 1 shows that the regex system performs better than any tf.idf schema used by the vs engine, so the pattern matcher provide better results than the vector space engine for automatic text categorization.
</nextsent>
<nextsent>however, we also observe in table 1 that the vs system gives better precision at high ranks (precisionat recall=0or mean reciprocal rank) than the regex system: this difference suggests that merging the classifiers could be effective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B643">
<title id=" C08-1010.xml">a classification of dialogue actions in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our taxonomy has been prepared by analysing acorpus of tutorial dialogues on mathematical theorem proving.
</prevsent>
<prevsent>we also detail an annotation experiment in which we apply the taxonomy and discuss idiosyncrasies in the data which influence the decisions in the dialogue move classification.
</prevsent>
</prevsection>
<citsent citstr=" N04-3002 ">
the field of intelligent tutoring systems has seen recent developments moving towards adding natural language capabilities to computer-based tutoring (graesser et al, 1999; zinn, 2004; litman and silliman, 2004), <papid> N04-3002 </papid>motivated by empirical investigations which point to the effectiveness of human tutors (bloom, 1984; moore, 1993; graesser et al,1995).</citsent>
<aftsection>
<nextsent>however, to be able to interact with student through the medium of natural language dialogue, the system must have model of how such tutorial dialogues can progress and what utterances are licenced.
</nextsent>
<nextsent>in order to develop such model of dialogue, we need to understand and describe the actions?
</nextsent>
<nextsent>performed with words, i.e. speech acts (austin, 1955) or dialogue moves.
</nextsent>
<nextsent>this involves identifying and categorising the functions ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B644">
<title id=" C08-1010.xml">a classification of dialogue actions in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>theda msl dialogue act taxonomy characterises utterances along four dimensions which correspond to four levels of functions utterances may have.
</prevsent>
<prevsent>the forward looking function describes the utterances effect on the following interaction, the backward looking function, its relation to previous dialogue,the communicative status describes the compre hensibility or interpret ability of the utterance, and the information level characterises the content of the utterance.
</prevsent>
</prevsection>
<citsent citstr=" W04-2307 ">
tsovaltzi and karagjosova (2004) <papid> W04-2307 </papid>proposed an extension of the damsl classification based on an analysis of tutorial dialogue corpora.</citsent>
<aftsection>
<nextsent>the proposed taxonomy adds task dimension which concentrates on tutor actions in the dialogue.
</nextsent>
<nextsent>1building on this work, we propose further extension to this taxonomy inspired by our analysis from the point of view of task-related goals.
</nextsent>
<nextsent>the classification we present (i) includes modifications of the damsl categorisation motivated by tutorial dialogue, (ii) accounts for students actions, and (iii) introduces task progress dimension whose purpose is to characterise the completion status ofa generally viewed task?, instantiated for the purpose of tutorial dialogue.
</nextsent>
<nextsent>we validated our dialogue act categorisation in small-scale annotation experiment whose results we present.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B647">
<title id=" C04-1082.xml">tagging with hidden markov models using ambiguous tags </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parser itself acts, in sense, as tagger since, while parsing the sentence, it chooses the right tag amonga set of possible tags for each word.
</prevsent>
<prevsent>the reason why we still need tagger and dont let the parser do the job is time and space complexity.parsers are usually more time and space consuming than taggers and highly ambiguous tags assignments can lead to prohibitive processing time and memory requirements.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the tagger described in this paper is basedon the standard hidden markov model architecture (charniak et al, 1993; brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>such taggers assign to sequence of words = w1 . . .
</nextsent>
<nextsent>wn , the part of speech tag sequencet?
</nextsent>
<nextsent>= t1 . . .
</nextsent>
<nextsent>tn which maximizes the joint probability (t,w ) where ranges over all possible tag sequences of length n. the probability (t,w ) is itself decomposed into product of 2n probabilities, lexical probabilities (wi|ti)(emission probabilities of the hmm) and syntactic probabilites (transition probabilities of thehmm).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B648">
<title id=" C04-1082.xml">tagging with hidden markov models using ambiguous tags </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>et al, 2000) reports experiments of pos tagging of hungarian with large tagset (about one thousand different tags).
</prevsent>
<prevsent>in order to reduce data sparseness problems, they devise reduced tagset which isused for tagging.
</prevsent>
</prevsection>
<citsent citstr=" P95-1039 ">
the same kind of idea is developed in (brants, 1995).<papid> P95-1039 </papid></citsent>
<aftsection>
<nextsent>the major difference between these approaches and ours, is that they devise the reduced tagset in such way that, after tagging, unique tag of the extended tagset can be recovered for each word.
</nextsent>
<nextsent>our perspective is significantly different since we allow unrecoverable ambiguity in the output of the tagger and leave to the other processing stages the task of reducing it.
</nextsent>
<nextsent>in the hmm based taggers framework, our work bears certain resemblance with(brants, 2000) <papid> A00-1031 </papid>who distinguishes between reliable and unreliable tag assignments using probabilities computed by the tagger.</nextsent>
<nextsent>unreliable tag assignments are those for which the probability is below given threshold.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B650">
<title id=" C04-1082.xml">tagging with hidden markov models using ambiguous tags </title>
<section> learning ambiguous tags from.  </section>
<citcontext>
<prevsection>
<prevsent>the process is iterated : the training corpus is tagged with mi, the most frequent error is used to constitue ti+1 and new tagger mi+1 is built, based on mi.the process continues until the result of the tagging on the development corpus converges or the number of iterations has reached given threshold.
</prevsent>
<prevsent>3.1 experiments.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the model described in section 2 has been tested on the brown corpus (francis and kucera, 1982), tagged with the 45 tags of the penn treebank tagset (marcus et al, 1993),<papid> J93-2004 </papid>which constitute the initial tagset t0.</citsent>
<aftsection>
<nextsent>the corpus has been divided in training corpus of 961, 3 words, development corpus of 118, 6 words and test corpus of 115, 6 words.
</nextsent>
<nextsent>the development corpus was used to detect the convergence and the final model was evaluated on the test corpus.
</nextsent>
<nextsent>the iterative tag learning algorithm converged after 50 iterations.a standard trigram model (without ambiguous tags) m0 was trained on the training corpus using the cmu-cambridge statistical language modeling toolkit (clarkson and rosenfeld, 1997).
</nextsent>
<nextsent>smoothing was done through backoff on bigrams and unigrams using linear discounting (ney et al, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B651">
<title id=" C04-1082.xml">tagging with hidden markov models using ambiguous tags </title>
<section> nn_vb 14 jj_nn_nnp.  </section>
<citcontext>
<prevsection>
<prevsent>an analysis of ambiguous tags showed that they do not always behave in the way expected; some of them introduce lot of ambiguity without correcting many mistakes.
</prevsent>
<prevsent>this work will be developed in two directions.the first one concerns the study of the different behaviour of ambiguous tags which couldbe influenced by computing differently the fictitious counts of each ambiguous tag, based on its behaviour on development corpus in order to force or prevent its introduction during tagging.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
the second direction concerns experiments onsupertagging (bangalore and joshi, 1999) <papid> J99-2004 </papid>followed by parsing stage the tagging stage associates to each word supertag.</citsent>
<aftsection>
<nextsent>the supertags are then combined by the parser to yield parseof the sentence.
</nextsent>
<nextsent>errors of the super tagger (al most one out of 5 words is attributed the wrong supertag) often impede the parsing stage.
</nextsent>
<nextsent>the idea is therefore to allow some ambiguity during the super tagging stage, leaving to the parser thetask of selecting the right super tag using syntactic constraints that are not available to the tagger.
</nextsent>
<nextsent>such experiments will constitute one way of testing the viability of our approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B652">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the canadian hansa rds task, we achieve an improvement of more than 30% relative.
</prevsent>
<prevsent>word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
obvious applications are the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>these applications depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></nextsent>
<nextsent>word alignment models were first introduced in statistical machine translation (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B653">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the canadian hansa rds task, we achieve an improvement of more than 30% relative.
</prevsent>
<prevsent>word-aligned bilingual corpora are an important knowledge source for many tasks in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
obvious applications are the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>these applications depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></nextsent>
<nextsent>word alignment models were first introduced in statistical machine translation (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B659">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>obvious applications are the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></prevsent>
<prevsent>these applications depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
word alignment models were first introduced in statistical machine translation (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>the alignment describes the mapping from source sentence words to target sentence words.
</nextsent>
<nextsent>using the ibm translation models ibm-1 to ibm-5 (brown et al, 1993), <papid> J93-2003 </papid>as well as the hidden-markov alignment model (vogel et al, 1996), <papid> C96-2141 </papid>we can produce alignments of good quality.</nextsent>
<nextsent>in (och and ney, 2003), <papid> J03-1002 </papid>it is shown that the statistical approach performs very well compared to alternative approaches,e.g. based on the dice coefficient or the competitive linking algorithm (melamed, 2000).<papid> J00-2004 </papid>a central component of the statistical translation models is the lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B663">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment models were first introduced in statistical machine translation (brown et al, 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>the alignment describes the mapping from source sentence words to target sentence words.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
using the ibm translation models ibm-1 to ibm-5 (brown et al, 1993), <papid> J93-2003 </papid>as well as the hidden-markov alignment model (vogel et al, 1996), <papid> C96-2141 </papid>we can produce alignments of good quality.</citsent>
<aftsection>
<nextsent>in (och and ney, 2003), <papid> J03-1002 </papid>it is shown that the statistical approach performs very well compared to alternative approaches,e.g. based on the dice coefficient or the competitive linking algorithm (melamed, 2000).<papid> J00-2004 </papid>a central component of the statistical translation models is the lexicon.</nextsent>
<nextsent>it models the word translation probabilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B664">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the alignment describes the mapping from source sentence words to target sentence words.
</prevsent>
<prevsent>using the ibm translation models ibm-1 to ibm-5 (brown et al, 1993), <papid> J93-2003 </papid>as well as the hidden-markov alignment model (vogel et al, 1996), <papid> C96-2141 </papid>we can produce alignments of good quality.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
in (och and ney, 2003), <papid> J03-1002 </papid>it is shown that the statistical approach performs very well compared to alternative approaches,e.g. based on the dice coefficient or the competitive linking algorithm (melamed, 2000).<papid> J00-2004 </papid>a central component of the statistical translation models is the lexicon.</citsent>
<aftsection>
<nextsent>it models the word translation probabilities.
</nextsent>
<nextsent>the standard training procedure of the statistical models uses the em algorithm.
</nextsent>
<nextsent>typically, the models are trained for one translation direction only.
</nextsent>
<nextsent>here, we will perform simultaneous training of both translation directions, source-to-target and target-to-source.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B721">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we chose them as baseline.
</prevsent>
<prevsent>compared to our work, these publications kept the training of the two translation directions strictly separate whereas we integrate both directions into one symmetrized training.
</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
additional linguistic knowledge sources such as dependency trees or parse trees were used in (cherry and lin, 2003) <papid> P03-1012 </papid>and (gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>in (cherry and lin, 2003) <papid> P03-1012 </papid>probability model pr(aj1 |fj1 , ei1) isused, which is symmetric per definition.</nextsent>
<nextsent>bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B723">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we chose them as baseline.
</prevsent>
<prevsent>compared to our work, these publications kept the training of the two translation directions strictly separate whereas we integrate both directions into one symmetrized training.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
additional linguistic knowledge sources such as dependency trees or parse trees were used in (cherry and lin, 2003) <papid> P03-1012 </papid>and (gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>in (cherry and lin, 2003) <papid> P03-1012 </papid>probability model pr(aj1 |fj1 , ei1) isused, which is symmetric per definition.</nextsent>
<nextsent>bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B726">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>additional linguistic knowledge sources such as dependency trees or parse trees were used in (cherry and lin, 2003) <papid> P03-1012 </papid>and (gildea, 2003).<papid> P03-1011 </papid></prevsent>
<prevsent>in (cherry and lin, 2003) <papid> P03-1012 </papid>probability model pr(aj1 |fj1 , ei1) isused, which is symmetric per definition.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>(melamed, 2000) <papid> J00-2004 </papid>uses an alignment model that enforces one-to-one alignments for non empty words.</nextsent>
<nextsent>in 2the base forms were determined using ling soft tools.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B728">
<title id=" C04-1006.xml">improved word alignment using a symmetric lexicon model </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>task: verb mobil canadian hansa rds precision[%] recall[%] aer[%] precision[%] recall[%] aer[%] base 93.3 96.0 5.5 96.6 86.0 8.2 lin.
</prevsent>
<prevsent>96.1 94.0 4.9 95.2 88.5 7.7 loglin.
</prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
95.2 95.3 4.7 93.6 90.8 7.5 (toutanova et al, 2002), <papid> W02-1012 </papid>extensions to the hmm-based alignment model are presented.</citsent>
<aftsection>
<nextsent>we have addressed the task of automatically generating word alignments for bilingual corpora.
</nextsent>
<nextsent>this problem is of great importance formany tasks in natural language processing, especially in the field of machine translation.
</nextsent>
<nextsent>we have presented lexicon symmetrization methods for statistical alignment models thatare trained using the em algorithm, in particular the five ibm models, the hmm and model 6.
</nextsent>
<nextsent>we have evaluated these methods on the verb mobil task and the canadian hansa rds task and compared our results to the state-of-the-art system of (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B734">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sentence ei1 =e1 . . .
</prevsent>
<prevsent>ei . . .
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
ei . among all possible target language sentences, we will choose the sentence with the highest probability: ei1 = argmax ei1 {pr(ei1|fj1 ) } = argmax ei1 {pr(ei1) ? pr(fj1 |ei1) } this decomposition into two knowledge sources is known as the source-channel approach to statistical machine translation (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>it allows an independent modeling of target language model pr(ei1) and translation model pr(fj1 |ei1).
</nextsent>
<nextsent>the target lan-guage model describes the well-formedness of the target language sentence.
</nextsent>
<nextsent>the translation model links the source language sentence tothe target language sentence.
</nextsent>
<nextsent>it can be further decomposed into alignment and lexicon model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B735">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the argmax operation denotes the search problem, i.e. the generation of the out put sentence in the target language.
</prevsent>
<prevsent>we have to maximize over all possible target language sentences.an alternative to the classical source channel approach is the direct modeling of the posterior probability pr(ei1|fj1 ).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
using log-linear model (och and ney, 2002), <papid> P02-1038 </papid>we obtain: pr(ei1|fj1 ) = exp ( m?</citsent>
<aftsection>
<nextsent>m=1 mhm(ei1, fj1 ) ) ? z(fj1 ) here, z(fj1 ) denotes the appropriate normal-ization constant.
</nextsent>
<nextsent>as decision rule, we obtain: ei1 = argmax ei1 { m?
</nextsent>
<nextsent>m=1 mhm(ei1, fj1 ) } this approach is generalization of thesource-channel approach.
</nextsent>
<nextsent>it has the advantage that additional models or feature functions can be easily integrated into the over all system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B736">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it has the advantage that additional models or feature functions can be easily integrated into the over all system.
</prevsent>
<prevsent>the model scaling factors m1 are trained according to the maximum entropy principle, e.g. using the gis algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
alternatively, one can train them with respect to the final translation quality measured by some error criterion (och, 2003).<papid> P03-1021 </papid>in this paper, we will investigate there ordering problem for phrase-based translation approaches.</citsent>
<aftsection>
<nextsent>as the word order in source and target language may differ, the search algorithm has to allow certain reorderings.
</nextsent>
<nextsent>if arbitrary reorderings are allowed, the search problem is np-hard (knight, 1999).<papid> J99-4005 </papid></nextsent>
<nextsent>to obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B737">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternatively, one can train them with respect to the final translation quality measured by some error criterion (och, 2003).<papid> P03-1021 </papid>in this paper, we will investigate there ordering problem for phrase-based translation approaches.</prevsent>
<prevsent>as the word order in source and target language may differ, the search algorithm has to allow certain reorderings.</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
if arbitrary reorderings are allowed, the search problem is np-hard (knight, 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>to obtain an efficient search algorithm, we can either restrict the possible reorderings or we have to use an approximation algorithm.
</nextsent>
<nextsent>note that in the latter case we cannot guarantee to find an optimal solution.the remaining part of this work is structured as follows: in the next section, we will review the baseline translation system,namely the alignment template approach.
</nextsent>
<nextsent>afterward, we will describe different reorderingconstraints.
</nextsent>
<nextsent>we will begin with the ibm constraints for phrase-based translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B738">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> alignment template approach.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4, we will present results for two japanese english translation tasks.
</prevsent>
<prevsent>in this section, we give brief description of the translation system, namely the alignment template approach.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
the key elements of this translation approach (och et al, 1999) <papid> W99-0604 </papid>are the alignment templates.</citsent>
<aftsection>
<nextsent>these are pairs of source and target language phrases with an alignment within the phrases.
</nextsent>
<nextsent>the alignment templates are build at the level of word classes.
</nextsent>
<nextsent>this improves the generalization capability of the alignment templates.
</nextsent>
<nextsent>we use maximum entropy to train the model scaling factors (och and ney, 2002).<papid> P02-1038 </papid>as feature functions we use phrase translation model as well as word translation model.additionally, we use two language model feature functions: word-based trigram mod eland class-based five-gram model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B744">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> reordering constraints.  </section>
<citcontext>
<prevsection>
<prevsent>the ibm constraints are illustrated in figure 1.
</prevsent>
<prevsent>for further details see e.g.
</prevsent>
</prevsection>
<citsent citstr=" J03-1005 ">
(tillmann and ney, 2003).<papid> J03-1005 </papid></citsent>
<aftsection>
<nextsent>for the phrase-based translation approach, we use the same idea.
</nextsent>
<nextsent>the target sentence is produced phrase by phrase.
</nextsent>
<nextsent>now, we allow skipping of up to phrases.
</nextsent>
<nextsent>if we set = 0, we obtain search that is monotone at the phrase level as special case.the search problem can be solved using dynamic programming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B746">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> reordering constraints.  </section>
<citcontext>
<prevsection>
<prevsent>setting = 0 results in search algorithm that is monotone at the phrase level.
</prevsent>
<prevsent>3.2 itg constraints.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
in this section, we describe the itg constraints (wu, 1995; wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>here, we interpret the input sentence as sequence of blocks.
</nextsent>
<nextsent>in the beginning, each alignment template is block of its own.
</nextsent>
<nextsent>then, the reordering process can be interpreted as follows: we select two consecutive blocks and merge them to single block by choosing between two options: either keep the target phrases in monotone order or invert the order.
</nextsent>
<nextsent>this idea is illustrated in figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B747">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> reordering constraints.  </section>
<citcontext>
<prevsection>
<prevsent>the idea is to start with the beam search decoder for unconstrained search and modify it in such way that it will produce only reorderings that do not violate the itgconstraints.
</prevsent>
<prevsent>now, we describe one way to obtain such decoder.
</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
it has been pointed outin (zens and ney, 2003) <papid> P03-1019 </papid>that the itg constraints can be characterized as follows: reordering violates the itg constraints if and only if it contains (3, 1, 4, 2) or (2, 4, 1, 3) as subsequence.</citsent>
<aftsection>
<nextsent>this means, if we select four columns and the corresponding rows from the alignment matrix and we obtain one of the two patterns illustrated in figure 4, this reordering cannot be generated with the itg constraints.
</nextsent>
<nextsent>now, we have to modify the beam search decoder such that it cannot produce these twopatterns.
</nextsent>
<nextsent>we implement this in the following way.
</nextsent>
<nextsent>during the search, we have cover age vector cov of the source sentence available for each partial hypothesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B748">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>the per compares the words in the two sentences ignoring the word order.
</prevsent>
<prevsent>bleu.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
this score measures the precision of unigrams, bigrams, trigrams and four grams with respect to reference translation with penalty for too short sentences (papineni etal., 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>the bleu score measures accuracy, i.e. large bleu scores are better.
</nextsent>
<nextsent>nist.
</nextsent>
<nextsent>this score is similar to bleu.
</nextsent>
<nextsent>it is weighted n-gram precision in combination with penalty for too short sentences (doddington, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B749">
<title id=" C04-1030.xml">reordering constraints for phrase based statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared to the monotone search, the bleu score for the itg constraints improves from 54.4% to 57.1%.
</prevsent>
<prevsent>recently, phrase-based translation approaches became more and more popular.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
marcu and wong (2002) <papid> W02-1018 </papid>present joint probability model for phrase-based translation.</citsent>
<aftsection>
<nextsent>in (koehn et 1the statistical significance test were done for the wer using boo strap resampling.
</nextsent>
<nextsent>table 5: translation performance wer[%] for the sldb task (330 sentences).
</nextsent>
<nextsent>sentence lengths: short:   10 words, long: ? 10 words; times in milliseconds per sentence.
</nextsent>
<nextsent>wer[%] sentence length reorder short long all time[ms] mon 32.0 52.6 48.1 911 skip 1 31.9 51.1 46.9 3 175 2 32.0 51.4 47.2 4 549 free 32.0 51.4 47.2 4 993 itg 31.8 50.9 46.7 4 472 table 6: translation performance for the sldb task (330 sentences).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B753">
<title id=" C08-1061.xml">classifying what type questions by head noun tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it classifies questions into several semantic categories which indicate the expected semantic type of answers to the questions.
</prevsent>
<prevsent>the semantic category helps to filter out irrelevant answer candidates, and determine the answer selection strategies.
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
1 the widely used question category criteria is two-layered taxonomy developed by li and roth (2002) <papid> C02-1150 </papid>from uiuc.</citsent>
<aftsection>
<nextsent>the hierarchy contains 6 coarse classes and 50 fine classes as shown in table 1.
</nextsent>
<nextsent>in this paper, we focus on fine-category classification.
</nextsent>
<nextsent>each fine category will be denoted as coarse:fine?, such as hum:individual?.
</nextsent>
<nextsent>a what-type question is defined as the one whose question word is what?, which?, name?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B759">
<title id=" C08-1061.xml">classifying what type questions by head noun tagging </title>
<section> feature sets.  </section>
<citcontext>
<prevsection>
<prevsent>besides words, part-of-speech, chunker, parser information and question length are used as syntactic features.
</prevsent>
<prevsent>all the words are lemmatized to root forms, and window size (here is 4) is set to utilize the surrounding words.
</prevsent>
</prevsection>
<citsent citstr=" H05-1059 ">
the part-of-speech (pos) tagging is completed by ss tagger (tsuruoka and tsujii, 2005), <papid> H05-1059 </papid>with our own improvement.</citsent>
<aftsection>
<nextsent>the noun phrase chunking (np chunking) module uses the basic np chunker software from 483 (ramshaw and marcus, 1995) <papid> W95-0107 </papid>to recognize the noun phrases in the question.</nextsent>
<nextsent>the importance of question syntactic structure is reported in (zhang and lee, 2002; nguyen et al. 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B760">
<title id=" C08-1061.xml">classifying what type questions by head noun tagging </title>
<section> feature sets.  </section>
<citcontext>
<prevsection>
<prevsent>all the words are lemmatized to root forms, and window size (here is 4) is set to utilize the surrounding words.
</prevsent>
<prevsent>the part-of-speech (pos) tagging is completed by ss tagger (tsuruoka and tsujii, 2005), <papid> H05-1059 </papid>with our own improvement.</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
the noun phrase chunking (np chunking) module uses the basic np chunker software from 483 (ramshaw and marcus, 1995) <papid> W95-0107 </papid>to recognize the noun phrases in the question.</citsent>
<aftsection>
<nextsent>the importance of question syntactic structure is reported in (zhang and lee, 2002; nguyen et al. 2007).
</nextsent>
<nextsent>they used complex machine learning method to capture the tree architecture.
</nextsent>
<nextsent>the ldcrfs based approach just selects parent node, relation with parent and governor for each target word generated from minipar(lin, 1999).
</nextsent>
<nextsent>the length of question is another important syntactic feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B761">
<title id=" C08-1061.xml">classifying what type questions by head noun tagging </title>
<section> feature sets.  </section>
<citcontext>
<prevsection>
<prevsent>named entity: named entity recognizer assigns semantic category to the noun phrase.
</prevsent>
<prevsent>it is widely used to provide semantic information in text mining.
</prevsent>
</prevsection>
<citsent citstr=" P05-1045 ">
in this paper, stanford named entity recognizer (finkel et al 2005) <papid> P05-1045 </papid>is used to classify noun phrases into four semantic categories: person, location, organizarion and misc.</citsent>
<aftsection>
<nextsent>noun hypernym: hypernyms can be considered as semantic abstractions.
</nextsent>
<nextsent>it helps to narrow the gap between training set and testing set.
</nextsent>
<nextsent>for example, what is maryland state bird??, if we recursively find the birds hypernym animal?, which appeared in training set, this question can be easily classified.
</nextsent>
<nextsent>in training set, we try to select appropriate hypernyms for each category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B775">
<title id=" C02-1008.xml">a transitive model for extracting translation equivalents of web queries through anchor text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these systems, queries submitted in source language normally have to be translated into target language by means of simple dictionary lookup.
</prevsent>
<prevsent>these dictionary-based techniques are limited in real-world applications, since the queries given by users often contain proper nouns.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
another kind of popular approaches to dealing with query translation based on corpus-based techniques uses parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (brown et al., 1993; <papid> J93-2003 </papid>dagan et al, 1993; <papid> W93-0301 </papid>smadja et al, 1996).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>although more reliable translation equivalents can be extracted by these techniques, the unavailability of large enough parallel corpora for various subject domains and multiple languages is still in thorny situation.
</nextsent>
<nextsent>on the other hand, the alternative approach using comparable or unrelated text corpora were studied by rapp (1999) <papid> P99-1067 </papid>and fung et al (1998).</nextsent>
<nextsent>this task is more difficult due to lack of parallel correlation between document or sentence pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B776">
<title id=" C02-1008.xml">a transitive model for extracting translation equivalents of web queries through anchor text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these systems, queries submitted in source language normally have to be translated into target language by means of simple dictionary lookup.
</prevsent>
<prevsent>these dictionary-based techniques are limited in real-world applications, since the queries given by users often contain proper nouns.
</prevsent>
</prevsection>
<citsent citstr=" W93-0301 ">
another kind of popular approaches to dealing with query translation based on corpus-based techniques uses parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (brown et al., 1993; <papid> J93-2003 </papid>dagan et al, 1993; <papid> W93-0301 </papid>smadja et al, 1996).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>although more reliable translation equivalents can be extracted by these techniques, the unavailability of large enough parallel corpora for various subject domains and multiple languages is still in thorny situation.
</nextsent>
<nextsent>on the other hand, the alternative approach using comparable or unrelated text corpora were studied by rapp (1999) <papid> P99-1067 </papid>and fung et al (1998).</nextsent>
<nextsent>this task is more difficult due to lack of parallel correlation between document or sentence pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B777">
<title id=" C02-1008.xml">a transitive model for extracting translation equivalents of web queries through anchor text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in these systems, queries submitted in source language normally have to be translated into target language by means of simple dictionary lookup.
</prevsent>
<prevsent>these dictionary-based techniques are limited in real-world applications, since the queries given by users often contain proper nouns.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
another kind of popular approaches to dealing with query translation based on corpus-based techniques uses parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (brown et al., 1993; <papid> J93-2003 </papid>dagan et al, 1993; <papid> W93-0301 </papid>smadja et al, 1996).<papid> J96-1001 </papid></citsent>
<aftsection>
<nextsent>although more reliable translation equivalents can be extracted by these techniques, the unavailability of large enough parallel corpora for various subject domains and multiple languages is still in thorny situation.
</nextsent>
<nextsent>on the other hand, the alternative approach using comparable or unrelated text corpora were studied by rapp (1999) <papid> P99-1067 </papid>and fung et al (1998).</nextsent>
<nextsent>this task is more difficult due to lack of parallel correlation between document or sentence pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B778">
<title id=" C02-1008.xml">a transitive model for extracting translation equivalents of web queries through anchor text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another kind of popular approaches to dealing with query translation based on corpus-based techniques uses parallel corpus containing aligned sentences whose translation pairs are corresponding to each other (brown et al., 1993; <papid> J93-2003 </papid>dagan et al, 1993; <papid> W93-0301 </papid>smadja et al, 1996).<papid> J96-1001 </papid></prevsent>
<prevsent>although more reliable translation equivalents can be extracted by these techniques, the unavailability of large enough parallel corpora for various subject domains and multiple languages is still in thorny situation.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
on the other hand, the alternative approach using comparable or unrelated text corpora were studied by rapp (1999) <papid> P99-1067 </papid>and fung et al (1998).</citsent>
<aftsection>
<nextsent>this task is more difficult due to lack of parallel correlation between document or sentence pairs.
</nextsent>
<nextsent>1 in our collected query logs, most of user queries contain only one or two words, so we use query term, query or term interchangeably in this paper.
</nextsent>
<nextsent>in our previous research we have developed an approach to extracting translations of web queries through mining of web anchor texts and link structures (lu, et al, 2001).
</nextsent>
<nextsent>this approach exploits web anchor texts as live bilingual corpora to reduce the existing difficulties of query translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B779">
<title id=" C02-1008.xml">a transitive model for extracting translation equivalents of web queries through anchor text mining </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although web anchor texts, undoubtedly, are live multilingual resources, not every particular pair of languages contains sufficient anchor texts.
</prevsent>
<prevsent>to deal with the problems, this paper extends the previous anchor-text-based approach by adding phase of indirect translation via an intermediate language.
</prevsent>
</prevsection>
<citsent citstr=" C00-1015 ">
for query term which is unable to be translated, our idea is to translate it into set of translation candidates in an intermediate language, and then seek for the most likely translation from the candidates, which are translated from the intermediate language into the target language (gollins et al, 2001; borin, 2000).<papid> C00-1015 </papid></citsent>
<aftsection>
<nextsent>we therefore propose transitive translation model to further exploit anchor text mining for translating web queries.
</nextsent>
<nextsent>a series of experiments has been conducted to realize the performance of the proposed approach.
</nextsent>
<nextsent>preliminary experimental results show that many query translations which cannot be obtained using the previous approach can be extracted with the improved approach.
</nextsent>
<nextsent>for query translation, the anchor-text-based approach is new technique compared with the bilingual-dictionary- and parallel-corpus-based approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B780">
<title id=" C04-1027.xml">learning theories from text </title>
<section> some background.  </section>
<citcontext>
<prevsection>
<prevsent>note also that the results ofthe theory induction process are perfectly comprehensible - the outcome is theory with some logical structure, rather than black box.
</prevsent>
<prevsent>the method requires fully parsed corpus with corresponding logical forms.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
using similar technique, we have experimented with slightly larger datasets, using the penn tree bank (marcus et al, 1994) <papid> H94-1020 </papid>since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least.</citsent>
<aftsection>
<nextsent>in practice, (liakata and pulman, 2002) <papid> C02-1105 </papid>report, it is by no means easy to do this.</nextsent>
<nextsent>it is possible to recover partial logical forms from large proportion of the treebank, but these are not complete or accurate enough to simply replicate the atis exper iment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B781">
<title id=" C04-1027.xml">learning theories from text </title>
<section> some background.  </section>
<citcontext>
<prevsection>
<prevsent>the method requires fully parsed corpus with corresponding logical forms.
</prevsent>
<prevsent>using similar technique, we have experimented with slightly larger datasets, using the penn tree bank (marcus et al, 1994) <papid> H94-1020 </papid>since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least.</prevsent>
</prevsection>
<citsent citstr=" C02-1105 ">
in practice, (liakata and pulman, 2002) <papid> C02-1105 </papid>report, it is by no means easy to do this.</citsent>
<aftsection>
<nextsent>it is possible to recover partial logical forms from large proportion of the treebank, but these are not complete or accurate enough to simply replicate the atis experiment.
</nextsent>
<nextsent>in the work reported here, we selected about 40 texts containing the verb resign?, all reporting, among other things, company succession?
</nextsent>
<nextsent>events, scenario familiar from the message understanding conference (muc) task (grishman and sundheim, 1995).
</nextsent>
<nextsent>the texts amounted to almost 4000 words in all.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B782">
<title id=" C04-1058.xml">why nitpicking works evidence for occams razor in error cor rectors </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in these nlp systems boosting is typically used as the ultimate stage in learned system.
</prevsent>
<prevsent>for example, shapire and singer(2000) applied it to text categorization while escudero et al(2000) used it to obtain good results on word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W02-2004 ">
more closely relevant to the experiments described here in, two of the best-performingthree teams in the conll-2002 named entity recognition shared task evaluation used boosting as their base system (carreras et al, 2002)(<papid> W02-2004 </papid>wu et al, 2002).<papid> W02-2035 </papid>however, precedents for improving performance after boosting are few.</citsent>
<aftsection>
<nextsent>at the conll-2002 shared task session, tjong kim sang (unpublished) described an experiment using voting to combine the ner outputs from the shared task participants which, predictably, produced better results than the individual systems.
</nextsent>
<nextsent>a couple of the individual systems were boosting models, so in some sense this could be regarded as an example.
</nextsent>
<nextsent>tsukamoto et al(2002) <papid> W02-2031 </papid>used piped adaboost.mhmodels for ner.</nextsent>
<nextsent>their experimental results were some what disappointing, but this could perhaps be attributable to various reasons including the feature engineering or not using cross-validation sampling in the stacking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B783">
<title id=" C04-1058.xml">why nitpicking works evidence for occams razor in error cor rectors </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in these nlp systems boosting is typically used as the ultimate stage in learned system.
</prevsent>
<prevsent>for example, shapire and singer(2000) applied it to text categorization while escudero et al(2000) used it to obtain good results on word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W02-2035 ">
more closely relevant to the experiments described here in, two of the best-performingthree teams in the conll-2002 named entity recognition shared task evaluation used boosting as their base system (carreras et al, 2002)(<papid> W02-2004 </papid>wu et al, 2002).<papid> W02-2035 </papid>however, precedents for improving performance after boosting are few.</citsent>
<aftsection>
<nextsent>at the conll-2002 shared task session, tjong kim sang (unpublished) described an experiment using voting to combine the ner outputs from the shared task participants which, predictably, produced better results than the individual systems.
</nextsent>
<nextsent>a couple of the individual systems were boosting models, so in some sense this could be regarded as an example.
</nextsent>
<nextsent>tsukamoto et al(2002) <papid> W02-2031 </papid>used piped adaboost.mhmodels for ner.</nextsent>
<nextsent>their experimental results were some what disappointing, but this could perhaps be attributable to various reasons including the feature engineering or not using cross-validation sampling in the stacking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B784">
<title id=" C04-1058.xml">why nitpicking works evidence for occams razor in error cor rectors </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>at the conll-2002 shared task session, tjong kim sang (unpublished) described an experiment using voting to combine the ner outputs from the shared task participants which, predictably, produced better results than the individual systems.
</prevsent>
<prevsent>a couple of the individual systems were boosting models, so in some sense this could be regarded as an example.
</prevsent>
</prevsection>
<citsent citstr=" W02-2031 ">
tsukamoto et al(2002) <papid> W02-2031 </papid>used piped adaboost.mhmodels for ner.</citsent>
<aftsection>
<nextsent>their experimental results were some what disappointing, but this could perhaps be attributable to various reasons including the feature engineering or not using cross-validation sampling in the stacking.
</nextsent>
<nextsent>appendix the following examples show the top 10 rules learned for english and spanish on the bracketing + classification task.
</nextsent>
<nextsent>(models m6 and m8) english ne -2=zzz ne -1=zzz word:[1,3]=21 nonnevocab 0=innonnevocab nevocab 0=innevocab captype 0=firstword-firstupper =  ne=i-org ne 1=o ne 2=o word -1=zzz nonnevocab 0=innonnevocab nevocab 0=not-innevocab captype 0=firstword-firstupper =  ne=ocaptype 0=notfirstword-firstupper captype -1=firstword-firstupper captype 1=number nonnevocab 0=innonnevocab nevocab 0=innevocab ne 0=i-loc =  ne=i org ne -1=zzz ne 0=i-org word 1=, nonnevocab 0=not-innonnevocab nevocab 0=not-innevocab captype 0=allupper =  ne=i-loc ne 0=i-per word:[1,3]=0 nonnevocab 0=not-innonnevocab nevocab 0=not-innevocab captype 0=notfirstword-firstupper =  ne=i-org ne 0=i-org ne 1=o ne 2=o nonnevocab 0=innonnevocab nevocab 0=not-innevocab captype 0=alllower =  ne=o ne 0=i-per ne 1=i-org =  ne=i-org ne -1=zzz ne 0=i-per word:[-3,-1]=zzz word:[1,3]=1 =  ne=i-org ne 0=i-org word:[-3,-1]=spd =  ne=b-org ne -1=i-org ne 0=i-per word:[1,3]=1 =  ne=i-org spanish wcap type 0=alllower ne -1=i-org ne 0=i-org ne 1=o =  ne=ocaptypelex -1=inlex captypegaz -1=not-ingaz wcap type -1=alllower ne -1=o ne 0=o captypelex 0=not-inlex captypegaz 0=not-ingaz wcap type 0=noneed firs tupper =  ne=i-org wcap type 0=noneed-firstupper wcap type -1=noneed-firstupper wcap type 1=alllower captypelex 0=not-inlex captypegaz 0=not-ingaz ne 0=o =  ne=i-org ne 0=o word 0=efe =  ne=i-org ne -1=o ne 0=o word 1=num word 2=.
</nextsent>
<nextsent>captypelex 0=not-inlex captypegaz 0=not-ingaz wcap type 0=allupper =  ne=i-misc pos -1=art pos 0=ncf wcap type 0=noneed-firstupper ne -1=o ne 0=o =  ne=i-org wcap type 0=alllower ne 0=i-per ne 1=o ne 2=o =  ne=o ne 0=o ne 1=i-misc word 2=num captypelex 0=not-inlex captypegaz 0=not-ingaz wcap type 0=allupper =  ne=i-misc ne 0=i-loc word:[-3,-1]=universidad =  ne=i-org ne 1=o ne 2=o word 0=de captypelex 0=not-inlex captypegaz 0=ingaz wcap type 0=alllower =  ne=o the adaboost.mh base models high accuracy sets high bar for error correction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B785">
<title id=" C04-1058.xml">why nitpicking works evidence for occams razor in error cor rectors </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we aim to further improve performance, and propose using piped error corrector.
</prevsent>
<prevsent>4.2 transformation-based learning.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
transformation-based learning (brill, 1995), <papid> J95-4004 </papid>or tbl, is one of the most successful rule-based machine learningalgorithms.</citsent>
<aftsection>
<nextsent>the central idea of tbl is to learn an ordered list of rules, each of which evaluates on there sults of those preceding it.
</nextsent>
<nextsent>an initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.
</nextsent>
<nextsent>transformation-based learning has been used to tacklea wide range of nlp problems, ranging from part-of speech tagging (brill, 1995) <papid> J95-4004 </papid>to parsing (brill, 1996) to segmentation and message understanding (day et al,1997).<papid> A97-1051 </papid></nextsent>
<nextsent>in general, it achieves state-of-the-art performances and is fairly resistant to overtraining.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B787">
<title id=" C04-1058.xml">why nitpicking works evidence for occams razor in error cor rectors </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the central idea of tbl is to learn an ordered list of rules, each of which evaluates on there sults of those preceding it.
</prevsent>
<prevsent>an initial assignment is made based on simple statistics, and then rules are greedily learned to correct the mistakes, until no net improvement can be made.
</prevsent>
</prevsection>
<citsent citstr=" A97-1051 ">
transformation-based learning has been used to tacklea wide range of nlp problems, ranging from part-of speech tagging (brill, 1995) <papid> J95-4004 </papid>to parsing (brill, 1996) to segmentation and message understanding (day et al,1997).<papid> A97-1051 </papid></citsent>
<aftsection>
<nextsent>in general, it achieves state-of-the-art performances and is fairly resistant to overtraining.
</nextsent>
<nextsent>we have investigated frequently raised questions aboutn-fold tem plated piped correction (ntpc), general purpose, conservative error correcting model, which has been shown to reliably deliver small but consistent gains on the accuracy of even high-performing base mod elson high-dimensional nlp tasks, with little risk of accidental degradation.
</nextsent>
<nextsent>experimental evidence shows that when error-correcting high-accuracy base models, simple models and hypotheses are more beneficial than complex ones, while the more complex and powerful models are surprisingly unreliable or damaging in practice.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B788">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over recent years, many natural language processing (nlp) techniques have been developed that might benefit from knowledge of distributionally similar words, i.e., words that occur in similar contexts.
</prevsent>
<prevsent>for example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
similarity-based smoothing (brown et al, 1992; <papid> J92-4003 </papid>dagan et al, 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events.other potential applications apply the hy pothesised relationship (harris, 1968) between distributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.one advantage of automatically generated the sau ruses (grefenstette, 1994; lin, 1998; <papid> P98-2127 </papid>curr anand moens, 2002) <papid> W02-0908 </papid>over large-scale manually created thesauruses such as wordnet (fellbaum,1998) is that they might be tailored to particular genre or domain.however, due to the lack of tight definition for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see section 2).</citsent>
<aftsection>
<nextsent>previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to manually created semantic resource (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2002) <papid> W02-0908 </papid>or be oriented towards particular task such as language modelling (dagan et al, 1999; lee, 1999).<papid> P99-1004 </papid></nextsent>
<nextsent>the first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is valid gold standard.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B789">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over recent years, many natural language processing (nlp) techniques have been developed that might benefit from knowledge of distributionally similar words, i.e., words that occur in similar contexts.
</prevsent>
<prevsent>for example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
similarity-based smoothing (brown et al, 1992; <papid> J92-4003 </papid>dagan et al, 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events.other potential applications apply the hy pothesised relationship (harris, 1968) between distributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.one advantage of automatically generated the sau ruses (grefenstette, 1994; lin, 1998; <papid> P98-2127 </papid>curr anand moens, 2002) <papid> W02-0908 </papid>over large-scale manually created thesauruses such as wordnet (fellbaum,1998) is that they might be tailored to particular genre or domain.however, due to the lack of tight definition for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see section 2).</citsent>
<aftsection>
<nextsent>previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to manually created semantic resource (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2002) <papid> W02-0908 </papid>or be oriented towards particular task such as language modelling (dagan et al, 1999; lee, 1999).<papid> P99-1004 </papid></nextsent>
<nextsent>the first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is valid gold standard.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B791">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over recent years, many natural language processing (nlp) techniques have been developed that might benefit from knowledge of distributionally similar words, i.e., words that occur in similar contexts.
</prevsent>
<prevsent>for example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events.
</prevsent>
</prevsection>
<citsent citstr=" W02-0908 ">
similarity-based smoothing (brown et al, 1992; <papid> J92-4003 </papid>dagan et al, 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events.other potential applications apply the hy pothesised relationship (harris, 1968) between distributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.one advantage of automatically generated the sau ruses (grefenstette, 1994; lin, 1998; <papid> P98-2127 </papid>curr anand moens, 2002) <papid> W02-0908 </papid>over large-scale manually created thesauruses such as wordnet (fellbaum,1998) is that they might be tailored to particular genre or domain.however, due to the lack of tight definition for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see section 2).</citsent>
<aftsection>
<nextsent>previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to manually created semantic resource (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2002) <papid> W02-0908 </papid>or be oriented towards particular task such as language modelling (dagan et al, 1999; lee, 1999).<papid> P99-1004 </papid></nextsent>
<nextsent>the first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is valid gold standard.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B795">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events.
</prevsent>
<prevsent>similarity-based smoothing (brown et al, 1992; <papid> J92-4003 </papid>dagan et al, 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events.other potential applications apply the hy pothesised relationship (harris, 1968) between distributional similarity and semantic similar ity; i.e., similarity in the meaning of words can be predicted from their distributional similarity.one advantage of automatically generated the sau ruses (grefenstette, 1994; lin, 1998; <papid> P98-2127 </papid>curr anand moens, 2002) <papid> W02-0908 </papid>over large-scale manually created thesauruses such as wordnet (fellbaum,1998) is that they might be tailored to particular genre or domain.however, due to the lack of tight definition for the concept of distributional similarity and the broad range of potential applications, alarge number of measures of distributional similarity have been proposed or adopted (see section 2).</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to manually created semantic resource (lin, 1998; <papid> P98-2127 </papid>curran and moens, 2002) <papid> W02-0908 </papid>or be oriented towards particular task such as language modelling (dagan et al, 1999; lee, 1999).<papid> P99-1004 </papid></citsent>
<aftsection>
<nextsent>the first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is valid gold standard.
</nextsent>
<nextsent>further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in particular application area.
</nextsent>
<nextsent>however, it is not at all obvious that one universally best measure exists for all applications (weeds and weir, 2003).<papid> W03-1011 </papid></nextsent>
<nextsent>thus, applying adistributional similarity technique to new application necessitates evaluating large number of distributional similarity measures in addition to evaluating the new model or algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B796">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is valid gold standard.
</prevsent>
<prevsent>further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in particular application area.
</prevsent>
</prevsection>
<citsent citstr=" W03-1011 ">
however, it is not at all obvious that one universally best measure exists for all applications (weeds and weir, 2003).<papid> W03-1011 </papid></citsent>
<aftsection>
<nextsent>thus, applying adistributional similarity technique to new application necessitates evaluating large number of distributional similarity measures in addition to evaluating the new model or algorithm.
</nextsent>
<nextsent>we propose shift in focus from attempting to discover the overall best distributional similarity measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures.
</nextsent>
<nextsent>this will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for particular application.
</nextsent>
<nextsent>further, we explore problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of syn onymy, antonymy and hyponymy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B797">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for particular application.
</prevsent>
<prevsent>further, we explore problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of syn onymy, antonymy and hyponymy.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
previous work on this problem (caraballo, 1999; <papid> P99-1016 </papid>lin et al., 2003) involves identifying specific phrasal patterns within text e.g., xs and other ys?</citsent>
<aftsection>
<nextsent>is used as evidence that is hyponym of y. our work explores the connection between relative frequency, distributional generality and semantic generality with promising results.
</nextsent>
<nextsent>the rest of this paper is organised as follows.in section 2, we present ten distributional similarity measures that have been proposed for use in nlp.
</nextsent>
<nextsent>in section 3, we analyse the variation in neighbour sets returned by these measures.
</nextsent>
<nextsent>in section 4, we take one fundamental statistical property (word frequency) and analyse correlation between this and the nearest neighbour setsgenerated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B805">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> compositionality of collocations.  </section>
<citcontext>
<prevsection>
<prevsent>how ever, some collocations such as strong tea are compositional, i.e., their meaning can be determined from their constituents, whereas others such as hot dog are not.
</prevsent>
<prevsent>both types are important in language generation since system must choose between alternatives but onlynon-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
baldwin et al (2003) <papid> W03-1812 </papid>explore empirical models of compositionality for noun-noun compounds and verb-particle constructions.</citsent>
<aftsection>
<nextsent>based on the observation (haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose model which considers the semantic similarity between collocation and its constituent words.mccarthy et al (2003) <papid> W03-1810 </papid>also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts.</nextsent>
<nextsent>they extract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents(e.g. rip) from the bnc using rasp and calculate the value of simlin between each phrasal verb and its simplex constituent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B806">
<title id=" C04-1146.xml">characterising measures of lexical distributional similarity </title>
<section> compositionality of collocations.  </section>
<citcontext>
<prevsection>
<prevsent>both types are important in language generation since system must choose between alternatives but onlynon-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary.
</prevsent>
<prevsent>baldwin et al (2003) <papid> W03-1812 </papid>explore empirical models of compositionality for noun-noun compounds and verb-particle constructions.</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
based on the observation (haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose model which considers the semantic similarity between collocation and its constituent words.mccarthy et al (2003) <papid> W03-1810 </papid>also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts.</citsent>
<aftsection>
<nextsent>they extract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents(e.g. rip) from the bnc using rasp and calculate the value of simlin between each phrasal verb and its simplex constituent.
</nextsent>
<nextsent>the test simplex score is used to rank the phrasal verbs according to their similarity with their simplexconstituent.
</nextsent>
<nextsent>this ranking is correlated with human judgements of the compositionality of the phrasal verbs using spear mans rank correlationcoefficient.
</nextsent>
<nextsent>the value obtained (0.0525) is disappointing since it is not statistically significant(the probability of this value under the null hypothesis of no correlation?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B809">
<title id=" C02-1103.xml">automatic text categorization using the importance of sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this case often comes out in documents with informal style such as news group and email.
</prevsent>
<prevsent>to overcome these problems, we have studied text summarization techniques with great interest.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
among text summarization techniques, there are statistical methods and linguistic methods (radev et al, 2000; <papid> W00-0403 </papid>marcu et al, 1999).</citsent>
<aftsection>
<nextsent>since the former methods are simpler and faster than the latter methods, we use the former methods to be applied to text categorization.
</nextsent>
<nextsent>therefore, we employ two kinds of text summarization techniques; one measures the importance of sentences by the similarity between the title and each sentence in document, and the other by the importance of terms in each sentence.
</nextsent>
<nextsent>in this paper, we use two kinds of text summarization techniques for classifying important sentences and unimportant sentences.
</nextsent>
<nextsent>the importance of each sentence is measured by these techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B810">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" D07-1031 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B812">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" P07-1094 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B813">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" P06-3002 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B815">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" D07-1023 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B816">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B817">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" P06-1109 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B818">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper explores the relationship between various measures of unsupervisedpart-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags.
</prevsent>
<prevsent>we find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance.
</prevsent>
</prevsection>
<citsent citstr=" P07-1049 ">
there has been great deal of recent interest in the unsupervised discovery of syntactic structure from text, both parts-of-speech (johnson, 2007; <papid> D07-1031 </papid>goldwater and griffiths, 2007; <papid> P07-1094 </papid>biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007) <papid> D07-1023 </papid>and deeper grammatical structure like constituency and dependency trees (klein and manning, 2004; <papid> P04-1061 </papid>smith, 2006; bod, 2006; <papid> P06-1109 </papid>seginer, 2007; <papid> P07-1049 </papid>van zaanen, 2001).</citsent>
<aftsection>
<nextsent>while some grammar induction systems operate on raw text, many of the most successful ones presume prior part-of-speech tagging.
</nextsent>
<nextsent>meanwhile, most recent work in part-of-speech induction focuses on increasing the degree to which their tags match hand-annotated ones such as those in the penn treebank.in this work our goal is to evaluate how improvements in part-of-speech tag induction affects grammar induction.
</nextsent>
<nextsent>using several different unsupervised taggers, we induce tags and train three grammar induction systems on the results.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B819">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tag induction can be thought of as clustering problem where, given corpus of words, we aim to group word tokens into syntactic classes.
</prevsent>
<prevsent>two tasks are commonly labeled unsupervised part-of-speech induction.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
in the first, tag induction systems are allowed the use of tagging dictionary, which specifies for each word set of possibleparts-of-speech (merialdo, 1994; <papid> J94-2001 </papid>smith and eisner, 2005; <papid> P05-1044 </papid>goldwater and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>in the second, only the word tokens and sentence boundaries are given.
</nextsent>
<nextsent>in this work we focus on this latter task to explore grammar induction in maximally unsupervised context.
</nextsent>
<nextsent>tag induction systems typically focus on two sorts of features: distributional and morphological. distributional refers to what sorts of words appear in close proximity to the word in question,while morphological refers to modeling the internal structure of word.
</nextsent>
<nextsent>all the systems below make use of distributional information, whereas only two use morphological features.we primarily focus on the metrics used to evaluate induced taggings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B820">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tag induction can be thought of as clustering problem where, given corpus of words, we aim to group word tokens into syntactic classes.
</prevsent>
<prevsent>two tasks are commonly labeled unsupervised part-of-speech induction.
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
in the first, tag induction systems are allowed the use of tagging dictionary, which specifies for each word set of possibleparts-of-speech (merialdo, 1994; <papid> J94-2001 </papid>smith and eisner, 2005; <papid> P05-1044 </papid>goldwater and griffiths, 2007).<papid> P07-1094 </papid></citsent>
<aftsection>
<nextsent>in the second, only the word tokens and sentence boundaries are given.
</nextsent>
<nextsent>in this work we focus on this latter task to explore grammar induction in maximally unsupervised context.
</nextsent>
<nextsent>tag induction systems typically focus on two sorts of features: distributional and morphological. distributional refers to what sorts of words appear in close proximity to the word in question,while morphological refers to modeling the internal structure of word.
</nextsent>
<nextsent>all the systems below make use of distributional information, whereas only two use morphological features.we primarily focus on the metrics used to evaluate induced taggings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B826">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>all the systems below make use of distributional information, whereas only two use morphological features.we primarily focus on the metrics used to evaluate induced taggings.
</prevsent>
<prevsent>the catalogue of recent part of-speech systems is large, and we can only test 329 the tagging metrics using few systems.
</prevsent>
</prevsection>
<citsent citstr=" C04-1052 ">
recent work that we do not explore explicitly includes (biemann, 2006; <papid> P06-3002 </papid>dasgupta and ng, 2007; freitag, 2004; <papid> C04-1052 </papid>smith and eisner, 2005).<papid> P05-1044 </papid></citsent>
<aftsection>
<nextsent>we have selected few systems, described below, that represent broad range of features and techniques to make our evaluation of the metrics as broad as possible.
</nextsent>
<nextsent>2.1 clustering using svd and k-means.
</nextsent>
<nextsent>schutze (1995) presents series of part-of-speech inducers based on distributional clustering.
</nextsent>
<nextsent>we implement the baseline system, which klein and manning (2002) <papid> P02-1017 </papid>use for their grammar induction experiments with induced part-of-speech tags.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B828">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 clustering using svd and k-means.
</prevsent>
<prevsent>schutze (1995) presents series of part-of-speech inducers based on distributional clustering.
</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
we implement the baseline system, which klein and manning (2002) <papid> P02-1017 </papid>use for their grammar induction experiments with induced part-of-speech tags.</citsent>
<aftsection>
<nextsent>for each word type in the vocabulary , the system forms feature row vector consisting of the number of times each of the most frequent words occur to the left of and to the right of w. it normalizes these row vectors and assembles them into |v |2f matrix.
</nextsent>
<nextsent>it then performs singular value decomposition on the matrix and rank reduces it to decrease its dimensionality to principle components (d   2f ).
</nextsent>
<nextsent>this results in representation of each word as point in d dimensional space.we follow klein and manning (2002) <papid> P02-1017 </papid>in using means to cluster the dimensional word vectors into parts-of-speech.</nextsent>
<nextsent>we use the = 500 most frequent words as left and right context features,and reduce to dimensionality of = 50.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B853">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>we use maximum marginal decoding, which johnson (2007) <papid> D07-1031 </papid>reports performs better than viterbi decoding.</prevsent>
<prevsent>2.3 systems with morphology.</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
clark (2003) <papid> E03-1009 </papid>presents several part-of-speech induction systems which incorporate morphological as well as distributional information.</citsent>
<aftsection>
<nextsent>we use the 330 implementation found on his website.1 2.3.1 ney-essen with morphology the simplest model is based on work by (ney etal., 1994).
</nextsent>
<nextsent>it uses bitag hmm, with the restriction that each word type in the vocabulary can only be generated by single part-of-speech.
</nextsent>
<nextsent>thus thetag induction task here reduces to finding multi way partition of the vocabulary.
</nextsent>
<nextsent>the learning algorithm greedily re assigns each word type to the part-of-speech that results in the greatest increase in likelihood.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B868">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> part-of-speech tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>although this results in situation where multiple induced tags may share single gold tag, it does not punish system for providing tags of finer granularity than the gold standard.in contrast, one-to-one?
</prevsent>
<prevsent>(1-to-1) accuracy restricts each gold tag to having single induced tag.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
the mapping typically is made to try to give the most favorable mapping in terms of accuracy, typically using greedy assignment (haghighi and klein, 2006).<papid> N06-1041 </papid></citsent>
<aftsection>
<nextsent>in cases where the number of gold tags is different than the number of induced tags,some must necessarily remain unassigned (john son, 2007).<papid> D07-1031 </papid>in addition to accuracy, there are several information theoretic criteria presented in the literature.these escape the problem of trying to find an appropriate mapping between induced and gold tags, at the expense of perhaps being less intuitive.</nextsent>
<nextsent>let i be the tag assignments to the wordsin the corpus created by an unsupervised tagger, and let gbe the gold standard tag assignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B883">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> grammar induction.  </section>
<citcontext>
<prevsection>
<prevsent>they attribute this to the fact that nouns end up in many induced tags.
</prevsent>
<prevsent>there has been quite bit of other work on constituency induction.
</prevsent>
</prevsection>
<citsent citstr=" P04-1062 ">
smith and eisner (2004) <papid> P04-1062 </papid>present an alternative estimation technique for ccm which uses annealing to try to escape local maxima.</citsent>
<aftsection>
<nextsent>bod (2006) <papid> P06-1109 </papid>describes an unsupervised system within the data-oriented-parsing frame work.</nextsent>
<nextsent>several approaches try to learn structure directly from raw text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B886">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> grammar induction.  </section>
<citcontext>
<prevsection>
<prevsent>seginer (2007) <papid> P07-1049 </papid>has an incremental parsing approach using novel representation called common-cover-links, which can be converted to constituent brackets.</prevsent>
<prevsent>van zaanen(2001)s abl attempts to align sentences to determine what sequences of words are substitutable.</prevsent>
</prevsection>
<citsent citstr=" P07-3008 ">
the work closest in spirit to this paper is cramer (2007), <papid> P07-3008 </papid>who evaluates several grammar induction systems on the eindhoven corpus (dutch).</citsent>
<aftsection>
<nextsent>oneof his experiments compares the grammar induction performance of these systems starting with tags induced using the system described by biemann (2006), <papid> P06-3002 </papid>to the performance of the systems on manually-marked tags.</nextsent>
<nextsent>however he does not evaluate to what degree better tagging performance leads to improvement in these systems.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B897">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as in the constituency case, smith (2006) presents several alternative estimation procedures for dmv, which try to minimize the local maximum problems inherent in em.
</prevsent>
<prevsent>it is thus possible these methods might yield better performance for the models when run off of induced tags.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
we induce tags with each system on the penn treebank wall street journal (marcus et al, 1994), <papid> H94-1020 </papid>sections 0-10, which contain 20,260 sentences.</citsent>
<aftsection>
<nextsent>we vary the number of tags (10, 20, 50) and run each system 10 times forgiven setting.
</nextsent>
<nextsent>the result of each run is used as the input to the ccm, dmv,and ccm+dmv systems.
</nextsent>
<nextsent>while the tags are induced from all sentences in the section, following the practice in (klein and manning, 2002; <papid> P02-1017 </papid>klein and manning, 2004), <papid> P04-1061 </papid>we remove punctuation, and consider only sentences of length not greater than10 in our grammar induction experiments.</nextsent>
<nextsent>tag gings are evaluated after punctuation is removed, but before filtering for length.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B900">
<title id=" C08-1042.xml">evaluating unsupervised partofspeech tagging for grammar induction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>these are ordered from best to worst according to the metric, so for instance h(t |t ) would give highest rank to its lowest value.
</prevsent>
<prevsent>we can compare the two rankings using ken dalls ?
</prevsent>
</prevsection>
<citsent citstr=" J06-4002 ">
(see lapata (2006) <papid> J06-4002 </papid>for an overview), non parametric measure of correspondence for rankings.</citsent>
<aftsection>
<nextsent>measures the difference between the number of concordant pairs (items the two rankings place in the same or der) and discordant pairs (those the rankings place in opposite order), divided by the total number ofpairs.
</nextsent>
<nextsent>a value of 1 indicates the rankings have perfect correspondence, -1 indicates they are in the opposite order, and 0 indicates they are independent.
</nextsent>
<nextsent>the ? values are shown in table 2.
</nextsent>
<nextsent>thescatter-plot in figure 1 shows the ? with the greatest magnitude.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B901">
<title id=" C02-1026.xml">the effectiveness of dictionary and web based answer reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems bear similar structure: (1) question analysis ? identify question keywords to be submitted to search engines (local or web), recognize question types, and suggest expected answer types.
</prevsent>
<prevsent>although most systems relyon taxonomy of expected answer types, the number of nodes in the taxonomy varies widely from single digits to few thousands.
</prevsent>
</prevsection>
<citsent citstr=" A00-1041 ">
for example, abney et al  (2000) <papid> A00-1041 </papid>used 5; ittycheriah et al  (2001), 31; hovy et al  (2001), 140; harabagiu et al  (2001), 8,797.</citsent>
<aftsection>
<nextsent>these taxonomies were mostly based on named entities and wordnet (fellbaum 1998).
</nextsent>
<nextsent>special types such definition questions (ex: what is an atom??)
</nextsent>
<nextsent>were added as necessary.
</nextsent>
<nextsent>(2) passage or sentence retrieval ? this aims to provide text pool of manageable size for extracting candidate answers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B902">
<title id=" C02-1026.xml">the effectiveness of dictionary and web based answer reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most top performing systems in trecs use their own retrieval methods for passages (brill et al  2001; clarke et al  2001; harabagiu et al  2001) or sentences (hovy et al  2001).
</prevsent>
<prevsent>(3) candidate answer extraction ? extract candidate answers according to answer types.
</prevsent>
</prevsection>
<citsent citstr=" A00-1023 ">
if the expected answer types are typical named entities, information extraction engines (bikel et al . 1999, srihari and li 2000) <papid> A00-1023 </papid>are used to extract candidate answers.</citsent>
<aftsection>
<nextsent>otherwise special answer patterns are used to pinpoint answers.
</nextsent>
<nextsent>for example, soubbotin and soubbotin (2001) create set of 6 answer patterns for definition questions.
</nextsent>
<nextsent>(4) answer ranking ? assign scores to candidate answers according to their frequency in top ranked passages (abney et al  2000; <papid> A00-1041 </papid>clarke et al  2001), similarity to candidate answers extracted from external sources such as the web (brill et al  2001; buchholz 2001) or wordnet (harabagiu et al . 2001; hovy et al  2001), density, distance, or order of question keywords around the candidates, similarity between the dependency structures of questions and candidate answers (harabagiu et al  2001; hovy et al  2001; ittycheriah et al  2001), and match of expected answer types.</nextsent>
<nextsent>in this paper, we describe an in-depth study of answer reranking for definition questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B905">
<title id=" C02-1026.xml">the effectiveness of dictionary and web based answer reranking </title>
<section> webclopedia: an automated.  </section>
<citcontext>
<prevsection>
<prevsent>question answering system webclopedias architecture follows the principle outlined in section 1.
</prevsent>
<prevsent>we briefly describe each stage in the following.
</prevsent>
</prevsection>
<citsent citstr=" C02-1042 ">
please refer to (hovy et al  2002) <papid> C02-1042 </papid>for more detail.</citsent>
<aftsection>
<nextsent>(1) question analysis: we used an in-house parser, contex (hermjakob 2001), <papid> W01-1203 </papid>to parse and analyze questions and relied on bbns identifinder (bikel et al , 1999) to provide basic named entity extraction capability.</nextsent>
<nextsent>(2) document retrieval/sentence ranking: their engine mg (witten et al  1994) was used to return at least 500 documents using boolean queries generated from the query formation stage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B906">
<title id=" C02-1026.xml">the effectiveness of dictionary and web based answer reranking </title>
<section> webclopedia: an automated.  </section>
<citcontext>
<prevsection>
<prevsent>we briefly describe each stage in the following.
</prevsent>
<prevsent>please refer to (hovy et al  2002) <papid> C02-1042 </papid>for more detail.</prevsent>
</prevsection>
<citsent citstr=" W01-1203 ">
(1) question analysis: we used an in-house parser, contex (hermjakob 2001), <papid> W01-1203 </papid>to parse and analyze questions and relied on bbns identifinder (bikel et al , 1999) to provide basic named entity extraction capability.</citsent>
<aftsection>
<nextsent>(2) document retrieval/sentence ranking: their engine mg (witten et al  1994) was used to return at least 500 documents using boolean queries generated from the query formation stage.
</nextsent>
<nextsent>however, fewer than 500 documents may be returned when very specific queries are given.
</nextsent>
<nextsent>to decrease the amount of text to be processed, the documents were broken into sentences.
</nextsent>
<nextsent>each sentence was scored using formula that rewards word and phrase overlap with the question and expanded query words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B907">
<title id=" C04-1016.xml">extending mt evaluation tools with translation complexity metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extended mt evaluation tools are available from the first authors web site: http://www.comp.leeds.ac.uk/bogdan/evalmt.html
</prevsent>
<prevsent>automated evaluation tools for mt systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
automated metrics such as bleu (papineni et al, 2002), <papid> P02-1040 </papid>red (akiba et al 2001), weighted n-gram model (wnm) (babych, 2004), syntactic relation / semantic vector model (rajman and hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters.</citsent>
<aftsection>
<nextsent>automated evaluation is much quicker and cheaper than human evaluation.
</nextsent>
<nextsent>another advantage of the scores produced by automated mt evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges may adjust their evaluation scale in order to discriminate between slightly better and slightly worse variants ? but only those variants which are present in the evaluation set.
</nextsent>
<nextsent>for example, absolute figures for human evaluation of set which includes mt output only are not directly comparable with the figures for another evaluation which might include mt plus non-native human translation, or several human translations of different quality.
</nextsent>
<nextsent>because of the instability of this intuitive scale, human evaluation figures should be treated as relative rather than absolute.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B908">
<title id=" C04-1016.xml">extending mt evaluation tools with translation complexity metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>evaluation scores (which change according to the complexity of the evaluated texts) is that the reported scores for different mt systems are not directly comparable.
</prevsent>
<prevsent>since there is no standard collection of texts used for benchmarking all mt systems, it is not clear how system that achieves, e.g., bleur4n4 1 score 0.556 tested on 490 utterances selected from the wsj?
</prevsent>
</prevsection>
<citsent citstr=" E03-1004 ">
(cmejrek et al 2003:<papid> E03-1004 </papid>89) may be compared to another system which achieves, e.g., the bleur1n4 score 0.240 tested on 10,150 sentences from the basic travel expression corpus?</citsent>
<aftsection>
<nextsent>(imamura et al, 2003:<papid> E03-1029 </papid>161).</nextsent>
<nextsent>moreover, even if there is no comparison involved, there is great degree of uncertainty in how to interpret the reported automated scores.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B909">
<title id=" C04-1016.xml">extending mt evaluation tools with translation complexity metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since there is no standard collection of texts used for benchmarking all mt systems, it is not clear how system that achieves, e.g., bleur4n4 1 score 0.556 tested on 490 utterances selected from the wsj?
</prevsent>
<prevsent>(cmejrek et al 2003:<papid> E03-1004 </papid>89) may be compared to another system which achieves, e.g., the bleur1n4 score 0.240 tested on 10,150 sentences from the basic travel expression corpus?</prevsent>
</prevsection>
<citsent citstr=" E03-1029 ">
(imamura et al, 2003:<papid> E03-1029 </papid>161).</citsent>
<aftsection>
<nextsent>moreover, even if there is no comparison involved, there is great degree of uncertainty in how to interpret the reported automated scores.
</nextsent>
<nextsent>for example, bleur2n4 0.3668 is the highest score for top mt system if mt performance is measured on news reports, but it is relatively poor score for corpus of e-mails, and score that is still beyond the state-of-the-art for corpus of legal documents.
</nextsent>
<nextsent>these levels of perfection have to be established experimentally for each type of text, and there is no way of knowing whether some reported automated score is better or worse if new type of text is involved in the evaluation.
</nextsent>
<nextsent>the need to use stable evaluation scores, normalised by the complexity of the evaluated task, has been recognised in other nlp areas, such as anaphora resolution, where the results may be relative with regard to specific evaluation set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B911">
<title id=" C02-1009.xml">a robust cross style bilingual sentences alignment model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to enhance the robustness, robust statistical model based on both transfer lexicon sand sentence lengths are proposed in this paper.
</prevsent>
<prevsent>after integrating the transfer lexicons into the model, 60% -measure error reduction (from 14.4% to 5.8%) is observed.
</prevsent>
</prevsection>
<citsent citstr=" C00-2163 ">
since the bilingual corpus is valuable resource for training statistical language models [dagon, 91; suet al, 95; su and chang, 99] and sentence alignment is the first step for most such tasks, many alignment approaches have been proposed in the literature [brown, 91; gale and church, 93; wu, 94; vogel et al, 96; och and ney, 2000].<papid> C00-2163 </papid></citsent>
<aftsection>
<nextsent>most of those reported approaches use the sentence length as the main feature to perform the alignment task.
</nextsent>
<nextsent>for example, brown et al (91) used the feature of number-of-words for alignment, and [gale and church,93] claimed that better performance can be achieved (5.8% error rate for english-french cor pus) if the number-of-characters is adopted instead.as cognates are reliable cues for language pairs derived from the same family, church (93) also attacked this problem by considering cognates additionally.
</nextsent>
<nextsent>because most of those reported work are performed on those indo-european language-pairs, for testing the performance on non-indo-europeanlanguages, wu (94) had tried both length and cognate features on the hongkong hansard english chinese corpus, and 7.9% error rate has been reported.
</nextsent>
<nextsent>besides, sentence alignment can also be indirectly achieved via more complicated word corresponding models [brown et al, 93; vogel et al,96; och and ney, 2000].<papid> C00-2163 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B915">
<title id=" C04-1156.xml">knowledge intensive word alignment with knowa </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for these reasons, we claim that in some cases algorithms based on external, linguistics resources, if available, can be useful alternative to statistics- based algorithms.
</prevsent>
<prevsent>in the rest of this paper we will compare the results obtained by statistics-based and linguistic resource-based algorithm when applied to the eurocor and multisemcor english/italian corpora.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the statistics-based algorithm to be evaluated is described in (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>for its evaluation we used an implementation by the authors themselves, called giza++, which is freely available to the scientific community (och, 2003).
</nextsent>
<nextsent>the second algorithm to be evaluated is crucially based on bilingual dictionary and morphological analyzer.
</nextsent>
<nextsent>it is called knowa (knowledge intensive word aligner) and has been developed at itc-irst by the authors of this paper.
</nextsent>
<nextsent>the results of the comparative evaluation show that, given specific application goals, and given the availability of italian/english resources, knowa obtains results that are comparable or better than the results obtained with giza++.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B916">
<title id=" C04-1156.xml">knowledge intensive word alignment with knowa </title>
<section> knowa ? the pivot extension.  </section>
<citcontext>
<prevsection>
<prevsent>once these pivotal correspondences have been established, the remaining alignments are derived using the pivots as fixed points.
</prevsent>
<prevsent>given that fact that these algorithms do not exploit bilingual dictionaries, the selection of the pivotal translation correspondent may be based on cognates, or specific frequency configurations.
</prevsent>
</prevsection>
<citsent citstr=" P00-1055 ">
see among others (simmard and plamondon, 1998) and (ribeiro et al, 2000).<papid> P00-1055 </papid></citsent>
<aftsection>
<nextsent>the results obtained by applying the one-to-one potential correspondence as criterion for selecting pivot words are illustrated further on in section 5.
</nextsent>
<nextsent>1-sizzling 2-temperatures 3-and 4-hot 5-summer 6-pavements 7-are 8-anything 9-but 10-kind 11-to 12-the 13-feet 1-il 2-clima 3-torrido 4-e 5-i 6-marciapiedi 7-dell?
</nextsent>
<nextsent>8-estate 9-rovente 10-non 11-sono 12-niente 13-di 14-buono 15-per 16-i 17-piedi table 3: pivot words involved in one-to-one potential correspondences 1-sizzling 2-temperatures 3-and 4-hot 5-summer 6-pavements 7-are 8-anything 9-but 10-kind ? 1-il 2-clima 3-torrido 4-e 5-i 6-marciapiedi 7-dell?
</nextsent>
<nextsent>8-estate 9-rovente 10-non ? table 4: typical potential correspondences for non-pivot words
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B919">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these measures are defined based on information that is automatically derived from the target knowledge base.
</prevsent>
<prevsent>an experimental evaluation shows that our method improved the success rate of retrieval by generating confirmation more efficiently than using conventional confidence measure.
</prevsent>
</prevsection>
<citsent citstr=" C02-1169 ">
information retrieval systems with spoken language have been studied (harabagiu et al, 2002; <papid> C02-1169 </papid>hori et al, 2003).</citsent>
<aftsection>
<nextsent>they require both automatic speech recognition (asr) and information retrieval (ir) technologies.
</nextsent>
<nextsent>as straight manifestation to create these systems, we can think of using asr results as an input for ir systems that retrieve text knowledge base (kb).
</nextsent>
<nextsent>how ever, two problems occur in the characteristics of speech.
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B920">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> redundancy included in spoken language.  </section>
<citcontext>
<prevsection>
<prevsent>expressions one is an asr error, which is basically inevitable in speech communications.
</prevsent>
<prevsent>therefore, an adequate confirmation is indispensable in spoken dialogue systems to eliminate the misunderstandings caused by asr errors.
</prevsent>
</prevsection>
<citsent citstr=" C00-1068 ">
if keywords to be confirmed are defined, the system can confirm them using confidence measures (komatani and kawahara, 2000; <papid> C00-1068 </papid>hazenet al, 2000) to manage the errors.</citsent>
<aftsection>
<nextsent>in conventional tasks for spoken dialogue systems in which their target of retrieval was well-defined, such as the relational database, keywords that are important to achieve the tasks correspond to items in the relational database.
</nextsent>
<nextsent>most spoken dialogue systems that have been developed, such as airline information systems (levin et al, 2000; potamianos et al, 2000; san-segundo et al., 2000) and train information systems (allen et al, 1996; <papid> P96-1009 </papid>sturm et al, 1999; lamel et al, 1999), are categorized here.</nextsent>
<nextsent>however, it is not feasible to define such keywords in retrieval for operation manuals (komatani et al, 2002) <papid> C02-1152 </papid>or www pages, where the target of retrieval is not organized and is written as natural language text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B921">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> redundancy included in spoken language.  </section>
<citcontext>
<prevsection>
<prevsent>if keywords to be confirmed are defined, the system can confirm them using confidence measures (komatani and kawahara, 2000; <papid> C00-1068 </papid>hazenet al, 2000) to manage the errors.</prevsent>
<prevsent>in conventional tasks for spoken dialogue systems in which their target of retrieval was well-defined, such as the relational database, keywords that are important to achieve the tasks correspond to items in the relational database.</prevsent>
</prevsection>
<citsent citstr=" P96-1009 ">
most spoken dialogue systems that have been developed, such as airline information systems (levin et al, 2000; potamianos et al, 2000; san-segundo et al., 2000) and train information systems (allen et al, 1996; <papid> P96-1009 </papid>sturm et al, 1999; lamel et al, 1999), are categorized here.</citsent>
<aftsection>
<nextsent>however, it is not feasible to define such keywords in retrieval for operation manuals (komatani et al, 2002) <papid> C02-1152 </papid>or www pages, where the target of retrieval is not organized and is written as natural language text.</nextsent>
<nextsent>another problem is that users utterances may include redundant expressions or out-of domain phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B922">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> redundancy included in spoken language.  </section>
<citcontext>
<prevsection>
<prevsent>in conventional tasks for spoken dialogue systems in which their target of retrieval was well-defined, such as the relational database, keywords that are important to achieve the tasks correspond to items in the relational database.
</prevsent>
<prevsent>most spoken dialogue systems that have been developed, such as airline information systems (levin et al, 2000; potamianos et al, 2000; san-segundo et al., 2000) and train information systems (allen et al, 1996; <papid> P96-1009 </papid>sturm et al, 1999; lamel et al, 1999), are categorized here.</prevsent>
</prevsection>
<citsent citstr=" C02-1152 ">
however, it is not feasible to define such keywords in retrieval for operation manuals (komatani et al, 2002) <papid> C02-1152 </papid>or www pages, where the target of retrieval is not organized and is written as natural language text.</citsent>
<aftsection>
<nextsent>another problem is that users utterances may include redundant expressions or out-of domain phrases.
</nextsent>
<nextsent>a speech interface has been said to have the advantage of ease of input.
</nextsent>
<nextsent>this means that redundant expressions, such as dis fluency and irrelevant phrases, are easily input.these do not directly contribute to task achievement and might even be harmful.
</nextsent>
<nextsent>asr results that may include such redundant portions are not adequate for an input of ir systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B923">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> redundancy included in spoken language.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 is an example of the database.
</prevsent>
<prevsent>the knowledge base is very large-scale, as shown in table 1.
</prevsent>
</prevsection>
<citsent citstr=" C02-1084 ">
the dialog navigator (kiyota et al, 2002) <papid> C02-1084 </papid>was developed in the university of tokyo as table 1: document set (knowledge base) text collection # of texts # of characters glossary 4,707 700,000 faq 11,306 6,000,000 support articles 23,323 22,000,000 text retrieval system for this knowledge base.the system accepts typed-text input as questions from users and outputs result of there trieval.</citsent>
<aftsection>
<nextsent>the system interprets input sentences taking syntactic dependency and synonymous expression into consideration for matching it with the knowledge base.
</nextsent>
<nextsent>the system can also navigate for the user when he/she makes vague questions based on scenarios (dialog card) that were described manually beforehand.
</nextsent>
<nextsent>hundreds of the dialog cards have been prepared to ask questions back to the users.
</nextsent>
<nextsent>if user question matches its input part, the system generates question based on its description.we adopted the dialog navigator as backend system and constructed text retrieval system with spoken dialogue interface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B924">
<title id=" C04-1158.xml">efficient confirmation strategy for largescale text retrieval systems with spoken dialogue interface </title>
<section> confirmation strategy using.  </section>
<citcontext>
<prevsection>
<prevsent>it enables us to detect portion that is not influential for retrieval or those portions that include asr errors.
</prevsent>
<prevsent>here, phrase, called bunsetsu1 in japanese, is adopted as portion for which the perplexity is calculated.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
we use syntactic parser knp (kurohashi and nagao, 1994) <papid> J94-4001 </papid>to divide the asr results into the phrases2.</citsent>
<aftsection>
<nextsent>1bunsetsu is commonly used linguistic unit in japanese, which consists of one or more content words and zero or more functional words that follow.
</nextsent>
<nextsent>2as the parser was designed for written language, the division often fails for portions including asr errors.
</nextsent>
<nextsent>the division error, however, does not affect the whole systems performance because the perplexity for the erroneous portions increases, indicating they are irrelevant.
</nextsent>
<nextsent> user utterance: atarashiku katta xp no pasokon de fax kinou wo tsukau niha doushitara iidesu ka??(please tell me how to use the facsimile function in the personal computer with windows xp that recently bought.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B926">
<title id=" C04-1122.xml">named entity discovery using comparable news articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>noun which doesnt appear in the corpora.
</prevsent>
<prevsent>several attempts have been made to tackle this problem by using unsupervised learning techniques, which make vast amount of corpora available to use.
</prevsent>
</prevsection>
<citsent citstr=" C96-2157 ">
(strzalkowski and wang, 1996) <papid> C96-2157 </papid>and(collins and singer, 1999) <papid> W99-0613 </papid>tried to obtain either lexical or contextual knowledge from seed given byhand.</citsent>
<aftsection>
<nextsent>they trained the two different kind of knowledge alternately at each iteration of training.
</nextsent>
<nextsent>(yan garber et al, 2002) <papid> C02-1154 </papid>tried to discover names with similar method.</nextsent>
<nextsent>however, these methods still suffer in the situation where the number of occurrences of certain name is rather small.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B927">
<title id=" C04-1122.xml">named entity discovery using comparable news articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>noun which doesnt appear in the corpora.
</prevsent>
<prevsent>several attempts have been made to tackle this problem by using unsupervised learning techniques, which make vast amount of corpora available to use.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
(strzalkowski and wang, 1996) <papid> C96-2157 </papid>and(collins and singer, 1999) <papid> W99-0613 </papid>tried to obtain either lexical or contextual knowledge from seed given byhand.</citsent>
<aftsection>
<nextsent>they trained the two different kind of knowledge alternately at each iteration of training.
</nextsent>
<nextsent>(yan garber et al, 2002) <papid> C02-1154 </papid>tried to discover names with similar method.</nextsent>
<nextsent>however, these methods still suffer in the situation where the number of occurrences of certain name is rather small.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B928">
<title id=" C04-1122.xml">named entity discovery using comparable news articles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(strzalkowski and wang, 1996) <papid> C96-2157 </papid>and(collins and singer, 1999) <papid> W99-0613 </papid>tried to obtain either lexical or contextual knowledge from seed given byhand.</prevsent>
<prevsent>they trained the two different kind of knowledge alternately at each iteration of training.</prevsent>
</prevsection>
<citsent citstr=" C02-1154 ">
(yan garber et al, 2002) <papid> C02-1154 </papid>tried to discover names with similar method.</citsent>
<aftsection>
<nextsent>however, these methods still suffer in the situation where the number of occurrences of certain name is rather small.
</nextsent>
<nextsent>in this paper we propose another method to strengthen the lexical knowledge for named entity tagging by using synchronicity of names in comparable documents.
</nextsent>
<nextsent>one can view comparable?
</nextsent>
<nextsent>document as an alternative expression of the samecontent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B929">
<title id=" C04-1122.xml">named entity discovery using comparable news articles </title>
<section> synchronicity of names.  </section>
<citcontext>
<prevsection>
<prevsent>several different newspapers published on the same day report lots of thesame events, therefore contain number of comparable documents.
</prevsent>
<prevsent>one can also take another view of comparable corpus, which is set of paraphraseddocuments.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
by exploiting this feature, one can extract paraphrastic expressions automatically from parallel corpora (barzilay and mckeown, 2001) <papid> P01-1008 </papid>or comparable corpora (shinyama and sekine, 2003).<papid> W03-1609 </papid></citsent>
<aftsection>
<nextsent>named entities incomparable documents have one notable characteristic: they tend to be preserved across comparable documents because it is generally difficult to paraphrase names.
</nextsent>
<nextsent>we think thatit is also hard to paraphrase product names or disease names, so they will also be preserved.
</nextsent>
<nextsent>therefore, if one named entity appears in one document, it should also appear in the comparable document.
</nextsent>
<nextsent>0 5 10 15 20 25 30 0 50 100 150 200 250 300 350 400 fre que ncy date latwpnytreute the occurrence of the word yigal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B930">
<title id=" C04-1122.xml">named entity discovery using comparable news articles </title>
<section> synchronicity of names.  </section>
<citcontext>
<prevsection>
<prevsent>several different newspapers published on the same day report lots of thesame events, therefore contain number of comparable documents.
</prevsent>
<prevsent>one can also take another view of comparable corpus, which is set of paraphraseddocuments.
</prevsent>
</prevsection>
<citsent citstr=" W03-1609 ">
by exploiting this feature, one can extract paraphrastic expressions automatically from parallel corpora (barzilay and mckeown, 2001) <papid> P01-1008 </papid>or comparable corpora (shinyama and sekine, 2003).<papid> W03-1609 </papid></citsent>
<aftsection>
<nextsent>named entities incomparable documents have one notable characteristic: they tend to be preserved across comparable documents because it is generally difficult to paraphrase names.
</nextsent>
<nextsent>we think thatit is also hard to paraphrase product names or disease names, so they will also be preserved.
</nextsent>
<nextsent>therefore, if one named entity appears in one document, it should also appear in the comparable document.
</nextsent>
<nextsent>0 5 10 15 20 25 30 0 50 100 150 200 250 300 350 400 fre que ncy date latwpnytreute the occurrence of the word yigal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B931">
<title id=" C04-1127.xml">cross lingual information extraction system evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>research in information extraction (ie) and its related fields has led to wide range of application sin many domains.
</prevsent>
<prevsent>the portability issue of ie systems across different domains, however, remains serious challenge.
</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
this problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (lin et al, 2003) and extraction patterns(yangarber, 2003), <papid> P03-1044 </papid>which require the user to provide only small set of lexical items of the target classes or extraction patterns for the target domain.</citsent>
<aftsection>
<nextsent>the idea of self-customizing ie system emerged recently with the improvement of pattern acquisition techniques (sudo et al, 2003<papid> P03-1029 </papid>b), where the ie system customizes itself across domains given by the users query.furthermore, there are demands for access to information in languages different from the users own.</nextsent>
<nextsent>however, it is more challenging to provide an ie system where the target language (here, en glish) is different from the source language (here, japanese): cross-lingual information extraction (clie) system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B932">
<title id=" C04-1127.xml">cross lingual information extraction system evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the portability issue of ie systems across different domains, however, remains serious challenge.
</prevsent>
<prevsent>this problem is being addressed through automatic knowledge acquisition methods, such as unsupervised learning for domain-specific lexicons (lin et al, 2003) and extraction patterns(yangarber, 2003), <papid> P03-1044 </papid>which require the user to provide only small set of lexical items of the target classes or extraction patterns for the target domain.</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
the idea of self-customizing ie system emerged recently with the improvement of pattern acquisition techniques (sudo et al, 2003<papid> P03-1029 </papid>b), where the ie system customizes itself across domains given by the users query.furthermore, there are demands for access to information in languages different from the users own.</citsent>
<aftsection>
<nextsent>however, it is more challenging to provide an ie system where the target language (here, en glish) is different from the source language (here, japanese): cross-lingual information extraction (clie) system.
</nextsent>
<nextsent>in this research, we explore various methods for efficient automatic pattern acquisition for theclie system, including the translation of the entire source document set into the target language.
</nextsent>
<nextsent>to achieve efficiency, the resulting clie system should (1) provide reasonable level of extraction performance (both accuracy and coverage) and (2) require little or no knowledge on the users part ofthe source language.
</nextsent>
<nextsent>today, there are basic linguistics tools available for many major languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B940">
<title id=" C04-1127.xml">cross lingual information extraction system evaluation </title>
<section> cross-lingual information extraction.  </section>
<citcontext>
<prevsection>
<prevsent>report personnel affair (that   c-person
</prevsent>
<prevsent>resigns)) ).
</prevsent>
</prevsection>
<citsent citstr=" C02-1070 ">
(riloff et al, 2002) <papid> C02-1070 </papid>present several approaches to cross-lingual information extraction (clie).</citsent>
<aftsection>
<nextsent>they describe the use of cross-language projection?
</nextsent>
<nextsent>for clie, exploiting the word alignment of documents in one language and the same documents translated into different language by machine translation (mt) system.
</nextsent>
<nextsent>they conducted experiments between two relatively close languages, english and french.
</nextsent>
<nextsent>in the experiment reported here, we will explore clie for two more disparate languages, english and japanese.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B946">
<title id=" C04-1127.xml">cross lingual information extraction system evaluation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this causes the translation-based qdie system to identify fewer pattern candidates from the relevant documents since pattern candidate must contain at least one of the ne types.to remedy this problem, we incorporated cross language projection?
</prevsent>
<prevsent>(riloff et al, 2002) <papid> C02-1070 </papid>only for named entities.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we used word alignment obtained by using giza++ (och and ney, 2003) <papid> J03-1002 </papid>to get names in the english translation from names in the original japanese sentences.</citsent>
<aftsection>
<nextsent>note that it is extremely difficult to make an alignment of case markers where one language explicitly renders marker as word and the other does not.
</nextsent>
<nextsent>so, direct application of(riloff et al, 2002) <papid> C02-1070 </papid>is not suitable for this experi ment.</nextsent>
<nextsent>we compare the following three systems in this experiment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B951">
<title id=" C02-1001.xml">disambiguation of finite state transducers </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J97-2003 ">
the objective of this work is to disambiguate transducers which have the following form: = d and to be able to apply the determinization algorithm described in (mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>our approach to disambiguating = d consists first of computing the composition and thereafter to disambiguate the transducer . we will give an important consequence of this result that allows us to compose any number of transducers with the transducer d, in contrast to the previous approach which consisted in first disambiguating transducers and to produce respectively d?
</nextsent>
<nextsent>and r?
</nextsent>
<nextsent>, then computing ? = r?
</nextsent>
<nextsent>d? where ? is unambiguous.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B952">
<title id=" C02-1001.xml">disambiguation of finite state transducers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>transducer is the pronunciations dictionary.it converts sequence of context independent phones to sequence of words.
</prevsent>
<prevsent>transducer represents language model: it converts sequences of words into sequences of words, while restricting the possible sequences or assigning score to the sequences.the speech recognition problem consists of finding the path of least cost in transducer ? , where is sequence of acoustic observations.the pronunciations dictionary representing the mapping from pronunciations to words can show an inherent ambiguity: sequence of phones can correspond to more than one word,so we cannot apply the transducer de terminization algorithm (an operation which reduces the redundancy, search time and possibly space).
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
this problem is usually handled by adding special symbols to the dictionary to remove the ambiguity in order to beable to apply the determinization algorithm (koskenniemi, 1990).<papid> C90-2040 </papid></citsent>
<aftsection>
<nextsent>nevertheless, when we compose the dictionary with the phonological rules, wemust take into account special symbols.
</nextsent>
<nextsent>this complicates the construction of transducers representing these rules and leads to size explosion.
</nextsent>
<nextsent>it would be simpler to compose the rules with the dictionary, then remove the ambiguity in the result and then apply the determinization algorithm.
</nextsent>
<nextsent>definitions formally, weighted transducer over semi ring = (k,?,?, 0?, 1?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B954">
<title id=" C04-1121.xml">sentiment classification on customer feedback data noisy data large feature vectors and the role of linguistic analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or topic.
</prevsent>
<prevsent>faced with the task of having to automatically classify piece of text as expressing positive or negative sentiment, reasonable first approach would consist of paying special attention to words that tend to express positive or negative attitude.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
pang et al  (2002) <papid> W02-1011 </papid>have demonstrated, however, that this is not as straightforward as one may think, given that sentiment is often expressed in more subtle and indirect ways.</citsent>
<aftsection>
<nextsent>the literature on sentiment classification can be divided into approaches that relyon semantic resources, such as sentiment or affect lexicon (nasukawa and yi 2003, subasic and huettner 2001), or large scale knowledge base (liu et al  2003) on the one hand, and approaches that try to learn patterns directly from tagged data, without additional resources (dave et al 2003, pang et al  2003).
</nextsent>
<nextsent>much research is also being directed at acquiring affect lexica automatically (turney 2002, <papid> P02-1053 </papid>turney and littman 2002).</nextsent>
<nextsent>there is also considerable amount of research on classification of text as subjective?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B956">
<title id=" C04-1121.xml">sentiment classification on customer feedback data noisy data large feature vectors and the role of linguistic analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pang et al  (2002) <papid> W02-1011 </papid>have demonstrated, however, that this is not as straightforward as one may think, given that sentiment is often expressed in more subtle and indirect ways.</prevsent>
<prevsent>the literature on sentiment classification can be divided into approaches that relyon semantic resources, such as sentiment or affect lexicon (nasukawa and yi 2003, subasic and huettner 2001), or large scale knowledge base (liu et al  2003) on the one hand, and approaches that try to learn patterns directly from tagged data, without additional resources (dave et al 2003, pang et al  2003).</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
much research is also being directed at acquiring affect lexica automatically (turney 2002, <papid> P02-1053 </papid>turney and littman 2002).</citsent>
<aftsection>
<nextsent>there is also considerable amount of research on classification of text as subjective?
</nextsent>
<nextsent>or objective?
</nextsent>
<nextsent>(wiebe et al 2001, yu and hatzivassiloglou 2003), <papid> W03-1017 </papid>task that is not relevant for the processing of very brief pieces of direct customer feedback.</nextsent>
<nextsent>in many studies, research on sentiment classification is conducted on review-type data, such as movie or restaurant reviews.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B957">
<title id=" C04-1121.xml">sentiment classification on customer feedback data noisy data large feature vectors and the role of linguistic analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is also considerable amount of research on classification of text as subjective?
</prevsent>
<prevsent>or objective?
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
(wiebe et al 2001, yu and hatzivassiloglou 2003), <papid> W03-1017 </papid>task that is not relevant for the processing of very brief pieces of direct customer feedback.</citsent>
<aftsection>
<nextsent>in many studies, research on sentiment classification is conducted on review-type data, such as movie or restaurant reviews.
</nextsent>
<nextsent>these data often consist of relatively well-formed, coherent and at least paragraph-length pieces of text.
</nextsent>
<nextsent>the results we present in this paper are based on customer feedback data from web surveys, which, as we will discuss below, are particularly noisy and fragmentary.
</nextsent>
<nextsent>for our purpose of automatic classification of customer feedback, we decided to use machine learning directly on the customer feedback, instead of relying on additional semantic resources of any kind.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B958">
<title id=" C04-1121.xml">sentiment classification on customer feedback data noisy data large feature vectors and the role of linguistic analysis </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>we used both techniques (and their combinations) in our experiments.
</prevsent>
<prevsent>the measure of predictiveness?
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we employed is log likelihood ratio with respect to the target variable (dunning 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>in the experiments described below, (in the top-ranked features) ranged from 1000 to 40,000.
</nextsent>
<nextsent>the different feature set combinations we used were: ? all features?
</nextsent>
<nextsent>no linguistic features?
</nextsent>
<nextsent>(only word ngrams) ? surface features?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B962">
<title id=" C04-1121.xml">sentiment classification on customer feedback data noisy data large feature vectors and the role of linguistic analysis </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>likelihood ratio.
</prevsent>
<prevsent>a second, more surprising result is that the use of abstract linguistic analysis features consistently contributes to the classification accuracy in sentiment classification.
</prevsent>
</prevsection>
<citsent citstr=" C04-1088 ">
while results like this have been reported in the area of style classification (baayen et al  1996, gamon 2004), <papid> C04-1088 </papid>they are noteworthy in domain where stylistic markers have not been considered in the past, indicating the need for more research into the stylistic correlations of affect in text.</citsent>
<aftsection>
<nextsent>we thank anthony aue and eric ringger (microsoft research) and hang li (microsoft research asia) for helpful comments and discussions, and chris moore (microsoft product support services uk) for the initial request for sentiment classification based on the needs of support services at microsoft.
</nextsent>
<nextsent>thanks also go to karin berghoefer of the butler-hill group for manually annotating subset of the data.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B963">
<title id=" C02-2003.xml">searching the web by voice </title>
<section> collocations.  </section>
<citcontext>
<prevsection>
<prevsent>a collocation is an expression of two or more words that corresponds to some conventional way of saying things?
</prevsent>
<prevsent>(manning and schutze, 1999).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
sometimes, the notion of collocation is defined in terms of syntax (by possible part-of-speech patterns) or in terms of semantics (requiringcollocations to exhibit non-compositional meaning) (smadja, 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>we adopt an empirical approach and consider any sequence of words that co occurs more often than chance potential collocation.
</nextsent>
<nextsent>3.1 the likelihood ratio.
</nextsent>
<nextsent>we adopted method for collocation discovery based on the likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></nextsent>
<nextsent>suppose we wish to test whether two words      </nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B964">
<title id=" C02-2003.xml">searching the web by voice </title>
<section> collocations.  </section>
<citcontext>
<prevsection>
<prevsent>we adopt an empirical approach and consider any sequence of words that co occurs more often than chance potential collocation.
</prevsent>
<prevsent>3.1 the likelihood ratio.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we adopted method for collocation discovery based on the likelihood ratio (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>suppose we wish to test whether two words
</nextsent>
<nextsent>forma collocation.
</nextsent>
<nextsent>under the independence hypothesis we assume that the probability of observing the second word
</nextsent>
<nextsent>is independent of the first word:
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B965">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>more specifically, the sense inventory used is multi wordnet (pianta et al, 2002), multilingual lexical database in which the italian component is strictly aligned with the english wordnet.
</prevsent>
<prevsent>2.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
the idea of obtaining linguistic information about text in one language by exploiting parallel or comparable texts in another language has been explored in the field of word sense disambiguation (wsd) since the early 90s, the most representative works being (brown et al, 1991), (<papid> P91-1034 </papid>gale et al, 1992), and (dagan and itai, 1994).<papid> J94-4003 </papid></citsent>
<aftsection>
<nextsent>in more recent years, ide et al (2002) <papid> W02-0808 </papid>present method to identify word meanings starting from multilingual corpus.</nextsent>
<nextsent>a by-product of applying this method is that once word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B966">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>more specifically, the sense inventory used is multi wordnet (pianta et al, 2002), multilingual lexical database in which the italian component is strictly aligned with the english wordnet.
</prevsent>
<prevsent>2.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
the idea of obtaining linguistic information about text in one language by exploiting parallel or comparable texts in another language has been explored in the field of word sense disambiguation (wsd) since the early 90s, the most representative works being (brown et al, 1991), (<papid> P91-1034 </papid>gale et al, 1992), and (dagan and itai, 1994).<papid> J94-4003 </papid></citsent>
<aftsection>
<nextsent>in more recent years, ide et al (2002) <papid> W02-0808 </papid>present method to identify word meanings starting from multilingual corpus.</nextsent>
<nextsent>a by-product of applying this method is that once word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B967">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 related work.
</prevsent>
<prevsent>the idea of obtaining linguistic information about text in one language by exploiting parallel or comparable texts in another language has been explored in the field of word sense disambiguation (wsd) since the early 90s, the most representative works being (brown et al, 1991), (<papid> P91-1034 </papid>gale et al, 1992), and (dagan and itai, 1994).<papid> J94-4003 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
in more recent years, ide et al (2002) <papid> W02-0808 </papid>present method to identify word meanings starting from multilingual corpus.</citsent>
<aftsection>
<nextsent>a by-product of applying this method is that once word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated.
</nextsent>
<nextsent>cross-language tagging is the goal of the work by diab and resnik (2002), <papid> P02-1033 </papid>who present method for word sense tagging both the source and target texts of parallel bilingual corpora with the wordnet sense inventory.</nextsent>
<nextsent>parallel to the studies regarding the projection of semantic information, more recently the nlp community has also explored the possibility of exploiting translation to project more syntax oriented annotations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B968">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>in more recent years, ide et al (2002) <papid> W02-0808 </papid>present method to identify word meanings starting from multilingual corpus.</prevsent>
<prevsent>a by-product of applying this method is that once word in one language is word-sense tagged, the translation equivalents in the parallel texts are also automatically annotated.</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
cross-language tagging is the goal of the work by diab and resnik (2002), <papid> P02-1033 </papid>who present method for word sense tagging both the source and target texts of parallel bilingual corpora with the wordnet sense inventory.</citsent>
<aftsection>
<nextsent>parallel to the studies regarding the projection of semantic information, more recently the nlp community has also explored the possibility of exploiting translation to project more syntax oriented annotations.
</nextsent>
<nextsent>yarowsky et al (2001) <papid> H01-1035 </papid>describe successful method consisting of (i) automatic annotation of english texts, (ii) cross language projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target language.</nextsent>
<nextsent>a further step is made in (hwa et al, 2002) and (cabezas et al, 2001), which address the task of acquiring dependency treebank by bootstrapping from existing linguistic resources for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B969">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>cross-language tagging is the goal of the work by diab and resnik (2002), <papid> P02-1033 </papid>who present method for word sense tagging both the source and target texts of parallel bilingual corpora with the wordnet sense inventory.</prevsent>
<prevsent>parallel to the studies regarding the projection of semantic information, more recently the nlp community has also explored the possibility of exploiting translation to project more syntax oriented annotations.</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
yarowsky et al (2001) <papid> H01-1035 </papid>describe successful method consisting of (i) automatic annotation of english texts, (ii) cross language projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target language.</citsent>
<aftsection>
<nextsent>a further step is made in (hwa et al, 2002) and (cabezas et al, 2001), which address the task of acquiring dependency treebank by bootstrapping from existing linguistic resources for english.
</nextsent>
<nextsent>finally, in (riloff et al, 2002) <papid> C02-1070 </papid>method is presented for rapidly creating information extraction (ie) systems for new languages by exploiting existing ie systems via cross-language projection.</nextsent>
<nextsent>the results of all the abovementioned studies show how previous major investments in english annotated corpora and tool development can be effectively leveraged across languages, allowing the development of accurate resources and tools in other languages without comparable human effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B970">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> the annotation transfer methodology.  </section>
<citcontext>
<prevsection>
<prevsent>yarowsky et al (2001) <papid> H01-1035 </papid>describe successful method consisting of (i) automatic annotation of english texts, (ii) cross language projection of annotations onto target language texts, and (iii) induction of noise-robust taggers for the target language.</prevsent>
<prevsent>a further step is made in (hwa et al, 2002) and (cabezas et al, 2001), which address the task of acquiring dependency treebank by bootstrapping from existing linguistic resources for english.</prevsent>
</prevsection>
<citsent citstr=" C02-1070 ">
finally, in (riloff et al, 2002) <papid> C02-1070 </papid>method is presented for rapidly creating information extraction (ie) systems for new languages by exploiting existing ie systems via cross-language projection.</citsent>
<aftsection>
<nextsent>the results of all the abovementioned studies show how previous major investments in english annotated corpora and tool development can be effectively leveraged across languages, allowing the development of accurate resources and tools in other languages without comparable human effort.
</nextsent>
<nextsent>the multisemcor project raises number of theoretical and practical issues.
</nextsent>
<nextsent>for instance: is translational language fully representative of the general use of language in the same way as original language is? to what extent are the lexica of different languages comparable?
</nextsent>
<nextsent>these theoretical issues have already been presented in (pianta and bentivogli, 2003) and will not be discussed here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B971">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> evaluation of the annotation transfer.  </section>
<citcontext>
<prevsection>
<prevsent>the word aligner used in the project is knowa (knowledge-intensive word aligner), an english/italian word aligner, developed at itc-irst, which relies mostly on information contained in the collins bilingual dictionary, available in electronic format.
</prevsent>
<prevsent>knowa also exploits morphological analyzer and multiword recognizer for both english and italian.
</prevsent>
</prevsection>
<citsent citstr=" C04-1156 ">
for detailed discussion of the characteristics of this tool, see (pianta and bentivogli, 2004).<papid> C04-1156 </papid></citsent>
<aftsection>
<nextsent>some characteristics of the multisemcor scenario make the alignment task easier for knowa.
</nextsent>
<nextsent>first, in semcor all multi words included in wordnet are explicitly marked.
</nextsent>
<nextsent>thus knowa does not need to recognize english multi words, although it still needs to recognize the italian ones.
</nextsent>
<nextsent>second, within multisemcor word alignment is done with the final aim of transferring lexical annotations from english to italian.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B972">
<title id=" C04-1053.xml">evaluating cross language annotation transfer in the multisemcor corpus </title>
<section> evaluation of the annotation transfer.  </section>
<citcontext>
<prevsection>
<prevsent>as expected, controlled translations produced better agreement rate between annotators.
</prevsent>
<prevsent>for assessing the performance of knowa, the standard notions of precision, recall, and coverage have been used following (vronis and langlais, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
see (och and ney, 2003) <papid> J03-1002 </papid>and arenberg et al, 2000) for different evaluation metrics.</citsent>
<aftsection>
<nextsent>the performance of knowa applied to the multisemcor gold standard in full-text alignment task is shown in table 1.
</nextsent>
<nextsent>these results, which compare well with those reported in the literature (vronis, 2000) show that, as expected, controlled translations allow for better alignment but also that free translations may be satisfactorily aligned.
</nextsent>
<nextsent>the evaluation of knowa with respect to the english content words which have semantic tag in semcor is reported in tables 2 and 3, for both free and controlled translations and broken down by part of speech.
</nextsent>
<nextsent>precision recall coverage free 83.5 57.9 60.0 controlled 88.4 67.5 74.9 table 1: knowa on full-text precision recall coverage nouns 93.7 81.1 86.5 verbs 85.6 70.3 82.1 adjectives 95.6 64.7 67.7 adverbs 88.4 38.5 43.5 total 91.2 68.2 74.8 table 2: knowa on sense-tagged words only (free translations) precision recall coverage nouns 95.9 82.5 86.1 verbs 90.7 76.8 84.7 adjectives 95.2 69.9 73.5 adverbs 90.4 51.6 57.1 total 93.9 74.6 79.5 table 3: knowa on sense-tagged words only (controlled translations) we can see that ignoring function words the performance of the word aligner improves in both precision and recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B973">
<title id=" C02-1021.xml">semiautomatic detection of errors in pos tagged corpora </title>
<section> automatic pos-tagging errors detection.  </section>
<citcontext>
<prevsection>
<prevsent>the approach as well as its impact on the correctness of the resulting corpus will be demonstrated on the version 2 of the negra?
</prevsent>
<prevsent>corpus of german (for the corpus itself seewww.coli.uni-sb.de/sfb378/negra-corpus, forde scription cf.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
skut et al (1997)).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>however, we believe the solutions developed and presented inthis paper are not bound particularly to correcting this corpus or to german, but hold generally.
</nextsent>
<nextsent>the error search we use has several phases which differ in the amount of context that has tobe taken into consideration during the error detection process.
</nextsent>
<nextsent>put plainly, the extent of context mirrors the linguistic complexity of the detection, or, in other words, at the moment when the objective is to search for  complex  errors, the  simple(r)  errors should be already eliminated.
</nextsent>
<nextsent>the first, preliminary phase, is thus the search for errors which are detectable absolutely locally, i.e. without any context at all.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B974">
<title id=" C02-1021.xml">semiautomatic detection of errors in pos tagged corpora </title>
<section> evaluation of the results.  </section>
<citcontext>
<prevsection>
<prevsent>1, however the prevailing part was that of incorrect tagging (only less than 8% were genuine source errors, about 26% were errors in segmentation).
</prevsent>
<prevsent>the whole resulted in changes on 3.774 lines of the corpus;the rectification of errors in segmentation resulted in reducing the number of corpus positions by over 700, from 355.096 to 354.3549.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
after finishing the corrections, we experimented with training and testing the tnt tagger (brants,2000) <papid> A00-1031 </papid>on the  old  and on the  corrected  version of negra?.</citsent>
<aftsection>
<nextsent>we used the same testing as described by brants, i.e. dividing each of the corpus into ten contiguous parts of equal size, 8 exempted are quotations and other meta linguistic contexts, such as der fluss heisst donau, peterbersetzte faust - eine tragdie ins englische als fist one tragedy, which, however, are as rule lexically specific and hence can be coped with as such.
</nextsent>
<nextsent>9 which is much nicer number than 355.096, and thus an additional motivation for correcting corpora
</nextsent>
<nextsent>each part having parallel starting and end position in each of the versions, and then running the system ten times, each time training on nine parts and testing on the tenth part, and finally computing the mean of the quality results.
</nextsent>
<nextsent>in doing so, we arrived at the following results: ? if both the training and the testing was performed on the  old  negra?, the tags assigned by the tnt tagger differed from the hand-assigned tags within the test sections on (together) 11.138 positions (out of the total of 355.096), which yields the error rate of 3,14% ? if both the training and the testing was performed on the  correct  negra?, the tags assigned by the tnt tagger differed from the hand-assigned tags of the test sections on (together) 10.889 positions (out of the total of 354.354), which yields the error rate of 3,07% ? in the most interesting final experiment, the training was performed on the  old  and the testing on the  correct  negra?; in the result, the tags assigned by tnt differed from thehand-assigned tags in the test sections on (to gether) 12.075 positions (out of the total of 354.354), yielding the error rate of 3,41%.these results show that there was only negligible (and, according to the 2 test, statistically insignificant) difference between the results in the cases when the tagger was both trained and tested on  old  corpus and both trained and tested on the  corrected  corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B975">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem of finding the most probable translation becomes graph-theoretic problem of finding the minimum path covering of the source language dependency tree.
</prevsent>
<prevsent>given source language sentence s, statistical machine translation (smt) model translates it by finding the target language sentence such that the probability p(t|s) is maximized.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
in word-based models, such as ibm model 1-5 (brown et al  1993), <papid> J93-2003 </papid>the probability p(t|s) is decomposed into statistical parameters involving words.</citsent>
<aftsection>
<nextsent>there have been many recent proposals to improve translation quality by decomposing p(t|s) into probabilities involving phrases.
</nextsent>
<nextsent>phrase-based smt approaches can be classified into two categories.
</nextsent>
<nextsent>one type of approach works with parse trees.
</nextsent>
<nextsent>in (yamada&knight; 2001), <papid> P01-1067 </papid>for example, the translation model applies three operations (re-order, insert, and translate) to an english parse tree to produce its chinese translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B976">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>phrase-based smt approaches can be classified into two categories.
</prevsent>
<prevsent>one type of approach works with parse trees.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
in (yamada&knight; 2001), <papid> P01-1067 </papid>for example, the translation model applies three operations (re-order, insert, and translate) to an english parse tree to produce its chinese translation.</citsent>
<aftsection>
<nextsent>a parallel corpus of english parse trees and chinese sentences are used to obtain the probabilities of the operations.
</nextsent>
<nextsent>in the second type of phrase-based smt models, phrases are defined as block in word aligned corpus such that words within the block are aligned with words inside the block (och et al 1999, <papid> W99-0604 </papid>marcu&wong; 2002).<papid> W02-1018 </papid></nextsent>
<nextsent>this definition will treat as phrases many word sequences that are not constituents in parse trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B977">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (yamada&knight; 2001), <papid> P01-1067 </papid>for example, the translation model applies three operations (re-order, insert, and translate) to an english parse tree to produce its chinese translation.</prevsent>
<prevsent>a parallel corpus of english parse trees and chinese sentences are used to obtain the probabilities of the operations.</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
in the second type of phrase-based smt models, phrases are defined as block in word aligned corpus such that words within the block are aligned with words inside the block (och et al 1999, <papid> W99-0604 </papid>marcu&wong; 2002).<papid> W02-1018 </papid></citsent>
<aftsection>
<nextsent>this definition will treat as phrases many word sequences that are not constituents in parse trees.
</nextsent>
<nextsent>this may look linguistically counter-intuitive.
</nextsent>
<nextsent>however, (koehn et al 2003) <papid> N03-1017 </papid>found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between there is? in english andes gibt?</nextsent>
<nextsent>(it gives?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B978">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (yamada&knight; 2001), <papid> P01-1067 </papid>for example, the translation model applies three operations (re-order, insert, and translate) to an english parse tree to produce its chinese translation.</prevsent>
<prevsent>a parallel corpus of english parse trees and chinese sentences are used to obtain the probabilities of the operations.</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
in the second type of phrase-based smt models, phrases are defined as block in word aligned corpus such that words within the block are aligned with words inside the block (och et al 1999, <papid> W99-0604 </papid>marcu&wong; 2002).<papid> W02-1018 </papid></citsent>
<aftsection>
<nextsent>this definition will treat as phrases many word sequences that are not constituents in parse trees.
</nextsent>
<nextsent>this may look linguistically counter-intuitive.
</nextsent>
<nextsent>however, (koehn et al 2003) <papid> N03-1017 </papid>found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between there is? in english andes gibt?</nextsent>
<nextsent>(it gives?)</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B979">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this definition will treat as phrases many word sequences that are not constituents in parse trees.
</prevsent>
<prevsent>this may look linguistically counter-intuitive.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however, (koehn et al 2003) <papid> N03-1017 </papid>found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between there is? in english andes gibt?</citsent>
<aftsection>
<nextsent>(it gives?)
</nextsent>
<nextsent>in german.
</nextsent>
<nextsent>in this paper, we present path-based transfer model for machine translation.
</nextsent>
<nextsent>the model is trained with word-aligned parallel corpus where the source language side consists of dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B980">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> acquisition of transfer rules.  </section>
<citcontext>
<prevsection>
<prevsent>connect cables to controller bran chez les cbles sur contrleur connect to controller bran chez sur contrleur connect cables bran chez les cbles power cables cbles  alimentation both cables deux cbles connect both power cables to the controller bran chez les deux cbles  alimentation sur le contrleur (a) (b) (c) (d) (e) (f) 1 2 3 4 5 6 7 8 9 connect to the controller bran chez sur le contrleur (g) figure 2.
</prevsent>
<prevsent>examples of transfer rules extracted from word-aligned corpus 3.1 spans.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
the rule extraction algorithm makes use of the notion of spans (fox 2002, <papid> W02-1039 </papid>lin&cherry; 2003<papid> W03-0302 </papid></citsent>
<aftsection>
<nextsent>given word alignment and node in the source dependency tree, the spans of induced by the word alignment are consecutive sequences of words in the target sentence.
</nextsent>
<nextsent>we define two types of spans: head span: the word sequence aligned with the node n. phrase span: the word sequence from the lower bound of the head spans of all nodes in the subtree rooted at to the upper bound of the same set of spans.
</nextsent>
<nextsent>for example, the spans of the nodes in fig.
</nextsent>
<nextsent>2(a) are listed in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B981">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> acquisition of transfer rules.  </section>
<citcontext>
<prevsection>
<prevsent>connect cables to controller bran chez les cbles sur contrleur connect to controller bran chez sur contrleur connect cables bran chez les cbles power cables cbles  alimentation both cables deux cbles connect both power cables to the controller bran chez les deux cbles  alimentation sur le contrleur (a) (b) (c) (d) (e) (f) 1 2 3 4 5 6 7 8 9 connect to the controller bran chez sur le contrleur (g) figure 2.
</prevsent>
<prevsent>examples of transfer rules extracted from word-aligned corpus 3.1 spans.
</prevsent>
</prevsection>
<citsent citstr=" W03-0302 ">
the rule extraction algorithm makes use of the notion of spans (fox 2002, <papid> W02-1039 </papid>lin&cherry; 2003<papid> W03-0302 </papid></citsent>
<aftsection>
<nextsent>given word alignment and node in the source dependency tree, the spans of induced by the word alignment are consecutive sequences of words in the target sentence.
</nextsent>
<nextsent>we define two types of spans: head span: the word sequence aligned with the node n. phrase span: the word sequence from the lower bound of the head spans of all nodes in the subtree rooted at to the upper bound of the same set of spans.
</nextsent>
<nextsent>for example, the spans of the nodes in fig.
</nextsent>
<nextsent>2(a) are listed in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B993">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>as in (koehn, et. al. 2003), <papid> N03-1017 </papid>1755 sentences of length 5-15 were used for testing.</prevsent>
<prevsent>we parsed the english side of the corpus with minipar2 (lin 2002).</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
we then performed word-align on the parsed corpus with the pro align system (cherry&lin; 2003, <papid> P03-1012 </papid>lin&cherry; 2003<papid> W03-0302 </papid>b).</citsent>
<aftsection>
<nextsent>from the training corpus, we extracted 2,040,565 distinct paths with one or more translations.
</nextsent>
<nextsent>the bleu score of our system on the test data is 0.2612.
</nextsent>
<nextsent>compared with the english to french results in (koehn et. al. 2003), <papid> N03-1017 </papid>this is higher than the ibm model 4 (0.2555), but lower than the phrasal model (0.3149).</nextsent>
<nextsent>6.1 transfer-based mt. both our system and transfer-based mt systems take parse tree in the source language and translate it into parse tree in the target language with transfer rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1000">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> related work and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>compared with the english to french results in (koehn et. al. 2003), <papid> N03-1017 </papid>this is higher than the ibm model 4 (0.2555), but lower than the phrasal model (0.3149).</prevsent>
<prevsent>6.1 transfer-based mt. both our system and transfer-based mt systems take parse tree in the source language and translate it into parse tree in the target language with transfer rules.</prevsent>
</prevsection>
<citsent citstr=" W02-1610 ">
there have been many recent proposals to acquire transfer rules automatically from word-aligned corpus (carbonell et al 2002, lavoie et al 2002, <papid> W02-1610 </papid>richardson et al 2001).</citsent>
<aftsection>
<nextsent>there are two main differences between our system and previous transfer-based approach: the unit of transfer and the generation module.
</nextsent>
<nextsent>the units of transfer in previous transfer based approach are usually subtrees in the source 1 http://www.isi.edu/~koehn/europarl/ 2 http://www.cs.ualberta.ca/~lindek/minipar.htm language parse tree.
</nextsent>
<nextsent>while the number of subtrees of tree is exponential in the size of the tree, the number of paths in tree is quadratic.
</nextsent>
<nextsent>the reduced number of possible transfer units makes the data less sparse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1001">
<title id=" C04-1090.xml">a path based transfer model for machine translation </title>
<section> related work and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>there is no separate generation module and we do not need target language grammar.
</prevsent>
<prevsent>6.2 translational divergence.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
the direct correspondence assumption (dca) states that the dependency tree in source and target language have isomorphic structures (hwa et. al. 2002).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>dca is often violated in the presence of translational divergence.
</nextsent>
<nextsent>it has been shown in (habash&dorr; 2002) that translational divergences are quite common (as much as 35% between english and spanish).
</nextsent>
<nextsent>for example, fig.
</nextsent>
<nextsent>6(a) is head swapping divergence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1002">
<title id=" C04-1135.xml">extracting hyponyms of pre specified hypernyms from itemizations and headings in web documents </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this paper describes method to acquire hyponyms forgiven hypernyms from html documents on the www.
</prevsent>
<prevsent>we assume that heading (or explanation) of an itemization (or list ing) in an html document is likely to contain hypernym of the items in the itemization, and we try to acquire hyponymy relations based onthis assumption.
</prevsent>
</prevsection>
<citsent citstr=" N04-1010 ">
our method is obtained by extending shinzatos method (shinzato and torisawa, 2004) <papid> N04-1010 </papid>where common hypernym forex pressions in itemizations in html documents is obtained by using statistical measures.</citsent>
<aftsection>
<nextsent>by using japanese html documents, we empirically show that our proposed method can obtain significant number of hyponymy relations which would otherwise be missed by alternative methods.
</nextsent>
<nextsent>hyponymy relations can play crucial role in various nlp systems, and there have been many attempts to develop automatic methods to acquire hy ponymy relations from text corpora (hearst, 1992; <papid> C92-2082 </papid>caraballo, 1999; <papid> P99-1016 </papid>imasumi, 2001; fleischman et al, 2003; <papid> P03-1001 </papid>morin and jacquemin, 2003; ando et al,2003).</nextsent>
<nextsent>most of these techniques have relied on particular linguistic patterns, such as np such as np.?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1004">
<title id=" C04-1135.xml">extracting hyponyms of pre specified hypernyms from itemizations and headings in web documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our method is obtained by extending shinzatos method (shinzato and torisawa, 2004) <papid> N04-1010 </papid>where common hypernym forex pressions in itemizations in html documents is obtained by using statistical measures.</prevsent>
<prevsent>by using japanese html documents, we empirically show that our proposed method can obtain significant number of hyponymy relations which would otherwise be missed by alternative meth ods.</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
hyponymy relations can play crucial role in various nlp systems, and there have been many attempts to develop automatic methods to acquire hy ponymy relations from text corpora (hearst, 1992; <papid> C92-2082 </papid>caraballo, 1999; <papid> P99-1016 </papid>imasumi, 2001; fleischman et al, 2003; <papid> P03-1001 </papid>morin and jacquemin, 2003; ando et al,2003).</citsent>
<aftsection>
<nextsent>most of these techniques have relied on particular linguistic patterns, such as np such as np.?
</nextsent>
<nextsent>the frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora.
</nextsent>
<nextsent>the effort of searching for other clues indicating hyponymy relations is thus significant.our aim is to extract hyponyms of pre specified hy pernyms from the www.
</nextsent>
<nextsent>we use itemizations (orlists) in html documents, such as the one in figure 1(a), and their headings (car company list?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1005">
<title id=" C04-1135.xml">extracting hyponyms of pre specified hypernyms from itemizations and headings in web documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our method is obtained by extending shinzatos method (shinzato and torisawa, 2004) <papid> N04-1010 </papid>where common hypernym forex pressions in itemizations in html documents is obtained by using statistical measures.</prevsent>
<prevsent>by using japanese html documents, we empirically show that our proposed method can obtain significant number of hyponymy relations which would otherwise be missed by alternative meth ods.</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
hyponymy relations can play crucial role in various nlp systems, and there have been many attempts to develop automatic methods to acquire hy ponymy relations from text corpora (hearst, 1992; <papid> C92-2082 </papid>caraballo, 1999; <papid> P99-1016 </papid>imasumi, 2001; fleischman et al, 2003; <papid> P03-1001 </papid>morin and jacquemin, 2003; ando et al,2003).</citsent>
<aftsection>
<nextsent>most of these techniques have relied on particular linguistic patterns, such as np such as np.?
</nextsent>
<nextsent>the frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora.
</nextsent>
<nextsent>the effort of searching for other clues indicating hyponymy relations is thus significant.our aim is to extract hyponyms of pre specified hy pernyms from the www.
</nextsent>
<nextsent>we use itemizations (orlists) in html documents, such as the one in figure 1(a), and their headings (car company list?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1006">
<title id=" C04-1135.xml">extracting hyponyms of pre specified hypernyms from itemizations and headings in web documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our method is obtained by extending shinzatos method (shinzato and torisawa, 2004) <papid> N04-1010 </papid>where common hypernym forex pressions in itemizations in html documents is obtained by using statistical measures.</prevsent>
<prevsent>by using japanese html documents, we empirically show that our proposed method can obtain significant number of hyponymy relations which would otherwise be missed by alternative meth ods.</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
hyponymy relations can play crucial role in various nlp systems, and there have been many attempts to develop automatic methods to acquire hy ponymy relations from text corpora (hearst, 1992; <papid> C92-2082 </papid>caraballo, 1999; <papid> P99-1016 </papid>imasumi, 2001; fleischman et al, 2003; <papid> P03-1001 </papid>morin and jacquemin, 2003; ando et al,2003).</citsent>
<aftsection>
<nextsent>most of these techniques have relied on particular linguistic patterns, such as np such as np.?
</nextsent>
<nextsent>the frequencies of use for such linguistic patterns are relatively low, though, and there can be many expressions that do not appear in such patterns even if we look at large corpora.
</nextsent>
<nextsent>the effort of searching for other clues indicating hyponymy relations is thus significant.our aim is to extract hyponyms of pre specified hy pernyms from the www.
</nextsent>
<nextsent>we use itemizations (orlists) in html documents, such as the one in figure 1(a), and their headings (car company list?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1015">
<title id=" C04-1135.xml">extracting hyponyms of pre specified hypernyms from itemizations and headings in web documents </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>as local document set for each hyponym candidate, we downloaded the top 100 documents in the ranking produced by search engine.
</prevsent>
<prevsent>in addition, we used 5.72106 japanese html documents (6.27 gb without tags) to obtain co-occurrence vectors to calculate the semantic similarities between expressions.
</prevsent>
</prevsection>
<citsent citstr=" C00-1060 ">
to derive co-occurrence vectors, we parsed the documents by using downgraded version of an existing parser (kanayama et al, 2000) <papid> C00-1060 </papid>and collected co-occurrences from the parsing results.as mentioned, we obtained 200 pairs of hyper nym and an hcs as the final heaih output.</citsent>
<aftsection>
<nextsent>all the hypernyms appearing in the output are listed in figure 4 along with their english translations and 4particularly, when only one itemization was obtained for hypernym, it was selected.
</nextsent>
<nextsent>hypernym hcs ??
</nextsent>
<nextsent>(emperor) (these are chinese emperors.)
</nextsent>
<nextsent>*?????????, (welfare *?????????, facilities) *???????????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1016">
<title id=" C02-1035.xml">semantics based representation for multimodal interpretation in conversational systems </title>
<section> multimodal interpretation.  </section>
<citcontext>
<prevsection>
<prevsent>such an overall understanding is then captured in representation called conversation unit.
</prevsent>
<prevsent>furthermore, mind also identifies how an input relates to the overall conversation discourse through discourse understanding.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
in particular, mind uses representation called conversation segment to group together inputs that contribute to same goal or sub-goal (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the result of discourse understanding is an evolving conversation history that reflects the over all progress of conversation.
</nextsent>
<nextsent>figure 2 shows conversation fragment between user and mind.
</nextsent>
<nextsent>in the first user input u1, the deictic figure 1.
</nextsent>
<nextsent>mind components gesture speech text multimodal interpreter discourse interpreter language interpreter gesture interpreter speech recognizer gesture recognizer modality unit (speech &amp; text) modality unit (gesture) conversation unit uni modal understanding discourse understanding multimodal understanding other ria components o versa tio h isto ry conversation segment mind o ain,visualc ntexts semantics-based representation for multimodal interpretation in conversational systems joyce chai ibm t. j. watson research center 19 skyline drive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1017">
<title id=" C02-1035.xml">semantics based representation for multimodal interpretation in conversational systems </title>
<section> semantics-based representation.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, these models are domain independent and can be applied to any information seeking applications (for structured information).
</prevsent>
<prevsent>3.1.3 representing user inputs given the semantic models of intention, attention and constraints, mind represents those models using combination of feature structures (carpenter, 1992).
</prevsent>
</prevsection>
<citsent citstr=" P97-1036 ">
this representation is inspired by the earlier works (johnston et al, 1997; <papid> P97-1036 </papid>johnston, 1998) <papid> P98-1102 </papid>and offers aflexibility to accommodate complex inputs.</citsent>
<aftsection>
<nextsent>specifically, mind represents intention, attention and constraints identified from user inputs as result of bothunimodal understanding and multimodal understanding.
</nextsent>
<nextsent>during uni modal understanding, mind applies adecision tree based semantic parser on natural language inputs (jelinek et al, 1994) <papid> H94-1052 </papid>to identify salient information.</nextsent>
<nextsent>for the gesture input, mind applies asimple geometry-based recognizer.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1018">
<title id=" C02-1035.xml">semantics based representation for multimodal interpretation in conversational systems </title>
<section> semantics-based representation.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, these models are domain independent and can be applied to any information seeking applications (for structured information).
</prevsent>
<prevsent>3.1.3 representing user inputs given the semantic models of intention, attention and constraints, mind represents those models using combination of feature structures (carpenter, 1992).
</prevsent>
</prevsection>
<citsent citstr=" P98-1102 ">
this representation is inspired by the earlier works (johnston et al, 1997; <papid> P97-1036 </papid>johnston, 1998) <papid> P98-1102 </papid>and offers aflexibility to accommodate complex inputs.</citsent>
<aftsection>
<nextsent>specifically, mind represents intention, attention and constraints identified from user inputs as result of bothunimodal understanding and multimodal understanding.
</nextsent>
<nextsent>during uni modal understanding, mind applies adecision tree based semantic parser on natural language inputs (jelinek et al, 1994) <papid> H94-1052 </papid>to identify salient information.</nextsent>
<nextsent>for the gesture input, mind applies asimple geometry-based recognizer.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1019">
<title id=" C02-1035.xml">semantics based representation for multimodal interpretation in conversational systems </title>
<section> semantics-based representation.  </section>
<citcontext>
<prevsection>
<prevsent>this representation is inspired by the earlier works (johnston et al, 1997; <papid> P97-1036 </papid>johnston, 1998) <papid> P98-1102 </papid>and offers aflexibility to accommodate complex inputs.</prevsent>
<prevsent>specifically, mind represents intention, attention and constraints identified from user inputs as result of bothunimodal understanding and multimodal understand ing.</prevsent>
</prevsection>
<citsent citstr=" H94-1052 ">
during uni modal understanding, mind applies adecision tree based semantic parser on natural language inputs (jelinek et al, 1994) <papid> H94-1052 </papid>to identify salient information.</citsent>
<aftsection>
<nextsent>for the gesture input, mind applies asimple geometry-based recognizer.
</nextsent>
<nextsent>as result, information from each uni modal input is represented in modality unit.
</nextsent>
<nextsent>we have seen several modality units (in figure 4, figure 6, and figure 7), where intention, attention and constraints are represented in featurestructures.
</nextsent>
<nextsent>note that only features that can be instan tiated by information from the user input are included in the feature structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1020">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" P04-1061 ">
mar. most recent work (e.g., (klein and manning,2004; <papid> P04-1061 </papid>dennis, 2005; bod, 2006<papid> W06-2912 </papid>a; smith and eisner, 2006; <papid> P06-1072 </papid>seginer, 2007)<papid> P07-1049 </papid>annotates text sentences using hierarchical bracketing (constituents) or ade pendency structure, and thus represents the induced grammar through its behavior in parsingtask.</citsent>
<aftsection>
<nextsent>solan et al (2005) uses graph representation, while (nakamura, 2006) simply uses grammar formalism such as pcfg.
</nextsent>
<nextsent>when the bracketing approach is taken, some algorithms label the resulting constituents, while most do not.each of these approaches can be justified or criticized; detailed discussion of this issue is beyond the scope of this paper.
</nextsent>
<nextsent>the algorithm presented here belongs to the first group, annotating given sentences with labeled bracketing structures.
</nextsent>
<nextsent>the main theoretical justification for this approach is that many linguistic and psycho-linguistic theories posit some kind of hierarchical labeled constituent (or constructional) structure, arguing that it has measurable psychological (cognitive) reality(e.g., (goldberg, 2006)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1021">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" W06-2912 ">
mar. most recent work (e.g., (klein and manning,2004; <papid> P04-1061 </papid>dennis, 2005; bod, 2006<papid> W06-2912 </papid>a; smith and eisner, 2006; <papid> P06-1072 </papid>seginer, 2007)<papid> P07-1049 </papid>annotates text sentences using hierarchical bracketing (constituents) or ade pendency structure, and thus represents the induced grammar through its behavior in parsingtask.</citsent>
<aftsection>
<nextsent>solan et al (2005) uses graph representation, while (nakamura, 2006) simply uses grammar formalism such as pcfg.
</nextsent>
<nextsent>when the bracketing approach is taken, some algorithms label the resulting constituents, while most do not.each of these approaches can be justified or criticized; detailed discussion of this issue is beyond the scope of this paper.
</nextsent>
<nextsent>the algorithm presented here belongs to the first group, annotating given sentences with labeled bracketing structures.
</nextsent>
<nextsent>the main theoretical justification for this approach is that many linguistic and psycho-linguistic theories posit some kind of hierarchical labeled constituent (or constructional) structure, arguing that it has measurable psychological (cognitive) reality(e.g., (goldberg, 2006)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1027">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" P06-1072 ">
mar. most recent work (e.g., (klein and manning,2004; <papid> P04-1061 </papid>dennis, 2005; bod, 2006<papid> W06-2912 </papid>a; smith and eisner, 2006; <papid> P06-1072 </papid>seginer, 2007)<papid> P07-1049 </papid>annotates text sentences using hierarchical bracketing (constituents) or ade pendency structure, and thus represents the induced grammar through its behavior in parsingtask.</citsent>
<aftsection>
<nextsent>solan et al (2005) uses graph representation, while (nakamura, 2006) simply uses grammar formalism such as pcfg.
</nextsent>
<nextsent>when the bracketing approach is taken, some algorithms label the resulting constituents, while most do not.each of these approaches can be justified or criticized; detailed discussion of this issue is beyond the scope of this paper.
</nextsent>
<nextsent>the algorithm presented here belongs to the first group, annotating given sentences with labeled bracketing structures.
</nextsent>
<nextsent>the main theoretical justification for this approach is that many linguistic and psycho-linguistic theories posit some kind of hierarchical labeled constituent (or constructional) structure, arguing that it has measurable psychological (cognitive) reality(e.g., (goldberg, 2006)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1028">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" P07-1049 ">
mar. most recent work (e.g., (klein and manning,2004; <papid> P04-1061 </papid>dennis, 2005; bod, 2006<papid> W06-2912 </papid>a; smith and eisner, 2006; <papid> P06-1072 </papid>seginer, 2007)<papid> P07-1049 </papid>annotates text sentences using hierarchical bracketing (constituents) or ade pendency structure, and thus represents the induced grammar through its behavior in parsingtask.</citsent>
<aftsection>
<nextsent>solan et al (2005) uses graph representation, while (nakamura, 2006) simply uses grammar formalism such as pcfg.
</nextsent>
<nextsent>when the bracketing approach is taken, some algorithms label the resulting constituents, while most do not.each of these approaches can be justified or criticized; detailed discussion of this issue is beyond the scope of this paper.
</nextsent>
<nextsent>the algorithm presented here belongs to the first group, annotating given sentences with labeled bracketing structures.
</nextsent>
<nextsent>the main theoretical justification for this approach is that many linguistic and psycho-linguistic theories posit some kind of hierarchical labeled constituent (or constructional) structure, arguing that it has measurable psychological (cognitive) reality(e.g., (goldberg, 2006)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1030">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the algorithm we present here requires pos tags for its labeling stages.
</prevsent>
<prevsent>parts-of-speech are widely considered to have psychological reality (at least in english,including when they are viewed as low-level constructions as in (croft, 2001)), so this kind of input is reasonable for theoretical research.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
moreover, as pos induction is of medium quality (clark, 2003), <papid> E03-1009 </papid>using manually pos tagged corpus enables us to measure the performance of other induction stage sin controlled manner.</citsent>
<aftsection>
<nextsent>since supervised pos tagging is of very high quality and very efficient computationally (brants, 2000), <papid> A00-1031 </papid>this requirement does not seriously limit the practical applicability of grammar induction algorithm.</nextsent>
<nextsent>our labeled bracketings induction algorithm consists of three stages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1031">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>parts-of-speech are widely considered to have psychological reality (at least in english,including when they are viewed as low-level constructions as in (croft, 2001)), so this kind of input is reasonable for theoretical research.
</prevsent>
<prevsent>moreover, as pos induction is of medium quality (clark, 2003), <papid> E03-1009 </papid>using manually pos tagged corpus enables us to measure the performance of other induction stage sin controlled manner.</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
since supervised pos tagging is of very high quality and very efficient computationally (brants, 2000), <papid> A00-1031 </papid>this requirement does not seriously limit the practical applicability of grammar induction algorithm.</citsent>
<aftsection>
<nextsent>our labeled bracketings induction algorithm consists of three stages.
</nextsent>
<nextsent>we first induce unlabeled bracketing trees using the algorithm given in(seginer, 2007)<papid> P07-1049 </papid>1.</nextsent>
<nextsent>we then induce initial labels using bayesian model merging (bmm) labeling algorithm (borensztajn and zuidema, 2007), which aims at minimizing the description length of the input data and the induced grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1033">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we 1the algorithm uses raw (not pos tagged) sentences.
</prevsent>
<prevsent>experimented with english (wsj10, brown10),german (negra10) and chinese (ctb10) corpora.
</prevsent>
</prevsection>
<citsent citstr=" P06-1111 ">
when comparing to previous work that used manually annotated corpora in its evaluation (haghighi and klein, 2006)<papid> P06-1111 </papid>2, we obtained 59.5% labeled f-score on the wsj10 setup vs. their 35.3%(section 5).</citsent>
<aftsection>
<nextsent>we also show substantial improvement over random baseline, and that the clustering stage of our algorithm improves the results of the second merging stage.
</nextsent>
<nextsent>section 2 discusses previous work.
</nextsent>
<nextsent>in section 3 we detail our algorithm.
</nextsent>
<nextsent>the experimental setup and results are presented in sections 4 and 5.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1034">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>unsupervised parsing has attracted researchers for decades (see (clark, 2001; klein, 2005) for recent reviews).
</prevsent>
<prevsent>many types of input, syntax formalisms, search procedures, and success criteria were used.
</prevsent>
</prevsection>
<citsent citstr=" P95-1031 ">
among the theoretical and practical motivations tothis problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (van zaanen, 2001), and improving language models (chen, 1995).<papid> P95-1031 </papid>in recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the wsj penntreebank.</citsent>
<aftsection>
<nextsent>recently,works along this line have for the first time outperformed the right branching heuristic baseline for english.
</nextsent>
<nextsent>these include the constituent context model (ccm) (klein and manning, 2002), <papid> P02-1017 </papid>its extension using dependency model (klein and manning, 2004), (<papid> P04-1061 </papid>u)dop based models (bod, 2006<papid> W06-2912 </papid>a; bod, 2006<papid> W06-2912 </papid>b; bod, 2007), <papid> P07-1051 </papid>an exemplar?</nextsent>
<nextsent>based approach (dennis, 2005), guiding em using contrastive estimation (smith and eisner, 2006), <papid> P06-1072 </papid>and the incremental parser of (seginer, 2007)<papid> P07-1049 </papid>.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1035">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>among the theoretical and practical motivations tothis problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (van zaanen, 2001), and improving language models (chen, 1995).<papid> P95-1031 </papid>in recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the wsj penntreebank.</prevsent>
<prevsent>recently,works along this line have for the first time outperformed the right branching heuristic baseline for english.</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
these include the constituent context model (ccm) (klein and manning, 2002), <papid> P02-1017 </papid>its extension using dependency model (klein and manning, 2004), (<papid> P04-1061 </papid>u)dop based models (bod, 2006<papid> W06-2912 </papid>a; bod, 2006<papid> W06-2912 </papid>b; bod, 2007), <papid> P07-1051 </papid>an exemplar?</citsent>
<aftsection>
<nextsent>based approach (dennis, 2005), guiding em using contrastive estimation (smith and eisner, 2006), <papid> P06-1072 </papid>and the incremental parser of (seginer, 2007)<papid> P07-1049 </papid>.</nextsent>
<nextsent>all of these use as input pos tag sequences, except of seginers algorithm, which uses plain text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1049">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>among the theoretical and practical motivations tothis problem are the study of human language acquisition (in particular, an empirical study of the poverty of stimulus hypothesis), preprocessing for constructing large treebanks (van zaanen, 2001), and improving language models (chen, 1995).<papid> P95-1031 </papid>in recent years efforts have been made to evaluate the algorithms on manually annotated corpora such as the wsj penntreebank.</prevsent>
<prevsent>recently,works along this line have for the first time outperformed the right branching heuristic baseline for english.</prevsent>
</prevsection>
<citsent citstr=" P07-1051 ">
these include the constituent context model (ccm) (klein and manning, 2002), <papid> P02-1017 </papid>its extension using dependency model (klein and manning, 2004), (<papid> P04-1061 </papid>u)dop based models (bod, 2006<papid> W06-2912 </papid>a; bod, 2006<papid> W06-2912 </papid>b; bod, 2007), <papid> P07-1051 </papid>an exemplar?</citsent>
<aftsection>
<nextsent>based approach (dennis, 2005), guiding em using contrastive estimation (smith and eisner, 2006), <papid> P06-1072 </papid>and the incremental parser of (seginer, 2007)<papid> P07-1049 </papid>.</nextsent>
<nextsent>all of these use as input pos tag sequences, except of seginers algorithm, which uses plain text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1057">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>for each pair (x , j ) of induced and target labels, let x ,y jbe the number of times they label constituent having the same span in the same sentence.
</prevsent>
<prevsent>following (haghighi and klein, 2006)<papid> P06-1111 </papid> we applieda greedy (many to one) mapping where the mapping is given by map(x ) = argmax j x ,y . this greedy mapping tends to map many induced labels to the same target label, and is therefore highly forgiving of large mismatches between the structures of the induced and target grammars.hence, we also applied label-to-label (ll) mapping, computed by reducing this problem to optimal assignment in weighted complete bipar tite graph, formally defined as follows.</prevsent>
</prevsection>
<citsent citstr=" D07-1043 ">
given weighted complete bipartite graph = (x ? ;x ? ) where edge (x , j ) has weight ij , 8excluding punctuation and null elements, according to the scheme of (klein, 2005).9there are many possible methods for evaluating clustering quality (rosenberg and hirschberg, 2007).<papid> D07-1043 </papid></citsent>
<aftsection>
<nextsent>for our task, overall f-score is very natural one.
</nextsent>
<nextsent>we will address other methods in future papers.find (one-to-one) matching from to having maximal weight.
</nextsent>
<nextsent>in our case, is the set ofmodel symbols, is the set of or most frequent target symbols (depending on the desired label set size used), and ij := x ,y , computed as in greedy mapping (the number of times i and j share constituent).
</nextsent>
<nextsent>to make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1058">
<title id=" C08-1091.xml">unsupervised induction of labeled parse trees by clustering with syntactic features </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>in our case, is the set ofmodel symbols, is the set of or most frequent target symbols (depending on the desired label set size used), and ij := x ,y , computed as in greedy mapping (the number of times i and j share constituent).
</prevsent>
<prevsent>to make the graph complete, we add zero weight edges between induced and target labels that do not share any constituent.
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
the kuhn-munkres algorithm (kuhn, 1955; munkres,1957) solves this problem, and we used it to perform the ll mapping (see also (luo, 2005)).<papid> H05-1004 </papid></citsent>
<aftsection>
<nextsent>we assessed the overall quality of our algorithm, the quality of its labeling stage and the quality ofthe syntactic clustering (sc) stage.
</nextsent>
<nextsent>for the overall quality of the induced grammar (both bracketing and labeling) we compare our results with (haghighi and klein, 2006)<papid> P06-1111 </papid>, using their setup10.</nextsent>
<nextsent>that setup was used for all numbers reported in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1064">
<title id=" C04-1047.xml">using a mixture of nbest lists from multiple mt systems in ranksumbased confidence measure for mt outputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the existing rscm does not always work well 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 co rre ct re jec tio r ate : correct acceptance rate: performance of existing method (a|bcd) j2e sat + existing method j2e hpat + existing method j2e d3 + existing method figure 1: performance of the existing rscm on three different types of japanese-to-english (j2e) mt sys tems: d3, hpat, and sat.
</prevsent>
<prevsent>the existing rscm tried to accept perfect mt outputs (grade in section 4) and to reject imperfect mt outputs (grades b, c, and in section 4).
</prevsent>
</prevsection>
<citsent citstr=" W03-0318 ">
on types of mt systems other than smt systems.figure 1 shows the differences among the performances, indicated by the receiver operating characteristics (roc) curve (section 4.1), of the existing rscm on each of three mt systems (section 4.2.1): d3, hpat, and sat (doi and sumita, 2003; <papid> W03-0318 </papid>imamura et al, 2003; <papid> P03-1057 </papid>watanabe et al, 2003).</citsent>
<aftsection>
<nextsent>only sat is an smt system; the others are not.
</nextsent>
<nextsent>the ideal roc curve is square (0), is square (1), (1),  (1), (1),  (0); thus, the closer the curve is to square, the better the performance of the rscm is. the performances of the existing rscm on the non-smt systems, d3 andhpat, are much worse than that on the smt system, sat.
</nextsent>
<nextsent>the performance of the existing rscm depend son the goodness/density of mt outputs in the best list from the system.
</nextsent>
<nextsent>however, the systemsn-best list does not always give good approximation of the total summation of the probability of all candidate translations given the source sentence/utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1065">
<title id=" C04-1047.xml">using a mixture of nbest lists from multiple mt systems in ranksumbased confidence measure for mt outputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the existing rscm does not always work well 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 co rre ct re jec tio r ate : correct acceptance rate: performance of existing method (a|bcd) j2e sat + existing method j2e hpat + existing method j2e d3 + existing method figure 1: performance of the existing rscm on three different types of japanese-to-english (j2e) mt sys tems: d3, hpat, and sat.
</prevsent>
<prevsent>the existing rscm tried to accept perfect mt outputs (grade in section 4) and to reject imperfect mt outputs (grades b, c, and in section 4).
</prevsent>
</prevsection>
<citsent citstr=" P03-1057 ">
on types of mt systems other than smt systems.figure 1 shows the differences among the performances, indicated by the receiver operating characteristics (roc) curve (section 4.1), of the existing rscm on each of three mt systems (section 4.2.1): d3, hpat, and sat (doi and sumita, 2003; <papid> W03-0318 </papid>imamura et al, 2003; <papid> P03-1057 </papid>watanabe et al, 2003).</citsent>
<aftsection>
<nextsent>only sat is an smt system; the others are not.
</nextsent>
<nextsent>the ideal roc curve is square (0), is square (1), (1),  (1), (1),  (0); thus, the closer the curve is to square, the better the performance of the rscm is. the performances of the existing rscm on the non-smt systems, d3 andhpat, are much worse than that on the smt system, sat.
</nextsent>
<nextsent>the performance of the existing rscm depend son the goodness/density of mt outputs in the best list from the system.
</nextsent>
<nextsent>however, the systemsn-best list does not always give good approximation of the total summation of the probability of all candidate translations given the source sentence/utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1066">
<title id=" C04-1047.xml">using a mixture of nbest lists from multiple mt systems in ranksumbased confidence measure for mt outputs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the n-best list is expected to approximate the total summation as closely as possible.
</prevsent>
<prevsent>this paper proposes method that eliminates unsatisfactory top output by using an alternativerscm based on mixture of n-best lists from multiple mt systems (figure 2).
</prevsent>
</prevsection>
<citsent citstr=" C02-1076 ">
the elimination system is intended to be used in the selector architecture, as in (akiba et al, 2002).<papid> C02-1076 </papid></citsent>
<aftsection>
<nextsent>the total translation quality of the selector architecture proved to be better than the translation quality of each elementmt system.
</nextsent>
<nextsent>the final output from the selection system is the best among the satisfactory top2 outputs from the elimination system.
</nextsent>
<nextsent>in the case of figure 2, the selection system can receive zero to three top mt outputs.
</nextsent>
<nextsent>when the selection system receive fewer than two top mt outputs, the selection system merely passes null output or the one top mt output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1081">
<title id=" C04-1047.xml">using a mixture of nbest lists from multiple mt systems in ranksumbased confidence measure for mt outputs </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>this sorted mixture is alternatively used instead of the systems n-best list in the existing rscm.
</prevsent>
<prevsent>that is, the proposed rscmchecks whether it accepts/rejects each top mt out put in the original m-best lists by using the sorted mixture; on the other hand, the existing rscmchecks whether it accepts/rejects the top mt out put in the systems n-best list by using the systems n-best.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
for scoring mt outputs, the proposed rscm uses score based on translation model called ibm4 (brown et al, 1993) (<papid> J93-2003 </papid>tm-score) and sco rebased on language model for the translation target language (lm-score).</citsent>
<aftsection>
<nextsent>as akiba et al (2002) <papid> C02-1076 </papid>reported, the products of tm-scores and lm-scores are statistical variables.</nextsent>
<nextsent>even in the case where the translation model (tm) and the language model for the translation target language (lm) are trained on sub-corpus of the same size, changing the training corpus also changes the tm-score, the lm-score,and their product.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1096">
<title id=" C04-1047.xml">using a mixture of nbest lists from multiple mt systems in ranksumbased confidence measure for mt outputs </title>
<section> experimental comparison.  </section>
<citcontext>
<prevsection>
<prevsent>4.2.3 training tms and lmsthe corpora used for training tms and lms described in section 3.2 were merged corpora (table 3).
</prevsent>
<prevsent>the number of trained tms/lms was three.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the translation models and language models were learned by using giza++ (och and ney, 2000) <papid> P00-1056 </papid>andthe cmu-cambridge toolkit (clarkson and rosenfeld, 1997), respectively.</citsent>
<aftsection>
<nextsent>4.3 experimental results and discussion.
</nextsent>
<nextsent>4.3.1 roc curvein order to plot the roc curve, the authors conducted the same experiment as shown in figure 1.
</nextsent>
<nextsent>that is, in the case where the grade of satisfactory translations is only grade a, each of the proposed and existing rscms tried to accept grade mt outputs and to reject grade b, c, or mt outputs.
</nextsent>
<nextsent>figures 5 to 7 show the roc curves for each of the three j2e mt systems (d3, hpat, and sat).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1097">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show statistically significant improvements of the alignment quality compared to the best results reported so far.
</prevsent>
<prevsent>on the verb mobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
word-aligned bilingual corpora provide important knowledge for many natural language processing tasks, such as the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>the solutions of these problems depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></nextsent>
<nextsent>word alignment models were first introduced in statistical machine translation (brown et al., 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1098">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show statistically significant improvements of the alignment quality compared to the best results reported so far.
</prevsent>
<prevsent>on the verb mobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
word-aligned bilingual corpora provide important knowledge for many natural language processing tasks, such as the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>the solutions of these problems depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></nextsent>
<nextsent>word alignment models were first introduced in statistical machine translation (brown et al., 1993).<papid> J93-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1106">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word-aligned bilingual corpora provide important knowledge for many natural language processing tasks, such as the extraction of bilingual word or phrase lexica (melamed, 2000; <papid> J00-2004 </papid>och and ney, 2000).<papid> P00-1056 </papid></prevsent>
<prevsent>the solutions of these problems depend heavily on the quality of the word alignment (och and ney, 2000).<papid> P00-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
word alignment models were first introduced in statistical machine translation (brown et al., 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>an alignment describes mapping from source sentence words to target sentence words.
</nextsent>
<nextsent>using the ibm translation models ibm-1 to ibm-5 (brown et al, 1993), <papid> J93-2003 </papid>as well as the hidden-markov alignment model (vogel et al., 1996), <papid> C96-2141 </papid>we can produce alignments of good quality.</nextsent>
<nextsent>however, all these models constrain the alignments so that source word can be aligned to at most one target word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1110">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment models were first introduced in statistical machine translation (brown et al., 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>an alignment describes mapping from source sentence words to target sentence words.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
using the ibm translation models ibm-1 to ibm-5 (brown et al, 1993), <papid> J93-2003 </papid>as well as the hidden-markov alignment model (vogel et al., 1996), <papid> C96-2141 </papid>we can produce alignments of good quality.</citsent>
<aftsection>
<nextsent>however, all these models constrain the alignments so that source word can be aligned to at most one target word.
</nextsent>
<nextsent>this constraint is useful to reduce the computational complexity of the model training, but makesit hard to align phrases in the target language (english) such as the day after tomorrow?
</nextsent>
<nextsent>to one word in the source language (ger man) ubermorgen?.
</nextsent>
<nextsent>we will present word alignment algorithm which avoids this constraint and produces symmetric word alignments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1111">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parameters of the ibm models and hmm,in particular the state occupation probabilities, will be used to determine the costs of aligning specific source word to target word.
</prevsent>
<prevsent>we will evaluate the suggested alignment methods on the german english verbmo bil task and the french english canadianhansards task.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we will show statistically significant improvements compared to state-of the-art results in (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>in this section, we will give an overview of the commonly used statistical word alignmenttechniques.
</nextsent>
<nextsent>they are based on the source channel approach to statistical machine translation (brown et al, 1993).<papid> J93-2003 </papid></nextsent>
<nextsent>we are givena source language sentence fj1 := f1...fj ...fjwhich has to be translated into target language sentence ei1 := e1...ei...ei . among all possible target language sentences, we will choose the sentence with the highest proba bility: ei1 = argmax ei1 {pr(ei1|fj1 ) } = argmax ei1 {pr(ei1) ? pr(fj1 |ei1) } this decomposition into two knowledge sources allows for an independent modeling of target language model pr(ei1) and translation model pr(fj1 |ei1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1149">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>that article also introduces the model 6; additionally, state-of-the-art results are presented for the verb mobil task and the canadian hansa rds task for various configurations.
</prevsent>
<prevsent>therefore, we chose them as baseline.
</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
additional linguistic knowledge sources such as dependeny trees or parse trees were used in(cherry and lin, 2003; <papid> P03-1012 </papid>gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>(melamed, 2000) <papid> J00-2004 </papid>uses an alignment model that enforces one-to-one alignments for non empty words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1150">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>that article also introduces the model 6; additionally, state-of-the-art results are presented for the verb mobil task and the canadian hansa rds task for various configurations.
</prevsent>
<prevsent>therefore, we chose them as baseline.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
additional linguistic knowledge sources such as dependeny trees or parse trees were used in(cherry and lin, 2003; <papid> P03-1012 </papid>gildea, 2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>(melamed, 2000) <papid> J00-2004 </papid>uses an alignment model that enforces one-to-one alignments for non empty words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1151">
<title id=" C04-1032.xml">symmetric word alignments for statistical machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, we chose them as baseline.
</prevsent>
<prevsent>additional linguistic knowledge sources such as dependeny trees or parse trees were used in(cherry and lin, 2003; <papid> P03-1012 </papid>gildea, 2003).<papid> P03-1011 </papid></prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
bilingual bracketing methods were used to produce word alignment in (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>(melamed, 2000) <papid> J00-2004 </papid>uses an alignment model that enforces one-to-one alignments for non empty words.</nextsent>
<nextsent>table 5: aer[%] for different alignment symmetrization methods and for various alignment models on the canadian hansa rds and the verb mobil tasks (mwec: minimum weight edge cover, ew: empty word).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1153">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>conversely, common situation in natural language generation is that one semantic representation can be verbal ised in multiple ways.
</prevsent>
<prevsent>this means that the relation between syntax and semantics is not functional at all, but rather true m-to-n relation.there is variety of approaches in the literature on syntax-semantics interfaces for coping with this situation, but none of them is completely satisfactory.
</prevsent>
</prevsection>
<citsent citstr=" P99-1039 ">
one way is to recast semantic ambiguity as syntactic ambiguity by compiling semantic distinctions into the syntax (montague, 1974; steedman, 1999; <papid> P99-1039 </papid>moortgat, 2002).</citsent>
<aftsection>
<nextsent>this restores functionality, but comes at the price of an artificial blow up of syntactic ambiguity.
</nextsent>
<nextsent>a second approach is to assume non-deterministic mapping from syntax to semantics as in generative grammar (chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation.
</nextsent>
<nextsent>for lfg, the operation of functional uncertaintainty allows for restricted form of relationality (kaplan and maxwell iii, 1988).<papid> C88-1060 </papid></nextsent>
<nextsent>finally, under specification (egg et al, 2001; gupta and lamping, 1998; <papid> P98-1077 </papid>copestake et al, 2004) introduces new level of representation,which can be computed functionally from syntactic analysis and encapsulates semantic ambiguity in way that supports the enumeration of all semantic readings by need.in this paper, we introduce completely relational syntax-semantics interface, building upon the under specification approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1154">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this restores functionality, but comes at the price of an artificial blow up of syntactic ambiguity.
</prevsent>
<prevsent>a second approach is to assume non-deterministic mapping from syntax to semantics as in generative grammar (chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation.
</prevsent>
</prevsection>
<citsent citstr=" C88-1060 ">
for lfg, the operation of functional uncertaintainty allows for restricted form of relationality (kaplan and maxwell iii, 1988).<papid> C88-1060 </papid></citsent>
<aftsection>
<nextsent>finally, under specification (egg et al, 2001; gupta and lamping, 1998; <papid> P98-1077 </papid>copestake et al, 2004) introduces new level of representation,which can be computed functionally from syntactic analysis and encapsulates semantic ambiguity in way that supports the enumeration of all semantic readings by need.in this paper, we introduce completely relational syntax-semantics interface, building upon the under specification approach.</nextsent>
<nextsent>we assume set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; grammatical analysis is tuple with one component for each dimension, and grammar describes setof such tuples.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1155">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a second approach is to assume non-deterministic mapping from syntax to semantics as in generative grammar (chomsky, 1965), but it is not always obvious how to reverse the relation, e. g. for generation.
</prevsent>
<prevsent>for lfg, the operation of functional uncertaintainty allows for restricted form of relationality (kaplan and maxwell iii, 1988).<papid> C88-1060 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1077 ">
finally, under specification (egg et al, 2001; gupta and lamping, 1998; <papid> P98-1077 </papid>copestake et al, 2004) introduces new level of representation,which can be computed functionally from syntactic analysis and encapsulates semantic ambiguity in way that supports the enumeration of all semantic readings by need.in this paper, we introduce completely relational syntax-semantics interface, building upon the under specification approach.</citsent>
<aftsection>
<nextsent>we assume set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; grammatical analysis is tuple with one component for each dimension, and grammar describes setof such tuples.
</nextsent>
<nextsent>while we make no priori functionality assumptions about the relation of the linguistic dimensions, functional mappings can be obtain edas special case.
</nextsent>
<nextsent>we formalise our syntax-semantics interface using extensible dependency grammar (xdg), new grammar formalism which gen eralises earlier work on topological dependency grammar (duchier and debusmann, 2001).<papid> P01-1024 </papid>the relational syntax-semantics interface is supported by parser for xdg based on constraint pro gramming.</nextsent>
<nextsent>the crucial feature of this parser is thatit supports the concurrent flow of possibly partial information between any two dimensions: once additional information becomes available on one dimension, it can be propagated to any other dimension.grammaticality conditions and preferences (e. g. selectional restrictions) can be specified on their natural level of representation, and inferences on each dimension can help reduce ambiguity on the others.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1156">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we assume set of linguistic dimensions, such as (syntactic) immediate dominance and predicate-argument structure; grammatical analysis is tuple with one component for each dimension, and grammar describes setof such tuples.
</prevsent>
<prevsent>while we make no priori functionality assumptions about the relation of the linguistic dimensions, functional mappings can be obtain edas special case.
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
we formalise our syntax-semantics interface using extensible dependency grammar (xdg), new grammar formalism which gen eralises earlier work on topological dependency grammar (duchier and debusmann, 2001).<papid> P01-1024 </papid>the relational syntax-semantics interface is supported by parser for xdg based on constraint pro gramming.</citsent>
<aftsection>
<nextsent>the crucial feature of this parser is thatit supports the concurrent flow of possibly partial information between any two dimensions: once additional information becomes available on one dimension, it can be propagated to any other dimension.grammaticality conditions and preferences (e. g. selectional restrictions) can be specified on their natural level of representation, and inferences on each dimension can help reduce ambiguity on the others.
</nextsent>
<nextsent>this generali ses the idea of underspecifica tion, which aims to represent and reduce ambiguity through inferences on single dimension only.the structure of this paper is as follows: in section 2, we give the general ideas behind xdg, its formal definition, and an overview of the constraint based parser.
</nextsent>
<nextsent>in section 3, we present the relationalsyntax-semantics interface, and go through examples that illustrate its operation.
</nextsent>
<nextsent>section 4 show show the semantics side of our syntax-semantics interface can be precisely related to mainstream semantics research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1158">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> extensible dependency grammar.  </section>
<citcontext>
<prevsection>
<prevsent>because propagation operates on all dimensions concurrently, the constraint solver can frequently infer information about one dimension from information on another, if there is multi-dimensionalprinciple linking the two dimensions.
</prevsent>
<prevsent>these inferences take place while the constraint problem is being solved, and they can often be drawn before the solver commits to any single solution.
</prevsent>
</prevsection>
<citsent citstr=" P02-1003 ">
because xdg allows us to write grammars with completely free word order, xdg solving is an np complete problem (koller and striegnitz, 2002).<papid> P02-1003 </papid></citsent>
<aftsection>
<nextsent>this means that the worst-case complexity of the solver is exponential, but the average-case complexity for the hand-crafted grammars we experimented with is often better than this result suggests.
</nextsent>
<nextsent>we hope there are useful fragments of xdg that would guarantee polynomial worst-case complexity.
</nextsent>
<nextsent>now that we have the formal and processing frameworks in place, we can define relational syntax semantics interface for xdg.
</nextsent>
<nextsent>we will first showhow we encode semantics within the xdg framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1159">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> a relational syntax-semantics interface </section>
<citcontext>
<prevsection>
<prevsent>this means that semantic information can beused to disambiguate syntactic ambiguities, and semantic information such as selectional preferences can be stated on their natural level of representation, rather than be forced into the id dimension directly.
</prevsent>
<prevsent>similarly, the introduction of new edges on sc could trigger similar reasoning process which would infer new pa-edges, and thus indirectly also new id-edges.
</prevsent>
</prevsection>
<citsent citstr=" C00-1067 ">
such new edges on sc could come from inferences with world or discourse knowledge (koller and niehren, 2000), <papid> C00-1067 </papid>scope preferences, or interactions with information structure (duchier and kruijff, 2003).</citsent>
<aftsection>
<nextsent>our syntax-semantics interface represents semantic information as graphs on the pa and sc dimensions.
</nextsent>
<nextsent>while this looks like radical departure from traditional semantic formalisms, we consider these graphs simply an alternative way of presenting more traditional representations.
</nextsent>
<nextsent>we devote the rest of the paper to demonstrating that pair of pa and scstructure can be interpreted as montague-style formula, and that partial analysis on these two dimensions can be seen as an underspecified semantic description.
</nextsent>
<nextsent>4.1 montague-style interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1160">
<title id=" C04-1026.xml">a relational syntax semantics interface based on dependency grammar </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>this means that we get bidirectional grammars for free.
</prevsent>
<prevsent>while the solver is reasonably efficient for many (hand-crafted) grammars, it is an important goalfor the future to ensure that it can handle large scale grammars imported from e.g. xtag (xtag research group, 2001) or induced from treebanks.one way in which we hope to achieve this is to identify fragments of xdg with prov ably polynomial parsing algorithms, and which contain most useful grammars.
</prevsent>
</prevsection>
<citsent citstr=" J93-4001 ">
such grammars would probably have to specify word orders that are not completely free, and we would have to control the combinatorics of the different dimensions (maxwell and kaplan,1993).<papid> J93-4001 </papid></citsent>
<aftsection>
<nextsent>one interesting question is also whether different dimensions can be compiled into single dimension, which might improve efficiency in some cases, and also sidestep the monostratal vs. multi stratal distinction.the crucial ingredient of xdg that make relational syntax-semantics processing possible are the declaratively specified principles.
</nextsent>
<nextsent>so far, we have only given some examples for principle specifi cations; while they could all be written as horn clauses, we have not committed to any particular representation formalism.
</nextsent>
<nextsent>the development of sucha representation formalism will of course be extremely important once we have experimented with more powerful grammars and have stable intuition about what principles are needed.at that point, it would also be highly interesting to define (logic) formalism that generali ses both xdg and dominance constraints, fragment of clls.
</nextsent>
<nextsent>such formalism would make it possible totake over the interface presented here, but use dominance constraints directly on the semantics dimensions, rather than via the encoding into pa and sc dimensions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1161">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with respect to part-of-speech tagging, we believe that the way forward from the relatively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and lexicon of words and their possible parts of speech.
</prevsent>
<prevsent>we believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for.
</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
unsupervised part-of-speech tagging, as defined above, has been attempted using variety of learning algorithms (brill 1995, <papid> W95-0101 </papid>church, 1988, cutting et. al. 1992, <papid> A92-1018 </papid>elworthy, 1994 <papid> A94-1009 </papid>kupiec 1992, merialdo 1991).</citsent>
<aftsection>
<nextsent>while this makes unsupervised part-of-speech tagging relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners.
</nextsent>
<nextsent>in this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.
</nextsent>
<nextsent>in addition, we explore two new ideas for improving tagging accuracy.
</nextsent>
<nextsent>first, we explore an hmm approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (lafferty et. al. 2001, toutanova et. al. 2003).<papid> N03-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1162">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with respect to part-of-speech tagging, we believe that the way forward from the relatively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and lexicon of words and their possible parts of speech.
</prevsent>
<prevsent>we believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
unsupervised part-of-speech tagging, as defined above, has been attempted using variety of learning algorithms (brill 1995, <papid> W95-0101 </papid>church, 1988, cutting et. al. 1992, <papid> A92-1018 </papid>elworthy, 1994 <papid> A94-1009 </papid>kupiec 1992, merialdo 1991).</citsent>
<aftsection>
<nextsent>while this makes unsupervised part-of-speech tagging relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners.
</nextsent>
<nextsent>in this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.
</nextsent>
<nextsent>in addition, we explore two new ideas for improving tagging accuracy.
</nextsent>
<nextsent>first, we explore an hmm approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (lafferty et. al. 2001, toutanova et. al. 2003).<papid> N03-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1163">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with respect to part-of-speech tagging, we believe that the way forward from the relatively small number of languages for which we can currently identify parts of speech in context with reasonable accuracy will make use of unsupervised methods that require only an untagged corpus and lexicon of words and their possible parts of speech.
</prevsent>
<prevsent>we believe this based on the fact that such lexicons exist for many more languages (in the form of conventional dictionaries) than extensive human-tagged training corpora exist for.
</prevsent>
</prevsection>
<citsent citstr=" A94-1009 ">
unsupervised part-of-speech tagging, as defined above, has been attempted using variety of learning algorithms (brill 1995, <papid> W95-0101 </papid>church, 1988, cutting et. al. 1992, <papid> A92-1018 </papid>elworthy, 1994 <papid> A94-1009 </papid>kupiec 1992, merialdo 1991).</citsent>
<aftsection>
<nextsent>while this makes unsupervised part-of-speech tagging relatively well-studied problem, published results to date have not been comparable with respect to the training and test data used, or the lexicons which have been made available to the learners.
</nextsent>
<nextsent>in this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.
</nextsent>
<nextsent>in addition, we explore two new ideas for improving tagging accuracy.
</nextsent>
<nextsent>first, we explore an hmm approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (lafferty et. al. 2001, toutanova et. al. 2003).<papid> N03-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1164">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we provide the first comprehensive comparison of methods for unsupervised part-of speech tagging.
</prevsent>
<prevsent>in addition, we explore two new ideas for improving tagging accuracy.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
first, we explore an hmm approach to tagging that uses context on both sides of the word to be tagged, inspired by previous work on building bidirectionality into graphical models (lafferty et. al. 2001, toutanova et. al. 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>second we describe method for sequential unsupervised training of tag sequence and lexical probabilities in an hmm, which we observe leads to improved accuracy over simultaneous training with certain types of models.
</nextsent>
<nextsent>in section 2, we provide brief description of the methods we evaluate and review published results.
</nextsent>
<nextsent>section 3 describes the contextual ized variation on hmm tagging that we have explored.
</nextsent>
<nextsent>in section 4 we provide direct comparison of several unsupervised part-of-speech taggers, which is followed by section 5, in which we present new method for training with sub optimal lexicons.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1166">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>words contained within the same equivalence classes are those which possess the same set of possible parts of speech.
</prevsent>
<prevsent>another highly-accurate method for part-of speech tagging from un labelled data is brills unsupervised transformation-based learner (utbl) (brill, 1995).<papid> W95-0101 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
derived from his supervised transformation-based tagger (brill, 1992), <papid> A92-1021 </papid>utbl uses information from the distribution of unambiguously tagged data to make informed labeling decisions in ambiguous contexts.</citsent>
<aftsection>
<nextsent>in contrast to the hmm taggers previously described, which make use of contextual information coming from the left side only, utbl considers both left and right contexts.
</nextsent>
<nextsent>reported tagging accuracies for these methods range from 87% to 96%, but are not directly comparable.
</nextsent>
<nextsent>kupiecs hmm class-based tagger, when trained on sample of 440,000 words of the original brown corpus, obtained test set accuracy of 95.7%.
</nextsent>
<nextsent>brill assessed his utbl tagger using 350,000 words of the brown corpus for training, and found that 96% of words in separate 200,000-word test set could be tagged correctly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1170">
<title id=" C04-1080.xml">partofspeech tagging in context </title>
<section> unsupervised tagging: comparison.  </section>
<citcontext>
<prevsection>
<prevsent>for our comparison of unsupervised tagging methods, we implemented the hmm taggers described in merialdo (1991) and kupiec (1992), as well as the utbl tagger described in brill (1995).<papid> W95-0101 </papid></prevsent>
<prevsent>we also implemented version of the contextual ized hmm using the type of word classes utilized in the kupiec model.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the algorithms were trained and tested using version 3 of the penn treebank, using the training, development, and test split described in collins (2002) <papid> W02-1001 </papid>and also employed by toutanova et al  (2003) <papid> N03-1033 </papid>in testing their supervised tagging algorithm.</citsent>
<aftsection>
<nextsent>specifically, we allocated sections 00 18 for training, 19-21 for development, and 22-24 for testing.
</nextsent>
<nextsent>to avoid the problem of unknown words, each learner was provided with lexicon constructed from tagged versions of the full treebank.
</nextsent>
<nextsent>we did not begin with any estimates of the likelihoods of tags for words, but only the knowledge of what tags are possible for each word in the lexicon, i.e., something we could obtain from manually-constructed dictionary.
</nextsent>
<nextsent>4.2 the effect of lexicon construction on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1174">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the corresponding phonemes shown in boldface originate from single proto-phoneme.
</prevsent>
<prevsent>from unprocessed bilingual word lists could be ofgreat assistance to historical linguists.
</prevsent>
</prevsection>
<citsent citstr=" J94-3004 ">
the reconstruction engine (lowe and mazaudon, 1994), <papid> J94-3004 </papid>setof programs designed to be an aid in language reconstruction, requires set of correspondences to be provided beforehand.</citsent>
<aftsection>
<nextsent>the determination of correspondences is closely related to another task that has been much studied in computational linguistics, the identification of cognates.
</nextsent>
<nextsent>cognates have been employed for sentence and word alignment in bitexts (simardet al, 1992; melamed, 1999), <papid> J99-1003 </papid>improving statistical machine translation models (al-onaizan et al, 1999), and inducing translation lexicons (koehn and knight, 2001).<papid> W01-0504 </papid></nextsent>
<nextsent>some of the proposed cognate identification algorithms implicitly determine and employ correspondences (tiedemann, 1999; mann and yarowsky, 2001).<papid> N01-1020 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1175">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the reconstruction engine (lowe and mazaudon, 1994), <papid> J94-3004 </papid>setof programs designed to be an aid in language reconstruction, requires set of correspondences to be provided beforehand.</prevsent>
<prevsent>the determination of correspondences is closely related to another task that has been much studied in computational linguistics, the identification of cognates.</prevsent>
</prevsection>
<citsent citstr=" J99-1003 ">
cognates have been employed for sentence and word alignment in bitexts (simardet al, 1992; melamed, 1999), <papid> J99-1003 </papid>improving statistical machine translation models (al-onaizan et al, 1999), and inducing translation lexicons (koehn and knight, 2001).<papid> W01-0504 </papid></citsent>
<aftsection>
<nextsent>some of the proposed cognate identification algorithms implicitly determine and employ correspondences (tiedemann, 1999; mann and yarowsky, 2001).<papid> N01-1020 </papid></nextsent>
<nextsent>although it may not be immediately apparent, there is strong similarity between the task of matching phonetic segments in pair of cognatewords, and the task of matching words in two sentences that are mutual translations (figure 1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1176">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the reconstruction engine (lowe and mazaudon, 1994), <papid> J94-3004 </papid>setof programs designed to be an aid in language reconstruction, requires set of correspondences to be provided beforehand.</prevsent>
<prevsent>the determination of correspondences is closely related to another task that has been much studied in computational linguistics, the identification of cognates.</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
cognates have been employed for sentence and word alignment in bitexts (simardet al, 1992; melamed, 1999), <papid> J99-1003 </papid>improving statistical machine translation models (al-onaizan et al, 1999), and inducing translation lexicons (koehn and knight, 2001).<papid> W01-0504 </papid></citsent>
<aftsection>
<nextsent>some of the proposed cognate identification algorithms implicitly determine and employ correspondences (tiedemann, 1999; mann and yarowsky, 2001).<papid> N01-1020 </papid></nextsent>
<nextsent>although it may not be immediately apparent, there is strong similarity between the task of matching phonetic segments in pair of cognatewords, and the task of matching words in two sentences that are mutual translations (figure 1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1177">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the determination of correspondences is closely related to another task that has been much studied in computational linguistics, the identification of cognates.
</prevsent>
<prevsent>cognates have been employed for sentence and word alignment in bitexts (simardet al, 1992; melamed, 1999), <papid> J99-1003 </papid>improving statistical machine translation models (al-onaizan et al, 1999), and inducing translation lexicons (koehn and knight, 2001).<papid> W01-0504 </papid></prevsent>
</prevsection>
<citsent citstr=" N01-1020 ">
some of the proposed cognate identification algorithms implicitly determine and employ correspondences (tiedemann, 1999; mann and yarowsky, 2001).<papid> N01-1020 </papid></citsent>
<aftsection>
<nextsent>although it may not be immediately apparent, there is strong similarity between the task of matching phonetic segments in pair of cognatewords, and the task of matching words in two sentences that are mutual translations (figure 1).
</nextsent>
<nextsent>the f u w nix iacet in terra onlies the groundsnowfigure 1: the similarity of word alignment in bi texts and phoneme alignment between cognates.
</nextsent>
<nextsent>consistency with which word in one language is translated into word in another language is mirrored by the consistency of sound correspondences.the former is due to the semantic relation of syn onymy, while the latter follows from the principle of the regularity of sound change.
</nextsent>
<nextsent>thus, as already asserted by guy (1994), it should be possible to use similar techniques for both tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1178">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments to be described in section 6 show that the method is capable of determining correspondences in bilingual word lists in which less than 30% of the pairs are cognates, and outperforms comparable algorithms on cognate identification.
</prevsent>
<prevsent>although the experiments focus on bilingual word lists, the approach presented in this paper could potentially be applied to other bitext-related tasks.
</prevsent>
</prevsection>
<citsent citstr=" N01-1014 ">
in schematic description of the comparative method, the two steps that precede the determination of correspondences are the identification of cognate pairs (kondrak, 2001), <papid> N01-1014 </papid>and their phonetic alignment (kondrak, 2000).<papid> A00-2038 </papid></citsent>
<aftsection>
<nextsent>indeed, if comprehensive set of correctly aligned cognate pairs is available, the correspondences could be extracted by simply following the alignment links.
</nextsent>
<nextsent>unfortunately, in order to make reliable judgments of cog nation, it is necessary to know in advance what the correspondences are.
</nextsent>
<nextsent>historical linguists solve this apparent circularity by guessing small number of likely cognates and refining the set of correspondences and cognates in an iterative fashion.
</nextsent>
<nextsent>guy (1994) outlines an algorithm for identifying cognates in bilingual word lists which is based oncorrespondences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1179">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments to be described in section 6 show that the method is capable of determining correspondences in bilingual word lists in which less than 30% of the pairs are cognates, and outperforms comparable algorithms on cognate identification.
</prevsent>
<prevsent>although the experiments focus on bilingual word lists, the approach presented in this paper could potentially be applied to other bitext-related tasks.
</prevsent>
</prevsection>
<citsent citstr=" A00-2038 ">
in schematic description of the comparative method, the two steps that precede the determination of correspondences are the identification of cognate pairs (kondrak, 2001), <papid> N01-1014 </papid>and their phonetic alignment (kondrak, 2000).<papid> A00-2038 </papid></citsent>
<aftsection>
<nextsent>indeed, if comprehensive set of correctly aligned cognate pairs is available, the correspondences could be extracted by simply following the alignment links.
</nextsent>
<nextsent>unfortunately, in order to make reliable judgments of cog nation, it is necessary to know in advance what the correspondences are.
</nextsent>
<nextsent>historical linguists solve this apparent circularity by guessing small number of likely cognates and refining the set of correspondences and cognates in an iterative fashion.
</nextsent>
<nextsent>guy (1994) outlines an algorithm for identifying cognates in bilingual word lists which is based oncorrespondences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1180">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> models of translational equivalence.  </section>
<citcontext>
<prevsection>
<prevsent>the difficulty lies in the fact that the mapping, or alignment, of words between two parts of bitext is not known in advance.
</prevsent>
<prevsent>algorithms for word alignment in bitexts aim at discovering word pairs that are mutual translations.a straightforward approach is to estimate the likelihood that words are mutual translations by computing similarity function based on co-occurrencestatistic, such as mutual information, dice coefficient, or the 2 test.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
the underlying assumption is that the association scores for different word pairs are independent of each other.melamed (2000) <papid> J00-2004 </papid>shows that the assumption of independence leads to invalid word associations, and proposes an algorithm for inducing models of translational equivalence that outperform the models thatare based solely on co-occurrence counts.</citsent>
<aftsection>
<nextsent>his models employ the one-to-one assumption, which formalizes the observation that most words in bitextsare translated to single word in the corresponding sentence.
</nextsent>
<nextsent>the algorithm, which is related tothe expectation-maximization (em) algorithm, iteratively re-estimates the likelihood scores which represent the probability that two word types are mutual translations.
</nextsent>
<nextsent>in the first step, the scores are initial ized according to the g2 statistic (dunning,1993).<papid> J93-1003 </papid></nextsent>
<nextsent>next, the likelihood scores are used to induce set of one-to-one links between word tokens in the bitext.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1181">
<title id=" C02-1016.xml">determining recurrent sound correspondences by inducing translation models </title>
<section> models of translational equivalence.  </section>
<citcontext>
<prevsection>
<prevsent>his models employ the one-to-one assumption, which formalizes the observation that most words in bitextsare translated to single word in the corresponding sentence.
</prevsent>
<prevsent>the algorithm, which is related tothe expectation-maximization (em) algorithm, iteratively re-estimates the likelihood scores which represent the probability that two word types are mutual translations.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
in the first step, the scores are initial ized according to the g2 statistic (dunning,1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>next, the likelihood scores are used to induce set of one-to-one links between word tokens in the bitext.
</nextsent>
<nextsent>the links are determined by greedy competitive linking algorithm, which proceeds to link pairs that have the highest likelihood scores.
</nextsent>
<nextsent>after the linking is completed, the link counts are used to re-estimate the likelihood scores, which in turn are applied to find new set of links.
</nextsent>
<nextsent>the process is repeated until the translation model converges to the desired degree.melamed presents three translation-model estimation methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1183">
<title id=" C08-1006.xml">verification and implementation of language based deception indicators in civil and criminal narratives </title>
<section> a deception indicator tagger  </section>
<citcontext>
<prevsection>
<prevsent>certain verb+ infini tive complement constructions, e.g. attempted to open the door, make up qualified assertion.
</prevsent>
<prevsent>syntactic structure is assigned by the cass chunk parser (abney, 1990).
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
part of speech tags are assigned by brills tagger (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>the di tag rules apply to the output of the parser and pos tagger.
</nextsent>
<nextsent>the subset of tags implemented in the tagger comprises 86% of all tags that occur in the training corpus.
</nextsent>
<nextsent>to see how well the di tagger covered the subset, we first ran the tagger on the training corpus.
</nextsent>
<nextsent>70% of the subset tags were correctly identified in that corpus, with 76% precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1184">
<title id=" C04-1132.xml">learning a robust word sense disambiguation model using hypernyms in definition sentences </title>
<section> naive bayes classifier using.  </section>
<citcontext>
<prevsection>
<prevsent>http://chasen.aist-nara.ac.jp/hiki/chasen/ next, we approximate equation (1) as (2): (s, c|f ) = (s|c, )p (c|f )
</prevsent>
<prevsent>p (s|c)p (c|f ) (2) the first term, (s|c, ), is the probabilistic model that predicts sense given feature set (and c).
</prevsent>
</prevsection>
<citsent citstr=" A00-2009 ">
it is similar to the ordinary naive bayes model for wsd (pedersen, 2000).<papid> A00-2009 </papid></citsent>
<aftsection>
<nextsent>however, we assume that this model can not be trained for low frequency words due to lack of training data.
</nextsent>
<nextsent>therefore, we approximate (s|c, ) to (s|c).using bayes?
</nextsent>
<nextsent>rule, equation (2) can be computed as follows: (s|c)p (c|f ) = (s)p (c|s) (c) (c)p (f |c) (f ) (3) = (s)p (f |c) (f ) (4) notice that (c|s) in (3) is equal to 1, because hypernym of sense is uniquely extracted by pattern matching (subsection 3.2).
</nextsent>
<nextsent>as all we want to do is to choose an s?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1185">
<title id=" C04-1132.xml">learning a robust word sense disambiguation model using hypernyms in definition sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, our method is to train the probabilistic model that predictsa hypernym of word, while most previous approaches use semantic classes as features (i.e., the condition of the posterior probability in the case of the naive bayes model).
</prevsent>
<prevsent>in facts, we also use features associated with semantic classes derived from the thesaurus, csent and (bcase;cnoun), as described in section 2.
</prevsent>
</prevsection>
<citsent citstr=" W02-0807 ">
several previous studies have used both corpus and machine readable dictionary for wsd (litkowski, 2002; <papid> W02-0807 </papid>rigau et al, 1997; <papid> P97-1007 </papid>stevenson and wilks, 2001).<papid> J01-3001 </papid></citsent>
<aftsection>
<nextsent>the difference between those methods and ours is the way we use information derived from the dictionary for wsd.
</nextsent>
<nextsent>training the probabilistic model that predicts hypernym in dictionary is our own approach.
</nextsent>
<nextsent>however, these various methods are not in competition with our method.
</nextsent>
<nextsent>in fact, the robustness of the wsd system would beeven more improved by combining these methods with that described in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1186">
<title id=" C04-1132.xml">learning a robust word sense disambiguation model using hypernyms in definition sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, our method is to train the probabilistic model that predictsa hypernym of word, while most previous approaches use semantic classes as features (i.e., the condition of the posterior probability in the case of the naive bayes model).
</prevsent>
<prevsent>in facts, we also use features associated with semantic classes derived from the thesaurus, csent and (bcase;cnoun), as described in section 2.
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
several previous studies have used both corpus and machine readable dictionary for wsd (litkowski, 2002; <papid> W02-0807 </papid>rigau et al, 1997; <papid> P97-1007 </papid>stevenson and wilks, 2001).<papid> J01-3001 </papid></citsent>
<aftsection>
<nextsent>the difference between those methods and ours is the way we use information derived from the dictionary for wsd.
</nextsent>
<nextsent>training the probabilistic model that predicts hypernym in dictionary is our own approach.
</nextsent>
<nextsent>however, these various methods are not in competition with our method.
</nextsent>
<nextsent>in fact, the robustness of the wsd system would beeven more improved by combining these methods with that described in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1187">
<title id=" C04-1132.xml">learning a robust word sense disambiguation model using hypernyms in definition sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, our method is to train the probabilistic model that predictsa hypernym of word, while most previous approaches use semantic classes as features (i.e., the condition of the posterior probability in the case of the naive bayes model).
</prevsent>
<prevsent>in facts, we also use features associated with semantic classes derived from the thesaurus, csent and (bcase;cnoun), as described in section 2.
</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
several previous studies have used both corpus and machine readable dictionary for wsd (litkowski, 2002; <papid> W02-0807 </papid>rigau et al, 1997; <papid> P97-1007 </papid>stevenson and wilks, 2001).<papid> J01-3001 </papid></citsent>
<aftsection>
<nextsent>the difference between those methods and ours is the way we use information derived from the dictionary for wsd.
</nextsent>
<nextsent>training the probabilistic model that predicts hypernym in dictionary is our own approach.
</nextsent>
<nextsent>however, these various methods are not in competition with our method.
</nextsent>
<nextsent>in fact, the robustness of the wsd system would beeven more improved by combining these methods with that described in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1188">
<title id=" C04-1155.xml">a flexible example annotation schema translation corresponding tree representation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, syntax transformation clues are also encapsulated at each node in the tct representation to capture the differentiation of grammatical structure between the source and target languages.
</prevsent>
<prevsent>with this annotation schema, translation examples are effectively represented and organized in the bilingual knowledge database that we need for the portuguese to chinese machine translation system.
</prevsent>
</prevsection>
<citsent citstr=" C90-3044 ">
the construction of bilingual knowledge base, in the development of example-based machine translation systems (sato and nagao, 1990), <papid> C90-3044 </papid>is vitally critical.</citsent>
<aftsection>
<nextsent>in the translation process, the application of bilingual examples concerns with how examples are used to facilitate translation, which involves the factor ization of an input sentence into the format of stored examples and the conversion of source texts into target texts in terms of the existing translations by referencing to the bilingual knowledge base.
</nextsent>
<nextsent>theoretically speaking, examples can be achieved from bilin 1 or bilingual knowledge base, we use the two terms inter-.
</nextsent>
<nextsent>changeably.
</nextsent>
<nextsent>gual corpus where the texts are aligned in sentential level, and technically, we need an example base for convenient storage and retrieval of examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1189">
<title id=" C04-1155.xml">a flexible example annotation schema translation corresponding tree representation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gual corpus where the texts are aligned in sentential level, and technically, we need an example base for convenient storage and retrieval of examples.
</prevsent>
<prevsent>the way of how the translation examples themselves are actually stored is closely related to the problem of searching for matches.
</prevsent>
</prevsection>
<citsent citstr=" P98-2139 ">
in structural example-based machine translation systems (grishman, 1994; meyers et al, 1998; <papid> P98-2139 </papid>watanabe et al, 2000), examples in the knowledge base are normally annotated with their constituency (kaji et al, 1992) <papid> C92-2101 </papid>or dependency structures (matsumoto et al, 1993; <papid> P93-1004 </papid>aramaki et al, 2001; aladhaileh et al, 2002), which allows the corresponding relations between source and target sentences to be established at the structural level.</citsent>
<aftsection>
<nextsent>all of these approaches annotate examples by mean of pair of analyzed structures, one for each language sentence, where the correspondences between inter levels of source and target structures are explicitly linked.
</nextsent>
<nextsent>however, we found that these approaches require the bilingual examples that have parallel?
</nextsent>
<nextsent>translations or close?
</nextsent>
<nextsent>syntactic structures (grishman, 1994), where the source sentence and target sentences have explicit correspondences in the sentences-pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1190">
<title id=" C04-1155.xml">a flexible example annotation schema translation corresponding tree representation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gual corpus where the texts are aligned in sentential level, and technically, we need an example base for convenient storage and retrieval of examples.
</prevsent>
<prevsent>the way of how the translation examples themselves are actually stored is closely related to the problem of searching for matches.
</prevsent>
</prevsection>
<citsent citstr=" C92-2101 ">
in structural example-based machine translation systems (grishman, 1994; meyers et al, 1998; <papid> P98-2139 </papid>watanabe et al, 2000), examples in the knowledge base are normally annotated with their constituency (kaji et al, 1992) <papid> C92-2101 </papid>or dependency structures (matsumoto et al, 1993; <papid> P93-1004 </papid>aramaki et al, 2001; aladhaileh et al, 2002), which allows the corresponding relations between source and target sentences to be established at the structural level.</citsent>
<aftsection>
<nextsent>all of these approaches annotate examples by mean of pair of analyzed structures, one for each language sentence, where the correspondences between inter levels of source and target structures are explicitly linked.
</nextsent>
<nextsent>however, we found that these approaches require the bilingual examples that have parallel?
</nextsent>
<nextsent>translations or close?
</nextsent>
<nextsent>syntactic structures (grishman, 1994), where the source sentence and target sentences have explicit correspondences in the sentences-pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1191">
<title id=" C04-1155.xml">a flexible example annotation schema translation corresponding tree representation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gual corpus where the texts are aligned in sentential level, and technically, we need an example base for convenient storage and retrieval of examples.
</prevsent>
<prevsent>the way of how the translation examples themselves are actually stored is closely related to the problem of searching for matches.
</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
in structural example-based machine translation systems (grishman, 1994; meyers et al, 1998; <papid> P98-2139 </papid>watanabe et al, 2000), examples in the knowledge base are normally annotated with their constituency (kaji et al, 1992) <papid> C92-2101 </papid>or dependency structures (matsumoto et al, 1993; <papid> P93-1004 </papid>aramaki et al, 2001; aladhaileh et al, 2002), which allows the corresponding relations between source and target sentences to be established at the structural level.</citsent>
<aftsection>
<nextsent>all of these approaches annotate examples by mean of pair of analyzed structures, one for each language sentence, where the correspondences between inter levels of source and target structures are explicitly linked.
</nextsent>
<nextsent>however, we found that these approaches require the bilingual examples that have parallel?
</nextsent>
<nextsent>translations or close?
</nextsent>
<nextsent>syntactic structures (grishman, 1994), where the source sentence and target sentences have explicit correspondences in the sentences-pair.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1193">
<title id=" C04-1155.xml">a flexible example annotation schema translation corresponding tree representation </title>
<section> translation corresponding tree.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1.
</prevsent>
<prevsent>an example of free translation?, where the translations of some words in portuguese sentence do not appear in target chinese sentence.
</prevsent>
</prevsection>
<citsent citstr=" C88-1013 ">
(tct) representation tct structure, as an extension of structure string tree correspondence representation (boitet and zaharin, 1988), <papid> C88-1013 </papid>is general structure that can flexibly associate not only the string of sentence to its syntactic structure in source language, but also allow the language annotator to explicitly associate the string from its translation in target language for the purpose to describe the correspondences between different languages.</citsent>
<aftsection>
<nextsent>2.1 the tct structure.
</nextsent>
<nextsent>the tct representation uses triple sequence intervals [snode(n)/stree(n)/stc(n)] encoded for each node in the tree to represent the corresponding relations between the structure of source sentence and the sub strings from both the source and target sentences.
</nextsent>
<nextsent>in tct structure, the correspondence is made up of three interrelated correspondences: 1) one between the node and the substring of source sentence encoded by the interval snode(n), which denotes the interval containing the substring corresponding to the node; 2) one between the subtree and the substring of source sentence represented by the interval stree(n), which indicates the interval of substring that is dominated by the subtree with the node as root; and 3) the other between the subtree of source sentence and the substring of target sentence expressed by the interval stc(n), which indicates the interval containing the substring in target sentence corresponding to the subtree of source sentence.
</nextsent>
<nextsent>the associated sub strings may be discontinuous in all cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1201">
<title id=" C02-1153.xml">generating the xtag english grammar using meta rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples of required tree templates are:declarative transitive (the example above); ditransi tive passive with wh-subject moved; and in transitive with pp object with the pp-object relativized.
</prevsent>
<prevsent>as early noticed by (vijay-shanker and schabes, 1992) the information regarding syntactic structure and feature equations in (feature-based) ltags is repeated across templates trees in quite regular way, that perhaps could be more concisely captured than by just having plain set of elementary trees.
</prevsent>
</prevsection>
<citsent citstr=" P95-1011 ">
besides the obvious linguistic relevance, as pure engineering issue, the success of such enterprise would result in enormous benefits for grammar development and maintenance.several approaches have been proposed in the literature describing compact representations methods for ltags, perhaps the best known being (vijayshanker and schabes, 1992), (candito, 1996; candito, 1998), (evans et al, 1995; <papid> P95-1011 </papid>evans et al, 2000), (xia et al, 1998; xia, 2001), and (becker, 1993; becker, 1994; becker, 2000).</citsent>
<aftsection>
<nextsent>we describe in this paper how we combined beckers meta rules with hierarchy of rule application to generate the verb tree templates in the xtag english grammar, from very small initial set of trees.3
</nextsent>
<nextsent>we present in this section an introductory example of metarules.4 consider the two trees in figure 3 3this work started years ago, already mentioned in (doran et al, 2000, p. 388).
</nextsent>
<nextsent>there has been some confusion on the issue, perhaps driven by somewhat ambiguous statement in(becker, 2000, p. 331): in this paper, we present the various patterns which are used in the implementation of meta rules which we added to the xtag system (doran et al 2000)?.
</nextsent>
<nextsent>the work of becker conceived and developed the idea of meta rules for tags (becker, 1993; becker, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1203">
<title id=" C02-1153.xml">generating the xtag english grammar using meta rules </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>a major strength of the approach is to have set target grammar with which to compare.
</prevsent>
<prevsent>a detailed qualitative evaluation of the mismatches between the existing and generated grammars was obtained that allows us to access not only the weaknesses of the generation process but also the problems of the original grammar development: e.g., the inconsistency in the treatment of the interface between the lexicon and the tree templates.future work in the xtag group includes the construction of graph based interface for meta rules that allows the application of meta rules according 10of course, this may not reflect beckers view.
</prevsent>
</prevsection>
<citsent citstr=" W02-1507 ">
to partial order, as well as distinct treatment for different families.11 we are also interested in aspects of the use of meta rules to enhance extracted grammars (kinyon and prolo, 2002).<papid> W02-1507 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1204">
<title id=" C02-2015.xml">context sensitive electronic dictionaries </title>
<section> comparison to other systems.  </section>
<citcontext>
<prevsection>
<prevsent>there are few pop-up dictionaries on the market: the most well-known are babylon, word point, clever learn, ifinger, langen scheidt pop-up dictionary and techo crafts robo word, but none of them have as many language technology features as mobimouse.
</prevsent>
<prevsent>there are some glossing?
</prevsent>
</prevsection>
<citsent citstr=" P98-2174 ">
programs in research laboratories (rxce, see feldweg and breidt 1996; or sharp, see poznanski et al 1998) <papid> P98-2174 </papid>that access dictionaries with context-sensitive look-up procedure.</citsent>
<aftsection>
<nextsent>however, they present the information to the user through their own graphical interface, and none of them have the basic featuere of mobimouse, namely, being context-sensitive instant comprehension tool for any running application.
</nextsent>
<nextsent>the above systems do not have access to more than one dictionary at the same time, unlike mobimouse.
</nextsent>
<nextsent>on the other hand, the treatment of multiword units in the idarex formalism (segond and breidt 1996) is more sophisticated than in mobimouse.
</nextsent>
<nextsent>another project with instant understanding is glosser, whose prototype (nerbonne et al 1997) <papid> A97-1020 </papid>performs morphological analysis of the sentence containing the selected word in similar manner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1205">
<title id=" C02-2015.xml">context sensitive electronic dictionaries </title>
<section> comparison to other systems.  </section>
<citcontext>
<prevsection>
<prevsent>the above systems do not have access to more than one dictionary at the same time, unlike mobimouse.
</prevsent>
<prevsent>on the other hand, the treatment of multiword units in the idarex formalism (segond and breidt 1996) is more sophisticated than in mobimouse.
</prevsent>
</prevsection>
<citsent citstr=" A97-1020 ">
another project with instant understanding is glosser, whose prototype (nerbonne et al 1997) <papid> A97-1020 </papid>performs morphological analysis of the sentence containing the selected word in similar manner.</citsent>
<aftsection>
<nextsent>in glosserunlike in mobimousethere is stochastic disambiguation step but everything is shown in a.separate window.
</nextsent>
<nextsent>the text acquisition techniques used in mo bimouse are independent from both the language and the writing system.
</nextsent>
<nextsent>hence it is rather different from most known applications that work with english characters only.
</nextsent>
<nextsent>most other pop-up dictionary applications start by pressing button or clicking the mouse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1206">
<title id=" C04-1167.xml">statistical language modeling with performance benchmarks using various levels of syntactic semantic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to model the linguistic structure that spans whole sentence or paragraph or even more, various approaches have been taken recently.
</prevsent>
<prevsent>these can be categorized into two main types : syntactically motivated and semantically motivated large span consideration.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
in the first type, probability of word is decided based on parse-tree information like grammatical headwords in sentence (charniak, 2001)(<papid> P01-1017 </papid>chelba and jelinek, 1998), <papid> P98-1035 </papid>or based on part of-speech (pos) tag information (galescu and ringger, 1999).</citsent>
<aftsection>
<nextsent>examples of the second type are (bellegarda, 2000) (coccaro and jurafsky, 1998), where latent semantic analysis (lsa)(landauer et al, 1998) is used to derive large span semantic dependencies.
</nextsent>
<nextsent>lsa uses word document co-occurrence statistics and matrixfactorization technique called singular value decomposition to derive semantic similarity measure between any two text units - words or documents.
</nextsent>
<nextsent>each of these approaches, when integrated with n-gram language model, has led to improved performance in terms of perplexity as well as speech recognition accuracy.while each of these approaches has been studied independently, it would be interesting to seehow they can be integrated in unified framework which looks at syntactic as well as semantic information in the large span.
</nextsent>
<nextsent>towards this direction, we describe in this paper mathematical framework called syntactically enhanced latent syntactic-semantic analysis (selsa).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1207">
<title id=" C04-1167.xml">statistical language modeling with performance benchmarks using various levels of syntactic semantic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to model the linguistic structure that spans whole sentence or paragraph or even more, various approaches have been taken recently.
</prevsent>
<prevsent>these can be categorized into two main types : syntactically motivated and semantically motivated large span consideration.
</prevsent>
</prevsection>
<citsent citstr=" P98-1035 ">
in the first type, probability of word is decided based on parse-tree information like grammatical headwords in sentence (charniak, 2001)(<papid> P01-1017 </papid>chelba and jelinek, 1998), <papid> P98-1035 </papid>or based on part of-speech (pos) tag information (galescu and ringger, 1999).</citsent>
<aftsection>
<nextsent>examples of the second type are (bellegarda, 2000) (coccaro and jurafsky, 1998), where latent semantic analysis (lsa)(landauer et al, 1998) is used to derive large span semantic dependencies.
</nextsent>
<nextsent>lsa uses word document co-occurrence statistics and matrixfactorization technique called singular value decomposition to derive semantic similarity measure between any two text units - words or documents.
</nextsent>
<nextsent>each of these approaches, when integrated with n-gram language model, has led to improved performance in terms of perplexity as well as speech recognition accuracy.while each of these approaches has been studied independently, it would be interesting to seehow they can be integrated in unified framework which looks at syntactic as well as semantic information in the large span.
</nextsent>
<nextsent>towards this direction, we describe in this paper mathematical framework called syntactically enhanced latent syntactic-semantic analysis (selsa).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1208">
<title id=" C04-1167.xml">statistical language modeling with performance benchmarks using various levels of syntactic semantic information </title>
<section> various levels of syntactic.  </section>
<citcontext>
<prevsection>
<prevsent>these are in decreasing order of complexity and provide finer to coarser levels of syntactic information.
</prevsent>
<prevsent>4.1 supertags.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
supertags are the elementary structures of lexicalized tree adjoining grammars (ltags)(bangalore and joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>they are combined by the operations of substitution and adjunction to yield parse for the sentence.
</nextsent>
<nextsent>each super tag is lexicalized i.e. associated with at least one lexical item - the anchor.
</nextsent>
<nextsent>further, all the arguments of the anchor of supertagare localized within the same super tag which allows the anchor to impose syntactic and semantic (predicate-argument) constraints directly on its arguments.
</nextsent>
<nextsent>as result, word is typically associated with one super tag for each syntactic configuration the word may appear in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1210">
<title id=" C04-1167.xml">statistical language modeling with performance benchmarks using various levels of syntactic semantic information </title>
<section> experiments and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in another experiment we used the part of-speech tag of the previous word (prevtag) within selsa, but it couldnt improve against the plain lsa.
</prevsent>
<prevsent>these results shows that phrase level information is somewhat useful if it can be predicted correctly, but previous pos tags are not useful.
</prevsent>
</prevsection>
<citsent citstr=" W02-1031 ">
model perplexity tri-gram only 103.12 lsa(125)+tri-gram 68.42 phrase-selsa(125)+tri-gram 64.78 prevtag-selsa(125)+tri-gram 69.12 table 3: perplexity of phrase/prevtag based selsa with the knowledge of content/function word type and the correct phrase/prevtag finally the utility of this language model can be tested in speech recognition experiment.here it can be most suitably applied in second pass rescoring framework where the output of first-pass could be the n-best list of either joint word-tag sequences (wang and harper, 2002) <papid> W02-1031 </papid>or word sequences which are then passed through syntax tagger.</citsent>
<aftsection>
<nextsent>both these approaches allow direct application of the results shown in above experiments, however there is possibility of error propagation if some word is incorrectly tagged.
</nextsent>
<nextsent>the other approach is to predict the tag left-to-right from the word-tag partial prefix followed by word prediction and then repeating the procedure for the next word.
</nextsent>
<nextsent>direction we presented the effect of incorporating various levels of syntactic information in statistical language model that uses the mathematical framework called syntactically enhanced lsa.selsa is an attempt to develop unified framework where syntactic and semantic dependencies can be jointly represented.
</nextsent>
<nextsent>it generalizes the lsa framework by incorporating various levels of the syntactic information along with the current word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1211">
<title id=" C08-1085.xml">recent advances in a feature rich framework for treebank annotation </title>
<section> data format.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, the library supports on the-fly xslt-based format conversions that can be easily plugged in via simple configuration file.
</prevsent>
<prevsent>consequently, the api can transparently handle even non-pml data formats.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
currently there are about dozen input/output conversion filters available, covering various existing data formats including the tiger xml format, the formats of the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>the conll-x shared task format (buchholz andmarsi, 2006), <papid> W06-2920 </papid>and the formats of the latin dependency (bamman and crane, 2006), sinica (chu ren et al, 2000), slovene dependency (dzeroski et al, 2006) (sdt), and alpino (van der beek et al., 2002) treebanks.</citsent>
<aftsection>
<nextsent>support for xces formats is planned as soon as final release of xces is available.
</nextsent>
<nextsent>this basic toolkit is further supplemented by various auxiliary tools, such as pmlcopy which allows one to copy, move, rename, or gzip sets of interconnected pml data files without breaking the internal url-based references.
</nextsent>
<nextsent>the heart of the annotation framework is multi platform graphical tree editor called tred, (hajic et al, 2001).
</nextsent>
<nextsent>tred was from the beginning designed to be annotation-schema independent, extensible and configurable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1212">
<title id=" C08-1085.xml">recent advances in a feature rich framework for treebank annotation </title>
<section> data format.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, the library supports on the-fly xslt-based format conversions that can be easily plugged in via simple configuration file.
</prevsent>
<prevsent>consequently, the api can transparently handle even non-pml data formats.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
currently there are about dozen input/output conversion filters available, covering various existing data formats including the tiger xml format, the formats of the penn treebank (marcus et al, 1994), <papid> H94-1020 </papid>the conll-x shared task format (buchholz andmarsi, 2006), <papid> W06-2920 </papid>and the formats of the latin dependency (bamman and crane, 2006), sinica (chu ren et al, 2000), slovene dependency (dzeroski et al, 2006) (sdt), and alpino (van der beek et al., 2002) treebanks.</citsent>
<aftsection>
<nextsent>support for xces formats is planned as soon as final release of xces is available.
</nextsent>
<nextsent>this basic toolkit is further supplemented by various auxiliary tools, such as pmlcopy which allows one to copy, move, rename, or gzip sets of interconnected pml data files without breaking the internal url-based references.
</nextsent>
<nextsent>the heart of the annotation framework is multi platform graphical tree editor called tred, (hajic et al, 2001).
</nextsent>
<nextsent>tred was from the beginning designed to be annotation-schema independent, extensible and configurable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1213">
<title id=" C04-1077.xml">corpus and evaluation measures for multiple document summarization with multiple sources </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>duc has included multiple document summarization among its tasks since the first conference.
</prevsent>
<prevsent>the text summarization challenge (tsc)2 has been held once in one and half years as part of the ntcir (nii-nacsis test collection for ir systems) project since 2001.
</prevsent>
</prevsection>
<citsent citstr=" W03-0507 ">
multiple document summarization was included for the first time as one of the tasks at tsc2 (in 2002) (okumura et al, 2003).<papid> W03-0507 </papid></citsent>
<aftsection>
<nextsent>multiple document summarization is now central issue for text summarization research.
</nextsent>
<nextsent>1http://duc.nist.gov 2http://www.lr.pi.titech.ac.jp/tsc in this paper, we detail the corpus construction and evaluation measures used at the text summarization challenge 3 (tsc3 hereafter), where multiple document summarization is the main issue.
</nextsent>
<nextsent>we also report the results of preliminary experiment on simple multiple document summarization systems.
</nextsent>
<nextsent>2.1 guidelines for corpus construction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1214">
<title id=" C04-1077.xml">corpus and evaluation measures for multiple document summarization with multiple sources </title>
<section> tsc3 corpus.  </section>
<citcontext>
<prevsection>
<prevsent>at duc 2002, extracts (important sentences) we reused, and this allowed us to evaluate sentence extraction.
</prevsent>
<prevsent>however, it is not possible to measure the effectiveness of redundant sentences reduction since the corpus was not annotated to show sentence with same content.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
in addition, this is the same even if we use the summbank corpus (radev et al, 2003).<papid> P03-1048 </papid>in any case, because many of the current summarization systems for multiple documents are based on sentence extraction, we believe these corpora to be unsuitable assets of documents for evaluation.on this basis, in tsc3, we assumed that the process of multiple document summarization consists of the following three steps, and we produce corpus for the evaluation of the system at each of the three steps4.</citsent>
<aftsection>
<nextsent>step 1 extract important sentences from given set of documents step 2 minimize redundant sentences from there sult of step 1 step 3 rewrite the result of step 2 to reduce the size of the summary to the specified number of characters or less.we have annotated not only the important sentences in the document set, but also those among them that have the same content.
</nextsent>
<nextsent>these are the corpora for steps 1 and 2.
</nextsent>
<nextsent>we have prepared human produced free summaries (abstracts) for step 3.in tsc3, since we have key data (a set of correct important sentences) for steps 1 and 2, we conducted automatic evaluation using scoring program.
</nextsent>
<nextsent>we adopted an intrinsic evaluation by human judges for step 3, which is currently under evaluation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1216">
<title id=" C04-1077.xml">corpus and evaluation measures for multiple document summarization with multiple sources </title>
<section> tsc3 corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we think that there are two kinds of extract.
</prevsent>
<prevsent>1.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
a set of sentences that human annotators judge as being important in document set (fukusima and okumura, 2001; zechner, 1996; <papid> C96-2166 </papid>paice, 1990).</citsent>
<aftsection>
<nextsent>4this is based on general ideas of summarization system and is not intended to impose any conditions on summarization system.
</nextsent>
<nextsent>mainichi articles yomiuri articles abstract (a) (b) (c) (d) doc.
</nextsent>
<nextsent>x doc.
</nextsent>
<nextsent>y figure 1: an example of an abstract and its sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1220">
<title id=" C04-1077.xml">corpus and evaluation measures for multiple document summarization with multiple sources </title>
<section> preliminary experiment.  </section>
<citcontext>
<prevsection>
<prevsent>w.c. lead short .426 .212 .326 long .539 .259 .369 tf ? idf short .497 .292 .397 long .604 .325 .434 pattern short .613 .305 .403 long .665 .298 .418table 4: evaluation results for pseudo question answering.?
</prevsent>
<prevsent>method length exact edit lead short .300 .589 long .275 .602 tf ? idf short .375 .643 long .393 .659 pattern short .390 .644 long .370 .640 by using statistical significance test such as the ? metric test and using 1,000 patterns.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
this is an extension of lins method (lin and hovy, 2000).<papid> C00-1072 </papid></citsent>
<aftsection>
<nextsent>the sentence score spat .
</nextsent>
<nextsent>^ 1 is defined by the following.
</nextsent>
<nextsent>spat la? ? f x?[?
</nextsent>
<nextsent>| ? lno (12) where ??.$1 is defined as follows: ? lo %wlaln?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1221">
<title id=" C04-1062.xml">multi human dialogue understanding for assisting artifact producing meetings </title>
<section> meeting assistant architecture.  </section>
<citcontext>
<prevsection>
<prevsent>the components for drawing and writing recognition and multimodal integration (kaiser et al, 2003) were developed at the oregon graduate institute (ogi) center for human computer communication3; the component for physical gesture recognition (ko et al, 2003) was developed at the massachusetts institute of technology (mit) ai lab4.
</prevsent>
<prevsent>integration between all components was performed by project members at those sites and at sri international5, and integration between our csli conversational intelligence architecture and ogis multimodal integrator (mi) was performed by members of both teams.
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
asr is done usingcmu sphinx6, from which the n-best list of results are passed to sris gemini parser (dowd ing et al, 1993).<papid> P93-1008 </papid></citsent>
<aftsection>
<nextsent>gemini incorporates suiteof techniques for handling noisy input, including fragment detection, and its dynamic grammar capabilities are used to register new lexical items, such as names of tasks that may be out of-grammar.an example of multimodal meeting conversation that the meeting assistant currently supports can be found in figure 2.7 there are two meeting participants in conference room with an electronic white board which can record their pen strokes and video camera that tracks their body movements; is standing at the white board and drawing while is sitting at the table.
</nextsent>
<nextsent>a gloss of how the system behaves in response to each utterance and gesture follows each utterance; these glosses will be explained in greater detail throughout the rest of the paper.
</nextsent>
<nextsent>the drawing made on the white board is in figure 3(a), and the chart artifact as it was constructed by the system is displayed in figure 3(b).
</nextsent>
<nextsent>architecture to meet the challenges presented by multi person meeting dialogue, we have extended and enhanced our previously used conversational intelligence architecture (cia).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1223">
<title id=" C04-1102.xml">detecting transliterated orthographic variants via two similarity metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from american.
</prevsent>
<prevsent>one may think that if back-transliteration we redone precisely, those variants would be back transliterated into one word, and they wouldbe recognized as variants.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
however, back transliteration is known to be very difcult task(knight and graehl, 1997).<papid> P97-1017 </papid></citsent>
<aftsection>
<nextsent>not only japanese but any language that has aphonetic spelling has this problem of transliterated orthographic variants.
</nextsent>
<nextsent>for example, english has variants for chinese proper noun as shan haiguan,?
</nextsent>
<nextsent>shanhaikwan,?
</nextsent>
<nextsent>or shanhaikuan.nowadays, it is well recognized that orthographic variant correction is an important processing step for achieving high performance in natural language processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1225">
<title id=" C04-1114.xml">improving statistical machine translation in the medical domain using the unified medical language system </title>
<section> machine translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>doctor: anemia can be very serious if left untreated.
</prevsent>
<prevsent>being anemic means your body lacks sufficient amount of red blood cells to carry oxygen through your body.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
figure 3: example test sentences (reference) the baseline system uses ibm1 lexicon transducers and different types of phrase transducers (zhang et al 2003, vogel et al 1996, <papid> C96-2141 </papid>vogel et al 2003).</citsent>
<aftsection>
<nextsent>the language model is trigram language model with good-turingsmoothing built with the sri-toolkit (sri, 1995 2004) using only the english part of the training data.
</nextsent>
<nextsent>the baseline system scores 0.171 bleu and 4.72 nist.
</nextsent>
<nextsent>[bleu and nist are well known.
</nextsent>
<nextsent>scoring methods for measuring machine translation quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1226">
<title id=" C04-1114.xml">improving statistical machine translation in the medical domain using the unified medical language system </title>
<section> machine translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>[bleu and nist are well known.
</prevsent>
<prevsent>scoring methods for measuring machine translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
both calculate the precision of translation by comparing it to reference translation and incorporating length penalty (doddington, 2001; papineni et al, 2002).]<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>3.2 extracting dictionaries from the umls.
</nextsent>
<nextsent>the first way to exploit the umls database for statistical machine translation system naturally is to extract additional spanish-english lexicons or phrasebooks.
</nextsent>
<nextsent>the umls meta thesaurus provides translation information as we can assume that spanish and english terms that are associated with the same concept are respective translations.
</nextsent>
<nextsent>for example as the english term arm?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1227">
<title id=" C04-1114.xml">improving statistical machine translation in the medical domain using the unified medical language system </title>
<section> machine translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>527 sentence pairs of the original 9,227 sentence pairs contained word or phrase pair from this dictionary.
</prevsent>
<prevsent>a retraining of the translation system with this changed training data resulted in transducer rules containing this body-part-tag.
</prevsent>
</prevsection>
<citsent citstr=" P00-1004 ">
by using cascaded transducers (vogel and ney, 2000) <papid> P00-1004 </papid>in the actual translation the first transducer, that is applied (in this case the body-part dictionary) replaces the spanish body part with its translation pair and the body-part tag.</citsent>
<aftsection>
<nextsent>the following transducers can apply their generalized rules containing the body-part-tag instead of the real body part.
</nextsent>
<nextsent>e.g. translation of the sentence: necesito examinar su antebrazo.
</nextsent>
<nextsent>first step apply body-part dictionary rule (antebrazoforearm) necesito examinar su @bodypart(antebrazoforearm).
</nextsent>
<nextsent>apply generalized transducer rule: (a rule could be: necesito examinar su @bodypart ? need to examine your @bodypart) need to examine your @bodypart(antebrazoforearm).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1228">
<title id=" C02-1116.xml">linking syntactic and semantic arguments in a dependency based formalism </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we propose formal characterization of variation in the syntactic realization of semantic arguments, using hierarchies of syntactic relations and thematic roles, and mechanism of lexical inheritance to obtain valency frames from individual linking types.
</prevsent>
</prevsection>
<citsent citstr=" P01-1024 ">
we embed the formalization in the new lexicalized,dependency-based grammar formalism of topological dependency grammar (tdg) (duchier and debusmann, 2001).<papid> P01-1024 </papid></citsent>
<aftsection>
<nextsent>we account for arguments that can be alternatively realized as np or pp, and model thematic role alternations.
</nextsent>
<nextsent>we also treat auxiliary constructions, where the correspond ance between syntactic and semantic argument hood is indirect.1
</nextsent>
<nextsent>this paper deals with the mapping (or linking) of semantic predicate-argument structure to surface syntactic realizations.
</nextsent>
<nextsent>we present formal architecture in the framework of multi-dimensional, heavily lexicalized, efficiently parsable dependency formalism (duchier and debusmann, 2001), <papid> P01-1024 </papid>which uses lexical inheritance as means to explicitly model syntactic variation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1230">
<title id=" C02-1116.xml">linking syntactic and semantic arguments in a dependency based formalism </title>
<section> linguistic data.  </section>
<citcontext>
<prevsection>
<prevsent>we then show how we use syntactic role hierarchies to account for the data in linguistically concise way and define admissibility conditions for tdg derivation.
</prevsent>
<prevsent>section 5 contrasts our analysis of the dative shift construction with the analysis of thematic role alternations.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
insights from corpus studies (e.g. the negra treebank for german (skut et al, 1998), or the material annotated in the framenet (baker et al, 1998) <papid> P98-1013 </papid>project on the basis of the bank of english (cobuild, 2001) show that the syntactic patterns specific verbs occur with vary stongly.</citsent>
<aftsection>
<nextsent>not only do we observe different patterns for different verbs, but also alternative patterns with the same verbs.
</nextsent>
<nextsent>(1) to (6) illustrate the well-known dative shift (levin,1993):45 phenomenon, which occurs with restricted class of verbs only.
</nextsent>
<nextsent>while the distinction between (2) and (4) can be explained in terms of lexical semantics, even semantically closely related verbs as english give and deliver can differ in their syntactic behaviour, as the contrast between (1) and (5) shows.
</nextsent>
<nextsent>(1) [the postman] gave [him] [a package].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1242">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, the state-of-the-art speech recognizer scan take dictation with satisfactory accuracy.
</prevsent>
<prevsent>although continuing attempts for improvements in predictive power are needed in the language modeling area for speech recognizers, another research topic, understanding of the dictation results, is coming into focus.
</prevsent>
</prevsection>
<citsent citstr=" P98-1035 ">
structured language models (slms) (chelba and jelinek, 1998; <papid> P98-1035 </papid>charniak, 2001; <papid> P01-1017 </papid>mori et al., 2001) were proposed for these purposes.</citsent>
<aftsection>
<nextsent>their predictive powers are reported to be slightly higher than an orthodox word tri-gram model if the slms are interpolated with word tri-gram model.
</nextsent>
<nextsent>in contrast with word n-gram models, slms use the syntactic structure (a partial parse tree) covering the preceding words at each step of word prediction.
</nextsent>
<nextsent>the syntactic structure also grows in parallel with the word prediction.
</nextsent>
<nextsent>thus after the prediction of the last word of sentence, slms are able to give syntactic structures covering all the words of aninput sentence (parse trees) with associated probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1243">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, the state-of-the-art speech recognizer scan take dictation with satisfactory accuracy.
</prevsent>
<prevsent>although continuing attempts for improvements in predictive power are needed in the language modeling area for speech recognizers, another research topic, understanding of the dictation results, is coming into focus.
</prevsent>
</prevsection>
<citsent citstr=" P01-1017 ">
structured language models (slms) (chelba and jelinek, 1998; <papid> P98-1035 </papid>charniak, 2001; <papid> P01-1017 </papid>mori et al., 2001) were proposed for these purposes.</citsent>
<aftsection>
<nextsent>their predictive powers are reported to be slightly higher than an orthodox word tri-gram model if the slms are interpolated with word tri-gram model.
</nextsent>
<nextsent>in contrast with word n-gram models, slms use the syntactic structure (a partial parse tree) covering the preceding words at each step of word prediction.
</nextsent>
<nextsent>the syntactic structure also grows in parallel with the word prediction.
</nextsent>
<nextsent>thus after the prediction of the last word of sentence, slms are able to give syntactic structures covering all the words of aninput sentence (parse trees) with associated probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1244">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the early slms refer to only limited and xed part of the histories for each step of wordand structure prediction in order to avoid data sparseness problem.
</prevsent>
<prevsent>for example, in an english model (chelba and jelinek, 2000) the next word is predicted from the two right-most exposed heads.
</prevsent>
</prevsection>
<citsent citstr=" C00-1081 ">
also in japanese model (mori et al, 2000) <papid> C00-1081 </papid>the next word is predicted from 1) all exposed heads depending on the next word and 2) the words depending on those exposed heads.</citsent>
<aftsection>
<nextsent>one of the natural improvements in predictive power for an slm can be achieved by adding some exibility to the history reference mechanism.
</nextsent>
<nextsent>for linear history, which is referred to by using word n-gram models, we can use context tree (ron et al, 1996) as exible history reference mechanism.
</nextsent>
<nextsent>in an n-gram model with context tree, the length of each n-gram is increased selectively according to an estimate of the resulting improvement in predictive quality.
</nextsent>
<nextsent>thus, in general, an n-gram model with context tree has more predictive power in smaller model.in slms, the history is not simple word sequence but sequence of partial parse trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1257">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>language model parsing accuracy slm with acts 87.8% (674/768) juman+knp 85.3% (655/768) baseline  62.4% (479/768) * each bunsetsu depends on the next one.
</prevsent>
<prevsent>function words.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
in order to compare our parser with one of the state-of-the-art parsers, we calculated the bunsetsu-based accuracies of our model and knp(kurohashi and nagao, 1994) <papid> J94-4001 </papid>on the rst 100 sentences of the test corpus.</citsent>
<aftsection>
<nextsent>first the sentences were segmented into words by juman (kurohashi et al, 1994) and the output word sequences are parsed by knp.
</nextsent>
<nextsent>next, the word-based dependencies output by our parser were changed into bunsetsu as used by knp, where the bunsetsu which is depended upon by bunsetsu is de ned as the bunsetsu containing the word depended upon by the last word of the source bunsetsu (see figure 5).
</nextsent>
<nextsent>table 3 shows the bunsetsu-based accuracies of our model and knp.
</nextsent>
<nextsent>inaccuracy, our parser outperformed knp, but the difference was not statistically signi cant.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1258">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this means, however, our method is not able to eciently use lexical information about the content words at this stage.
</prevsent>
<prevsent>some model re nement should be explored for further improvements.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
historically, the structures of natural languages have been described by cfg and most parsers (fujisaki et al, 1989; pereira and schabes, 1992; <papid> P92-1017 </papid>charniak, 1997) are based on it.</citsent>
<aftsection>
<nextsent>an slm for english (chelba and jelinek, 2000), proposed as language model for speech recognition, is also based on cfg.
</nextsent>
<nextsent>on the other hand, an slm for japanese (mori et al, 2000) <papid> C00-1081 </papid>is based on markov model by introducing limit on language structures caused by our human memory limitations (yngve, 1960; miller, 1956).</nextsent>
<nextsent>we introduced the same limitation into our language model and our parser is also based on markov model.in the last decade, the importance of the lexicon has come into focus in the area of stochastic parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1263">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, an slm for japanese (mori et al, 2000) <papid> C00-1081 </papid>is based on markov model by introducing limit on language structures caused by our human memory limitations (yngve, 1960; miller, 1956).</prevsent>
<prevsent>we introduced the same limitation into our language model and our parser is also based on markov model.in the last decade, the importance of the lexicon has come into focus in the area of stochastic parsers.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
nowadays, many state-of-the-art parsers are based on lexicalized models (charniak, 1997;collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>in these papers, they reported signi cant improvement in parsing accuracy by lexicalization.
</nextsent>
<nextsent>our model is also lexicalized, the lexicalization is limited to grammatical function words because of the sparseness of data at the step of next word prediction.
</nextsent>
<nextsent>the greatest dierence between our parser and many state-of-the-art parsers is that our parser is based on generative language model,which works as language model of speech recognizer.
</nextsent>
<nextsent>therefore, speech recognizer equipped with our parser as its language model should be useful for spoken language understanding system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1265">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, speech recognizer equipped with our parser as its language model should be useful for spoken language understanding system.
</prevsent>
<prevsent>the greatest advantage of our model over other structured language models is the ablity to refer to variable part of the structured history by using acts.
</prevsent>
</prevsection>
<citsent citstr=" P98-1083 ">
there have been several attempts at japanese parsers (kurohashi and nagao, 1994; <papid> J94-4001 </papid>haruno et al,1998; <papid> P98-1083 </papid>fujio and matsumoto, 1998; kudo and matsumoto, 2000).<papid> W00-1303 </papid></citsent>
<aftsection>
<nextsent>these japanese parsers have all been based on unique phrasal unit called bunsetsu, concatenation of one or more content words followed by some grammatical function words.
</nextsent>
<nextsent>unlike these parsers, our model describes dependencies between words.
</nextsent>
<nextsent>thus our parser can more easily be extended to other languages.
</nextsent>
<nextsent>in addition, since almost allpasers in other languages than japanese output relationships between words, the output of our pars ercan be used by post-parser language processing systems proposed for many other languages (such as aword-level structural alignment of sentences in different languages).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1266">
<title id=" C02-1157.xml">a stochastic parser based on an slm with arboreal context trees </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, speech recognizer equipped with our parser as its language model should be useful for spoken language understanding system.
</prevsent>
<prevsent>the greatest advantage of our model over other structured language models is the ablity to refer to variable part of the structured history by using acts.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
there have been several attempts at japanese parsers (kurohashi and nagao, 1994; <papid> J94-4001 </papid>haruno et al,1998; <papid> P98-1083 </papid>fujio and matsumoto, 1998; kudo and matsumoto, 2000).<papid> W00-1303 </papid></citsent>
<aftsection>
<nextsent>these japanese parsers have all been based on unique phrasal unit called bunsetsu, concatenation of one or more content words followed by some grammatical function words.
</nextsent>
<nextsent>unlike these parsers, our model describes dependencies between words.
</nextsent>
<nextsent>thus our parser can more easily be extended to other languages.
</nextsent>
<nextsent>in addition, since almost allpasers in other languages than japanese output relationships between words, the output of our pars ercan be used by post-parser language processing systems proposed for many other languages (such as aword-level structural alignment of sentences in different languages).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1267">
<title id=" C04-1202.xml">using gene expression programming to construct sentence ranking functions for text summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we proposed novel summarization system architecture which employs gene expression programming technique as its learning mechanism.
</prevsent>
<prevsent>the preliminary experimental results have shown that our prototype system outperforms the baseline systems.
</prevsent>
</prevsection>
<citsent citstr=" P03-1048 ">
automatic text summarization has been studied for decades (edmundson 1969) and is still very active area (salton et al 1994; kupiec et al 1995; brandow et al 1995; lin 1999; aone et al 1999; sekine and nobata 2001; mani 2001; mckeown et al 2001; radev et al 2003).<papid> P03-1048 </papid></citsent>
<aftsection>
<nextsent>only few have tried using machine learning to accomplish this difficult task (lin 1999; aone et al 1999; neto et al 2002).
</nextsent>
<nextsent>most research falls into combining statistical methods with linguistic analysis.
</nextsent>
<nextsent>we regard the summarization as problem of empowering machine to learn from human-summarized text documents.
</nextsent>
<nextsent>we employ an evolutionary algorithm, gene expression programming (gep) (ferreira 2001), as the learning mechanism in our adaptive text summarization (ats) system to learn sentence ranking functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1269">
<title id=" C04-1202.xml">using gene expression programming to construct sentence ranking functions for text summarization </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>will be returned as the summary of that document and presented in their nature order.
</prevsent>
<prevsent>in the testing stage, different document set is supplied to test the similarity between the machine summarized text and the human or other system summarized text.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
in addition to the traditional way of extracting the highest ranked sentences in document to compose summary as in (edmundson 1969; lin 1999; kupiec et al 1995; brandow 1995; zechner 1996), <papid> C96-2166 </papid>we embedded machine learning mechanism in our system.</citsent>
<aftsection>
<nextsent>the system architecture is shown in figure 1 where the gep module is highlighted.
</nextsent>
<nextsent>in the training stage, each of the training documents is passed to the gep module after being preprocessed into set of sentence feature vectors.
</nextsent>
<nextsent>the gep runs generations, and in each generation population of sentence scoring functions in the form of chromosomes in gep is generated.
</nextsent>
<nextsent>every candidate scoring function is then applied to sentence feature vectors from every training document and produces score accordingly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1273">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is both important, because there is strong demand for all kindsof computer support for healthcare and clinical services, which aim at improving their quality and decreasing their costs, and challenging ? given the miracles of medical sublanguage, the various text genres one encounters and the enormous breadth of expertise surfacing as medical terminology.
</prevsent>
<prevsent>however, the development of human language technology for written language material has, up until now, almost exclusively focused on newswire or newspaper genres.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this is most prominently evidenced by the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>its value as one of the most widely used language resources mainly derives from two features.
</nextsent>
<nextsent>first, it supplies everyday, non-specialist document sources, such as the wall street journal, and,second, it contains value-added, viz.
</nextsent>
<nextsent>annotated, linguistic data.
</nextsent>
<nextsent>since the understanding of newspaper material does not impose particular requirements on its reader, other than the mastery of general english and common-sense knowledge, it is easy for almost everybody to deal with.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1275">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, the question arises, how portable results are from the newspaper domain to the medical domain?
</prevsent>
<prevsent>we will deal with these issues, focusing on the portability of taggers, from two perspectives.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
we first pick up off-the-shelf technology, in our case the rule-based brill tagger (brill, 1995) <papid> J95-4004 </papid>and the statistically-based tnt tagger (brants, 2000), <papid> A00-1031 </papid>both trained on newspaper data, and run it on medical text data.</citsent>
<aftsection>
<nextsent>one may wonder how the taggers trained on newspaper language perform with medical language.
</nextsent>
<nextsent>furthermore, one may ask whether it is necessary (and, if so, costly) to retrain these taggers on medical corpus, if one were at hand?
</nextsent>
<nextsent>these questions seem to be of particular importance, be cause the use of off-the-shelf language technology for mlp applications has recently been questioned (campbell and johnson, 2001).
</nextsent>
<nextsent>answers will be given in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1277">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, the question arises, how portable results are from the newspaper domain to the medical domain?
</prevsent>
<prevsent>we will deal with these issues, focusing on the portability of taggers, from two perspectives.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we first pick up off-the-shelf technology, in our case the rule-based brill tagger (brill, 1995) <papid> J95-4004 </papid>and the statistically-based tnt tagger (brants, 2000), <papid> A00-1031 </papid>both trained on newspaper data, and run it on medical text data.</citsent>
<aftsection>
<nextsent>one may wonder how the taggers trained on newspaper language perform with medical language.
</nextsent>
<nextsent>furthermore, one may ask whether it is necessary (and, if so, costly) to retrain these taggers on medical corpus, if one were at hand?
</nextsent>
<nextsent>these questions seem to be of particular importance, be cause the use of off-the-shelf language technology for mlp applications has recently been questioned (campbell and johnson, 2001).
</nextsent>
<nextsent>answers will be given in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1284">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> medical tagging with off-the-shelf.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 experiment 1: medical tagging with.
</prevsent>
<prevsent>standard tagset trained on negra the german default version of tnt was trained on negra, the largest publicly available manually annotated german newspaper corpus (composed of355,095 tokens and pos-tagged with the general purpose stts tagset; cf.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
(skut et al, 1997)).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>the brill tagger comes with an english default version also trained on general-purpose language corpora like the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid>in order to compare the performance of both taggers on german data, the brill tagger was retrained on the german negra newspaper corpus, with parameters recommended in the training manual.</nextsent>
<nextsent>in second round, we set aside subset of newly developed german-language medical corpus (21,000 tokens, with 1800 sentences).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1289">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this seems to be even more notable inasmuch as the taggers retraining was done on comparatively small-sized corpus (90,000 tokens).
</prevsent>
<prevsent>these experiments suggest two explanations.first, annotating medical texts with medically enhanced tagset took care of medical sublanguage properties not covered by general-purpose tagsets.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
second, several tagging experiments on newspaper language, whether statistical (ratnaparkhi, 1996; <papid> W96-0213 </papid>brants, 2000) <papid> A00-1031 </papid>or rule-based (brill, 1995), <papid> J95-4004 </papid>report that the tagging accuracy for unknown words is much lower than the overall accuracy.2 thus, the lower percentage of unknown words in medical texts seems to be sublanguage feature beneficial to pos taggers, whereas the higher proportion of unknown words in newspaper language seems to bea prominent source of tagging errors.</citsent>
<aftsection>
<nextsent>this is witnessed by the tagging accuracy for unknown words,which is much higher for the framed-trained tagger than for the newspaper-trained one.
</nextsent>
<nextsent>for the medical tagger, there is only 5 percentage point difference between overall and unknown word accuracy at training point 90,000, whereas, for the newspaper tagger, this difference amounts to 8.8 percentage points.
</nextsent>
<nextsent>this may be interrelated with another property of sublanguages, viz.
</nextsent>
<nextsent>their lower number of word types: at each training point, the lexicon of the framed tagger is 20 percentage points smaller than that of the newspaper tagger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1294">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this was partly achieved by computationally more expensive models than tnts efficienct unidirectional markov ian one.
</prevsent>
<prevsent>for example, gimenez and ma`rquez (2003) report an accuracy of 97.13% for their svm-basedpower tagger.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
the best automatically learned pos tagging result reported so far (97.24%) is toutanova et al (2003)<papid> N03-1033 </papid>s feature-based cyclic dependency network tagger.</citsent>
<aftsection>
<nextsent>although reaching the 98% accuracy level constitutes breakthrough, it is of course conditioned by the medical sublanguage we are working with.
</nextsent>
<nextsent>still, the application of language technologies in certain sublanguage domains like medicine, and more recently, genomics and biology, is gaining rapid importance, and thus, our results also have to be considered from this perspective.
</nextsent>
<nextsent>2these authors report on differences between 7.7 and 11.5 percentage points.
</nextsent>
<nextsent>we collected experimental evidence, contrary to recent claims (campbell and johnson, 2001), that off the-shelf nlp tools can be applied to mlp in straightforward way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1295">
<title id=" C04-1140.xml">high performance tagging on medical texts </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we collected experimental evidence, contrary to recent claims (campbell and johnson, 2001), that off the-shelf nlp tools can be applied to mlp in straightforward way.
</prevsent>
<prevsent>we explain this finding with statistically significant pos n-gram type overlaps of newspaper language and medical sublanguage, which has not been recognized before.to the best of our knowledge, this is the first tagging study that reaches 98% accuracy level for data-driven tagger (which must be distinguished from linguistically backuped taggers which come with heavy?
</prevsent>
</prevsection>
<citsent citstr=" P97-1032 ">
parsing machinery (samuelsson and voutilainen, 1997)).<papid> P97-1032 </papid></citsent>
<aftsection>
<nextsent>still, we deal with specialized sublanguage simpler in structure compared with newspaper language, although we kept it diverse through the various text genres.
</nextsent>
<nextsent>acknowledgements.
</nextsent>
<nextsent>we would like to thank our students, inka benthin, lucas champollion and caspar hasenclever, for their excellent work as human taggers.
</nextsent>
<nextsent>this work was partly supported by dfg grant kl 640/5-1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1296">
<title id=" C02-1151.xml">probabilistic reasoning for entity and relation recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in our framework, classifiers that identify entities and relations among them are first learned from local information in the sentence; this information, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities.
</prevsent>
<prevsent>our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately.
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
recognizing and classifying entities and relations in text data is key task in many nlp problems such as information extraction (ie) (califf and mooney, 1999;freitag, 2000; roth and yih, 2001), question answering (qa) (voorhees, 2000) and story comprehension (hirschman et al , 1999).<papid> P99-1042 </papid></citsent>
<aftsection>
<nextsent>in typical ie application of constructing jobs database from unstructured text, the system has to extract meaningful entities like title and salary and, ideally, to determine whether the entities are associated with the same position.
</nextsent>
<nextsent>in qa system, many questions ask for specific entities involved in some relations.
</nextsent>
<nextsent>for example, the question where was poe born??
</nextsent>
<nextsent>in trec-9 asks for the location entity in which poe was born.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1297">
<title id=" C02-1151.xml">probabilistic reasoning for entity and relation recognition </title>
<section> computational approach.  </section>
<citcontext>
<prevsection>
<prevsent>although the labels of entities and relations from sentence mutually depend on each other, two basic classifiers for entities and relations are first learned, in whicha multi-class classifier for e(or r) is learned as function of all other known?
</prevsent>
<prevsent>properties of the observation.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
the classifier for entities is named entity classifier, in which the boundary of an entity is predefined (collinsand singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>on the other hand, the relation classifier is given pair of entities, which denote the two arguments of the target relation.
</nextsent>
<nextsent>accurate predictions of these two classifiers seem to relyon complicated syntax analysis and semantics related information of the wholesentence.
</nextsent>
<nextsent>however, we derive weak classifiers by treating these two learning tasks as shallow text processing problems.
</nextsent>
<nextsent>this strategy has been successfully applied on several nlp tasks, such as information extraction (califf and mooney, 1999; freitag, 2000; roth and yih, 2001) and chunking (i.e. shallow paring) (munoz et al , 1999).<papid> W99-0621 </papid>it assumes that the class labels can be decided by local properties, such as the information provided by the words around or inside the target.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1298">
<title id=" C02-1151.xml">probabilistic reasoning for entity and relation recognition </title>
<section> computational approach.  </section>
<citcontext>
<prevsection>
<prevsent>accurate predictions of these two classifiers seem to relyon complicated syntax analysis and semantics related information of the wholesentence.
</prevsent>
<prevsent>however, we derive weak classifiers by treating these two learning tasks as shallow text processing problems.
</prevsent>
</prevsection>
<citsent citstr=" W99-0621 ">
this strategy has been successfully applied on several nlp tasks, such as information extraction (califf and mooney, 1999; freitag, 2000; roth and yih, 2001) and chunking (i.e. shallow paring) (munoz et al , 1999).<papid> W99-0621 </papid>it assumes that the class labels can be decided by local properties, such as the information provided by the words around or inside the target.</citsent>
<aftsection>
<nextsent>examples include the spelling of word, part-of-speech, and semantic related attributes acquired from external resources such as wordnet.
</nextsent>
<nextsent>the propositional learner we use is snow (roth,1998; carle son et al , 1999) 1 snow is multi-class classifier that is specifically tailored for large scale learning tasks.
</nextsent>
<nextsent>the learning architecture makes use of network of linear functions, in which the targets (entity classes or relation classes, in this case) are represented as linear 1available at http://l2r.cs.uiuc.edu/cogcomp/cc-software.html functions over common feature space.
</nextsent>
<nextsent>within snow, we use here learning algorithm which is variation of winnow (littlestone, 1988), feature efficient algorithm that is suitable for learning in nlp-like domains, where the number of potential features is very large, but only few of them are active in each example, and only small fraction of them are relevant to the target concept.while typically snow is used as classifier, and predicts using winner-take-all mechanism over the activation value of the target classes, here we rely directly on the raw activation value it outputs, which is the weighted linear sum of the features, to estimate the posteriors.it can be verified that the resulting values are monotonic with the confidence in the prediction, therefore is good source of probability estimation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1299">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> formal model for information selection.  </section>
<citcontext>
<prevsection>
<prevsent>current summarization systems often represent concepts indirectly via textual features that give high scores to the textual units that contain important information and should be used in the summary and low scores to those textual units which are not likely to contain information worth to be included in the final output.
</prevsent>
<prevsent>thus, many summarization approaches use as conceptual units lexical features like tf*idf weighing of wordsin the input text(s), words used in the titles and section headings of the source documents (luhn, 1959; h.p.edmundson, 1968), or certain cue phrases like significant, important and in conclusion (kupiec et al., 1995; teufel and moens, 1997).
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (barzilay and elhadad, 1997) <papid> W97-0703 </papid>or syntactic constraints between representations of concepts (hatzivassiloglou et al, 2001).</citsent>
<aftsection>
<nextsent>conceptual units do not have to be directly observable as text snippets; they can represent abstract properties that particular text units may or may not satisfy, for example, stat usas first sentence in paragraph or generally position in the source text (lin and hovy, 1997).<papid> A97-1042 </papid></nextsent>
<nextsent>some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997), <papid> W97-0713 </papid>while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (hatzivassiloglou et al, 2001).no matter how these important concepts are defined, different systems use text-observable features that either correspond to the concepts of interest (e.g., words and their frequencies) or point out those text units that potentially contain important concepts (e.g., position or discourse properties of the text unit in the source document).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1300">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> formal model for information selection.  </section>
<citcontext>
<prevsection>
<prevsent>thus, many summarization approaches use as conceptual units lexical features like tf*idf weighing of wordsin the input text(s), words used in the titles and section headings of the source documents (luhn, 1959; h.p.edmundson, 1968), or certain cue phrases like significant, important and in conclusion (kupiec et al., 1995; teufel and moens, 1997).
</prevsent>
<prevsent>conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (barzilay and elhadad, 1997) <papid> W97-0703 </papid>or syntactic constraints between representations of concepts (hatzivassiloglou et al, 2001).</prevsent>
</prevsection>
<citsent citstr=" A97-1042 ">
conceptual units do not have to be directly observable as text snippets; they can represent abstract properties that particular text units may or may not satisfy, for example, stat usas first sentence in paragraph or generally position in the source text (lin and hovy, 1997).<papid> A97-1042 </papid></citsent>
<aftsection>
<nextsent>some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997), <papid> W97-0713 </papid>while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (hatzivassiloglou et al, 2001).no matter how these important concepts are defined, different systems use text-observable features that either correspond to the concepts of interest (e.g., words and their frequencies) or point out those text units that potentially contain important concepts (e.g., position or discourse properties of the text unit in the source document).</nextsent>
<nextsent>the former classof features can be directly converted to conceptual units in our representation, while the latter can be accounted for by postulating abstract conceptual units associated with particular status (e.g., first sentence) for particular textual unit.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1301">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> formal model for information selection.  </section>
<citcontext>
<prevsection>
<prevsent>conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (barzilay and elhadad, 1997) <papid> W97-0703 </papid>or syntactic constraints between representations of concepts (hatzivassiloglou et al, 2001).</prevsent>
<prevsent>conceptual units do not have to be directly observable as text snippets; they can represent abstract properties that particular text units may or may not satisfy, for example, stat usas first sentence in paragraph or generally position in the source text (lin and hovy, 1997).<papid> A97-1042 </papid></prevsent>
</prevsection>
<citsent citstr=" W97-0713 ">
some summarization systems assume that the importance of sentence is derivable from rhetorical representation of the source text (marcu, 1997), <papid> W97-0713 </papid>while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (hatzivassiloglou et al, 2001).no matter how these important concepts are defined, different systems use text-observable features that either correspond to the concepts of interest (e.g., words and their frequencies) or point out those text units that potentially contain important concepts (e.g., position or discourse properties of the text unit in the source document).</citsent>
<aftsection>
<nextsent>the former classof features can be directly converted to conceptual units in our representation, while the latter can be accounted for by postulating abstract conceptual units associated with particular status (e.g., first sentence) for particular textual unit.
</nextsent>
<nextsent>we assume that each conceptual unit has an associated importance weight wi that indicates how important unit ci is to the overall summary or answer.
</nextsent>
<nextsent>2.1 first model: full correspondence having formally defined the sets and of textual and conceptual units, the part that remains in order to have the complete picture of the constraints given by the data and summarization approach is the mapping between textual units and conceptual units.
</nextsent>
<nextsent>this mapping, function : tc ? [0, 1], tells us how well each conceptual unit is covered by given textual unit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1302">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> formal model for information selection.  </section>
<citcontext>
<prevsection>
<prevsent>this approach is similar to that taken in evaluations that keep the length of the output summary within certain bounds, such as the recent major summarization evaluations in the document understanding conferences from 2001 to the present (harman and voorhees, 2001).
</prevsent>
<prevsent>another approach would beto combine the two components and assign composite score to each summary, essentially mandating specific tradeoff between recall and precision;for example, the total score can be defined as linear combination of i(s) and (s), in which casethe weights specify the relative importance of coverage and precision/brevity, as well as accounting for scale differences between the two metrics.
</prevsent>
</prevsection>
<citsent citstr=" N03-2037 ">
this approach is similar to the calculation of recall, precision, and f-measure adopted in the recent nist evaluation of long answers for definitional questions (voorhees, 2003).<papid> N03-2037 </papid></citsent>
<aftsection>
<nextsent>in this paper, we will follow the first tactic of maximizing i(s) with limit on (s) rather than attempting to solve the thorny issues of weighing the two components appropriately.
</nextsent>
<nextsent>summarization redundancy of information has been found useful in determining what text pieces should be included during summarization, on the basis that information that is repeated is likely to be central to the topic orevent being discussed.
</nextsent>
<nextsent>earlier work has also recognized that, while it is good idea to select among the passages repeating information, it is also important to avoid repetition of the same information in the final output.
</nextsent>
<nextsent>two main approaches have been proposed for avoiding redundancy in the output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1303">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> handling redundancy in.  </section>
<citcontext>
<prevsection>
<prevsent>one approach relies on grouping together potential output text units on the basis of their similarity, and outputtingonly representative from each group (hatzivas siloglou et al, 2001).
</prevsent>
<prevsent>sentences can be clustered in this manner according to word overlap, or by using additional content similarity features.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
this approach has been recently applied to the construction of paragraph-long answers (e.g., (blair-goldensohn et al, 2003; yu and hatzivassiloglou, 2003)).<papid> W03-1017 </papid>an alternative approach, proposed for the synthesis of information during query-based passage retrieval is the maximum marginal relevance (mmr)method (goldstein et al, 2000).</citsent>
<aftsection>
<nextsent>this approach assigns to each potential new sentence in the output similarity score with the sentences already included in the summary.
</nextsent>
<nextsent>only those sentences that contain substantial amount of new information can get into the summary.
</nextsent>
<nextsent>mmr bases this similarity score on word overlap and additional information about the time when each document was released, and thuscan fail to identify repeated information when paraphrasing is used to convey the same meaning.in contrast to these approaches, our model handles redundancy in the output at the same time it selects the output sentences.
</nextsent>
<nextsent>it is clear from equations (1)?(3) that each conceptual unit is counted only once whether it appears in one or multiple textual units.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1304">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>filatova and hatzivassiloglou (2003) define the procedure for extracting atomic events in detail, and show that these triplets capture the most important relations connecting the major constituent parts of events, such as location, dates and participants.
</prevsent>
<prevsent>our hypothesis is that using these events as conceptual units would provide reasonable basis for summarizing texts that are supposed to describe one or more events.evaluation metric given the difficulties in coming up with universally accepted evaluation measure for summarization, and the fact that judgments by humans are time-consuming and labor-intensive, we adopted an automated process for comparing system-produced summaries to the ideal summaries written by humans.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the rouge method (lin and hovy, 2003) <papid> N03-1020 </papid>is based on n-gram overlap between the system-produced and ideal summaries.</citsent>
<aftsection>
<nextsent>as such, it is recall-based measure, and it requires thatthe length of the summaries be controlled in order to allow for meaningful comparisons.
</nextsent>
<nextsent>although rouge is only proxy measure of summary quality, it offers the advantage that it can be readily applied to compare the performance of different systems on the same set of documents, assuming that ideal summaries are available for those documents.
</nextsent>
<nextsent>baseline our baseline method does not consider the overlap in information content between selected textual units.
</nextsent>
<nextsent>instead, we fix the score of each sentence as the sum of tf*idf values or atomic eventscores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1305">
<title id=" C04-1057.xml">a formal model for information selection in multi sentence text extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>events tend to be particularly good representation for document sets with well-defined constituent parts (such as specific participants) that cluster around narrow event.
</prevsent>
<prevsent>events not only give us higher absolute performance when compared to just words but also lead to more pronounced improvement when our model is employed.
</prevsent>
</prevsection>
<citsent citstr=" W04-1017 ">
a more detailed analysis of the above experiments together with the discussion of advantages and disadvantages of our evaluation schema can be found in (filatova and hatzivassiloglou, 2004).<papid> W04-1017 </papid></citsent>
<aftsection>
<nextsent>in this paper we proposed formal model for information selection and redundancy avoidance in summarization and question-answering.
</nextsent>
<nextsent>within this two-dimensional model, summarization and question-answering entail mapping textual units onto conceptual units, and optimizing the selection of subset of textual units that maximizes the information content of the covered conceptual units.
</nextsent>
<nextsent>the formalization of the process allows us to benefit from theoretical results, including suitable approximation algorithms.
</nextsent>
<nextsent>experiments using duc data showed that this approach does indeed lead to improvements due to better information packing over straightforward content selection method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1306">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, when some examples conflict during re = = = null0 show1 me2 the3 one4 in5 the6 window7 uindo1 no2 shinamono3 o4 mise5 telidasai6 7 0 4 0 1 1( ) figure 1: example of word alignment between english and japanese (watanabe and sumita, 2003)trieval, example-based mt selects the best example scored by the similarity between the input and the source part of the example.
</prevsent>
<prevsent>this implies that example-based mt does not check whether the translation of the given input sentence is correct or not.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
on the other hand, statistical mt employing ibm models (brown et al, 1993) <papid> J93-2003 </papid>translates an in put sentence by the combination of word transfer and word re-ordering.</citsent>
<aftsection>
<nextsent>therefore, when it is applied to language pair in which the word order is quite different (e.g., english and japanese, figure 1), it becomes difficult to find globally optimal solution due to the enormous search space (watan abe and sumita, 2003).
</nextsent>
<nextsent>statistical mt could generate high-quality translations if it succeeded in finding globally optimal solution.
</nextsent>
<nextsent>therefore, the models employed by statistical mt are superior indicators of the quality of machine translation.
</nextsent>
<nextsent>using this feature, akiba et al (2002) <papid> C02-1076 </papid>achieved selection of the best translation among those output by multiple mt engines.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1307">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>statistical mt could generate high-quality translations if it succeeded in finding globally optimal solution.
</prevsent>
<prevsent>therefore, the models employed by statistical mt are superior indicators of the quality of machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C02-1076 ">
using this feature, akiba et al (2002) <papid> C02-1076 </papid>achieved selection of the best translation among those output by multiple mt engines.</citsent>
<aftsection>
<nextsent>this paper presents an example-based mt method based on syntactic transfer, which selects the best translation by using models of statistical mt. this method is roughly structured using two modules (figure 2).
</nextsent>
<nextsent>one is an example-based syntactic transfer module.
</nextsent>
<nextsent>this module constructs input sentence output sentence example-based syntactic transfer thesaurus preprocessing postprocessing statistical generation translation dictionary transfer rules translation model language model figure 2: structure of proposed method tree structures of the target language by parsing and mapping the input sentence while referring to transfer rules.
</nextsent>
<nextsent>the other is statistical generation module, which selects the best word sequence ofthe target language in the same manner as statistical mt. therefore, this method is sequentially combined example-based and statistical mt.the proposed method has the following advantages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1308">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>2.3 usage of source examples.
</prevsent>
<prevsent>example-based transfer utilizes the source examples for disambiguation of mapping and parsing.
</prevsent>
</prevsection>
<citsent citstr=" P91-1024 ">
specifically, the semantic distance (sumita andiida, 1991) <papid> P91-1024 </papid>is calculated between the source examples and the headwords of the input sentence, andthe transfer rules that contain the nearest example are used to construct the target tree structure.</citsent>
<aftsection>
<nextsent>the semantic distance between words is define das the distance from the leaf node to the most specific common abstraction (msca) in thesaurus (ohno and hama nishi, 1984).
</nextsent>
<nextsent>for example, if the input phrase ie (home) ni kaeru (return)?
</nextsent>
<nextsent>is given, rules 1 to 3 in figure 3 are used for the syntactic transfer, and three target nodes are generated without any disambiguation.
</nextsent>
<nextsent>however, when we compare the source examples with the headword of the variables (ie) and y(kaeru), only rule 2 is used for the transfer be cause the semantic distance of the example (soko (there), yuku (go)) is the nearest.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1309">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>transfer rules transfer rules were acquired from the training set using hierarchical phrase alignment, and low-frequency rules that appeared less than twice were removed.
</prevsent>
<prevsent>the number of rules was 24,310.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
translation model and language model we used lexicon model of ibm model 4 learned by giza++ (och and ney, 2003) <papid> J03-1002 </papid>and word bigram and trigram models learned by cmu-cambridge statistical language modeling toolkit (clarkson and rosenfeld, 1997).compared methods we compared the following four methods.</citsent>
<aftsection>
<nextsent>baseline (example-based transfer only)the best translation that had the same semantic distance was randomly selected from the the bus tm: -0.07lm: -1.94 bus tm: -0.07lm: -0.0 xnp n-best n-best n-best will yvp leave at 11 oclock tm: -2.72lm: -4.58 start at 11 oclock tm: -3.62lm: -4.17 leaves at 11 oclock tm: -2.72lm: -3.11 yvp leave tm: -1.88lm: -0.0 start tm: -2.78lm: -0.0 leaves tm: -1.88lm: -0.0 xpp at 11 oclock tm: -0.84lm: -2.79 at 11 tm: -4.91lm: -2.26 bus tm: -0.07lm: -2.11 bus will start at 11 oclock the bus will leave at 11 oclock bus will leave at 11 oclock tm: -7.13lm: -14.30 tm: -8.03 lm: -13.84 tm: -7.13 lm: -13.54    /s  figure 5: example of bottom-up generation (tm and lm denote log probabilities of the translation and language models, respectively) tree that was output from the example-based transfer module.
</nextsent>
<nextsent>the translation words were selected in advance as those having the highest frequency in the training corpus.
</nextsent>
<nextsent>this is the baseline for translating sentence when using only the example-based transfer.
</nextsent>
<nextsent>bottom-up the bottom-up generation selects the best translation from the outputs of the example based transfer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1310">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>the purpose of this experiment is to measure the influence of the translation model.evaluation metrics from the test set, 510 sentences were evaluated by the following automatic and subjective evaluation metrics.
</prevsent>
<prevsent>the number of reference translations for automatic evaluation was 16 per sentence.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
bleu: automatic evaluation by bleu score (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>nist: automatic evaluation by nist score (doddington, 2002).
</nextsent>
<nextsent>mwer: the mean rate by calculating the word error rates between the mt results and all reference translations, where the lowest rate is selected.
</nextsent>
<nextsent>subjective evaluation: subjective evaluation by an english native speaker into the four ranks of a: perfect, b: fair, c: acceptable, and d: nonsense.
</nextsent>
<nextsent>automatic evaluation subjective evaluation translation speed method bleu nist mwer a+b a+b+c mean worst (sec./sent.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1311">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>5 discussion.
</prevsent>
<prevsent>we incorporated example-based mt in models of statistical mt. however, some methods to obtain initial solutions of statistical mt by example based mt have already been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P01-1050 ">
for example, marcu (2001) <papid> P01-1050 </papid>proposed method inwhich initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (germann et al, 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>watanabe and sumita (2003) proposed decoding algorithm in which translations that are similar tothe input sentence are retrieved from bilingual corpora and then modified by greedy decoding.
</nextsent>
<nextsent>the difference between our method and these methods involves whether modification is applied.
</nextsent>
<nextsent>our approach simply selects the best translation from candidates that are output from example based mt. even though example-based mt can output appropriate translations to some degree, our method assumes that the candidates contain globally optimal solution.
</nextsent>
<nextsent>this means that the upper bound of mt quality is limited by the example-based transfer, so we have to improve this stage in order to further improve mt quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1312">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>5 discussion.
</prevsent>
<prevsent>we incorporated example-based mt in models of statistical mt. however, some methods to obtain initial solutions of statistical mt by example based mt have already been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
for example, marcu (2001) <papid> P01-1050 </papid>proposed method inwhich initial translations are constructed by combining bilingual phrases from translation memory, which is followed by modifying the translations by greedy decoding (germann et al, 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>watanabe and sumita (2003) proposed decoding algorithm in which translations that are similar tothe input sentence are retrieved from bilingual corpora and then modified by greedy decoding.
</nextsent>
<nextsent>the difference between our method and these methods involves whether modification is applied.
</nextsent>
<nextsent>our approach simply selects the best translation from candidates that are output from example based mt. even though example-based mt can output appropriate translations to some degree, our method assumes that the candidates contain globally optimal solution.
</nextsent>
<nextsent>this means that the upper bound of mt quality is limited by the example-based transfer, so we have to improve this stage in order to further improve mt quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1313">
<title id=" C04-1015.xml">example based machine translation based on syntactic transfer with statistical models </title>
<section> s ? x </section>
<citcontext>
<prevsection>
<prevsent>our approach simply selects the best translation from candidates that are output from example based mt. even though example-based mt can output appropriate translations to some degree, our method assumes that the candidates contain globally optimal solution.
</prevsent>
<prevsent>this means that the upper bound of mt quality is limited by the example-based transfer, so we have to improve this stage in order to further improve mt quality.
</prevsent>
</prevsection>
<citsent citstr=" P03-1057 ">
for instance, example-based mt can be improved by applying an optimization algorithm that uses an automatic evaluation of mt quality (imamura et al, 2003).<papid> P03-1057 </papid></citsent>
<aftsection>
<nextsent>this paper demonstrated that example-based mt can be improved by incorporating it in models of statistical mt. the example-based mt used in this paper is based on syntactic transfer, so word reordering is achieved in the transfer module.
</nextsent>
<nextsent>using this feature, the best translation was selected by using only lexicon model and an n-gram language model.
</nextsent>
<nextsent>in addition, bottom-up generation achieved faster translation speed by using the tree structure of the target sentence.
</nextsent>
<nextsent>acknowledgements the authors would like to thank kadokawa publishers, who permitted us to use the hierarchy of ruigo-shin-jiten.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1314">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>problem as problem of translating from source language of queries into target language of documents, represented as snippets.
</prevsent>
<prevsent>since both queries and snippets are arguably natural language, statistical machine translation technology (smt) is readily applicable to this task.
</prevsent>
</prevsection>
<citsent citstr=" P07-1059 ">
in previous work,this has been done successfully for question answering tasks (riezler et al , 2007; <papid> P07-1059 </papid>soricut and brill, 2006; echihabi and marcu, 2003; <papid> P03-1003 </papid>berger et al ., 2000), but not for web search in general.</citsent>
<aftsection>
<nextsent>cui et al .s (2002) model is to our knowledge the first to deploy query-document relations for direct extraction of expansion terms for general web retrieval.
</nextsent>
<nextsent>our smt approach has two main advantages overcui et al model: firstly, cui et al model relates document terms to query terms by using simple term frequency counts in session data, without considering smoothing techniques.
</nextsent>
<nextsent>our approach deploys sophisticated machine learning approach to word alignment, including smoothing techniques, to map query phrases to snippetphrases.
</nextsent>
<nextsent>secondly, cui et al model only indirectly uses context information to disambiguate expansion terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1315">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>problem as problem of translating from source language of queries into target language of documents, represented as snippets.
</prevsent>
<prevsent>since both queries and snippets are arguably natural language, statistical machine translation technology (smt) is readily applicable to this task.
</prevsent>
</prevsection>
<citsent citstr=" P03-1003 ">
in previous work,this has been done successfully for question answering tasks (riezler et al , 2007; <papid> P07-1059 </papid>soricut and brill, 2006; echihabi and marcu, 2003; <papid> P03-1003 </papid>berger et al ., 2000), but not for web search in general.</citsent>
<aftsection>
<nextsent>cui et al .s (2002) model is to our knowledge the first to deploy query-document relations for direct extraction of expansion terms for general web retrieval.
</nextsent>
<nextsent>our smt approach has two main advantages overcui et al model: firstly, cui et al model relates document terms to query terms by using simple term frequency counts in session data, without considering smoothing techniques.
</nextsent>
<nextsent>our approach deploys sophisticated machine learning approach to word alignment, including smoothing techniques, to map query phrases to snippetphrases.
</nextsent>
<nextsent>secondly, cui et al model only indirectly uses context information to disambiguate expansion terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1316">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> query-snippet translation.  </section>
<citcontext>
<prevsection>
<prevsent>the only attempt at smoothing that is made in this approach is recurrence to words in query context, using equation 2, when equation 1 assigns zero probability to unseen pairs.
</prevsent>
<prevsent>738
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
the smt system deployed in our approach isan implementation of the alignment template approach of och and ney (och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>the basic features of the model consist of translation model and language model which go back to the noisy channel formulation of machine translation in brown et al  (1993).<papid> J93-2003 </papid></nextsent>
<nextsent>their fundamental equation of machine translation?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1317">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> query-snippet translation.  </section>
<citcontext>
<prevsection>
<prevsent>738
</prevsent>
<prevsent>the smt system deployed in our approach isan implementation of the alignment template approach of och and ney (och and ney, 2004).<papid> J04-4002 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the basic features of the model consist of translation model and language model which go back to the noisy channel formulation of machine translation in brown et al  (1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>their fundamental equation of machine translation?
</nextsent>
<nextsent>defines the job of translation system as finding the english string ? that is translation of foreign string such that ? = argmax p (e|f) = argmax p (f |e)p (e) (3) equation 3 allows for separation of language model (e), and translation model (f |e).
</nextsent>
<nextsent>ochand ney (2004) <papid> J04-4002 </papid>reformulate equation 3 as linear combination of feature functions m (e, f) and weights ? m, including feature functions for translation models i (e, f) = (f |e) and language models j (e) = (e): ? = argmax m ? m=1 ? h (e, f) (4) the translation model used in our approach isbased on the sequence of alignment models described in och and ney (2003).<papid> J03-1002 </papid></nextsent>
<nextsent>the relationship of translation model and alignment model for source language string = j 1 and target string = i 1 is via hidden variable describing an alignment mapping from source position to target position j : (f 1 |e 1 ) = ? j 1 (f 1 , j 1 |e 1 ) (5) the alignment j 1 contains so-called null-word alignments j = 0 that align source words to the empty word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1319">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> query-snippet translation.  </section>
<citcontext>
<prevsection>
<prevsent>their fundamental equation of machine translation?
</prevsent>
<prevsent>defines the job of translation system as finding the english string ? that is translation of foreign string such that ? = argmax p (e|f) = argmax p (f |e)p (e) (3) equation 3 allows for separation of language model (e), and translation model (f |e).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
ochand ney (2004) <papid> J04-4002 </papid>reformulate equation 3 as linear combination of feature functions m (e, f) and weights ? m, including feature functions for translation models i (e, f) = (f |e) and language models j (e) = (e): ? = argmax m ? m=1 ? h (e, f) (4) the translation model used in our approach isbased on the sequence of alignment models described in och and ney (2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the relationship of translation model and alignment model for source language string = j 1 and target string = i 1 is via hidden variable describing an alignment mapping from source position to target position j : (f 1 |e 1 ) = ? j 1 (f 1 , j 1 |e 1 ) (5) the alignment j 1 contains so-called null-word alignments j = 0 that align source words to the empty word.
</nextsent>
<nextsent>the different alignment models described in och and ney (2003) <papid> J03-1002 </papid>each parameterize equation 5 differently so as to capture different properties of source and target mappings.</nextsent>
<nextsent>all models are based on estimating parameters ? by maximizing the likelihood of training data consisting of sentence-aligned, but not word-aligned strings {(f , s) : = 1, . . .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1322">
<title id=" C08-1093.xml">translating queries into snippets for improved query expansion </title>
<section> query-snippet translation.  </section>
<citcontext>
<prevsection>
<prevsent>language modeling in our approach deploys an n-gram language model that assigns the following probability to string l 1 of words (see brants et al .
</prevsent>
<prevsent>(2007)): (w 1 ) = ? i=1 (w |w i1 1 ) (7) ? ? i=1 (w |w i1 in+1 ) (8) estimation of n-gram probabilities is done by counting relative frequencies of n-grams in corpus of user queries.
</prevsent>
</prevsection>
<citsent citstr=" D07-1090 ">
remedies against sparse data problems are achieved by various smoothing techniques, as described in brants et al  (2007).<papid> D07-1090 </papid>for applications of the system to translate unseen queries, standard dynamic-programming beam-search decoder (och and ney, 2004) <papid> J04-4002 </papid>that tightly integrates translation model and language model is used.</citsent>
<aftsection>
<nextsent>expansion terms are taken from those terms in the 5-best translations of the query that have not been seen in the original query string.
</nextsent>
<nextsent>in our opinion, the advantages of using an alignment-based translation model to correlate document terms with query terms, instead of relying on term frequency counts as in equation 1, areas follows.
</nextsent>
<nextsent>the formalization of translation models as involving hidden alignment variable allows us to induce probability distribution that assigns some probability of being translated into target word to every source word.
</nextsent>
<nextsent>this is crucial step towards solving the problem of the lexical gap?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1326">
<title id=" C02-1096.xml">word form and class based prediction of the components of german nominal compounds in an aac system </title>
<section> word prediction for aac.  </section>
<citcontext>
<prevsection>
<prevsent>in section 6, we report about our preliminary experiments with the integration of compound and simple word prediction.
</prevsent>
<prevsent>finally, in section 7,we summarize the main results we obtained and indicate directions for further work.
</prevsent>
</prevsection>
<citsent citstr=" W97-0506 ">
word prediction systems based on n-gram statistics are an important component of aac devices, i.e.,software and possibly hardware typing aids for disabled users (copestake, 1997; <papid> W97-0506 </papid>carlberger, 1998).word predictors provide the user with prediction window, i.e. menu that, at any time, lists the most likely next word candidates, given the input that the user has typed until the current character.</citsent>
<aftsection>
<nextsent>if the word that the user intends to type next is in the prediction window, the user can select it from there.
</nextsent>
<nextsent>otherwise, the user will keep typing letters,until the target word appears in the prediction window (or until she finishes typing the word).
</nextsent>
<nextsent>the (percentage) keystroke savings rate (ksr) is standard measure used in aac research to evaluate word predictors.
</nextsent>
<nextsent>the ksr can be thought of as the percentage of keystrokes that perfect?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1327">
<title id=" C02-1096.xml">word form and class based prediction of the components of german nominal compounds in an aac system </title>
<section> the split compound prediction model.  </section>
<citcontext>
<prevsection>
<prevsent>thus, if you have to predict the head of compound given fruit modifier, it would be reasonable, all else being equal, to guess some kind of sweet.
</prevsent>
<prevsent>4.1.1 class-based modifier-head bigrams while semantically-driven prediction makes sense in principle, clustering nouns into semantic classe sis certainly not trivial job, and, if large input lexicon must be partitioned, it is not task that couldbe accomplished by human expert.
</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
drawing inspiration from brown et al (1990), we constructed instead semantic classes using clustering algorithm extracting them from corpus, on the basis of the average mutual information (mi) between pairs of words (rosenfeld, 1996).5 4even if the model handled other compound types, very few pos combinations are attested within compounds.5we are aware of the fact that other measures of lexical association have been proposed (evert and krenn, 2001, <papid> P01-1025 </papid>and mi values were computed using adam bergers trigger toolkit (berger, 1997).6 the same training corpus of about 25.5m words (and with n+n compounds split) that we describe below was used to collect mi values for noun pairs.</citsent>
<aftsection>
<nextsent>all modifiers and heads of n+n compounds and all corpus words that were parsed as nouns by the xerox morphological analyzer (karttunen et al, 1997) were counted as nouns for this purpose.
</nextsent>
<nextsent>mi was computed only for pairs that co-occurred at least three times in the corpus (thus, only subset of the input nouns appears in the output list).
</nextsent>
<nextsent>valid co-occurrences were bound by maximal distance between elements of 500 words, and minimal distance of 2 words (to avoid lexicalized phrases, such as proper names or phrasal loanwords).
</nextsent>
<nextsent>having obtained list of pairs from the toolkit, the next step was to cluster them into classes, by grouping together nouns with high mi.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1328">
<title id=" C04-1022.xml">automatic learning of language model structure </title>
<section> factored language models.  </section>
<citcontext>
<prevsection>
<prevsent>this in turn results in high perplexity and in large number of out-of-vocabulary (oov) words when applying trained language model to new unseen text.
</prevsent>
<prevsent>2.1 factored word representations.
</prevsent>
</prevsection>
<citsent citstr=" N03-2002 ">
a recently developed approach that addresses this problem is that of factored language models (flms) (kirchhoff et al, 2002; bilmes and kirchhoff, 2003), <papid> N03-2002 </papid>whose basic idea is to decompose words into sets of features (or factors) instead of viewing them as unanalyzable wholes.probabilistic language models can then be constructed over (sub)sets of word features instead of, or in addition to, the word variables them selves.</citsent>
<aftsection>
<nextsent>for instance, words can be decomposed into stems/lexemes and pos tags indicating their morphological features, as shown below: word: stock prices are rising stem: stock price be rise tag: nsg n3pl v3pl vpart such representation serves to express lexical and syntactic generalizations, which would otherwise remain obscured.
</nextsent>
<nextsent>it is comparable toclass-based representations employed in standard class-based language models; however, in flms several simultaneous class assignments are allowed instead of single one.
</nextsent>
<nextsent>in general, we assume that word is equivalent to fixed number (k) of factors, i.e. ? f1:k . the task then is to produce statistical model over the resulting representation - using trigram approximation, the resulting probability model is as follows: p(f1:k1 , f1:k2 , ..., f1:kt ) ? ? t=3 p(f1:kt |f1:kt1 , f1:kt2 ) (2)thus, each word is dependent not only on single stream of temporally ordered word variables,but also on additional parallel (i.e. simultaneously occurring) features.
</nextsent>
<nextsent>this factored representation can be used in two different ways to improve over standard lms: by using product model or backoff model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1330">
<title id=" C04-1022.xml">automatic learning of language model structure </title>
<section> factored language models.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to different choices for g, different discounting parameters can be chosen at different levels in the backoff graph.
</prevsent>
<prevsent>for instance, at the topmost node, kneser-ney discounting might be chosen whereas at lower node good-turing might be applied.
</prevsent>
</prevsection>
<citsent citstr=" N04-4034 ">
flms have been implemented as an add-on to the widely-used srilm toolkit1 and have been used successfully for the purpose of morpheme-based language modeling (bilmes and kirchhoff, 2003), <papid> N03-2002 </papid>multi-speaker language modeling (ji and bilmes, 2004), <papid> N04-4034 </papid>and speech recognition (kirchhoff et al, 2003).</citsent>
<aftsection>
<nextsent>in order to use an flm, three types of parameters need to be specified: the initial conditioning factors, the backoff graph, and the smoothing options.
</nextsent>
<nextsent>the goal of structure learning is to find the parameter combinations that create flms that achieve low perplexity on unseen test data.
</nextsent>
<nextsent>the resulting model spaceis extremely large: given factored word representation with total of factors, there are n=1 (k )possible subsets of initial conditioning factors.
</nextsent>
<nextsent>for set of conditioning factors, there are up to m!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1331">
<title id=" C04-1022.xml">automatic learning of language model structure </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>a sample annotation (for the word yararlanmak, consisting of the root yarar plus three inflectional groups) is shown below: yararmanlak: yarar+noun+a3sg+pnon+nom db+verb+acquire+pos db+noun+inf+a3sg+pnon+nom we removed segmentation marks (for titles and paragraph boundaries) from the corpus but included punctuation.
</prevsent>
<prevsent>words may have different numbers of inflectional groups, but the flm representation requires the same number of factors for each word; we therefore had to map the original morphological tags to fixed-length factored representation.
</prevsent>
</prevsection>
<citsent citstr=" P99-1033 ">
this was done using linguistic knowledge: according to (oflazer, 1999), <papid> P99-1033 </papid>the final inflectional group in each dependent word has special status since it determines inflectional markings on headwords following the dependent word.</citsent>
<aftsection>
<nextsent>the final inflectional group was therefore analyzed into separate factors indicating the number (n), case (c), part-of-speech (p) and all other information (o).
</nextsent>
<nextsent>additional factors for the word are the root (r) and all remaining information in the original tag not subsumed by the other factors (g).
</nextsent>
<nextsent>the word itself is used as another factor (w).
</nextsent>
<nextsent>thus, the above example would be factor ized as follows: w:yararlanmak/r:yarar/p:nouninf-n:a3sg/ c:nom/o:pnon/g:nouna3sgpnonnom+verb +acquire+pos other factorizations are certainly possible; however, our primary goal is not to find the best possible encoding for our data but to demonstrate the effectiveness of the flm approach, which is largely independent of the choice of factors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1332">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd), the task of identifying the intended meaning (sense) of words in context, is long-standing problem in natural language processing.
</prevsent>
<prevsent>sense disambiguation is of ten characterized as an intermediate task, which is not an end in itself, but has the potential to improve many applications.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
examples include summarization (barzilay and elhadad, 1997), <papid> W97-0703 </papid>question answering (ramakrishnan et al , 2003) <papid> W03-1201 </papid>and machine translation (chan and ng, 2007).wsd is commonly treated as supervised classification task.</citsent>
<aftsection>
<nextsent>assuming we have access to data that has been hand-labeled with correct word senses, we can train classifier to assign senses to unseen words in context.
</nextsent>
<nextsent>while this approach often achieves high accuracy, adequately large sense labeled datasets are unfortunately difficult to obtain.
</nextsent>
<nextsent>for many words, domains, languages, and sense inventories they are unavailable, and ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1333">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word sense disambiguation (wsd), the task of identifying the intended meaning (sense) of words in context, is long-standing problem in natural language processing.
</prevsent>
<prevsent>sense disambiguation is of ten characterized as an intermediate task, which is not an end in itself, but has the potential to improve many applications.
</prevsent>
</prevsection>
<citsent citstr=" W03-1201 ">
examples include summarization (barzilay and elhadad, 1997), <papid> W97-0703 </papid>question answering (ramakrishnan et al , 2003) <papid> W03-1201 </papid>and machine translation (chan and ng, 2007).wsd is commonly treated as supervised classification task.</citsent>
<aftsection>
<nextsent>assuming we have access to data that has been hand-labeled with correct word senses, we can train classifier to assign senses to unseen words in context.
</nextsent>
<nextsent>while this approach often achieves high accuracy, adequately large sense labeled datasets are unfortunately difficult to obtain.
</nextsent>
<nextsent>for many words, domains, languages, and sense inventories they are unavailable, and ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1334">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.in most cases it is unreasonable to expect to acquire them.
</prevsent>
</prevsection>
<citsent citstr=" W97-0201 ">
ng (1997) <papid> W97-0201 </papid>estimates that high accuracy domain-independent system for wsd would probably need corpus of about 3.2 million sense tagged words.</citsent>
<aftsection>
<nextsent>at throughput of one word per minute (edmonds, 2000), this would require about 27 person-years of human annotation effort.semcor (fellbaum, 1998) is one of the few corpora that have been manually annotated for all words ? it contains sense labels for 23,346 lemmas.
</nextsent>
<nextsent>inspite of being widely used, semcor contains too few tagged instances for the majority of polysemous words (typically fewer than 10 each).
</nextsent>
<nextsent>supervised methods require much larger datasets than this to perform adequately.
</nextsent>
<nextsent>the problem of obtaining sufficient labeled data, often referred to as the data acquisition bottleneck, creates significant barrier to the use of supervised wsd methods in real world applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1335">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>despite its simplicity, the algorithm provides good baseline for comparison.
</prevsent>
<prevsent>cover age can be increased by augmenting the dictionary definition (gloss) of each sense with the glosses of related words and senses (banerjee and pedersen, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
although most algorithms disambiguate word senses in context, mccarthy et al  (2004) <papid> P04-1036 </papid>propose method that does not relyon contextual cues.their algorithm capitalizes on the fact that the distribution of word senses is highly skewed.</citsent>
<aftsection>
<nextsent>a large number of frequent words is often associated with one dominant sense.
</nextsent>
<nextsent>indeed, current supervised methods rarely outperform the simple heuristic of choosing the most common sense in the training data (henceforth the first sense heuristic?), despite taking local context into account.
</nextsent>
<nextsent>rather than obtaining the first sense via annotating word senses manually, mccarthy et al  propose to acquire first senses automatically and use them for disambiguation.
</nextsent>
<nextsent>thus, by design, their algorithm assigns the same sense to all instances of polysemous word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1336">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>pseudo-labels as training instances gale et al  (1992) pioneered the use of parallel corpora as source of sense-tagged data.
</prevsent>
<prevsent>their key insight is that different translations of an ambiguous word can serve to distinguish its senses.
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
ng et al  (2003) <papid> P03-1058 </papid>extend this approach further and demonstrate that it is feasible for large scale wsd.</citsent>
<aftsection>
<nextsent>they gather examples from english-chinese parallel corpora and use automatic word alignment as means of obtaining translation dictionary.
</nextsent>
<nextsent>translations are next assigned to senses of english ambiguous words.
</nextsent>
<nextsent>english instances corresponding to these translations serve as training data.
</nextsent>
<nextsent>it has become common to use related words from dictionary to learn contextual cues for wsd (mihalcea, 2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1338">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their system then queries the web with these related words.
</prevsent>
<prevsent>the contexts surrounding the relatives of specific sense are presumed to be indicators of that sense, and used for disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
a similar idea, proposed by yarowsky (1992), <papid> C92-2070 </papid>is to use thesaurus and acquire informative contexts from words in the same category as the target.our own work uses insights gained from unsupervised methods with the aim of creating large datasets of sense-labeled instances without explicit manual coding.</citsent>
<aftsection>
<nextsent>unlike ng et al  (2003) <papid> P03-1058 </papid>our algorithm works on monolingual corpora, which are 66 much more abundant than parallel ones, and is fully automatic.</nextsent>
<nextsent>in their approach translations and their english senses must be associated manually.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1350">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, wordnet, which is our semantic resource and point of comparison, has wide coverage of nouns.
</prevsent>
<prevsent>also, for many tasks and applications (e.g., web queries) nouns are the most frequently encountered part-of-speech (jansen et al , 2000).we made use of the coarse-grained sense groupings provided for both senseval datasets.
</prevsent>
</prevsection>
<citsent citstr=" D07-1107 ">
for many applications (e.g., information retrieval) coarsely defined senses are more useful (see snow et al  (2007) <papid> D07-1107 </papid>for discussion).our training data was created from the bnc using different ways of obtaining the neighbors of the target word.</citsent>
<aftsection>
<nextsent>as described in section 3 we retrieved neighbors using lins (1998) similarity measure on rasp parsed (briscoe and carroll, 2002) version of the bnc.
</nextsent>
<nextsent>we used subject and object dependencies, as well as adjective and noun modifier dependencies.
</nextsent>
<nextsent>we also created training datasets using collocational neighbors.
</nextsent>
<nextsent>specifically, using the infomap toolkit 2 , we constructed vector-based representations for individual words from the bnc using term-document matrix and the cosine similarity measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1351">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 features.
</prevsent>
<prevsent>we used rich feature space based on lemmas,part-of-speech (pos) tags and variety of positional and syntactic relationships of the target word capturing both immediate local context and wider context.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
these feature types have been widely used in wsd algorithms (see lee and ng (2002) <papid> W02-1006 </papid>for an evaluation of their effectiveness).</citsent>
<aftsection>
<nextsent>their use is illustrated on sample english sentence for the target word sense in table 1.
</nextsent>
<nextsent>4.3 supervised classifiers.
</nextsent>
<nextsent>one of our evaluation goals was to examine the effect of our training-data creation procedure on different types of classifiers and determine which ones are most suited for use with our method.
</nextsent>
<nextsent>we therefore chose three supervised classifiers (sup port vector machines, maximum entropy, and labelpropagation) which are based on different learning paradigms and have shown competitive performance in wsd (niu et al , 2005; <papid> P05-1049 </papid>preiss and yarowsky, 2001; mihalcea and edmonds, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1352">
<title id=" C08-1009.xml">good neighbors make good senses exploiting distributional similarity for unsupervised wsd </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4.3 supervised classifiers.
</prevsent>
<prevsent>one of our evaluation goals was to examine the effect of our training-data creation procedure on different types of classifiers and determine which ones are most suited for use with our method.
</prevsent>
</prevsection>
<citsent citstr=" P05-1049 ">
we therefore chose three supervised classifiers (sup port vector machines, maximum entropy, and labelpropagation) which are based on different learning paradigms and have shown competitive performance in wsd (niu et al , 2005; <papid> P05-1049 </papid>preiss and yarowsky, 2001; mihalcea and edmonds, 2004).</citsent>
<aftsection>
<nextsent>we summarize below their main characteristics and differences.support vector machines svms model classification as the problem of finding separating hyper plane in high dimensional vector space.
</nextsent>
<nextsent>they focus on differentiating between the most problematic cases ? instances which are close to each other in the high dimensional space, but have different labels.
</nextsent>
<nextsent>they are discriminative, rather than generative, and do not explicitly model the classes.
</nextsent>
<nextsent>svms have been applied successfully inmany nlp tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1357">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>typical target relations include reaction?
</prevsent>
<prevsent>and production?
</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
(pantel and pennacchiootti, 2006), <papid> P06-1015 </papid>person-affiliation?</citsent>
<aftsection>
<nextsent>and organization-location?
</nextsent>
<nextsent>(zelenko et al, 2002), part-whole?
</nextsent>
<nextsent>(berland and charniak, 1999; <papid> P99-1008 </papid>girjuet al, 2006) and temporal precedence relations between events (chklovski and pantel, 2004; <papid> W04-3205 </papid>torisawa, 2006).<papid> N06-1008 </papid></nextsent>
<nextsent>our current task of acquiring objecttrouble?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1358">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and organization-location?
</prevsent>
<prevsent>(zelenko et al, 2002), part-whole?
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
(berland and charniak, 1999; <papid> P99-1008 </papid>girjuet al, 2006) and temporal precedence relations between events (chklovski and pantel, 2004; <papid> W04-3205 </papid>torisawa, 2006).<papid> N06-1008 </papid></citsent>
<aftsection>
<nextsent>our current task of acquiring objecttrouble?
</nextsent>
<nextsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</nextsent>
<nextsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</nextsent>
<nextsent>another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1359">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and organization-location?
</prevsent>
<prevsent>(zelenko et al, 2002), part-whole?
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
(berland and charniak, 1999; <papid> P99-1008 </papid>girjuet al, 2006) and temporal precedence relations between events (chklovski and pantel, 2004; <papid> W04-3205 </papid>torisawa, 2006).<papid> N06-1008 </papid></citsent>
<aftsection>
<nextsent>our current task of acquiring objecttrouble?
</nextsent>
<nextsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</nextsent>
<nextsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</nextsent>
<nextsent>another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1360">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>and organization-location?
</prevsent>
<prevsent>(zelenko et al, 2002), part-whole?
</prevsent>
</prevsection>
<citsent citstr=" N06-1008 ">
(berland and charniak, 1999; <papid> P99-1008 </papid>girjuet al, 2006) and temporal precedence relations between events (chklovski and pantel, 2004; <papid> W04-3205 </papid>torisawa, 2006).<papid> N06-1008 </papid></citsent>
<aftsection>
<nextsent>our current task of acquiring objecttrouble?
</nextsent>
<nextsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</nextsent>
<nextsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</nextsent>
<nextsent>another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1361">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</prevsent>
<prevsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></citsent>
<aftsection>
<nextsent>clearly troubles should be associated with negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation.
</nextsent>
<nextsent>while recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (kobayashi et al, 2007), <papid> D07-1114 </papid>these attributes usually concern intrinsic properties of the objects, such as digital cameras colors ? they do not extend to sentiment-related factors external to the object like traffic jams?</nextsent>
<nextsent>for theme parks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1362">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</prevsent>
<prevsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</prevsent>
</prevsection>
<citsent citstr=" E06-1026 ">
another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></citsent>
<aftsection>
<nextsent>clearly troubles should be associated with negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation.
</nextsent>
<nextsent>while recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (kobayashi et al, 2007), <papid> D07-1114 </papid>these attributes usually concern intrinsic properties of the objects, such as digital cameras colors ? they do not extend to sentiment-related factors external to the object like traffic jams?</nextsent>
<nextsent>for theme parks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1363">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relations is new and object-trouble relations are inherently more abstract and indirect than relations like person-affiliation?
</prevsent>
<prevsent>they crucially depend on additional knowledge about whether and how given objects use might be hampered by specific trouble.
</prevsent>
</prevsection>
<citsent citstr=" P06-2059 ">
another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></citsent>
<aftsection>
<nextsent>clearly troubles should be associated with negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation.
</nextsent>
<nextsent>while recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (kobayashi et al, 2007), <papid> D07-1114 </papid>these attributes usually concern intrinsic properties of the objects, such as digital cameras colors ? they do not extend to sentiment-related factors external to the object like traffic jams?</nextsent>
<nextsent>for theme parks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1364">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another line of research closely related to our work is the recognition of semantic orientation and sentiment analysis (turney, 2002; <papid> P02-1053 </papid>takamura et al,2006; <papid> E06-1026 </papid>kaji and kitsuregawa, 2006).<papid> P06-2059 </papid></prevsent>
<prevsent>clearly troubles should be associated with negative orientation of an expression, but studies on the acquisition of semantic orientation traditionally do not bother with the context of evaluation.</prevsent>
</prevsection>
<citsent citstr=" D07-1114 ">
while recent work on sentiment analysis has started to associate sentiment-related attribute-evaluation pairs to objects (kobayashi et al, 2007), <papid> D07-1114 </papid>these attributes usually concern intrinsic properties of the objects, such as digital cameras colors ? they do not extend to sentiment-related factors external to the object like traffic jams?</citsent>
<aftsection>
<nextsent>for theme parks.
</nextsent>
<nextsent>the acquisition method proposed in this work addresses both these matters.finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word trouble?.
</nextsent>
<nextsent>although we used the classical lexico-syntactic patterns for hyponymy acquisition (hearst, 1992; <papid> C92-2082 </papid>imasumi, 2001; ando et al, 2003) to reflect this intuition, our experiments show wewere unable to attain satisfactory performance using lexico-syntactic patterns alone.</nextsent>
<nextsent>thus, we alsouse verb-noun dependencies as evidence in learning (pantel and ravichandran, 2004; <papid> N04-1041 </papid>shinzato and torisawa, 2004).<papid> N04-1010 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1365">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for theme parks.
</prevsent>
<prevsent>the acquisition method proposed in this work addresses both these matters.finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word trouble?.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
although we used the classical lexico-syntactic patterns for hyponymy acquisition (hearst, 1992; <papid> C92-2082 </papid>imasumi, 2001; ando et al, 2003) to reflect this intuition, our experiments show wewere unable to attain satisfactory performance using lexico-syntactic patterns alone.</citsent>
<aftsection>
<nextsent>thus, we alsouse verb-noun dependencies as evidence in learning (pantel and ravichandran, 2004; <papid> N04-1041 </papid>shinzato and torisawa, 2004).<papid> N04-1010 </papid></nextsent>
<nextsent>we treat the evidences uniformly as elements in feature vector given to supervised learning method, which allowed us to extract considerably larger number of trouble expressions than could be acquired by sparse lexico syntactic patterns alone, while still keeping decent precision.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1366">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the acquisition method proposed in this work addresses both these matters.finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word trouble?.
</prevsent>
<prevsent>although we used the classical lexico-syntactic patterns for hyponymy acquisition (hearst, 1992; <papid> C92-2082 </papid>imasumi, 2001; ando et al, 2003) to reflect this intuition, our experiments show wewere unable to attain satisfactory performance using lexico-syntactic patterns alone.</prevsent>
</prevsection>
<citsent citstr=" N04-1041 ">
thus, we alsouse verb-noun dependencies as evidence in learning (pantel and ravichandran, 2004; <papid> N04-1041 </papid>shinzato and torisawa, 2004).<papid> N04-1010 </papid></citsent>
<aftsection>
<nextsent>we treat the evidences uniformly as elements in feature vector given to supervised learning method, which allowed us to extract considerably larger number of trouble expressions than could be acquired by sparse lexico syntactic patterns alone, while still keeping decent precision.
</nextsent>
<nextsent>what kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in nlp.
</nextsent>
<nextsent>in this work we show thatat least trouble expressions can successfully be acquired based on noun-verb dependency information alone.
</nextsent>
<nextsent>their acquisition in section 1 we have characterized trouble expressions as kind of trouble?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1367">
<title id=" C08-1024.xml">looking for trouble </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the acquisition method proposed in this work addresses both these matters.finally, our task of acquiring trouble expressions can be regarded as hyponymy acquisition, where target expressions are hyponyms of the word trouble?.
</prevsent>
<prevsent>although we used the classical lexico-syntactic patterns for hyponymy acquisition (hearst, 1992; <papid> C92-2082 </papid>imasumi, 2001; ando et al, 2003) to reflect this intuition, our experiments show wewere unable to attain satisfactory performance using lexico-syntactic patterns alone.</prevsent>
</prevsection>
<citsent citstr=" N04-1010 ">
thus, we alsouse verb-noun dependencies as evidence in learning (pantel and ravichandran, 2004; <papid> N04-1041 </papid>shinzato and torisawa, 2004).<papid> N04-1010 </papid></citsent>
<aftsection>
<nextsent>we treat the evidences uniformly as elements in feature vector given to supervised learning method, which allowed us to extract considerably larger number of trouble expressions than could be acquired by sparse lexico syntactic patterns alone, while still keeping decent precision.
</nextsent>
<nextsent>what kind of hyponymy relations can be acquired by noun-verb dependencies is still an open question in nlp.
</nextsent>
<nextsent>in this work we show thatat least trouble expressions can successfully be acquired based on noun-verb dependency information alone.
</nextsent>
<nextsent>their acquisition in section 1 we have characterized trouble expressions as kind of trouble?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1369">
<title id=" C04-1009.xml">type inheritance combinatory categorial grammar </title>
<section> i discuss </section>
<citcontext>
<prevsection>
<prevsent>it is worth noting that do not adopt the  -calculus semantics typical of ccg but opt instead for the minimal recur sion semantics (mrs) (copestake et al, 1999) native to the lkb.3however, the so-called spurious?
</prevsent>
<prevsent>parses are in fact motivated by into national and information structural phrases, as argued by steedman (2000), although tccg does not implement any prosody information.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
has focused on spurious ambiguity and its effects on efficiency (see karttunen 1986; see vijay-shankar and weir 1990 for proof of polynominal-time parsing algorithm and clark and curran 2004<papid> P04-1014 </papid>b for statistical models of ccg parsing), however most ofthese solutions are parser based.</citsent>
<aftsection>
<nextsent>rather than making proprietary modifications to the lkbs parser, instead adopt eisners (1996) ccg normal form to eliminate spurious ambiguity.
</nextsent>
<nextsent>eisner demonstrates that the parse forest assigned to given string can be partitioned into semantic equivalence classes such that there is only one canonical?
</nextsent>
<nextsent>(normal form) structure per equivalence class, where the normal form prefers application over and right-branching   over left-branching   (and vice versa for  b).4 these preferences are sta table as constraint son what may serve as the primary functors of different combinators.
</nextsent>
<nextsent>i implement this by assigning one of the values in (4) to the feature nf: (4) nf tr bc-tr ot bc-ot-tr fc-ot-tr fc fc-tr fc-ot bc bc-otan nf value fc marks sign as being the output of   b, bc as the output of  b, ot as lexical item or the output of application, and tr as the output of t. the sub types are disjunctive, so thatfc-ot-tr is either lexeme or the output of   b, application, or t. each combinator constrains the nf features of its output and daughters to be of specific value.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1371">
<title id=" C04-1009.xml">type inheritance combinatory categorial grammar </title>
<section> i discuss </section>
<citcontext>
<prevsection>
<prevsent>the cumulative effect results in only one canonical?
</prevsent>
<prevsent>parse for each reading of given string.
</prevsent>
</prevsection>
<citsent citstr=" P96-1011 ">
for more discussion of the efficiency of this approach see eisner (1996) <papid> P96-1011 </papid>and clark and curran (2004<papid> P04-1014 </papid>a).</citsent>
<aftsection>
<nextsent>for purposes of tccg, however, eliminating spurious ambiguity facilitates exploration of tccgs hybrid nature by making direct comparisons possible between typesof grammatical encoding in tccg and more standard hpsg/ccg approaches, which turn to next.
</nextsent>
<nextsent>4eisners constraints on   only apply to ccgs with  for &amp; (  and are thus ignored.
</nextsent>
<nextsent>i do augment eisners system by restricting to only occur when needed for b. 3 comparison of ccg and hpsgin this section briefly review some major differences between ccg and hpsg.
</nextsent>
<nextsent>both theories share roots in the same strand of lexical ist syntax,wherein grammatical information is lexically encoded and combination is category driven.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1374">
<title id=" C04-1009.xml">type inheritance combinatory categorial grammar </title>
<section> i discuss </section>
<citcontext>
<prevsection>
<prevsent>the result is richly structured set of relationships between linguistic units that reduces redundancy and can be exploited to state grammatical and typo logical generalizations.
</prevsent>
<prevsent>as noted in   1, the respective advantages of these theories are compatible, and much previous work has exploited this fact.
</prevsent>
</prevsection>
<citsent citstr=" C86-1045 ">
use of unification (a core operation in hpsg) in cg dates at least as far back as karttunen (1986), back as karttunen (1989), uszkoreit (1986),<papid> C86-1045 </papid>and zeevat (1988).</citsent>
<aftsection>
<nextsent>work on incorporating inheritance hierarchies into ccg is relatively more recent.most notably villavicencio (2001) implements hybrid ccg/hpsg grammar in the lkb for purposes of exploring principles and parameters acquisition model, defining parameters in terms of underspecified type hierarchies that the learner makes more precise during the learning process.5 moving 5note that tccg employs different type of cg than beyond acquisition, baldridge (2002) argues more generally for type-hierarchy approach to the structure of ccg lexicon so as to reduce redundancy and capture broader typo logical generalizations, although he does not explicitly flesh out this proposal.6 with tccg build directly on this previous work by applying villavicenios type inheritance techniques to the issues raised by baldridge,addressing head on the advantages of hybrid approach and comparing it to prior hpsg and ccg analyses.
</nextsent>
<nextsent>in the following sections outline several case studies of this approach.7 4 advantages of tccg over ccg.
</nextsent>
<nextsent>i turn first to the use of type hierarchies and lexical mapping rules in tccg and the elimination of redundancy this brings to ccg.
</nextsent>
<nextsent>using as my case study the hierarchy of verbal signs, in ccg the following categories are assigned to various verb types (note that in tccg cps are categorially finite nps): (6) (a) in transitive (sleep): s
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1375">
<title id=" C04-1009.xml">type inheritance combinatory categorial grammar </title>
<section> i discuss </section>
<citcontext>
<prevsection>
<prevsent>constraint on verb-lxm in (9) defines english as an sv language.
</prevsent>
<prevsent>for language like irish this type could encode general vs constraint (e.g. verb-lxm:=  np$).
</prevsent>
</prevsection>
<citsent citstr=" W02-1502 ">
thus the type hierarchy provides an explicit means for encoding broad typo logical parameters not directly sta table in ccg (see bender et al 2002 <papid> W02-1502 </papid>for further discussion and villavicencio 2001 on acquisition of word order parameters).however, even (6) is not exhaustive of all possible verbal categories, since each verb carries not just its basic?</citsent>
<aftsection>
<nextsent>category but also cluster of other categories corresponding to various lexical operations.
</nextsent>
<nextsent>for example, give is associated with several categories, including but not limited to: (10) (a) double object: ((s
</nextsent>
<nextsent>np)  np)  np (b) np-pp complement: ((s
</nextsent>
<nextsent>np)  pp fl ff )  np (c) passivized double object, no agent: (s
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1376">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" W06-0705 ">
swering (harabagiu and hickl, 2006).<papid> W06-0705 </papid></citsent>
<aftsection>
<nextsent>in recent years spectrum of approaches to robust, open domain nli have been explored within the context of the recognizing textual entailment challenge (dagan et al , 2005).
</nextsent>
<nextsent>up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of lexical or semantic overlap (jijkoun and de rijke, 2005), pattern-based relation extraction (ro mano et al , 2006), <papid> E06-1052 </papid>or approximate matching of predicate-argument structure (hickl et al , 2006).</nextsent>
<nextsent>such methods, while robust and often effective, are at best partial solutions, unable to explain even simple forms of logical inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1377">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>swering (harabagiu and hickl, 2006).<papid> W06-0705 </papid></prevsent>
<prevsent>in recent years spectrum of approaches to robust, open domain nli have been explored within the context of the recognizing textual entailment challenge (dagan et al , 2005).</prevsent>
</prevsection>
<citsent citstr=" E06-1052 ">
up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of lexical or semantic overlap (jijkoun and de rijke, 2005), pattern-based relation extraction (ro mano et al , 2006), <papid> E06-1052 </papid>or approximate matching of predicate-argument structure (hickl et al , 2006).</citsent>
<aftsection>
<nextsent>such methods, while robust and often effective, are at best partial solutions, unable to explain even simple forms of logical inference.
</nextsent>
<nextsent>for example, most shallow approaches would fail to license the introduction of large in the following example: (1) every firm saw costs grow more than expected, even after adjusting for inflation.
</nextsent>
<nextsent>every large firm saw costs grow.at the other extreme, some researchers have approached nli as logical deduction, building on work in theoretical semantics to translate sentences into first-order logic (fol), and then applying theorem prover or model builder (akhmatova,2005; fowler et al , 2005).
</nextsent>
<nextsent>regrettably, such approaches tend to founder on the myriad complexities of full semantic interpretation, including tense,aspect, causality, intensionality, modality, vagueness, idioms, indexicals, ellipsis, and many other issues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1378">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> a theory of natural logic </section>
<citcontext>
<prevsection>
<prevsent>a natural logic system can thus achieve the expressivity and precision needed to handle great variety of simple logical inferences, while sidestepping the difficulties of full semantic interpretation.
</prevsent>
<prevsent>the natural logic approach originated in traditional logic (e.g., aristotles syllogisms), and was revived informal form by van benthem (1986) and sanchez valencia (1991), who proposed natural logic based on categorial grammar to handle inferences involving containment relations and upward and downward monotonicity, such as (1).
</prevsent>
</prevsection>
<citsent citstr=" W06-3907 ">
theirmonotonicity calculus explains inferences involving even nested inversions of monotonicity, but be cause it lacks any representation of exclusion (as opposed to containment), it cannot explain simple inferences such as (38) and (205) in table 2, below.another model which arguably follows the natural logic tradition (though not presented as such)was developed by nairn et al  (2006) <papid> W06-3907 </papid>to explain inversions and nestings of implicative (and factive) predicates, as in ed did not forget to force dave toleave |= dave left.</citsent>
<aftsection>
<nextsent>their implication projection algorithm bears some resemblance to the monotonic ity calculus, but does not incorporate containment relations or explain interactions between implica tives and monotonicity, and thus fails to license john refused to dance |= john didnt tango.
</nextsent>
<nextsent>we propose new model of natural logic which generalizes the monotonicity calculus to cover inferences involving exclusion, and (partly) unifies it with nairn et al model of implicatives.
</nextsent>
<nextsent>we (1) augment the set of entailment relations used in monotonicity calculus to include representations of exclusion; (2) generalize the concept of mono toni city to one of projectivity, which describes how the ent ailments of compound expression depend on the ent ailments of its parts; and (3) describe aweak proof procedure based on composing entailment relations across chains of atomic edits.
</nextsent>
<nextsent>1natural logic should not be confused with natural deduction, proof system for first-order logic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1379">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> the natlog system.  </section>
<citcontext>
<prevsection>
<prevsent>are upward monotone (attempt to tango @ attempt to dance); signatures ?/+, ?/?, and ?/+ are downward monotone (refuse to dance @ refuse to tango); and signatures +/+, ?/?, and ?/?
</prevsent>
<prevsent>are non-monotone (think dancing is fun # think tan going is fun).
</prevsent>
</prevsection>
<citsent citstr=" W05-1201 ">
our implementation of natural logic, the natlog system, uses multi-stage architecture like those of (marsi and krahmer, 2005; <papid> W05-1201 </papid>maccartney et al ,2006), <papid> N06-1006 </papid>comprising (1) linguistic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition.</citsent>
<aftsection>
<nextsent>well use the following inference as running example: (2) jimmy dean refused to move without blue jeans.
</nextsent>
<nextsent>james dean didnt dance without pants.the example is admittedly contrived, but it compactly exhibits containment, exclusion, and im plicativity.
</nextsent>
<nextsent>how the natlog system handles this example is depicted in table 1.linguistic analysis.
</nextsent>
<nextsent>relative to other nli systems, the natlog system does comparatively little linguistic pre-processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1380">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> the natlog system.  </section>
<citcontext>
<prevsection>
<prevsent>are upward monotone (attempt to tango @ attempt to dance); signatures ?/+, ?/?, and ?/+ are downward monotone (refuse to dance @ refuse to tango); and signatures +/+, ?/?, and ?/?
</prevsent>
<prevsent>are non-monotone (think dancing is fun # think tan going is fun).
</prevsent>
</prevsection>
<citsent citstr=" N06-1006 ">
our implementation of natural logic, the natlog system, uses multi-stage architecture like those of (marsi and krahmer, 2005; <papid> W05-1201 </papid>maccartney et al ,2006), <papid> N06-1006 </papid>comprising (1) linguistic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition.</citsent>
<aftsection>
<nextsent>well use the following inference as running example: (2) jimmy dean refused to move without blue jeans.
</nextsent>
<nextsent>james dean didnt dance without pants.the example is admittedly contrived, but it compactly exhibits containment, exclusion, and im plicativity.
</nextsent>
<nextsent>how the natlog system handles this example is depicted in table 1.linguistic analysis.
</nextsent>
<nextsent>relative to other nli systems, the natlog system does comparatively little linguistic pre-processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1381">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> the natlog system.  </section>
<citcontext>
<prevsection>
<prevsent>how the natlog system handles this example is depicted in table 1.linguistic analysis.
</prevsent>
<prevsent>relative to other nli systems, the natlog system does comparatively little linguistic pre-processing.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we relyon the stanford parser (klein and manning, 2003), <papid> P03-1054 </papid>penntreebank-trained statistical parser, for tokenization, lemmatization, part-of-speech tagging, and phrase-structure parsing.</citsent>
<aftsection>
<nextsent>by far the most important analysis performed at this stage, however, is projectivity marking, in which we compute the effective projectivity for each token span in each input sentence.
</nextsent>
<nextsent>in the premise of (2), for example, we want to determine that the effective projectivity is upward monotone 3 fact ives, however, do not fit as neatly as implicatives: for example, deleting signature +/+ generates @ (jim forgot that dancing is fun @ dancing is fun); yet under negation, thisis projected not as a, but as | (jim didnt forget that dancing is fun | dancing isnt fun).
</nextsent>
<nextsent>the problem arises because the implication carried by factive is not an entailment, but presupposition.
</nextsent>
<nextsent>as is well known, the projection behavior of pre suppositions differs from that of ent ailments (van der sandt, 1992).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1382">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> the natlog system.  </section>
<citcontext>
<prevsection>
<prevsent>composition tends to degenerate?
</prevsent>
<prevsent>towards #, in the sense that the composition of chain of randomly-selected relations tends toward # asthe chain grows longer.
</prevsent>
</prevsection>
<citsent citstr=" W07-1423 ">
this chaining of entail ments across edits can be compared to the method presented in (harmeling, 2007); <papid> W07-1423 </papid>however, that approach assigns to each edit merely probability of preserving truth, not an entailment relation.</citsent>
<aftsection>
<nextsent>the last line of table 1 shows the cumulative composition of the atomic entailment relations in the line above.
</nextsent>
<nextsent>particular noteworthy is the fact that | and compose to yield @.
</nextsent>
<nextsent>(to illustrate: if excludes (fish | human) and is the negation of (human ? nonhuman), then entails (fish @ nonhuman).)
</nextsent>
<nextsent>the final entailment relation in this line, @, is natlogs final (and correct) answer for our example problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1383">
<title id=" C08-1066.xml">modeling semantic containment and exclusion in natural language inference </title>
<section> evaluating on fracas problems.  </section>
<citcontext>
<prevsection>
<prevsent>each problem consists of one or more premise sentences, question sentence, and one of three answers: yes (the union of @ and =), no (the union of | and )?, or unknown (the union of a, `, and #).
</prevsent>
<prevsent>table 2 shows some example problems.
</prevsent>
</prevsection>
<citsent citstr=" W07-1431 ">
to facilitate comparison with previous work, we have evaluated our system using version of the fracas data prepared by (maccartney and manning, 2007), <papid> W07-1431 </papid>in which multiple-premise problems(44% of the total) and problems lacking hypothesis or well-defined answer (3% of the total) are excluded; question sentences have been converted 525 ? id premise hypothesis ans 1 38 no delegate finished the report.</citsent>
<aftsection>
<nextsent>some delegate finished the report on time.
</nextsent>
<nextsent>no 1 48 at most ten commissioners spend time at home.
</nextsent>
<nextsent>at most ten c...s spend lot of time at home.
</nextsent>
<nextsent>yes 2 83 either smith, jones or anderson signed the contract.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1385">
<title id=" C02-1122.xml">fertilization of case frame dictionary for robust japanese case analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it describes what kinds of cases each verb has and what kinds of nouns can fill case slot.
</prevsent>
<prevsent>since these relations have millions of combinations,it is difficult to construct case frame dictionary by hand.
</prevsent>
</prevsection>
<citsent citstr=" H01-1043 ">
we proposed method to construct japanese case frame dictionary automatically by arranging large volumes of parse results by coupling verb and its closest case component (kawahara and kurohashi, 2001).<papid> H01-1043 </papid></citsent>
<aftsection>
<nextsent>this case frame dictionary, however, could not handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change.this paper proposes method of fertilizing the case frame dictionary to handle these complicated expressions.
</nextsent>
<nextsent>we take an iterative method which consists of two stages.
</nextsent>
<nextsent>this means gradual learning of what is understood by an analyzer in each stage.
</nextsent>
<nextsent>in the first stage, we parse large raw corpus and construct japanese case frame dictionary automatically from the parse results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1394">
<title id=" C02-1003.xml">learning chinese bracketing knowledge based on a bilingual language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the past few years have seen great success in automatic acquisition of monolingual parsing knowledge and grammars.
</prevsent>
<prevsent>the availability of large tagged and syntactically bracketed corpora, such as penn tree bank, makes it possible to extract syntactic structure and grammar rules automatically (marcus 1993).
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
substantial improvements have been made to parse western language such as english, and many powerful models have been proposed (brill 1993, collins 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>however, very limited progress has been achieved in chinese.
</nextsent>
<nextsent>knowledge acquisition is bottleneck for real appication of chinese parsing.
</nextsent>
<nextsent>while some methods have been proposed to learn syntactic knowledge from annotated chinese corpus, most of the methods depended on the annotated or partial annotated data(zhou 1997, streiter 2000).
</nextsent>
<nextsent>due to the limited avail bility of chinese annotated corpus, tests of these methods are still small in scale.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1395">
<title id=" C02-1003.xml">learning chinese bracketing knowledge based on a bilingual language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation and discussion are given in section 4.
</prevsent>
<prevsent>we conclude with discussion of future work.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
1 bilingual language model ? itg wu (1997) <papid> J97-3002 </papid>has proposed bilingual language model called inversion transduction grammar (itg), which can be used to parse bilingual sentence pairs simultaneously.</citsent>
<aftsection>
<nextsent>we will give brief description here.
</nextsent>
<nextsent>for details please refer to (wu 1995, <papid> P95-1033 </papid>wu 1997).<papid> J97-3002 </papid></nextsent>
<nextsent>the inversion transduction grammar is bilingual context-free grammar that generates two matched output languages (referred to as l1 and l2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1396">
<title id=" C02-1003.xml">learning chinese bracketing knowledge based on a bilingual language model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 bilingual language model ? itg wu (1997) <papid> J97-3002 </papid>has proposed bilingual language model called inversion transduction grammar (itg), which can be used to parse bilingual sentence pairs simultaneously.</prevsent>
<prevsent>we will give brief description here.</prevsent>
</prevsection>
<citsent citstr=" P95-1033 ">
for details please refer to (wu 1995, <papid> P95-1033 </papid>wu 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>the inversion transduction grammar is bilingual context-free grammar that generates two matched output languages (referred to as l1 and l2).
</nextsent>
<nextsent>it also differs from standard context-free grammars in that the itg allows right-hand side production in two directions: straight or inverted.
</nextsent>
<nextsent>the following examples are two itg productions: -  [a b] -   b  each nonterminal symbol stands for pair of matched strings.
</nextsent>
<nextsent>for example, the nonterminal stands for the string-pair (a1, a2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1411">
<title id=" C02-1003.xml">learning chinese bracketing knowledge based on a bilingual language model </title>
<section> english parsing supervised bilingual.  </section>
<citcontext>
<prevsection>
<prevsent>the remaining productions are all lexical.
</prevsent>
<prevsent>bij is the translation probability that source word ui translates into target word vj.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
bij can be obtained using statistical word-translation model (melamed 2000) <papid> J00-2004 </papid>or word alignment(l? 2001a).</citsent>
<aftsection>
<nextsent>the last two productions denote that the word in one language has no counterpart on other side of the bitext.
</nextsent>
<nextsent>a small constant can be chosen for the probabilities bie and bej.
</nextsent>
<nextsent>in btg, no language specific syntactic grammar is used.
</nextsent>
<nextsent>the maximum-likelihood parser selects the parse tree that best satisfies the combined lexical translation preferences, as expressed by the bij probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1423">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.
</prevsent>
<prevsent>the framework is flexible, allowing fast adaptation to applications and it is scalable.we apply it in combination with tera byte corpus to answer natural language tests, achieving encouraging results.
</prevsent>
</prevsection>
<citsent citstr=" C02-1033 ">
modeling term co-occurrence is important for many natural language applications, such as topic segmentation (ferret, 2002), <papid> C02-1033 </papid>query expansion (vech tomova et al, 2003), machine translation (tanaka, 2002), <papid> C02-1065 </papid>language modeling (dagan et al, 1999; yuret, 1998), and term weighting (hisamitsu andniwa, 2002).<papid> C02-1125 </papid></citsent>
<aftsection>
<nextsent>for these applications, we are interested in terms that co-occur in close proximity more often than expected by chance, for example,  new?,york?
</nextsent>
<nextsent> ,  accurate?,exact?
</nextsent>
<nextsent> and  gasoline?,crude?
</nextsent>
<nextsent> . these pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for eachother.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1424">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.
</prevsent>
<prevsent>the framework is flexible, allowing fast adaptation to applications and it is scalable.we apply it in combination with tera byte corpus to answer natural language tests, achieving encouraging results.
</prevsent>
</prevsection>
<citsent citstr=" C02-1065 ">
modeling term co-occurrence is important for many natural language applications, such as topic segmentation (ferret, 2002), <papid> C02-1033 </papid>query expansion (vech tomova et al, 2003), machine translation (tanaka, 2002), <papid> C02-1065 </papid>language modeling (dagan et al, 1999; yuret, 1998), and term weighting (hisamitsu andniwa, 2002).<papid> C02-1125 </papid></citsent>
<aftsection>
<nextsent>for these applications, we are interested in terms that co-occur in close proximity more often than expected by chance, for example,  new?,york?
</nextsent>
<nextsent> ,  accurate?,exact?
</nextsent>
<nextsent> and  gasoline?,crude?
</nextsent>
<nextsent> . these pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for eachother.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1425">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in comparison with previous models, which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models, in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus.
</prevsent>
<prevsent>the framework is flexible, allowing fast adaptation to applications and it is scalable.we apply it in combination with tera byte corpus to answer natural language tests, achieving encouraging results.
</prevsent>
</prevsection>
<citsent citstr=" C02-1125 ">
modeling term co-occurrence is important for many natural language applications, such as topic segmentation (ferret, 2002), <papid> C02-1033 </papid>query expansion (vech tomova et al, 2003), machine translation (tanaka, 2002), <papid> C02-1065 </papid>language modeling (dagan et al, 1999; yuret, 1998), and term weighting (hisamitsu andniwa, 2002).<papid> C02-1125 </papid></citsent>
<aftsection>
<nextsent>for these applications, we are interested in terms that co-occur in close proximity more often than expected by chance, for example,  new?,york?
</nextsent>
<nextsent> ,  accurate?,exact?
</nextsent>
<nextsent> and  gasoline?,crude?
</nextsent>
<nextsent> . these pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for eachother.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1426">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent> and  gasoline?,crude?
</prevsent>
<prevsent> . these pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for eachother.
</prevsent>
</prevsection>
<citsent citstr=" N03-1032 ">
examples of such affinities include synonyms (terra and clarke, 2003), <papid> N03-1032 </papid>verb similarities (resnik and diab, 2000) and word associations (rapp, 2002).<papid> C02-1007 </papid>ideally, language model would capture the patterns of co-occurrences representing the affinity between terms.</citsent>
<aftsection>
<nextsent>unfortunately, statistical models used to capture language characteristics often do not take contextual information into account.
</nextsent>
<nextsent>many models incorporating contextual information use only select group of content words and the end product is model for sequences of adjacent words (rosenfeld,1996; beeferman et al, 1997; <papid> P97-1048 </papid>niesler and wood land, 1997).practical problems exist when modeling text statistically, since we require reasonably sized corpus in order to overcome sparseness problems, butat the same time we face the difficulty of scaling our algorithms to larger corpora (rosenfeld, 2000).</nextsent>
<nextsent>attempts to scale language models to large corpora, in particular to the web, have often used general-purpose search engines to generate term statistics (berger and miller, 1998; zhu and rosenfeld, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1427">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent> and  gasoline?,crude?
</prevsent>
<prevsent> . these pairs of terms represent distinct lexical-semantic phenomena, and as consequence the terms have an affinity for eachother.
</prevsent>
</prevsection>
<citsent citstr=" C02-1007 ">
examples of such affinities include synonyms (terra and clarke, 2003), <papid> N03-1032 </papid>verb similarities (resnik and diab, 2000) and word associations (rapp, 2002).<papid> C02-1007 </papid>ideally, language model would capture the patterns of co-occurrences representing the affinity between terms.</citsent>
<aftsection>
<nextsent>unfortunately, statistical models used to capture language characteristics often do not take contextual information into account.
</nextsent>
<nextsent>many models incorporating contextual information use only select group of content words and the end product is model for sequences of adjacent words (rosenfeld,1996; beeferman et al, 1997; <papid> P97-1048 </papid>niesler and wood land, 1997).practical problems exist when modeling text statistically, since we require reasonably sized corpus in order to overcome sparseness problems, butat the same time we face the difficulty of scaling our algorithms to larger corpora (rosenfeld, 2000).</nextsent>
<nextsent>attempts to scale language models to large corpora, in particular to the web, have often used general-purpose search engines to generate term statistics (berger and miller, 1998; zhu and rosenfeld, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1428">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples of such affinities include synonyms (terra and clarke, 2003), <papid> N03-1032 </papid>verb similarities (resnik and diab, 2000) and word associations (rapp, 2002).<papid> C02-1007 </papid>ideally, language model would capture the patterns of co-occurrences representing the affinity between terms.</prevsent>
<prevsent>unfortunately, statistical models used to capture language characteristics often do not take contextual information into account.</prevsent>
</prevsection>
<citsent citstr=" P97-1048 ">
many models incorporating contextual information use only select group of content words and the end product is model for sequences of adjacent words (rosenfeld,1996; beeferman et al, 1997; <papid> P97-1048 </papid>niesler and wood land, 1997).practical problems exist when modeling text statistically, since we require reasonably sized corpus in order to overcome sparseness problems, butat the same time we face the difficulty of scaling our algorithms to larger corpora (rosenfeld, 2000).</citsent>
<aftsection>
<nextsent>attempts to scale language models to large corpora, in particular to the web, have often used general-purpose search engines to generate term statistics (berger and miller, 1998; zhu and rosenfeld, 2001).
</nextsent>
<nextsent>however, many researchers are recognizing the limitations of relying on the statistics provided by commercial search engines (zhu and rosenfeld, 2001; keller and lapata, 2003).<papid> J03-3005 </papid></nextsent>
<nextsent>acl 2004 features workshop devoted to the problem of scaling human language technologies to tera byte scale corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1431">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many models incorporating contextual information use only select group of content words and the end product is model for sequences of adjacent words (rosenfeld,1996; beeferman et al, 1997; <papid> P97-1048 </papid>niesler and wood land, 1997).practical problems exist when modeling text statistically, since we require reasonably sized corpus in order to overcome sparseness problems, butat the same time we face the difficulty of scaling our algorithms to larger corpora (rosenfeld, 2000).</prevsent>
<prevsent>attempts to scale language models to large corpora, in particular to the web, have often used general-purpose search engines to generate term statistics (berger and miller, 1998; zhu and rosenfeld, 2001).</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
however, many researchers are recognizing the limitations of relying on the statistics provided by commercial search engines (zhu and rosenfeld, 2001; keller and lapata, 2003).<papid> J03-3005 </papid></citsent>
<aftsection>
<nextsent>acl 2004 features workshop devoted to the problem of scaling human language technologies to tera byte scale corpora.
</nextsent>
<nextsent>another approach to capturing lexical affinity is through the use of similarity measures (lee, 2001;terra and clarke, 2003).<papid> N03-1032 </papid></nextsent>
<nextsent>turney (2001) used statistics supplied by the alta vista search engine to compute word similarity measures, solving set of synonym questions taken from series of practice exams for toefl (test of english as foreign lan guage).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1435">
<title id=" C04-1147.xml">fast computation of lexical affinity models </title>
<section> models for word co-occurrence.  </section>
<citcontext>
<prevsection>
<prevsent>any pair of word represents parameter in distance models.
</prevsent>
<prevsent>therefore, these models have to deal with combinatorial explosion problems, especially when longer sequences are considered.
</prevsent>
</prevsection>
<citsent citstr=" W03-1011 ">
functional models use the underlying syntactic function of words to measure co-occurrence frequency (weeds and weir, 2003; <papid> W03-1011 </papid>niesler and woodland, 1997; grefenstette, 1993).</citsent>
<aftsection>
<nextsent>the need for parsing affects the scala bility of these models.
</nextsent>
<nextsent>note that both distance and functional models rely only on pairs of terms comprised of single word.
</nextsent>
<nextsent>consider the pair of terms new york?
</nextsent>
<nextsent>and terrorism?, or any pair where one of the two items is itself collocation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1444">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>highly ambiguous words pose continuing problems for nlp applications.they can lead to irrelevant document retrieval in information retrieval systems, and inaccurate translations in machine translation systems (palmer et al, 2000).
</prevsent>
<prevsent>for example, the chinese word (jian4) hasmany different senses, one of which can be translated into english as see?, and another as show?.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
correctly sense-tagging the chinese word in context can prove to be highly beneficial for lexical choice in chinese-english machine translation.several efforts have been made to develop automatic wsd systems that can provide accurate sense tagging (ide and veronis, 1998), <papid> J98-1001 </papid>with current emphasis on creating manually sense-taggeddata for supervised training of statistical wsd systems, as evidenced by senseval-1 (kilgarriff and palmer, 2000) and senseval-2 (edmonds and cotton, 2001).</citsent>
<aftsection>
<nextsent>highly polysemous verbs, which have several distinct but related senses, pose the greatest challenge for these systems (palmer et al,2001).
</nextsent>
<nextsent>predicate-argument information and selectional restrictions are hypothesized to be particularly useful for disambiguating verb senses.
</nextsent>
<nextsent>maximum entropy models can be used to solve any classification task and have been applied to awide range of nlp tasks, including sentence boundary detection, part-of-speech tagging, and parsing (ratnaparkhi, 1998).
</nextsent>
<nextsent>assigning sense tags to wordsin context can be viewed as classification task similar to part-of-speech tagging, except that separate set of tags is required for each vocabulary item to besense-tagged.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1445">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>maximum entropy models can be used to solve any classification task and have been applied to awide range of nlp tasks, including sentence boundary detection, part-of-speech tagging, and parsing (ratnaparkhi, 1998).
</prevsent>
<prevsent>assigning sense tags to wordsin context can be viewed as classification task similar to part-of-speech tagging, except that separate set of tags is required for each vocabulary item to besense-tagged.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
under the maximum entropy framework (berger et al, 1996), <papid> J96-1002 </papid>evidence from different features can be combined with no assumptions of feature independence.</citsent>
<aftsection>
<nextsent>the automatic tagger estimates the conditional probability that word has sense  given that it occurs in context  , where is conjunction of features.
</nextsent>
<nextsent>the estimated probability is derived from feature weights which are determined automatically from training data so asto produce probability distribution that has maximum entropy, under the constraint that it is consistent with observed evidence.
</nextsent>
<nextsent>with existing tools for learning maximum entropy models, the bulk of our work is in defining the types of features to look for in the data.
</nextsent>
<nextsent>our goal is to see if sense-tagging of verbs can be improved by combining linguistic features that capture information about predicate arguments and selectional restrictions.in this paper we report on our experiments on automatic wsd using maximum entropy approach for both english and chinese verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1446">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal is to see if sense-tagging of verbs can be improved by combining linguistic features that capture information about predicate arguments and selectional restrictions.in this paper we report on our experiments on automatic wsd using maximum entropy approach for both english and chinese verbs.
</prevsent>
<prevsent>we compare the difficulty of the sense-tagging tasks in the two languages and investigate the types of contextual features that are useful for each language.
</prevsent>
</prevsection>
<citsent citstr=" W02-0813 ">
we find that while richer linguistic features are useful for english wsd, they do not prove to be as beneficial for chinese.the maximum entropy system performed competitively with the best systems on the english verbs in senseval-1 and senseval-2 (dang and palmer, 2002).<papid> W02-0813 </papid></citsent>
<aftsection>
<nextsent>however, while senseval-2 made it possible to compare many different approaches over many different languages, data for the chinese lexical sample task was not made available in time for any systems to compete.
</nextsent>
<nextsent>instead, we report on two experiments that we ran using our own lexicon and two separate chinese corpora that are very similar in style (news articles from the peoples republic of china), but have different types and levels of annotation ? the penn chinese treebank (ctb)(xia et al, 2000), and the peoples daily news (pdn)corpus from beijing university.
</nextsent>
<nextsent>we discuss the utility of different types of annotation for successful automatic word sense disambiguation.
</nextsent>
<nextsent>our maximum entropy wsd system was designed to combine information from many different sources, using as much linguistic knowledge as could be gathered automatically by current nlptools.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1447">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> english experiment.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss the utility of different types of annotation for successful automatic word sense disambiguation.
</prevsent>
<prevsent>our maximum entropy wsd system was designed to combine information from many different sources, using as much linguistic knowledge as could be gathered automatically by current nlptools.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
in order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using maximum entropy tagger (ratnaparkhi, 1998) and parsed using the collins parser (collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>in addition, an automatic named entity tagger (bikel et al, 1997) <papid> A97-1029 </papid>was run on the sentences to map proper nouns to small set of semantic classes.</nextsent>
<nextsent>chodorow, leacock and miller (chodorow et al, 2000) found that different combinations of topic aland local features were most effective for disam biguating different words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1448">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> english experiment.  </section>
<citcontext>
<prevsection>
<prevsent>our maximum entropy wsd system was designed to combine information from many different sources, using as much linguistic knowledge as could be gathered automatically by current nlptools.
</prevsent>
<prevsent>in order to extract the linguistic features necessary for the model, all sentences were first automatically part-of-speech-tagged using maximum entropy tagger (ratnaparkhi, 1998) and parsed using the collins parser (collins, 1997).<papid> P97-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
in addition, an automatic named entity tagger (bikel et al, 1997) <papid> A97-1029 </papid>was run on the sentences to map proper nouns to small set of semantic classes.</citsent>
<aftsection>
<nextsent>chodorow, leacock and miller (chodorow et al, 2000) found that different combinations of topic aland local features were most effective for disam biguating different words.
</nextsent>
<nextsent>following their work, we divided the possible model features into topical features and several types of local contextual features.topical features looked for the presence of keywords occurring anywhere in the sentence and any surrounding sentences provided as context (usually one or two sentences).
</nextsent>
<nextsent>the set of 200-300 keywords is specific to each lemma to be disambiguated, and is determined automatically from training data so as to minimize the entropy of the probability of the senses conditioned on the keyword.the local features for verb  in particular sentence tend to look only within the smallest clause containing  . they include collocational features requiring no linguistic preprocessing beyond partof-speech tagging (1), syntactic features that capture relations between the verb and its complements(2-4), and semantic features that incorporate information about noun classes for subjects and objects (5-6): 1.
</nextsent>
<nextsent>the word  , the part of speech of  , the partof speech of words at positions -1 and +1 relative to  , and words at positions -2, -1, +1, +2, relative to  2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1449">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> chinese experiments.  </section>
<citcontext>
<prevsection>
<prevsent>about 200 sentences for each word were selected randomly from pdn and sense tagged as with the ctb.
</prevsent>
<prevsent>we automatically annotated the pdn data to yield the same types of annotation that had been available in the ctb.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
we used maximum matching algorithm and dictionary compiled fromthe ctb (sproat et al, 1996; <papid> J96-3004 </papid>xue, 2001) to do segmentation, and trained maximum entropy part-of speech tagger (ratnaparkhi, 1998) and tag-based parser (bikel and chiang, 2000) <papid> W00-1201 </papid>on the ctb to do tagging and parsing.4 then the same feature extraction and model-training was done for the pdn corpus as for the ctb.</citsent>
<aftsection>
<nextsent>the system performance is much lower for the pdn than for the ctb, for several reasons.
</nextsent>
<nextsent>first, the pdn corpus is more balanced than the ctb, which contains primarily financial articles.
</nextsent>
<nextsent>a wider range of usages of the words was expressed in pdn than in ctb, making the disambiguation task more difficult; the average number of senses for the pdn words was 8.2 (compared to 3.5 for ctb), and the4on held-out portions of the ctb, the accuracy of the segmentation and part-of-speech tagging are over 95%, and the accuracy of the parsing is 82%, which are comparable to the performance of the english preprocessors.
</nextsent>
<nextsent>the performance of these pre processors is naturally expected to degrade when transferred to different domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1450">
<title id=" C02-1143.xml">simple features for chinese word sense disambiguation </title>
<section> chinese experiments.  </section>
<citcontext>
<prevsection>
<prevsent>about 200 sentences for each word were selected randomly from pdn and sense tagged as with the ctb.
</prevsent>
<prevsent>we automatically annotated the pdn data to yield the same types of annotation that had been available in the ctb.
</prevsent>
</prevsection>
<citsent citstr=" W00-1201 ">
we used maximum matching algorithm and dictionary compiled fromthe ctb (sproat et al, 1996; <papid> J96-3004 </papid>xue, 2001) to do segmentation, and trained maximum entropy part-of speech tagger (ratnaparkhi, 1998) and tag-based parser (bikel and chiang, 2000) <papid> W00-1201 </papid>on the ctb to do tagging and parsing.4 then the same feature extraction and model-training was done for the pdn corpus as for the ctb.</citsent>
<aftsection>
<nextsent>the system performance is much lower for the pdn than for the ctb, for several reasons.
</nextsent>
<nextsent>first, the pdn corpus is more balanced than the ctb, which contains primarily financial articles.
</nextsent>
<nextsent>a wider range of usages of the words was expressed in pdn than in ctb, making the disambiguation task more difficult; the average number of senses for the pdn words was 8.2 (compared to 3.5 for ctb), and the4on held-out portions of the ctb, the accuracy of the segmentation and part-of-speech tagging are over 95%, and the accuracy of the parsing is 82%, which are comparable to the performance of the english preprocessors.
</nextsent>
<nextsent>the performance of these pre processors is naturally expected to degrade when transferred to different domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1451">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.this paper presents novel terminology extraction method applied to the french-english part of the database.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
there is long tradition of research into bilingual terminology extraction (kupiec, 1993), (<papid> P93-1003 </papid>gaussier, 1998).<papid> P98-1074 </papid></citsent>
<aftsection>
<nextsent>in most systems, candidate terms are first identified in the source language based on predefined pos patterns ? for french, n, prep n, and adj are typical patterns.
</nextsent>
<nextsent>in second step, the translation candidates are extracted from the bilingual corpus based on word alignments.
</nextsent>
<nextsent>in recent work, itagaki et al (2007) use the phrase table derived from the giza++ alignments to identify the translations.
</nextsent>
<nextsent>we use different and more flexible approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1452">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.this paper presents novel terminology extraction method applied to the french-english part of the database.
</prevsent>
</prevsection>
<citsent citstr=" P98-1074 ">
there is long tradition of research into bilingual terminology extraction (kupiec, 1993), (<papid> P93-1003 </papid>gaussier, 1998).<papid> P98-1074 </papid></citsent>
<aftsection>
<nextsent>in most systems, candidate terms are first identified in the source language based on predefined pos patterns ? for french, n, prep n, and adj are typical patterns.
</nextsent>
<nextsent>in second step, the translation candidates are extracted from the bilingual corpus based on word alignments.
</nextsent>
<nextsent>in recent work, itagaki et al (2007) use the phrase table derived from the giza++ alignments to identify the translations.
</nextsent>
<nextsent>we use different and more flexible approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1453">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> sub-sentential alignment.  </section>
<citcontext>
<prevsection>
<prevsent>each test corpus contains approximately 4,500 words.we also compiled development corpus containing sentences of varying sentence length to debug the system and to determine the value of the thresholds used in the system.
</prevsent>
<prevsent>the formal characteristics of the test corpora and the training corpus are given in table 2.
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
sub-sentential alignments ? and the underlying word alignments ? are used in the context of machine translation to create phrase tables forphrase-based statistical machine translation systems (koehn et al, 2007)<papid> P07-2045 </papid></citsent>
<aftsection>
<nextsent>a stand-alone sub sentential alignment module however, is also useful for human translators if incorporated in cat tools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (gotti et al, 2005).
</nextsent>
<nextsent>a quite obvious application of sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (melamed, 2000), (<papid> J00-2004 </papid>itagaki et al, 2007).</nextsent>
<nextsent>in the context of statistical machine translation, giza++ is one of the most widely used word alignment toolkits.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1455">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> sub-sentential alignment.  </section>
<citcontext>
<prevsection>
<prevsent>sub-sentential alignments ? and the underlying word alignments ? are used in the context of machine translation to create phrase tables forphrase-based statistical machine translation systems (koehn et al, 2007)<papid> P07-2045 </papid></prevsent>
<prevsent>a stand-alone sub sentential alignment module however, is also useful for human translators if incorporated in cat tools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (gotti et al, 2005).</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
a quite obvious application of sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (melamed, 2000), (<papid> J00-2004 </papid>itagaki et al, 2007).</citsent>
<aftsection>
<nextsent>in the context of statistical machine translation, giza++ is one of the most widely used word alignment toolkits.
</nextsent>
<nextsent>giza++ implements the ibm models and is used in moses (koehn et al, 2007)<papid> P07-2045 </papid>to generate the initial source-to-target and targetto-source word alignments after which some sym metrization heuristics combine the alignments of both translation directions.</nextsent>
<nextsent>we present an alternative ? linguistically-based ? approach, that starts from lexical probabilistic bilingual dictionary generated by ibm model one.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1457">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> sub-sentential alignment.  </section>
<citcontext>
<prevsection>
<prevsent>4the english pos tagger often tags past participle erroneously as past tense.
</prevsent>
<prevsent>532 we adapted the annotation guidelines of macken (2007) to the french-english language pair, and used three different types of links: regular links for straightforward correspondences, fuzzy links for translation-specific shifts of various kinds, and null links for words for which no correspondence could be indicated.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
figure 2 shows an example.figure 2: manual reference: regular links are indicated by xs, fuzzy links and null links by 0s to evaluate the systems performance, we used the evaluation methodology of och and ney (2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>och and ney distinguished sure alignments (s)and possible alignments (p) and introduced the following redefined precision and recall measures: precision = |a ? | |a| , recall = |a ? s| |s| (1) and the alignment error rate (aer): aer(s, ;a) = 1 ? |a ? | + |a ? s| |a| + |s| (2)we consider all regular links of the manual reference as sure alignments and all fuzzy and null links as possible alignments to compare the output of our system with the manual reference.
</nextsent>
<nextsent>we trained statistical translation models using moses.
</nextsent>
<nextsent>moses uses the giza++ toolkit (ibm model 1-4) in both translation directions (source to target, target to source) and allows for differentsymmetrization heuristics to combine the alignments of both translation directions.
</nextsent>
<nextsent>we used three different heuristics: grow-diag-final (default), intersection and union.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1458">
<title id=" C08-1067.xml">linguistically based sub sentential alignment for terminology extraction from a bilingual automotive corpus </title>
<section> terminology extraction.  </section>
<citcontext>
<prevsection>
<prevsent>we have created frequency list for both corpora and calculated the log-likelihood values for each word inthis frequency list.
</prevsent>
<prevsent>in the formula below, corresponds to the number of words in the corpus, whereas the observed values correspond to thereal frequencies of word in the corpus.
</prevsent>
</prevsection>
<citsent citstr=" W00-0901 ">
the formula for calculating both the expected values (e)and the log-likelihood have been described in detail by (rayson and garside, 2000).<papid> W00-0901 </papid></citsent>
<aftsection>
<nextsent>e = i ? o ? n (3)we used the resulting expected values for calculating the log-likelihood: 2ln?
</nextsent>
<nextsent>= 2 ? o ln( i i ) (4)manual inspection of the log-likelihood figures confirmed our hypothesis that more domain specific terms in our corpus got high ll-values.as we are mainly interested in finding distinctive terms in the automotive corpus, we have only kept those terms showing positive expected values in our domain-specific corpus combined with user-defined log-likelihood values.
</nextsent>
<nextsent>examples of french-english translation pairs that are filtered out using the ll values are: fr: tout ? en: entire fr: propre ? en: clean fr: interdits ? en: prohibited fr: nombre ? en: number 4.2.2 mutual expectation measure dias and kaalep (2003) have developed the mutual expectation measure for evaluating the degree of cohesiveness between words in text.
</nextsent>
<nextsent>we have applied this metric on our list of multiword terms, to exclude multiword terms which components do not occur together more often than expected bychance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1459">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper deals with the problem of how to disambiguate the readings of sentences, analyzed by given unification-based grammar (ubg).
</prevsent>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</citsent>
<aftsection>
<nextsent>(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</nextsent>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1460">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper deals with the problem of how to disambiguate the readings of sentences, analyzed by given unification-based grammar (ubg).
</prevsent>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
</prevsection>
<citsent citstr=" E95-1012 ">
all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</citsent>
<aftsection>
<nextsent>(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</nextsent>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1461">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper deals with the problem of how to disambiguate the readings of sentences, analyzed by given unification-based grammar (ubg).
</prevsent>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</citsent>
<aftsection>
<nextsent>(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</nextsent>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1463">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper deals with the problem of how to disambiguate the readings of sentences, analyzed by given unification-based grammar (ubg).
</prevsent>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
</prevsection>
<citsent citstr=" P98-1022 ">
all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</citsent>
<aftsection>
<nextsent>(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</nextsent>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1464">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper deals with the problem of how to disambiguate the readings of sentences, analyzed by given unification-based grammar (ubg).
</prevsent>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</citsent>
<aftsection>
<nextsent>(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</nextsent>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1465">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>apparently, there are many different approaches for almost as many different unification-based grammar formalisms on the market that tackle this difficult problem.
</prevsent>
<prevsent>all approaches have in common that they try to model probability distribution over the readings of the ubg, which can be used to rank the competing analyses of given sentence; see, e.g., briscoe and carroll (1993), <papid> J93-1002 </papid>eisele (1994), brew (1995), <papid> E95-1012 </papid>abney (1997), <papid> J97-4005 </papid>goodman (1997), bod and kaplan (1998), <papid> P98-1022 </papid>johnson et al (1999), <papid> P99-1069 </papid>riezler et al.</prevsent>
</prevsection>
<citsent citstr=" C00-1085 ">
(2000), osborne (2000), <papid> C00-1085 </papid>bouma et al (2001), or schmid (2002).</citsent>
<aftsection>
<nextsent>unfortunately, most of the proposed probability models are not mathematically clean in that the probabilities of all possible ubg readings do not sum to the value 1, problem which is discussed intensively by eisele (1994), abney (1997), <papid> J97-4005 </papid>and schmid (2002).</nextsent>
<nextsent>in addition, many of the newer approaches use log-linear (or exponential) models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1468">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> context-free approximation.  </section>
<citcontext>
<prevsection>
<prevsent>a c s figure 1: the readings of sentence, analyzed by ubg (top) and its cfg approximation (bottom).
</prevsent>
<prevsent>the picture illustrates that (i) each ubg reading of the sentence is associated with non-empty set of syntax trees according to the cfg approximation, and (ii) that the sentence may have cfg trees, which can not be replayed by the ubg, since the cfg over generates (or at best is correct approximation of the ubg).
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
1994) or patr-ii (shieber, 1985) <papid> P85-1018 </papid>into context-free grammars (cfg).</citsent>
<aftsection>
<nextsent>the method was introduced by kiefer and krieger (2000).
</nextsent>
<nextsent>the approximation method can be seen as the construction of the least fixpoint of certain monotonic function and shares similarities with the in stantiation of rules in bottom-up passive chart parser or with partial evaluation in logic programming.
</nextsent>
<nextsent>the basic idea of the approach is as follows.
</nextsent>
<nextsent>in first step, one generalizes the set of all lexicon entries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1470">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> context-free approximation.  </section>
<citcontext>
<prevsection>
<prevsent>we note here that adding (and perhaps removing) fss during the iteration can be achieved in different ways: either by employing feature structure equivalence  (structural equiva lence) or by using fs subsumption  . it is clear thatthe resulting cfgs will behave differently (see figure 4).
</prevsent>
<prevsent>an in-depth description of the method, containing lots of details, plus mathematical underpinning is presented in (kiefer and krieger, 2000) and (kiefer and krieger, 2002).
</prevsent>
</prevsection>
<citsent citstr=" C00-2155 ">
the application ofthe method to mid-size ubg of english, and large size hpsgs of english and japanese is described in (kiefer and krieger, 2002) and (kiefer et al, 2000).<papid> C00-2155 </papid></citsent>
<aftsection>
<nextsent>(kiefer and krieger, 2000) suggest that, given ubg, the approximated cfg can be used as cheap filter during two-stage parsing approach.
</nextsent>
<nextsent>the idea is to let the cfg explore the search space, whereas the ubg deterministically replays the derivations, proposed by the cfg.
</nextsent>
<nextsent>to be able to carry out the replay, during the creation of the cf grammar, each cf production is correlated with the ubg rules it was produced from.
</nextsent>
<nextsent>the abovementioned two-stage parsing approach not only speeds up parsing (see figure 4), but can also be starting point for an efficient stochastic parsing model, even though the ubg might encode an infinite number of categories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1471">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>measure 89 1058 temperature 72 1028 at 7 all 60 three 55 1018 decks 10 1033 951 929 960 873 687 304 1017 measure 89 1058 temperature 72 1028 960 at 7 all 60 three 55 1018 decks 10 1033 951 183 873 687 304 1017 figure 3: alternative readings licensed by the context-free approximation of the gemini grammar.
</prevsent>
<prevsent>approximation.
</prevsent>
</prevsection>
<citsent citstr=" P01-1022 ">
(dowding et al, 2001) <papid> P01-1022 </papid>compared (moore, 1999)s approach to grammar approximation to (kiefer and krieger, 2000).</citsent>
<aftsection>
<nextsent>as basis for the comparison, they chose an english grammar written in the gemini/cle formalism.
</nextsent>
<nextsent>the motivation for this enterprise comes from the use of the resulting cfg as context-free language model for the nuance speech recognizer.
</nextsent>
<nextsent>john dowding kindly provided the gemini grammar and corpus of 500 sentences, allowing us to measure the quality of our approximation method for realistic mid-size grammar, both under  and  (see section 2).1 the gemini grammar consisted of 57 unification rules and small lexicon of 216 entries which expanded into 425 full forms.
</nextsent>
<nextsent>since the grammar allows for atomic dis junctions (and makes heavy use of them), we ended in overall 1,886 type definition sin our system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1474">
<title id=" C02-1075.xml">a novel disambiguation method for unification based grammars using probabilistic context free approximations </title>
<section> related work and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>a comparison of the models with our random baseline shows an increase in precision of about 16%.
</prevsent>
<prevsent>although we tried hard to improve this gainby varying the starting parameters, we wish to report that we found no better starting parameters than uniform probabilities for the grammar rules.
</prevsent>
</prevsection>
<citsent citstr=" P00-1061 ">
the most direct points of comparison of our method are the approaches of johnson et al (1999) <papid> P99-1069 </papid>and riezler et al (2000), <papid> P00-1061 </papid>esp. since they use the same evaluation criteria than we use.</citsent>
<aftsection>
<nextsent>in the first approach, log-linear models for lfg grammars were trained on treebanks of about 400sentences.
</nextsent>
<nextsent>precision was evaluated for an ambiguity rate of 10 (using cross-validation), and achieved 59%.
</nextsent>
<nextsent>if compared to this, our best models achievea gain of about 28%.
</nextsent>
<nextsent>however, comparison is difficult, since the disambiguation task is more easy for our models, due to the low ambiguity rate ofour testing corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1476">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments using comparative sentences from product reviews and forum posts show that the approach is effective.
</prevsent>
<prevsent>in the past few years, there was growing interest in mining opinions in the user-generated content (ugc) on the web, e.g., customer reviews, forum posts, and blogs.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
one major focus is sentiment classification and opinion mining (e.g., pang et al 2002; <papid> W02-1011 </papid>turney 2002; <papid> P02-1053 </papid>hu and liu 2004; wilson et al 2004; kim and hovy 2004; <papid> C04-1200 </papid>popescu and etzioni 2005) ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>however, these studies mainly center on direct opinions or sentiments expressed on entities.
</nextsent>
<nextsent>little study has been done on comparisons, which represent another type of opinion-bearing text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1478">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments using comparative sentences from product reviews and forum posts show that the approach is effective.
</prevsent>
<prevsent>in the past few years, there was growing interest in mining opinions in the user-generated content (ugc) on the web, e.g., customer reviews, forum posts, and blogs.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
one major focus is sentiment classification and opinion mining (e.g., pang et al 2002; <papid> W02-1011 </papid>turney 2002; <papid> P02-1053 </papid>hu and liu 2004; wilson et al 2004; kim and hovy 2004; <papid> C04-1200 </papid>popescu and etzioni 2005) ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>however, these studies mainly center on direct opinions or sentiments expressed on entities.
</nextsent>
<nextsent>little study has been done on comparisons, which represent another type of opinion-bearing text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1479">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments using comparative sentences from product reviews and forum posts show that the approach is effective.
</prevsent>
<prevsent>in the past few years, there was growing interest in mining opinions in the user-generated content (ugc) on the web, e.g., customer reviews, forum posts, and blogs.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
one major focus is sentiment classification and opinion mining (e.g., pang et al 2002; <papid> W02-1011 </papid>turney 2002; <papid> P02-1053 </papid>hu and liu 2004; wilson et al 2004; kim and hovy 2004; <papid> C04-1200 </papid>popescu and etzioni 2005) ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>however, these studies mainly center on direct opinions or sentiments expressed on entities.
</nextsent>
<nextsent>little study has been done on comparisons, which represent another type of opinion-bearing text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1484">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining.
</prevsent>
<prevsent>sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (pang et al 2002; <papid> W02-1011 </papid>turney 2002).<papid> P02-1053 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
sentiment classification at the sentence-level has also been studied (e.g., riloff and wiebe 2003; <papid> W03-1014 </papid>kim and hovy 2004; <papid> C04-1200 </papid>wilson et al 2004; gamon et al  242 2005; stoyanov and cardie 2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>these works are different from ours as we study comparatives.
</nextsent>
<nextsent>the works in (hu and liu 2004; liu et al 2005; popescu and etzioni 2005; mei et al 2007) perform opinion mining at the feature level.
</nextsent>
<nextsent>the task involves (1) extracting entity features (e.g., picture quality?
</nextsent>
<nextsent>and battery life?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1487">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two main directions are sentiment classification at the document and sentence levels, and feature-based opinion mining.
</prevsent>
<prevsent>sentiment classification at the document level investigates ways to classify each evaluative document (e.g., product review) as positive or negative (pang et al 2002; <papid> W02-1011 </papid>turney 2002).<papid> P02-1053 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-0302 ">
sentiment classification at the sentence-level has also been studied (e.g., riloff and wiebe 2003; <papid> W03-1014 </papid>kim and hovy 2004; <papid> C04-1200 </papid>wilson et al 2004; gamon et al  242 2005; stoyanov and cardie 2006).<papid> W06-0302 </papid></citsent>
<aftsection>
<nextsent>these works are different from ours as we study comparatives.
</nextsent>
<nextsent>the works in (hu and liu 2004; liu et al 2005; popescu and etzioni 2005; mei et al 2007) perform opinion mining at the feature level.
</nextsent>
<nextsent>the task involves (1) extracting entity features (e.g., picture quality?
</nextsent>
<nextsent>and battery life?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1488">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers.
</prevsent>
<prevsent>again, our work is different because we deal with comparisons.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words (hatzivassiloglou and mckeown 1997; <papid> P97-1023 </papid>kanayama and nasukawa 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>both works use conjunction rules to find such words from large domain corpora.
</nextsent>
<nextsent>one conjunction rule states that when two opinion words are linked by and?, their opinions are the same.
</nextsent>
<nextsent>our method is different in three aspects.
</nextsent>
<nextsent>first, we argue that finding domain opinion words is problematic because in the same domain the same word may indicate different opinions depending on what features it is applied to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1489">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in camera review) and (2) finding orientations (positive, negative or neutral) of opinions expressed on the features by reviewers.
</prevsent>
<prevsent>again, our work is different because we deal with comparisons.
</prevsent>
</prevsection>
<citsent citstr=" W06-1642 ">
discovering orientations of context dependent opinion comparative words is related to identifying domain opinion words (hatzivassiloglou and mckeown 1997; <papid> P97-1023 </papid>kanayama and nasukawa 2006).<papid> W06-1642 </papid></citsent>
<aftsection>
<nextsent>both works use conjunction rules to find such words from large domain corpora.
</nextsent>
<nextsent>one conjunction rule states that when two opinion words are linked by and?, their opinions are the same.
</nextsent>
<nextsent>our method is different in three aspects.
</nextsent>
<nextsent>first, we argue that finding domain opinion words is problematic because in the same domain the same word may indicate different opinions depending on what features it is applied to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1490">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>as discussed in the introduction, closely related work to ours is (jindal and liu 2006).
</prevsent>
<prevsent>however, it does not find which entities are preferred by authors.
</prevsent>
</prevsection>
<citsent citstr=" W06-1602 ">
bos and nissim (2006) <papid> W06-1602 </papid>proposes method to extract some useful items from superlative sentences.</citsent>
<aftsection>
<nextsent>fiszman et al (2007) <papid> W07-1018 </papid>studied the problem of identifying which entity has more of certain features in comparative sen tences.</nextsent>
<nextsent>it does not find which entity is preferred.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1491">
<title id=" C08-1031.xml">mining opinions in comparative sentences </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, it does not find which entities are preferred by authors.
</prevsent>
<prevsent>bos and nissim (2006) <papid> W06-1602 </papid>proposes method to extract some useful items from superlative sentences.</prevsent>
</prevsection>
<citsent citstr=" W07-1018 ">
fiszman et al (2007) <papid> W07-1018 </papid>studied the problem of identifying which entity has more of certain features in comparative sen tences.</citsent>
<aftsection>
<nextsent>it does not find which entity is preferred.
</nextsent>
<nextsent>definition (entity and feature): an entity is the name of person, product, company, location, etc, under comparison in comparative sentence.
</nextsent>
<nextsent>a feature is part or attribute of the entity that is being compared.
</nextsent>
<nextsent>for example, in the sentence, camera xs battery life is longer than that of camera y?, camera x?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1493">
<title id=" C04-1019.xml">the queens agents using collaborating object based dialogue agents in the queens communicator </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(allen et al, 2000) ? little published research addresses the question of how established techniques of object-oriented software engineering (booch, 1994) (booch et al, 1998) can contribute to the dialogue management task.
</prevsent>
<prevsent>some research groups confirm the suitability of java for the development of interactive, agent based systems ? for example collagen (rich et al. 2001).
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
indeed, the collagen architecture, like that of the queens communicator, manages discourse using focus stack?, classical idea in the theory of discourse structure (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>for dialogues that are not primarily transaction- based or frame-based, and where the system must establish the users broader objectives before offering advice or presenting options, discourse management strategy based on problem-solving (ps) objects (objectives, recipes, actions and resources) is appropriate (blaylock et al, 2003).
</nextsent>
<nextsent>we are currently investigating means of using ps objects to orient dialogue, before using expertise like that currently encapsulated in our domain agents to complete those frame-filling tasks that are needed to support the users objectives.
</nextsent>
<nextsent>we have decomposed the cross-domain dialogue management task intuitively into number of sub dialogues, each conducted by an implemented domain specialist with its own expert rules and associated frame of information to collect.
</nextsent>
<nextsent>by using inheritance we easily establish common approach to dialogue management, independent of domain: all experts inherit the same confirmation strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1494">
<title id=" C04-1194.xml">discovering word senses from a network of lexical cooccurrences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main semantic resources with wide coverage that can be exploited by computers are lexico-semantic networks such as wordnet.
</prevsent>
<prevsent>be cause of the way they were built, mainly by hand, these networks are not fundamentally different from traditional dictionaries.
</prevsent>
</prevsection>
<citsent citstr=" W99-0501 ">
hence, it is not very surprising that they were criticized, as in (harabagiu et al, 1999), <papid> W99-0501 </papid>for not being suitable for natural language processing.</citsent>
<aftsection>
<nextsent>they were criticized both about the nature of the senses they discriminate and the way they characterize them.
</nextsent>
<nextsent>their senses are considered as too fine-grained but also incomplete.
</nextsent>
<nextsent>moreover, they are generally defined through their relations with synonyms, hyponyms and hyperonyms but not by elements that describe the contexts in which they occur.
</nextsent>
<nextsent>one of the solutions for solving this problem consists in automatically discovering the senses of words from corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1495">
<title id=" C04-1194.xml">discovering word senses from a network of lexical cooccurrences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first one, represented by (pantel and lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from distributional ist viewpoint, hence to gather words that are mainly synonyms.
</prevsent>
<prevsent>in the case of (pantel and lin, 2002), the discovering of word senses is side effect of the clustering algorithm, cluster by committee, used for building classes of words: as word can belong to several classes, each of them can be considered as one of its senses.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
the second main trend, found in (schtze, 1998), (pedersen and bruce, 1997) <papid> W97-0322 </papid>and (puran dare, 2003), <papid> N03-3004 </papid>represents each instance of target word by set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances.</citsent>
<aftsection>
<nextsent>each cluster is then considered as sense of the target word.
</nextsent>
<nextsent>the last trend, explored by (vronis, 2003), (dorow and widdows, 2003) <papid> E03-1020 </papid>and (rapp, 2003), starts from the cooccurrents of word recorded from corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.</nextsent>
<nextsent>our work takes place in this last trend.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1496">
<title id=" C04-1194.xml">discovering word senses from a network of lexical cooccurrences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first one, represented by (pantel and lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from distributional ist viewpoint, hence to gather words that are mainly synonyms.
</prevsent>
<prevsent>in the case of (pantel and lin, 2002), the discovering of word senses is side effect of the clustering algorithm, cluster by committee, used for building classes of words: as word can belong to several classes, each of them can be considered as one of its senses.
</prevsent>
</prevsection>
<citsent citstr=" N03-3004 ">
the second main trend, found in (schtze, 1998), (pedersen and bruce, 1997) <papid> W97-0322 </papid>and (puran dare, 2003), <papid> N03-3004 </papid>represents each instance of target word by set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances.</citsent>
<aftsection>
<nextsent>each cluster is then considered as sense of the target word.
</nextsent>
<nextsent>the last trend, explored by (vronis, 2003), (dorow and widdows, 2003) <papid> E03-1020 </papid>and (rapp, 2003), starts from the cooccurrents of word recorded from corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.</nextsent>
<nextsent>our work takes place in this last trend.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1497">
<title id=" C04-1194.xml">discovering word senses from a network of lexical cooccurrences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second main trend, found in (schtze, 1998), (pedersen and bruce, 1997) <papid> W97-0322 </papid>and (puran dare, 2003), <papid> N03-3004 </papid>represents each instance of target word by set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances.</prevsent>
<prevsent>each cluster is then considered as sense of the target word.</prevsent>
</prevsection>
<citsent citstr=" E03-1020 ">
the last trend, explored by (vronis, 2003), (dorow and widdows, 2003) <papid> E03-1020 </papid>and (rapp, 2003), starts from the cooccurrents of word recorded from corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity.</citsent>
<aftsection>
<nextsent>our work takes place in this last trend.
</nextsent>
<nextsent>the starting point of the method we present in this article is network of lexical cooccurrences, that is graph whose vertices are the significant words of corpus and edges represent the cooccurrences between these words in the corpus.
</nextsent>
<nextsent>the discovering of word senses is performed word by word and the processing of word only relies on the subgraph that contains its cooccurrents.
</nextsent>
<nextsent>the first step of the method consists in building matrix of similarity between these cooccurrents by exploiting their relations in the subgraph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1505">
<title id=" C04-1194.xml">discovering word senses from a network of lexical cooccurrences </title>
<section> networks of lexical cooccurrences.  </section>
<citcontext>
<prevsection>
<prevsent>cooccurrences were classically extracted by moving fixed-size window on texts.
</prevsent>
<prevsent>parameters were chosen in order to catch topical relations: the window was rather large, 20 word wide, and took into account the boundaries of texts; moreover, cooccurrences were indifferent to word order.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
as (church and hanks, 1990), <papid> J90-1003 </papid>we adopted an evaluation of mutual information as cohesion measure of each cooccurrence.</citsent>
<aftsection>
<nextsent>this measure was normalized according to the maximal mutual information relative to the considered corpus.
</nextsent>
<nextsent>after filtering the less significant cooccurrences (cooccurrences with less than 10 occurrences and cohesion lower than 0.1), we got network with approximately 23,000 words and 5.2 million cooccurrences for french, 30,000 words and 4.8 million cooccurrences for english.
</nextsent>
<nextsent>4.1 building of the similarity matrix be-.
</nextsent>
<nextsent>tween cooccurrents the number and the extent of the clusters built by clustering algorithm generally depend on set of parameters that can be tuned in one way or an other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1515">
<title id=" C08-1098.xml">estimation of conditional probabilities with decision trees and an application to fine grained pos tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is based on three ideas: (1) splitting of the pos tags into attribute vectors and decomposition of the contextual pos probabilities of the hmm into aproduct of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order hmms.
</prevsent>
<prevsent>in experiments on german and czech data, our tagger outperformed state of-the-art pos taggers.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
a hidden-markov-model part-of-speech tagger (brants, 2000, <papid> A00-1031 </papid>e.g.) computes the most probable pos tag sequence ? n 1 = ? 1 , ..., ? n forgiven word sequence n 1 . ? n 1 = argmax n 1 p(t 1 , n 1 )the joint probability of the two sequences is defined as the product of context probabilities and lexical probabilities over all pos tags: p(t 1 , n 1 ) = ? i=1 p(t |t i1 ik ) ? ??</citsent>
<aftsection>
<nextsent>context prob.
</nextsent>
<nextsent>p(w |t ) ? ??
</nextsent>
<nextsent>lexical prob.
</nextsent>
<nextsent>(1)hmm taggers are fast and were successfully applied to wide range of languages and training corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1517">
<title id=" C08-1098.xml">estimation of conditional probabilities with decision trees and an application to fine grained pos tagging </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>some of the 54 stts labels were mapped to new labels with dots, which reduced the number of main categories to 23.
</prevsent>
<prevsent>examples are the nominal pos tags nn and ne which were mapped ton.reg and n.name.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
some lexically decidable distinctions missing in the tiger corpus have been 7 it was planned to include also the stanford tagger (toutanova et al , 2003) <papid> N03-1033 </papid>in this comparison, but it was not possible to train it on the tiger data.automatically added.</citsent>
<aftsection>
<nextsent>examples are the distinction between definite and indefinite articles, and the distinction between hyphens, slashes, left and right parentheses, quotation marks, and other symbols which the tiger treebank annotates with ?$(?.a supplementary lexicon was created by analyzing word list which included all words from the training, development, and test data with german computational morphology.
</nextsent>
<nextsent>the analyses generated by the morphology were mapped to the tiger tagset.
</nextsent>
<nextsent>note that only the words, but not the pos tags from the test and development data were used, here.
</nextsent>
<nextsent>therefore, it is always possible to create asupplementary lexicon for the corpus to be pro cessed.in case of the tnt tagger, the entries of the supplementary lexicon were added to the regular lexicon with default frequency of 1 if the word/tagpair was unknown, and with frequency proportional to the prior probability of the tag if the wordwas unknown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1519">
<title id=" C08-1098.xml">estimation of conditional probabilities with decision trees and an application to fine grained pos tagging </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>the difference is significant.
</prevsent>
<prevsent>our tagger combines two ideas, the decomposition of the probability of complex pos tags into product of feature probabilities, and the estimation of the conditional probabilities with decision trees.
</prevsent>
</prevsection>
<citsent citstr=" C94-1025 ">
a similar idea was previously presented in kempe (1994), <papid> C94-1025 </papid>but apparently never applied again.</citsent>
<aftsection>
<nextsent>the tagging accuracy reported by kempe was below that of traditional trigram tagger.
</nextsent>
<nextsent>unlike him, we found that our tagging method out-performed state-of-the-art pos taggers on fine-grained pos tagging even if only trigram context was used.schmid (1994) and m`arquez (1999) used decision trees for the estimation of contextual tag probabilities, but without decomposition of the tagprobability.
</nextsent>
<nextsent>magerman (1994) applied probabilistic decision trees to parsing, but not with generative model.provost &amp; domingos (2003) noted that well known decision tree induction algorithms such as c4.5 (quinlan, 1993) or cart (breiman et al ,1984) fail to produce accurate probability estimates.
</nextsent>
<nextsent>they proposed to grow the decision trees to their maximal size without pruning, and to smooth the probability estimates with add-1 smoothing (also known as the laplace correction).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1520">
<title id=" C02-1056.xml">paraphrasing of chinese utterances </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using the implemented paraphraser and the obtained patterns, paraphrasing experiment was conducted and the results were evaluated.
</prevsent>
<prevsent>in spoken language translation one of the key issues is how to deal with unrestricted expressions in spontaneous utterances.
</prevsent>
</prevsection>
<citsent citstr=" C02-1163 ">
to resolve this problem, we have proposed paraphrasing approach in which the utterances are automatically paraphrased prior to transfer (yamamotoet al, 2001; yamamoto, 2002).<papid> C02-1163 </papid></citsent>
<aftsection>
<nextsent>the paraphrasing process aims to bridge the gap between the unrestricted expressions in the input and the limited expressions that the transfer can translate.
</nextsent>
<nextsent>in fact, paraphrasing actions are often seenin daily communication.
</nextsent>
<nextsent>when listener can not understand what speaker said, the speaker usually says it again using other words, i.e., heparaphrases.
</nextsent>
<nextsent>in chinese-japanese spoken language translation system, the pre-processing of chinese utterances is involved and we attempt to apply paraphrasing approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1521">
<title id=" C04-1079.xml">generating overview summaries of ongoing email thread discussions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the summary thus provides the question-answer pair intact, thereby improving the coherence.
</prevsent>
<prevsent>question answer summaries would presumably besuited to discussions which support an information provision task, complementary task to the one we examine.
</prevsent>
</prevsection>
<citsent citstr=" N04-4027 ">
rambow et al (2004) <papid> N04-4027 </papid>apply sentence extraction techniques to the thread to construct generic summary.</citsent>
<aftsection>
<nextsent>though not specifically using dialogue structure, one feature used marks if sentence is question or not.
</nextsent>
<nextsent>work has also been done on more accurately constructing the dialogue structure.
</nextsent>
<nextsent>newman and blitzer (2003) focus on clustering related news group messages into dialogue segments.
</nextsent>
<nextsent>the segments are then linked using email header information to form hierarchical structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1522">
<title id=" C04-1079.xml">generating overview summaries of ongoing email thread discussions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an overview of the differences between asynchronous and synchronous modes of communication is provided by clark (1991) and simpson-young et al.
</prevsent>
<prevsent>(2000).
</prevsent>
</prevsection>
<citsent citstr=" W00-1420 ">
alexandersson et al (2000) <papid> W00-1420 </papid>note that in speech there is tendency not to repeat shared conversation context.</citsent>
<aftsection>
<nextsent>they use the preceding dialogue structure, modeled using dialogue representation theory, to provide additional ellipsed information.
</nextsent>
<nextsent>it is unclear how such an approach might apply to an email corpus which has the potential to cover broader set of domains.
</nextsent>
<nextsent>more recently, zechner and lavie (2001) identify question-answer dialogue segments in order to extract the pair as whole.
</nextsent>
<nextsent>hillard et al (2003) <papid> N03-2012 </papid>have also produced system which generates summaries of speech discussions supporting decision-making process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1523">
<title id=" C04-1079.xml">generating overview summaries of ongoing email thread discussions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is unclear how such an approach might apply to an email corpus which has the potential to cover broader set of domains.
</prevsent>
<prevsent>more recently, zechner and lavie (2001) identify question-answer dialogue segments in order to extract the pair as whole.
</prevsent>
</prevsection>
<citsent citstr=" N03-2012 ">
hillard et al (2003) <papid> N03-2012 </papid>have also produced system which generates summaries of speech discussions supporting decision-making process.</citsent>
<aftsection>
<nextsent>their work differs from ours in that they focus on categorizing the polarity of responses in order to summarize consensus.
</nextsent>
<nextsent>to make the problem more manageable we make the following assumptions about the types of threads that our algorithm will handle.
</nextsent>
<nextsent>to begin with, we assume that the threads have been correctly constructed and classified as discussions supporting decision-making.
</nextsent>
<nextsent>needless to say, the first assumption is little unrealistic given that thread construction is difficult problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1524">
<title id=" C04-1079.xml">generating overview summaries of ongoing email thread discussions </title>
<section> extract top ranking sentence.  </section>
<citcontext>
<prevsection>
<prevsent>gong and liu use svd for text segmentation and summarization purposes.
</prevsent>
<prevsent>hoffman describes the results of svd within probabilistic framework.
</prevsent>
</prevsection>
<citsent citstr=" W03-1202 ">
for more complete summary of our interpretation of the svd analysis see wan et al (2003).<papid> W03-1202 </papid></citsent>
<aftsection>
<nextsent>to begin with, we construct the matrix as in the centro id method.
</nextsent>
<nextsent>the matrix provides representation of each sentence in dimensionality, where is the size of the vocabulary of the thread.
</nextsent>
<nextsent>the svd analysis1 is the product of three matrices u, and transpose.
</nextsent>
<nextsent>in the following equation, dimensionality is indicated by the subscripts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1526">
<title id=" C08-1027.xml">syntactic reordering integrated with phrase based smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on an english-danish task, we achieve an absolute improvement in translation quality of 1.1 % bleu.
</prevsent>
<prevsent>manual evaluation supports the claim that the present approach is significantly superior to previous approaches.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the emergence of phrase-based statistical machine translation (psmt) (koehn et al, 2003) <papid> N03-1017 </papid>has beenone of the major developments in statistical approaches to translation.</citsent>
<aftsection>
<nextsent>allowing translation of word sequences (phrases) instead of single words provides smt with robustness in word selection and local word reordering.
</nextsent>
<nextsent>psmt has two means of reordering the words.either phrase pair has been learned where the target word order differs from the source (phrase internal reordering), or distance penalized orderings of target phrases are attempted in decoding (phrase ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1527">
<title id=" C08-1027.xml">syntactic reordering integrated with phrase based smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems clear that reordering often depends on higher level linguistic information, which is absent from psmt.
</prevsent>
<prevsent>in recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering.
</prevsent>
</prevsection>
<citsent citstr=" C04-1073 ">
in works such as (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; wang et al, 2007; <papid> D07-1077 </papid>habash,2007), reordering decisions are done deter minis tically?, thus placing these decisions outside the actual psmt system by learning to translate from reordered source language.</citsent>
<aftsection>
<nextsent>(crego and marino, 2007; zhang et al, 2007; li et al, 2007) <papid> P07-1091 </papid>are more in the spirit of psmt, in that multiple reorderings are presented to the psmt system as (possibly weighted) options.</nextsent>
<nextsent>still, there remains basic conflict between the syntactic reordering rules and the psmt system:one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (zhang et al, 2007) point out.in this paper, we proceed in the spirit of the non deterministic approaches by providing the decoder with multiple source reorderings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1528">
<title id=" C08-1027.xml">syntactic reordering integrated with phrase based smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it seems clear that reordering often depends on higher level linguistic information, which is absent from psmt.
</prevsent>
<prevsent>in recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering.
</prevsent>
</prevsection>
<citsent citstr=" D07-1077 ">
in works such as (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; wang et al, 2007; <papid> D07-1077 </papid>habash,2007), reordering decisions are done deter minis tically?, thus placing these decisions outside the actual psmt system by learning to translate from reordered source language.</citsent>
<aftsection>
<nextsent>(crego and marino, 2007; zhang et al, 2007; li et al, 2007) <papid> P07-1091 </papid>are more in the spirit of psmt, in that multiple reorderings are presented to the psmt system as (possibly weighted) options.</nextsent>
<nextsent>still, there remains basic conflict between the syntactic reordering rules and the psmt system:one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (zhang et al, 2007) point out.in this paper, we proceed in the spirit of the non deterministic approaches by providing the decoder with multiple source reorderings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1529">
<title id=" C08-1027.xml">syntactic reordering integrated with phrase based smt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent work, there has been some progress towards integrating syntactic information with the statistical approach to reordering.
</prevsent>
<prevsent>in works such as (xia and mccord, 2004; <papid> C04-1073 </papid>collins et al, 2005; wang et al, 2007; <papid> D07-1077 </papid>habash,2007), reordering decisions are done deter minis tically?, thus placing these decisions outside the actual psmt system by learning to translate from reordered source language.</prevsent>
</prevsection>
<citsent citstr=" P07-1091 ">
(crego and marino, 2007; zhang et al, 2007; li et al, 2007) <papid> P07-1091 </papid>are more in the spirit of psmt, in that multiple reorderings are presented to the psmt system as (possibly weighted) options.</citsent>
<aftsection>
<nextsent>still, there remains basic conflict between the syntactic reordering rules and the psmt system:one that is most likely due to the discrepancy between the translation units (phrases) and units of the linguistic rules, as (zhang et al, 2007) point out.in this paper, we proceed in the spirit of the non deterministic approaches by providing the decoder with multiple source reorderings.
</nextsent>
<nextsent>but instead of scoring the input word order, we score the order of the output.
</nextsent>
<nextsent>by doing this, we avoid the integration problems of previous approaches.it should be noted that even though the experi 209ments are conducted within source reordering approach, this scoring is also compatible with other approach.
</nextsent>
<nextsent>we will, however, not look further into this possiblity in the present paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1540">
<title id=" C08-1027.xml">syntactic reordering integrated with phrase based smt </title>
<section> reordering rules.  </section>
<citcontext>
<prevsection>
<prevsent>the rules are extracted from the hand-aligned,copenhagen danish-english dependency tree bank (buch-kromann et al, 2007).
</prevsent>
<prevsent>5478 sentences from the news paper domain containing 111,805 english words and 100,185 danish words.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the english side is parsed using state-of-the-art statistical english parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>4.3 integrating rule-based reordering in.
</nextsent>
<nextsent>psmt the integration of the rule-based reordering in our psmt system is carried out in two separate stages: 1.
</nextsent>
<nextsent>reorder the source sentence to assimilate the.
</nextsent>
<nextsent>word order of the target language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1542">
<title id=" C02-1022.xml">varying cardinality in metonymic extensions to nouns </title>
<section> approaches to metonymy.  </section>
<citcontext>
<prevsection>
<prevsent>such contexts reflect prototypical knowledge derived from agentive or telic roles of the lexical entry for  book , which are prominent roles in the qualia structure of nouns.
</prevsent>
<prevsent>particularities of the qualia structure of nouns regulate the acceptability of leaving metonymic relation implicit (pustejovsky and bouillon 1995).
</prevsent>
</prevsection>
<citsent citstr=" P93-1012 ">
stallard (1993) <papid> P93-1012 </papid>indirectly addresses the second aspect by taking into account scoping relations and impacts on pronominal reference.</citsent>
<aftsection>
<nextsent>he introduces distinction between referential and predicative metonymy, depending on whether the intended or the literal argument is accessible for subsequent pronominal reference.
</nextsent>
<nextsent>this distinction manifests itself in different scope relations that hold between these arguments in the corresponding logical forms.
</nextsent>
<nextsent>we will argue against his usage of scoping and the resulting strict distinction of pronominal accessibility.
</nextsent>
<nextsent>markert and hahn (1997) address interactions of metonymic relation extension and anaphora resolution, which enables them to handle textual ellipsis references.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1543">
<title id=" C02-1085.xml">detecting shifts in news stories for paragraph extraction </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a set of 25 target events were defined.
</prevsent>
<prevsent>each document is labeled according to whether or not the document discusses the target event.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
all 15,863 documents were tagged by part-of-speech tagger(brill, 1992) <papid> A92-1021 </papid>and stemmed using wordnet information(fellbaum, 1998).</citsent>
<aftsection>
<nextsent>we extracted all nouns in the documents.
</nextsent>
<nextsent>5.1 tracking task.
</nextsent>
<nextsent>table 1 summarizes the results which were obtained using the standard tdt evaluation measure1.
</nextsent>
<nextsent>table 1: tracking results nt miss f/a prec f1 1 31% 0.16% 70% 0.68 2 27% 0.16% 79% 0.78 4 24% 0.09% 87% 0.78 8 23% 0.09% 87% 0.79 16 22% 0.09% 86% 0.79 nt?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1544">
<title id=" C02-1085.xml">detecting shifts in news stories for paragraph extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>figure 5: recall with and without detecting shift figure 6: precision with and without detecting shift
</prevsent>
<prevsent>most of the work on summarization task by paragraph or sentence extraction has applied statistical techniques based on word distribution to the target document(kupiec et al , 1995).
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
more recently, other approaches have investigated the use of machine learning to find patterns in documents(strzalkowskiet al , 1998) and the utility of parameterized modules so as to deal with different genres or corpora(goldstein et al , 2000).<papid> W00-0405 </papid></citsent>
<aftsection>
<nextsent>some of these approaches to single document summarization have been extended to deal with multi-document sum marization(mani and e.bloedorn, 1997), (barzilay et al , 1999), (<papid> P99-1071 </papid>mckeown et al , 1999).</nextsent>
<nextsent>our work differs from the earlier work in several important respects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1545">
<title id=" C02-1085.xml">detecting shifts in news stories for paragraph extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the work on summarization task by paragraph or sentence extraction has applied statistical techniques based on word distribution to the target document(kupiec et al , 1995).
</prevsent>
<prevsent>more recently, other approaches have investigated the use of machine learning to find patterns in documents(strzalkowskiet al , 1998) and the utility of parameterized modules so as to deal with different genres or corpora(goldstein et al , 2000).<papid> W00-0405 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
some of these approaches to single document summarization have been extended to deal with multi-document sum marization(mani and e.bloedorn, 1997), (barzilay et al , 1999), (<papid> P99-1071 </papid>mckeown et al , 1999).</citsent>
<aftsection>
<nextsent>our work differs from the earlier work in several important respects.
</nextsent>
<nextsent>first, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events(radev et al , 2000).<papid> W00-0403 </papid></nextsent>
<nextsent>detecting subject shift from the documents in the target event, however, presents special difficulties, since these documents are collected from very restricted domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1546">
<title id=" C02-1085.xml">detecting shifts in news stories for paragraph extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some of these approaches to single document summarization have been extended to deal with multi-document sum marization(mani and e.bloedorn, 1997), (barzilay et al , 1999), (<papid> P99-1071 </papid>mckeown et al , 1999).</prevsent>
<prevsent>our work differs from the earlier work in several important respects.</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
first, our method focuses on subject shift of the documents from the target event rather than the sets of documents from different events(radev et al , 2000).<papid> W00-0403 </papid></citsent>
<aftsection>
<nextsent>detecting subject shift from the documents in the target event, however, presents special difficulties, since these documents are collected from very restricted domain.
</nextsent>
<nextsent>we thus present window adjustment algorithm which automatically adjusts the optimal window in the training documents, so as to include only the data which are sufficiently related to the current subject.
</nextsent>
<nextsent>second,our approach works in living way, while many approaches are stable ones, i.e., they use documents which are prepared in advance and apply variety of techniques to create summaries.
</nextsent>
<nextsent>we are interested in substantially smaller number of initial training documents, which are then utilized to extract paragraphs from documents relevant to the initial documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1547">
<title id=" C04-1145.xml">morpheme based derivation of bipolar semantic orientation of chinese words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we concluded that morphemes in chinese, as in any language, constitute distinct sub-lexical unit which, though small in number, has greater linguistic significance than words, as seen by the significant enhancement of results with much smaller corpus than that required by turney.
</prevsent>
<prevsent>the semantic orientation (so) of word indicates the direction in which the word deviates from the norm for its semantic group or lexical field (lehrer, 1974).
</prevsent>
</prevsection>
<citsent citstr=" C00-1044 ">
words that encode desirable state (e.g., beautiful) have positive so, while words that represent undesirable states (e.g. absurd) have negative so (hatzivassiloglou and wiebe, 2000).<papid> C00-1044 </papid></citsent>
<aftsection>
<nextsent>hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>used the words and?, or?, and but?</nextsent>
<nextsent>as linguistic cues to extract adjective pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1548">
<title id=" C04-1145.xml">morpheme based derivation of bipolar semantic orientation of chinese words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the semantic orientation (so) of word indicates the direction in which the word deviates from the norm for its semantic group or lexical field (lehrer, 1974).
</prevsent>
<prevsent>words that encode desirable state (e.g., beautiful) have positive so, while words that represent undesirable states (e.g. absurd) have negative so (hatzivassiloglou and wiebe, 2000).<papid> C00-1044 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
hatzivassiloglou and mckeown (1997) <papid> P97-1023 </papid>used the words and?, or?, and but?</citsent>
<aftsection>
<nextsent>as linguistic cues to extract adjective pairs.
</nextsent>
<nextsent>turney (2003) assessed the so of words using their occurrences near strongly polarized words like excellent?
</nextsent>
<nextsent>and poor?
</nextsent>
<nextsent>with accuracy from 61% to 82%, subject to corpus size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1552">
<title id=" C04-1145.xml">morpheme based derivation of bipolar semantic orientation of chinese words </title>
<section> related work and applications.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm was evaluated on 3,596 words (1,614 positive and 1,982 negative) including adjectives, adverbs, nouns, and verbs.
</prevsent>
<prevsent>an accuracy of 82.8% was attained in corpus of hundred billion words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
so can be used to classify reviews (e.g., movie reviews) as positive or negative (turney, 2002), <papid> P02-1053 </papid>and applied to subjectivity analysis such as recognizing hostile messages, classifying emails, mining reviews (wiebe et al, 2001).<papid> W01-1626 </papid></citsent>
<aftsection>
<nextsent>the first step of those applications is to recognize that the text is subjective and then the second step, naturally, is to determine the so of the subjective text.
</nextsent>
<nextsent>also, it can be used to summarize argumentative articles like editorials of news media.
</nextsent>
<nextsent>a summarization system would benefit from distinguishing sentences intended to present factual materials from those intended to present opinions, since many summaries are meant to include only facts.
</nextsent>
<nextsent>turney (2003) examined so-pmi (pointwise mutual information) and so-lsa (latent semantic analysis).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1553">
<title id=" C04-1145.xml">morpheme based derivation of bipolar semantic orientation of chinese words </title>
<section> related work and applications.  </section>
<citcontext>
<prevsection>
<prevsent>the algorithm was evaluated on 3,596 words (1,614 positive and 1,982 negative) including adjectives, adverbs, nouns, and verbs.
</prevsent>
<prevsent>an accuracy of 82.8% was attained in corpus of hundred billion words.
</prevsent>
</prevsection>
<citsent citstr=" W01-1626 ">
so can be used to classify reviews (e.g., movie reviews) as positive or negative (turney, 2002), <papid> P02-1053 </papid>and applied to subjectivity analysis such as recognizing hostile messages, classifying emails, mining reviews (wiebe et al, 2001).<papid> W01-1626 </papid></citsent>
<aftsection>
<nextsent>the first step of those applications is to recognize that the text is subjective and then the second step, naturally, is to determine the so of the subjective text.
</nextsent>
<nextsent>also, it can be used to summarize argumentative articles like editorials of news media.
</nextsent>
<nextsent>a summarization system would benefit from distinguishing sentences intended to present factual materials from those intended to present opinions, since many summaries are meant to include only facts.
</nextsent>
<nextsent>turney (2003) examined so-pmi (pointwise mutual information) and so-lsa (latent semantic analysis).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1554">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>our method is based on two existing methods for chinese or japanese word segmentation, and we explain them in this section.
</prevsent>
<prevsent>2.1 the markov model-based method.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
word-based markov models are used in english part-of-speech (pos) tagging (charniak et al ,1993; brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>this method identifies postags = t1, . . .
</nextsent>
<nextsent>, tn, given sentence as word sequence = w1, . . .
</nextsent>
<nextsent>, wn, where is the number of words in the sentence.
</nextsent>
<nextsent>the method assumes that each word has state which is the same as the pos of the word and the sequence of states is markovchain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1555">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: the b, i, e, s?
</prevsent>
<prevsent>tag set the lattice.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
the candidates of unknown words can be generated by heuristic rules(matsumoto et al , 2001)or statistical word models which predict the probabilities for any strings to be unknown words (sproatet al , 1996; <papid> J96-3004 </papid>nagata, 1999).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>however, such heuristic rules or word models must be carefully designed for specific language, and it is difficult to properly process wide variety of unknown words.
</nextsent>
<nextsent>2.2 the character tagging method.
</nextsent>
<nextsent>this method carries out word segmentation by tagging each character in given sentence, and inthis method, the tags indicate word-internal positions of the characters.
</nextsent>
<nextsent>we call such tags position of-character (poc) tags (xue, 2003) in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1556">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: the b, i, e, s?
</prevsent>
<prevsent>tag set the lattice.
</prevsent>
</prevsection>
<citsent citstr=" P99-1036 ">
the candidates of unknown words can be generated by heuristic rules(matsumoto et al , 2001)or statistical word models which predict the probabilities for any strings to be unknown words (sproatet al , 1996; <papid> J96-3004 </papid>nagata, 1999).<papid> P99-1036 </papid></citsent>
<aftsection>
<nextsent>however, such heuristic rules or word models must be carefully designed for specific language, and it is difficult to properly process wide variety of unknown words.
</nextsent>
<nextsent>2.2 the character tagging method.
</nextsent>
<nextsent>this method carries out word segmentation by tagging each character in given sentence, and inthis method, the tags indicate word-internal positions of the characters.
</nextsent>
<nextsent>we call such tags position of-character (poc) tags (xue, 2003) in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1557">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>this method carries out word segmentation by tagging each character in given sentence, and inthis method, the tags indicate word-internal positions of the characters.
</prevsent>
<prevsent>we call such tags position of-character (poc) tags (xue, 2003) in this paper.
</prevsent>
</prevsection>
<citsent citstr=" W98-1120 ">
several poc-tag sets have been studied (sang and veenstra, 1999; sekine et al , 1998), <papid> W98-1120 </papid>and we use the b, i, e, s?</citsent>
<aftsection>
<nextsent>tag set shown in table 1 1.
</nextsent>
<nextsent>figure 2 shows an example of poc-tagging.
</nextsent>
<nextsent>the poc-tags can represent word boundaries for any sentences, and the word segmentation task can be reformulated as the poc-tagging task.
</nextsent>
<nextsent>the tagging task can be solved by using general machine learning techniques such as maximum entropy (me) models (xue, 2003) and support vector machines (yoshida et al , 2003; asahara et al , 2003).<papid> W03-1720 </papid>1the b, i, e, s?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1558">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>figure 2 shows an example of poc-tagging.
</prevsent>
<prevsent>the poc-tags can represent word boundaries for any sentences, and the word segmentation task can be reformulated as the poc-tagging task.
</prevsent>
</prevsection>
<citsent citstr=" W03-1720 ">
the tagging task can be solved by using general machine learning techniques such as maximum entropy (me) models (xue, 2003) and support vector machines (yoshida et al , 2003; asahara et al , 2003).<papid> W03-1720 </papid>1the b, i, e, s?</citsent>
<aftsection>
<nextsent>tags are also called op-cn, cn-cn, cn cl, op-cl? tags (sekine et al , 1998) <papid> W98-1120 </papid>or ll, mm, rr, lr?</nextsent>
<nextsent>tags (xue, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1560">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> previous work on word segmentation.  </section>
<citcontext>
<prevsection>
<prevsent>tags (xue, 2003).
</prevsent>
<prevsent>figure 2: example of the character tagging method: word boundaries are indicated by vertical lines (?|?).this character tagging method can easily handle unknown words, because known words and unknown words are treated equally and no other exceptional processing is necessary.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
this approach isalso used in base-np chunking (ramshaw and marcus, 1995) <papid> W95-0107 </papid>and named entity recognition (sekine et al ., 1998) <papid> W98-1120 </papid>as well as word segmentation.</citsent>
<aftsection>
<nextsent>and character-level information we saw the two methods for word segmentation in the previous section.
</nextsent>
<nextsent>it is observed that the markov model-based method has high overall accuracy, however, the accuracy drops for unknown words, and the character tagging method has high accuracy for unknown words but lower accuracy for known words (yoshida et al , 2003; xue, 2003;sproat and emerson, 2003).<papid> W03-1719 </papid></nextsent>
<nextsent>this seems natural be cause words are used as processing unit in the markov model-based method, and therefore much information about known words (e.g., pos or word bigram probability) can be used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1562">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> word segmentation using word-level.  </section>
<citcontext>
<prevsection>
<prevsent>this approach isalso used in base-np chunking (ramshaw and marcus, 1995) <papid> W95-0107 </papid>and named entity recognition (sekine et al ., 1998) <papid> W98-1120 </papid>as well as word segmentation.</prevsent>
<prevsent>and character-level information we saw the two methods for word segmentation in the previous section.</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
it is observed that the markov model-based method has high overall accuracy, however, the accuracy drops for unknown words, and the character tagging method has high accuracy for unknown words but lower accuracy for known words (yoshida et al , 2003; xue, 2003;sproat and emerson, 2003).<papid> W03-1719 </papid></citsent>
<aftsection>
<nextsent>this seems natural be cause words are used as processing unit in the markov model-based method, and therefore much information about known words (e.g., pos or word bigram probability) can be used.
</nextsent>
<nextsent>however, unknown words cannot be handled directly by this method itself.
</nextsent>
<nextsent>on the other hand, characters are used as unit in the character tagging method.
</nextsent>
<nextsent>in general, the number of characters is finite and far fewer than that of words which continuously increases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1565">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> word segmentation using word-level.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3: example of the hybrid meth odin order to handle various character-level features, we calculate word emission probabilities for poc-tags by bayes?
</prevsent>
<prevsent>theorem: (wi|ti) = (ti|wi, ti ? tpoc)p (wi, ti ? tpoc)p (ti) , = (ti|wi, ti ? tpoc) ? ttpoc (wi, t) (ti) , (6) where tpoc = {b, i,e,s}, wi is character and ti is poc-tag.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
in the above equation, (ti) and (wi, t) are estimated by the maximum-likelihood method, and the probability of poc tag ti, given character wi (p (ti|wi, ti ? tpoc)) is estimated using me models (berger et al , 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>we use the following features for me models, where cx is the xth character in sentence, wi = ci?
</nextsent>
<nextsent>and yx is the character type of cx (table 2 shows the definition of character types we used): (1) characters (ci2, ci1, ci?
</nextsent>
<nextsent>, ci?+1, ci?+2) (2) pairs of characters (ci2ci1, ci1ci?
</nextsent>
<nextsent>, ci1ci?+1, cici?+1, ci?+1ci?+2) (3) character types (yi2, yi1, yi?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1569">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 experiments of japanese word.
</prevsent>
<prevsent>segmentation we use the rwcp corpus, which is japanese word-segmented and pos-tagged corpus.
</prevsent>
</prevsection>
<citsent citstr=" C00-1004 ">
we use the following systems for comparison: chasen the word segmentation and pos-tagging system based on extended markov models (asahara and matsumoto, 2000; <papid> C00-1004 </papid>matsumoto et al ., 2001).</citsent>
<aftsection>
<nextsent>this system carries out unknown wordprocessing using heuristic rules.
</nextsent>
<nextsent>maximum matching the same system used in the chinese experiments.
</nextsent>
<nextsent>character tagging the same system used in the chinese experiments.
</nextsent>
<nextsent>as dictionary for chasen, maximum matching and the hybrid method, we use ipadic (matsumoto and asahara, 2001) which is attached to chasen.statistical information of these data is shown in table 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1570">
<title id=" C04-1067.xml">chinese and japanese word segmentation using word level and character level information </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>words which are in the training corpus but not in the dictionary are handled as unknown words in the calculations.
</prevsent>
<prevsent>the number of known/unknown words of the rwcp corpus shown in table 3 is also calculated in the same way.
</prevsent>
</prevsection>
<citsent citstr=" W01-0512 ">
uchimoto et al  (2001) <papid> W01-0512 </papid>studied japanese word segmentation using me models.</citsent>
<aftsection>
<nextsent>although their method is word-based, no word dictionaries are used directly and known and unknown words are handled in same way.
</nextsent>
<nextsent>the method estimates how likely string is to be word using me. given sentence, the method estimates the probabilities for every sub strings in the sentence.
</nextsent>
<nextsent>word segmentation is conducted by finding division of the sentence which maximizes the product of probabilities that each divided substring is word.
</nextsent>
<nextsent>compared to our method, their method can handle some types of features for unknown words such as the word starts with an alphabet and ends with numeral?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1573">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the need for the automatic acquisition of lexical information arised from the so-called lexical bottleneck?
</prevsent>
<prevsent>in nlp systems: no matter whether symbolic or statistical, all systems need more andmore lexical information in order to be able to predict words behaviour, and this information is very hard and costly to encode manually.
</prevsent>
</prevsection>
<citsent citstr=" W99-0503 ">
in recent research in the field, the main effort has been to infer semantic classes for verbs, in english (stevenson et al, 1999) <papid> W99-0503 </papid>and german (schulte imwalde and brew, 2002).</citsent>
<aftsection>
<nextsent>in this paper, we concentrate on adjectives, which have received less attention (see though hatzivassiloglou and mckeown(1993) <papid> P93-1023 </papid>and lapata (2001)).<papid> N01-1009 </papid></nextsent>
<nextsent>our aim is to establish semantic classes for adjectives in catalan by means of clustering, using only shallow syntacticevidence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1574">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in nlp systems: no matter whether symbolic or statistical, all systems need more andmore lexical information in order to be able to predict words behaviour, and this information is very hard and costly to encode manually.
</prevsent>
<prevsent>in recent research in the field, the main effort has been to infer semantic classes for verbs, in english (stevenson et al, 1999) <papid> W99-0503 </papid>and german (schulte imwalde and brew, 2002).</prevsent>
</prevsection>
<citsent citstr=" P93-1023 ">
in this paper, we concentrate on adjectives, which have received less attention (see though hatzivassiloglou and mckeown(1993) <papid> P93-1023 </papid>and lapata (2001)).<papid> N01-1009 </papid></citsent>
<aftsection>
<nextsent>our aim is to establish semantic classes for adjectives in catalan by means of clustering, using only shallow syntacticevidence.
</nextsent>
<nextsent>we compare the results with set of adjectives classified by human judges according to semantic characteristics.
</nextsent>
<nextsent>thus, we intend to induce semantic properties from syntactic distribution.
</nextsent>
<nextsent>we now justify each of the choices: why adjectives, why clustering, and why shallow features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1575">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in nlp systems: no matter whether symbolic or statistical, all systems need more andmore lexical information in order to be able to predict words behaviour, and this information is very hard and costly to encode manually.
</prevsent>
<prevsent>in recent research in the field, the main effort has been to infer semantic classes for verbs, in english (stevenson et al, 1999) <papid> W99-0503 </papid>and german (schulte imwalde and brew, 2002).</prevsent>
</prevsection>
<citsent citstr=" N01-1009 ">
in this paper, we concentrate on adjectives, which have received less attention (see though hatzivassiloglou and mckeown(1993) <papid> P93-1023 </papid>and lapata (2001)).<papid> N01-1009 </papid></citsent>
<aftsection>
<nextsent>our aim is to establish semantic classes for adjectives in catalan by means of clustering, using only shallow syntacticevidence.
</nextsent>
<nextsent>we compare the results with set of adjectives classified by human judges according to semantic characteristics.
</nextsent>
<nextsent>thus, we intend to induce semantic properties from syntactic distribution.
</nextsent>
<nextsent>we now justify each of the choices: why adjectives, why clustering, and why shallow features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1576">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> classification and hypothesis.  </section>
<citcontext>
<prevsection>
<prevsent>however, it is most useful for semantic tasks.
</prevsent>
<prevsent>for instance, object adjective scan evoke arguments when combined with predica tive nouns (presidential visit - president visits x).
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
for projects such as framenet (baker et al, 1998), <papid> P98-1013 </papid>1note that we do not state that adjectives denote objects or events, but that they imply an object or event in their denota tion.</citsent>
<aftsection>
<nextsent>this kind of adjectives denotes properties or states, but with an embedded or shadow?
</nextsent>
<nextsent>argument (pustejovsky, 1995), similarly to verbs like to butter.
</nextsent>
<nextsent>these kinds of relationships could be automatically extracted if information on the class were available.
</nextsent>
<nextsent>the same applies to event adjectives, this time being predicates (flipping coin - coin flips).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1577">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>however, there are too few data for this suspicion to be statistically testable.
</prevsent>
<prevsent>landis and koch (1977) consider values  0.61 to indicate substancious agreement, whereas3the low agreement is probably the result of both the fuzziness of the limits between polysemy and vagueness for adjectives, and the way the instructions were written, as they induced judges to make hard choices and did not state clearly enough the conditions under which an item could be classified in more than one class.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
carletta (1996) <papid> J96-2004 </papid>says that 0.67  0.8 allows just tentative conclusions to be drawn?.</citsent>
<aftsection>
<nextsent>merlo and stevenson (2001) <papid> J01-3003 </papid>report inter-judge  values of 0.53 to 0.66 for task we consider to be comparable to ours, that of classifying verbs into unergative,unaccusative and object-drop, and argue that car lettas is too stringent scale for our task, which is qualitatively quite different from content analysis?</nextsent>
<nextsent>(merlo and stevenson, 2001, <papid> J01-3003 </papid>396).the results reported in tables 2 and 3 are significantly higher tan those of merlo and stevenson (2001).<papid> J01-3003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1578">
<title id=" C04-1161.xml">acquisition of semantic classes for adjectives from distributional evidence </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>landis and koch (1977) consider values  0.61 to indicate substancious agreement, whereas3the low agreement is probably the result of both the fuzziness of the limits between polysemy and vagueness for adjectives, and the way the instructions were written, as they induced judges to make hard choices and did not state clearly enough the conditions under which an item could be classified in more than one class.
</prevsent>
<prevsent>carletta (1996) <papid> J96-2004 </papid>says that 0.67  0.8 allows just tentative conclusions to be drawn?.</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
merlo and stevenson (2001) <papid> J01-3003 </papid>report inter-judge  values of 0.53 to 0.66 for task we consider to be comparable to ours, that of classifying verbs into unergative,unaccusative and object-drop, and argue that car lettas is too stringent scale for our task, which is qualitatively quite different from content analysis?</citsent>
<aftsection>
<nextsent>(merlo and stevenson, 2001, <papid> J01-3003 </papid>396).the results reported in tables 2 and 3 are significantly higher tan those of merlo and stevenson (2001).<papid> J01-3003 </papid></nextsent>
<nextsent>although they are still not all above 0.8, aswould be desirable according to carletta, we consider them to be strong enough to back up both the classification and the feasibility of the task by hu mans.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1581">
<title id=" C04-1195.xml">creative discovery in lexical ontologies </title>
<section> creative exploration in the lmh space.  </section>
<citcontext>
<prevsection>
<prevsent>4.2.2 external validation in contrast, the external validation set for compound m-h is the set of distinct documents that contain the compound term h?, as acquired using web search engine.
</prevsent>
<prevsent>for instance, given the wordnet concepts naval-engineer, software-engineer and naval-academy, rule (5) generates the hypothesis software-academy, which cannot be validated internally yet which retrieves over 1000 web documents to atest to its validity.
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
this web strategy is motivated by keller and lapatas (2003) <papid> J03-3005 </papid>finding that the number of documents containing novel compound reliably predicts the human plausibility scores for the compound.</citsent>
<aftsection>
<nextsent>nonetheless, external validation in this way is by no means robust process.
</nextsent>
<nextsent>since web documents are not sense tagged, one cannot be sure that compound occurs with the sense that it is hypothesized to have.
</nextsent>
<nextsent>indeed, it may not even occur as compound at all, but as coincidental juxtaposition of terms from different phrases or sentences.
</nextsent>
<nextsent>finally, even if found with the correct syntactic and semantic form, one cannot be sure that the usage is not that of non-native, second language learner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1582">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1583">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1584">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" P01-1064 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1586">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1587">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1588">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" P98-2243 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1589">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic analysis, which aims at identifying the topics of text, delimiting their extend and finding the relations between the resulting segments, has recently raised an important interest.
</prevsent>
<prevsent>the largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the tdt (topic detection and tracking) initiative (fiscus et al, 1999), which addresses all the tasks we have mentioned but from domain-dependent view point and not necessarily in an integrated way.systems that implement this work can be categorized according to what kind of knowledge they use.
</prevsent>
</prevsection>
<citsent citstr=" P99-1077 ">
most of those that achieve text segmentation only relyon the intrinsic characteristics of texts: word distribution, as in (hearst, 1997), (<papid> J97-1003 </papid>choi,2000) <papid> A00-2004 </papid>and (utiyama and isahara, 2001), <papid> P01-1064 </papid>or linguistic cues as in (passonneau and litman, 1997).<papid> J97-1005 </papid>they can be applied without restriction about domains but have low results when text doesnt characterize its topical structure by surface clues.some systems exploit domain-independent knowledge about lexical cohesion: network of words built from dictionary in (kozima, 1993); <papid> P93-1041 </papid>large set of collocations collected from corpus in (fer ret, 1998), (<papid> P98-2243 </papid>kaufmann, 1999) <papid> P99-1077 </papid>and (choi, 2001).</citsent>
<aftsection>
<nextsent>tosome extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains.
</nextsent>
<nextsent>the last main type of systems relies on knowledge about the topics they may encounter in the texts they process.
</nextsent>
<nextsent>this is typically the kind of approach developed in tdt where this knowledge is automatically built from set of reference texts.
</nextsent>
<nextsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1591">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the work of bigi (bigi et al, 1998) stands in the same perspective but focuses on much larger topics than tdt.
</prevsent>
<prevsent>these systems have limited scope due to their topic representations but they are also more precise for the same reason.
</prevsent>
</prevsection>
<citsent citstr=" P98-1100 ">
hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such combination: (jobbins andevett, 1998) <papid> P98-1100 </papid>combined word recurrence, collocations and thesaurus; (beeferman et al, 1999) relied on both collocations and linguistic cues.</citsent>
<aftsection>
<nextsent>the topic analysis we propose implements such hybrid approach: it relies on general language resource, collocation network, but exploits it together with word recurrence in texts.
</nextsent>
<nextsent>moreover, it simultaneously achieves topic segmentation andlink detection, i.e. determining whether two segments discuss the same topic.
</nextsent>
<nextsent>we detail in this paper the implementation of this analysis by the topicoll system, we report evaluations of its capabilities concerning segmentation for two languages, french and english, and finally, we propose an evaluation measure that integrates both segmentation and link detection.
</nextsent>
<nextsent>in accordance with much work about discourse analysis, topicoll processes texts linearly: it detects topic shifts and finds links between segments without delaying its decision, i.e., by only taking into account the part of text that has been already analyzed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1598">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> collocation networks.  </section>
<citcontext>
<prevsection>
<prevsent>first, the initial corpus was preprocessed in order to characterize texts by their topically significant words.
</prevsent>
<prevsent>thus, we retained only the lemmatized form of plain words, that is, nouns, verbs and adjectives.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
collocations were extracted according to the method described in (church and hanks, 1990) <papid> J90-1003 </papid>by moving window on texts.</citsent>
<aftsection>
<nextsent>parameters were chosen in order to catch topical relations: the window was rather large, 20-word wide, and took into account the boundaries of texts; moreover, collocations were indifferent to word order.
</nextsent>
<nextsent>we also adopted an evaluation of mutual information as cohesion measure of eachcollocation.
</nextsent>
<nextsent>this measure was normalized according to the maximal mutual information relative to the considered corpus.
</nextsent>
<nextsent>after filtering the less significant collocations (collocations with less than 10 occurrences and cohesion lower than 0.1), we got network with approximately 23,000 words and 5.2 million collocations for french, 30,000 words and 4.8 million collocations for english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1615">
<title id=" C02-1033.xml">using collocations for topic segmentation and link detection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>each column of table 3 states for an interval of values for n. systems 3-11 3-5 6-8 9-11 baseline 0.45 0.38 0.39 0.36 cwm 0.09 0.10 0.07 0.05 u00 0.10 0.09 0.07 0.05 c99 0.12 0.11 0.09 0.09 dotplot 0.18 0.20 0.15 0.12 segmenter 0.36 0.23 0.33 0.43 text tiling 0.46 0.44 0.43 0.48 topicoll1 0.30 0.28 0.27 0.34 topicoll2 0.31 0.28 0.28 0.34 table 3 ? pk for c99 corpus the first seven lines of table 3 results from choisexperiments (choi, 2001).
</prevsent>
<prevsent>the baseline is procedure that partitions document into 10 segments of equal length.
</prevsent>
</prevsection>
<citsent citstr=" W98-1123 ">
cwm is described in (choi, 2001), u00 in (utiyama and isahara, 2001), <papid> P01-1064 </papid>c99 in (choi, 2000), <papid> A00-2004 </papid>dotplot in (reynar, 1998) and segmenter in (kan et al, 1998).<papid> W98-1123 </papid></citsent>
<aftsection>
<nextsent>table 3 confirms first that the link detection partof topicoll doesnt debase its segmentation capabilities.
</nextsent>
<nextsent>it also shows that topicolls results on this corpus are significantly lower than its results on the le monde corpus.
</nextsent>
<nextsent>this is partially due to our collocation network for english: its density, i.e. the ratio between the size of its vocabulary and its number of collocations, is 30% lower than the density of the network for french, which has certainly significant effect.
</nextsent>
<nextsent>table 3 also shows that topicoll has worse results than systems such as cwm, u00, c99 or dotplot.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1616">
<title id=" C02-1077.xml">improved iterative scaling can yield multiple globally optimal models with radically differing performance levels </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>experimental results,from the domain of parse selection for stochastic attribute value grammars, shows the wide variation in performance that can be found when estimating models using iis.
</prevsent>
<prevsent>further results show that the in uence of the initial model can be diminished by selecting either uniform weights, or else by model averaging.
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
when statistically modelling linguistic phenomena of one sort or another, researchers typically loglinear models to the data (for example (johnson et al., 1999)).<papid> P99-1069 </papid></citsent>
<aftsection>
<nextsent>there are (at least) three reasons forthe popularity of such models: they do not make unwarranted independence assumptions, the maximum likelihood solution of such models coincides with the maximum entropy solution, and nally, they can beeciently estimated (using algorithms such as improved iterative scaling (iis) (laerty et al, 1997)).
</nextsent>
<nextsent>now, the solution found by iis is guaranteed to approach global maximum for both likelihood and entropy under certain conditions.
</nextsent>
<nextsent>although this is appealing, in realistic situations it turns out that multiple models exist, all of which are equivalent in terms of likelihood but dierent from each other interms of their performance at some task.
</nextsent>
<nextsent>in particular, the initial weight settings can in uence the quality of the nal model, even though this nal model is the maximum entropy solution (as found by iis).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1617">
<title id=" C02-1077.xml">improved iterative scaling can yield multiple globally optimal models with radically differing performance levels </title>
<section> log-linear modelling of.  </section>
<citcontext>
<prevsection>
<prevsent>5 the exact behaviour of the algorithm will depend on the training data, so this is not the only imaginable outcome, but it is certainly plausible one.
</prevsent>
<prevsent>attribute-value grammars here we show how attribute-value grammars may be modelled using log-linear models.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
abney gives fuller details (abney, 1997).<papid> J97-4005 </papid></citsent>
<aftsection>
<nextsent>let be an attribute-value grammar, and a set of sentences within the string-set de ned by l(g).
</nextsent>
<nextsent>a log-linear model, , consist of two components: set of features, and set of weights, .
</nextsent>
<nextsent>the (unnormalised) total weight of parse x, (x), is function of the features that are `active  on parse: (x) = exp( x i=1  f (x)) (6) the probability of parse, (x m), is simply the result of normalising the total weight associated with that parse: (x jm) = 1 (x) (7) = y2 (y) (8) is the union of the set of parses assigned to each sentence in by the grammar g, such that each parse in is unique in terms of the features that are active on it.
</nextsent>
<nextsent>normally parse can be viewed as the set of features that are active on it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1618">
<title id=" C02-1077.xml">improved iterative scaling can yield multiple globally optimal models with radically differing performance levels </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this consisted of sample of 53795 parses (produced from sentences at most 15 tokens long, with at most 15 parses per sentence).
</prevsent>
<prevsent>the sentences were drawn from the parsed wall street journal, and all could be parsed using our grammar.
</prevsent>
</prevsection>
<citsent citstr=" C00-1085 ">
the motivation for this choice of training set came from the fact that when the sample of sentences is too small, the resulting model will tend to under t. likewise, when the training set is too large, the model will tend to over t. sample of appropriate size (which can be found using simple search, as osborne (2000) <papid> C00-1085 </papid>demon strated) will therefore neither signi cantly under tnor over t. quite apart from estimation issues related to sample size, because we repeatedly estimate models, using sample that is just suciently large(and no larger) allows us to make signi cant computational savings.</citsent>
<aftsection>
<nextsent>we used disjoint development set and testing set.
</nextsent>
<nextsent>the development set consisted of 2620 parses, derived from parsing sentences at most 30 tokens long, with at most 100 parses per sentence.
</nextsent>
<nextsent>the testing set was randomly sampled from the wall street journal, and consisted of 469 sentences, with each sentence at most 30 tokens long, with at most 100 parses per sentence.
</nextsent>
<nextsent>each sentence has on average 60:0 parses per sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1619">
<title id=" C02-1077.xml">improved iterative scaling can yield multiple globally optimal models with radically differing performance levels </title>
<section> comments.  </section>
<citcontext>
<prevsection>
<prevsent> model averaging can also cancel out variations caused by particular choice of initial settings.
</prevsent>
<prevsent>however, this implies greater computational burden as iis will need to be run many times in order to gain representative sample of models.
</prevsent>
</prevsection>
<citsent citstr=" W00-0709 ">
the number of features in the model could be reduced using feature selection methods (for example (mullen and osborne, 2000)).<papid> W00-0709 </papid>although iis is useful tool for estimating loglinear models, we have since moved-on to estimating models using limited-memory variable-metric methods (malouf, 2002).<papid> W02-2018 </papid></citsent>
<aftsection>
<nextsent>our ndings show that convergence, for range of problems, is faster.
</nextsent>
<nextsent>an interesting question is seeing the extent to which other numerical methods for estimating log-linear models are sensitive to initial parameter values.
</nextsent>
<nextsent>finally, it should be noted that our theoretical results apply toa more general setting than that of log-linear models trained using the iis algorithm.
</nextsent>
<nextsent>the problem of overlapping features could in principle occur in any situation in which model has linear combination of features, and `hill-climbing  algorithm is used to seek maximum-likelihood solution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1621">
<title id=" C02-1077.xml">improved iterative scaling can yield multiple globally optimal models with radically differing performance levels </title>
<section> comments.  </section>
<citcontext>
<prevsection>
<prevsent> model averaging can also cancel out variations caused by particular choice of initial settings.
</prevsent>
<prevsent>however, this implies greater computational burden as iis will need to be run many times in order to gain representative sample of models.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
the number of features in the model could be reduced using feature selection methods (for example (mullen and osborne, 2000)).<papid> W00-0709 </papid>although iis is useful tool for estimating loglinear models, we have since moved-on to estimating models using limited-memory variable-metric methods (malouf, 2002).<papid> W02-2018 </papid></citsent>
<aftsection>
<nextsent>our ndings show that convergence, for range of problems, is faster.
</nextsent>
<nextsent>an interesting question is seeing the extent to which other numerical methods for estimating log-linear models are sensitive to initial parameter values.
</nextsent>
<nextsent>finally, it should be noted that our theoretical results apply toa more general setting than that of log-linear models trained using the iis algorithm.
</nextsent>
<nextsent>the problem of overlapping features could in principle occur in any situation in which model has linear combination of features, and `hill-climbing  algorithm is used to seek maximum-likelihood solution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1622">
<title id=" C04-1201.xml">a language independent method for question classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly, the advantage of such classification relies on having the ability of extracting from the documents such instances.
</prevsent>
<prevsent>in other words, good question classification module may be useless if we lackan accurate named entity extractor for the document collection.results of the error analysis of an open domain qa system showed that 36.4% of the errors were generated by the question classification module (moldovan et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
thus it is not surprising that an increasing interest has arisen aimed at developing accurate question classifiers (zhang and lee, 2003; li and roth, 2002; <papid> C02-1150 </papid>suzuki et al, 2003).<papid> W03-1208 </papid></citsent>
<aftsection>
<nextsent>however, most of these approaches are targeted to the englishlanguage.
</nextsent>
<nextsent>besides, the machine learning algorithms used are trained on features extracted by natural language processing tools that are language dependent, and for some languages these tools are not available.
</nextsent>
<nextsent>this implies that if we want to reproduce the results of these methods in different language we need first to solve the problem of making available the appropriate analyzers in the given language.we present here flexible method for question classification.
</nextsent>
<nextsent>we claim that the method is language-independent since no complex natural language processing tools are needed; we use plain lexical features that can be extracted automatically from the questions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1623">
<title id=" C04-1201.xml">a language independent method for question classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>clearly, the advantage of such classification relies on having the ability of extracting from the documents such instances.
</prevsent>
<prevsent>in other words, good question classification module may be useless if we lackan accurate named entity extractor for the document collection.results of the error analysis of an open domain qa system showed that 36.4% of the errors were generated by the question classification module (moldovan et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-1208 ">
thus it is not surprising that an increasing interest has arisen aimed at developing accurate question classifiers (zhang and lee, 2003; li and roth, 2002; <papid> C02-1150 </papid>suzuki et al, 2003).<papid> W03-1208 </papid></citsent>
<aftsection>
<nextsent>however, most of these approaches are targeted to the englishlanguage.
</nextsent>
<nextsent>besides, the machine learning algorithms used are trained on features extracted by natural language processing tools that are language dependent, and for some languages these tools are not available.
</nextsent>
<nextsent>this implies that if we want to reproduce the results of these methods in different language we need first to solve the problem of making available the appropriate analyzers in the given language.we present here flexible method for question classification.
</nextsent>
<nextsent>we claim that the method is language-independent since no complex natural language processing tools are needed; we use plain lexical features that can be extracted automatically from the questions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1628">
<title id=" C04-1201.xml">a language independent method for question classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they experimented with 68question types and compared performance of using bag-of-words against using more elaborated combinations of attributes, namely named entities and semantic information.
</prevsent>
<prevsent>their best results, an accuracy of 94.8% at the first level of the hierarchy, were obtained when using svm trained on bag-of-words together with named entities and semantic information.the idea of using the internet in qa system is not new.
</prevsent>
</prevsection>
<citsent citstr=" W02-1033 ">
what is new, however, is that we are using the internet to obtain values for features in our question classification process, as opposed to previous approaches where the redundancy of information available on the internet has been used in the answer extraction process (brill et al, 2002; <papid> W02-1033 </papid>lin et al, 2002; katz et al, 2003).</citsent>
<aftsection>
<nextsent>question classification is very similar to text classification.
</nextsent>
<nextsent>one thing they have in common is that in both cases we need to assign class, from finite set of possible classes, to natural language text.
</nextsent>
<nextsent>another similarity is attribute information; what has been used as attributes for text classification can also be extracted and used in question classification.
</nextsent>
<nextsent>finally, in both cases we have high dimensional attributes: if we want to use the bag-of-words approach, we will face the problem of having very large attribute sets.an important difference is that question classification introduces the problem of dealing with short sentences, compared with text documents, and thus we have less information available on each question instance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1629">
<title id=" C04-1201.xml">a language independent method for question classification </title>
<section> learning question classifiers.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 using internet.
</prevsent>
<prevsent>as kilgarriff and grefenstette wrote, the internet is fabulous linguists?
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
playground (kil garriff and grefenstette, 2003).<papid> J03-3001 </papid></citsent>
<aftsection>
<nextsent>it has become the greatest information source available world wide, and although english is the dominant language represented on the internet it is very likely that one can find information in almost any desired language.
</nextsent>
<nextsent>considering this, and thefact that the texts are written in natural language, we believe that new methods that take advantage of this large corpus must be devised.in this work we propose using the internet in order to acquire information that can be used as attributes in our classification problem.
</nextsent>
<nextsent>this attribute information can be extracted automatically from the web and the goal is to provide an estimate about the possible semantic class of the question.
</nextsent>
<nextsent>the procedure for gathering this information from the web is as follows: we use set of heuristics to extract from the question word w, or set of words, that will complement the queries submitted for the search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1632">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>specifically, we increase the recall of an error detection method by abstracting theword to be disambiguated to representation containing information about some of its inherent properties, namely the set of categories it can potentially have.
</prevsent>
<prevsent>this work thus provides insights into the relation of corpus categories to categories derived from local contexts.
</prevsent>
</prevsection>
<citsent citstr=" E03-1009 ">
category induction techniques generally relyon local contexts, i.e., surrounding words, to cluster word types together (e.g., clark, 2003; <papid> E03-1009 </papid>schutze, 1995), using information of kind also found in human category acquisition tasks (e.g., mintz, 2002, 2003).</citsent>
<aftsection>
<nextsent>such information is also at the core of standard part-of-speech (pos) tagging, or disambiguation, methods (see, e.g., manning and schutze, 1999, ch.
</nextsent>
<nextsent>10), with the contexts generally abstracted to pos tags.
</nextsent>
<nextsent>the contextual information is similar in both tasks because induction is founded in part upon the notion that local contexts are useful for disambiguation: one morphosyntac tically clusters words which should have the same category in the same contexts.
</nextsent>
<nextsent>but which contexts count as being the same??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1633">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>as outlined in sections 2 and 3, we can use such studies as starting point for generalizing error detection.
</prevsent>
<prevsent>2.1 the variation n-gram method.
</prevsent>
</prevsection>
<citsent citstr=" E03-1068 ">
the error detection method we build from is the variation n-gram method (dickinson and meurers, 2003; <papid> E03-1068 </papid>dickinson, 2005).</citsent>
<aftsection>
<nextsent>the approach detects items which occur multiple times in the corpus with varying annotation, the so-called variation nuclei.
</nextsent>
<nextsent>a nucleus with its repeated surrounding context is referred to as variation n-gram.every detected variation in the annotation of nucleus is classified as an error or genuine ambiguity using basic heuristic requiring at least one word of context on each side of the nucleus.
</nextsent>
<nextsent>for example, in the wsj corpus, part of the penn treebank 3 release (marcus et al, 1993), <papid> J93-2004 </papid>the string in (1) is variation 12-gram since off is variation nucleus that is tagged preposition (in)in one corpus occurrence and particle (rp) in an other.</nextsent>
<nextsent>1 dickinson (2005) shows that examining those cases with identical local context in this case, looking at ward off aresults in an estimated error detection precision of 92.5%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1634">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the approach detects items which occur multiple times in the corpus with varying annotation, the so-called variation nuclei.
</prevsent>
<prevsent>a nucleus with its repeated surrounding context is referred to as variation n-gram.every detected variation in the annotation of nucleus is classified as an error or genuine ambiguity using basic heuristic requiring at least one word of context on each side of the nucleus.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for example, in the wsj corpus, part of the penn treebank 3 release (marcus et al, 1993), <papid> J93-2004 </papid>the string in (1) is variation 12-gram since off is variation nucleus that is tagged preposition (in)in one corpus occurrence and particle (rp) in an other.</citsent>
<aftsection>
<nextsent>1 dickinson (2005) shows that examining those cases with identical local context in this case, looking at ward off aresults in an estimated error detection precision of 92.5%.
</nextsent>
<nextsent>(1) toward off hostile takeover attempt by two european shipping concerns this method can be applied to syntactic annotation, and for this annotation, one can increase the recall of errors found by abstracting the nuclei to pos tags (boyd et al, 2007).
</nextsent>
<nextsent>clearly, this is nota feasible abstraction here, given that we are attempting to detect errors in pos annotation.
</nextsent>
<nextsent>1 to distinguish variation nuclei, we shade them in gray and underline the immediately surrounding context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1636">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> an appropriate level of abstraction.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 complete ambiguity classes.
</prevsent>
<prevsent>ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
4and ambiguity classes have been shown to be successfully employed, in variety of ways, to improve pos tagging (e.g., cutting et al, 1992; <papid> A92-1018 </papid>daelemans et al, 1996; <papid> W96-0102 </papid>dickinson, 2007; goldberg et al, 2008; <papid> P08-1085 </papid>tseng et al, 2005).</citsent>
<aftsection>
<nextsent>only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context.
</nextsent>
<nextsent>as an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3).
</nextsent>
<nextsent>there are at least 27 4 one could group affixes by ambiguity class for languages like chinese (cf.
</nextsent>
<nextsent>ctbmorph features in tseng et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1637">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> an appropriate level of abstraction.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 complete ambiguity classes.
</prevsent>
<prevsent>ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
4and ambiguity classes have been shown to be successfully employed, in variety of ways, to improve pos tagging (e.g., cutting et al, 1992; <papid> A92-1018 </papid>daelemans et al, 1996; <papid> W96-0102 </papid>dickinson, 2007; goldberg et al, 2008; <papid> P08-1085 </papid>tseng et al, 2005).</citsent>
<aftsection>
<nextsent>only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context.
</nextsent>
<nextsent>as an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3).
</nextsent>
<nextsent>there are at least 27 4 one could group affixes by ambiguity class for languages like chinese (cf.
</nextsent>
<nextsent>ctbmorph features in tseng et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1638">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> an appropriate level of abstraction.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 complete ambiguity classes.
</prevsent>
<prevsent>ambiguity classes capture the relevant property we are interested in: words with the same category possibilities are grouped together.
</prevsent>
</prevsection>
<citsent citstr=" P08-1085 ">
4and ambiguity classes have been shown to be successfully employed, in variety of ways, to improve pos tagging (e.g., cutting et al, 1992; <papid> A92-1018 </papid>daelemans et al, 1996; <papid> W96-0102 </papid>dickinson, 2007; goldberg et al, 2008; <papid> P08-1085 </papid>tseng et al, 2005).</citsent>
<aftsection>
<nextsent>only certain words can take one of two (or more) tags, and these should be disambiguated in the same way in context.
</nextsent>
<nextsent>as an example of how using ambiguity classes as variation nuclei can increase recall, consider the frame being by in example (3).
</nextsent>
<nextsent>there are at least 27 4 one could group affixes by ambiguity class for languages like chinese (cf.
</nextsent>
<nextsent>ctbmorph features in tseng et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1640">
<title id=" C08-1026.xml">representations for category disambiguation </title>
<section> representations for disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>we have shown that local lexical context provides generally unambiguous context for corpus tags, given sufficient information about the word to be disambiguated.
</prevsent>
<prevsent>the information need not be very abstract, either: frames using ambiguity class nuclei only require words category possibilities.
</prevsent>
</prevsection>
<citsent citstr=" C04-1080 ">
even for many unsupervised situations, this is available from lexicon (e.g., banko and moore, 2004; <papid> C04-1080 </papid>goldberg et al, 2008).<papid> P08-1085 </papid></citsent>
<aftsection>
<nextsent>we have only looked at cases with variation in tagging; fully gauging the accuracy of such data representation for disambiguation requires more of the framed nuclei from the corpus, including those without variation.
</nextsent>
<nextsent>for this, we could take all framed nuclei from corpus and compare the level of ambiguity for differing abstractions.
</nextsent>
<nextsent>how ever, most framed nuclei occur only once, and it is not clear how meaningful it is to say that these are unambiguous.
</nextsent>
<nextsent>thus, we examine framed nuclei which occur at least twice and report in table 1 206 for the wsj how unambiguous particular level of nucleus abstraction is. 5 abstraction unamb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1646">
<title id=" C02-2022.xml">cross linguistic phoneme correspondences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe the metaphoneme inventory defined for dutch, english and german, comparing the results for vowels and consonants.
</prevsent>
<prevsent>we also describe some of the unexpected information that arose from the analysis of cognate forms we undertook to find the meta phoneme correspondences.
</prevsent>
</prevsection>
<citsent citstr=" C00-2171 ">
tiberius and cahill (2000) <papid> C00-2171 </papid>presented the theory ofcross-linguistic phoneme correspondences (metaphonemes) with an example pilot study of the vowels of dutch, english and german.</citsent>
<aftsection>
<nextsent>the aim of this work is to allow the type of generalisation that is permitted by the use of phonemes with allophonic variation to be taken one level higher, i.e. above the level of the single language.
</nextsent>
<nextsent>the idea behind it is to represent the near-identities that closely related languages such as dutch, english and german so often share.
</nextsent>
<nextsent>for example, the dutch word kat?
</nextsent>
<nextsent>/kat/ has the english equivalent cat?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1655">
<title id=" C02-1125.xml">a measure of term representativeness based on the number of cooccurring salient words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>representativeness (see section 1).
</prevsent>
<prevsent>in this paper we employ the basic idea of the baseline method?
</prevsent>
</prevsection>
<citsent citstr=" C00-1047 ">
proposed by hisamitsu (hisamitsu et al 2000).<papid> C00-1047 </papid></citsent>
<aftsection>
<nextsent>the idea is that the distribution of words co-occurring with representative term should be biased according to the word distribution of the whole corpus.
</nextsent>
<nextsent>concretely, for any term and any measure for the degree of bias of word occurrences in d(t), set of words co-occurring with t, according to those of the whole corpus d0, the baseline method defines representativeness of term by normalizing m(d(t)).
</nextsent>
<nextsent>in what follows, d0 is an archive of newspaper articles and d(t) is defined as the set of all articles containing t. the normalization of m(d(t)) is done by function bm, called the baseline function, which estimates the value of m(drand) using #drand for any randomly sampled document (in our case, article?)
</nextsent>
<nextsent>set drand, where #drand stands for the total number of words contained in drand.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1657">
<title id=" C02-1125.xml">a measure of term representativeness based on the number of cooccurring salient words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>among number of measures introduced there, the most commonly used one is tf-idf proposed by salton et al there are variety of modifications of tf-idf (for example, singhal et al 1996) but all share the basic feature that word appearing more frequently in fewer documents is assigned higher value.
</prevsent>
<prevsent>in nlp domains several measures concentrating on the unithood of word sequence have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
for instance, the mutual information (church et al 1990) and log-likelihood ratio (dunning 1993; <papid> J93-1003 </papid>cohen 1995) have been widely used for extracting word bigrams.</citsent>
<aftsection>
<nextsent>some measures for termhood have also been proposed, such as imp (nakagawa 2000), c-value and nc-value (mima et al. 2000).
</nextsent>
<nextsent>although certain existing measures are widely used, they have major problems as follows: (1) classical measures such as tf-idf are so sensitive to term frequencies that they fail to avoid uninformative words that occur very frequently; (2) measures based on unithood cannot handle single-word terms; and (3) the threshold value for term to be considered as being representative is difficult to define or can only be defined in an ad hoc manner.
</nextsent>
<nextsent>it is reported that measures defined by the baseline method do not have these problems (hisamitsu et al 2000).<papid> C00-1047 </papid></nextsent>
<nextsent>1.2 baseline method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1664">
<title id=" C04-1130.xml">trajectory based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kilgarriff and rosenzweig (2000) presented the first empirical study.
</prevsent>
<prevsent>they combined the output of the participating senseval1 systems via simple voting.
</prevsent>
</prevsection>
<citsent citstr=" A00-2009 ">
pedersen (2000) <papid> A00-2009 </papid>built an ensemble of nave bayesian classifiers, each of which is based on lexical features that represent co-occurring words in varying sized windows of context.</citsent>
<aftsection>
<nextsent>the sense that receives majority of the votes was assigned as the final selection.
</nextsent>
<nextsent>stevenson and wilks (2001) presented classifier combination framework where three different disambiguation modules were combined using memory-based approach.
</nextsent>
<nextsent>hoste et al (2002) used word experts consisted of four memory-based learners trained on different context.
</nextsent>
<nextsent>output of the word experts is based on majority voting or weighted voting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1665">
<title id=" C04-1130.xml">trajectory based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hoste et al (2002) used word experts consisted of four memory-based learners trained on different context.
</prevsent>
<prevsent>output of the word experts is based on majority voting or weighted voting.
</prevsent>
</prevsection>
<citsent citstr=" W02-1004 ">
florian et al(2002) and florian and yarowsky (2002) <papid> W02-1004 </papid>used six different classifiers as components of their combination.</citsent>
<aftsection>
<nextsent>they compared several different strategies of combination, which include combining the posterior distribution, combination based on order statistics and several different voting.
</nextsent>
<nextsent>klein et al (2002) <papid> W02-0811 </papid>combined number of different first-order classifiers using majority voting, weighted voting and maximum entropy.</nextsent>
<nextsent>in park (2003), committee of classifiers was used to learn from the unlabeled examples.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1666">
<title id=" C04-1130.xml">trajectory based word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>florian et al(2002) and florian and yarowsky (2002) <papid> W02-1004 </papid>used six different classifiers as components of their combination.</prevsent>
<prevsent>they compared several different strategies of combination, which include combining the posterior distribution, combination based on order statistics and several different voting.</prevsent>
</prevsection>
<citsent citstr=" W02-0811 ">
klein et al (2002) <papid> W02-0811 </papid>combined number of different first-order classifiers using majority voting, weighted voting and maximum entropy.</citsent>
<aftsection>
<nextsent>in park (2003), committee of classifiers was used to learn from the unlabeled examples.
</nextsent>
<nextsent>the label of an unlabeled example is predicted by weighted majority voting.
</nextsent>
<nextsent>frank at al.
</nextsent>
<nextsent>(2003) presented locally weighted nave bayesian model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1667">
<title id=" C04-1130.xml">trajectory based word sense disambiguation </title>
<section> the trajectory of sense selection.  </section>
<citcontext>
<prevsection>
<prevsent>what is the best window size for wsd has been long for problem.
</prevsent>
<prevsent>weaver (1955) hoped we could find minimum value of the window size which can lead to the correct choice of sense for the target ambiguous word.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
yarowsky (1994) <papid> P94-1013 </papid>argued the optimal value is sensitive to the type of ambiguity.</citsent>
<aftsection>
<nextsent>semantic or topic-based ambiguities warrant larger window (from 20 to 50), while more local syntactic ambiguities warrant smaller window (3 or 4).
</nextsent>
<nextsent>leacock at el (1998) showed the local context is superior to topical context as an indicator of word sense.
</nextsent>
<nextsent>yarowsky (2002) suggested that different algorithms prefer different window sizes.
</nextsent>
<nextsent>followed by these works, it is clear that different window sizes might cause different sense selection for an occurrence of the target word even when same algorithm is used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1668">
<title id=" C04-1130.xml">trajectory based word sense disambiguation </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in order to do comparative study, we have implemented not only our algorithm, but also four other related algorithms in our experiments.
</prevsent>
<prevsent>they fall into two classes.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
nb (manning and schutze 1999) and knn (ng and lee 1996) <papid> P96-1006 </papid>are two components of our approach.</citsent>
<aftsection>
<nextsent>locally weighted nb(lwnb, frank et al 2003) and ensemble nb(enb pedersen 2000) <papid> A00-2009 </papid>are two com binational approaches.</nextsent>
<nextsent>since our aim is to compare not only the performance but also the robustness of these algorithms, we implemented each algorithm in following way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1673">
<title id=" C02-1090.xml">taxonomy learning  factoring the structure of a taxonomy into a semantic classification decision </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specifics of the task are big number of classes into which new words need to be classified and hence lot of poorly predictable semantic distinctions that have to be taken into account.
</prevsent>
<prevsent>for this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task.
</prevsent>
</prevsection>
<citsent citstr=" W93-0113 ">
its previous applications (e.g., grefenstette 1993, <papid> W93-0113 </papid>hearst and schuetze 1993, taku naga et al 1997, lin 1998, <papid> P98-2127 </papid>caraballo 1999) <papid> P99-1016 </papid>demonstrated that cooccurrence statistics on target word is often sufficient for its automatical classification into one of numerous classes such as synsets of wordnet.</citsent>
<aftsection>
<nextsent>distributional techniques, however, are poorly applicable to rare words, i.e., those words for which corpus does not contain enough cooccurrence data to judge about their meaning.
</nextsent>
<nextsent>such words are the primary concern of many practical nlp applications: as rule, they are semantically focused words and carry lot of important information.
</nextsent>
<nextsent>if one has to do with specific domain of lexicon, sparse data is problem particularly difficult to overcome.
</nextsent>
<nextsent>the major challenge for the application of the distributional approach in this area is, therefore, the development of ways to minimize the amount of corpus data required to successfully carry out task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1674">
<title id=" C02-1090.xml">taxonomy learning  factoring the structure of a taxonomy into a semantic classification decision </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specifics of the task are big number of classes into which new words need to be classified and hence lot of poorly predictable semantic distinctions that have to be taken into account.
</prevsent>
<prevsent>for this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
its previous applications (e.g., grefenstette 1993, <papid> W93-0113 </papid>hearst and schuetze 1993, taku naga et al 1997, lin 1998, <papid> P98-2127 </papid>caraballo 1999) <papid> P99-1016 </papid>demonstrated that cooccurrence statistics on target word is often sufficient for its automatical classification into one of numerous classes such as synsets of wordnet.</citsent>
<aftsection>
<nextsent>distributional techniques, however, are poorly applicable to rare words, i.e., those words for which corpus does not contain enough cooccurrence data to judge about their meaning.
</nextsent>
<nextsent>such words are the primary concern of many practical nlp applications: as rule, they are semantically focused words and carry lot of important information.
</nextsent>
<nextsent>if one has to do with specific domain of lexicon, sparse data is problem particularly difficult to overcome.
</nextsent>
<nextsent>the major challenge for the application of the distributional approach in this area is, therefore, the development of ways to minimize the amount of corpus data required to successfully carry out task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1675">
<title id=" C02-1090.xml">taxonomy learning  factoring the structure of a taxonomy into a semantic classification decision </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the specifics of the task are big number of classes into which new words need to be classified and hence lot of poorly predictable semantic distinctions that have to be taken into account.
</prevsent>
<prevsent>for this reason, knowledge-poor approaches such as the distributional approach are particularly suited for this task.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
its previous applications (e.g., grefenstette 1993, <papid> W93-0113 </papid>hearst and schuetze 1993, taku naga et al 1997, lin 1998, <papid> P98-2127 </papid>caraballo 1999) <papid> P99-1016 </papid>demonstrated that cooccurrence statistics on target word is often sufficient for its automatical classification into one of numerous classes such as synsets of wordnet.</citsent>
<aftsection>
<nextsent>distributional techniques, however, are poorly applicable to rare words, i.e., those words for which corpus does not contain enough cooccurrence data to judge about their meaning.
</nextsent>
<nextsent>such words are the primary concern of many practical nlp applications: as rule, they are semantically focused words and carry lot of important information.
</nextsent>
<nextsent>if one has to do with specific domain of lexicon, sparse data is problem particularly difficult to overcome.
</nextsent>
<nextsent>the major challenge for the application of the distributional approach in this area is, therefore, the development of ways to minimize the amount of corpus data required to successfully carry out task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1676">
<title id=" C02-1090.xml">taxonomy learning  factoring the structure of a taxonomy into a semantic classification decision </title>
<section> classification methods.  </section>
<citcontext>
<prevsection>
<prevsent>thus the number of calculations is reduced to the number of classes.
</prevsent>
<prevsent>thereby, class representation may be derived from set of vectors corresponding to one synonym set (as is done by taku naga et al  1997) or set of vectors corresponding to synonym set and some or all subordinate synonym sets (resnik 1992).
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
another way to prepare representation of word class is what may be called the centro id based approach (e.g., pereira et al  1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>it is almost exactly like the category-based method, the only difference being that class vector is computed slightly differently.
</nextsent>
<nextsent>all vectors corresponding to class members are added up and the resulting vector is divided by to compute the centro id between the vectors.
</nextsent>
<nextsent>saurus the classification methods described above presuppose that semantic classes being augmented exist independently of each other.
</nextsent>
<nextsent>for most existing thesauri this is not the case: they typically encode taxonomic relations between word classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1677">
<title id=" C02-1090.xml">taxonomy learning  factoring the structure of a taxonomy into a semantic classification decision </title>
<section> classification methods.  </section>
<citcontext>
<prevsection>
<prevsent>the obtained frequencies of cooccurrence were weighted by the 1+log weight function.
</prevsent>
<prevsent>the distributional similarity was measured by means of three different similarity measures: the jac cards coefficient, l1 distance, and the skew divergence.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
this choice of similarity measures was motivated by results of studies by (levy et al  1998) and (lee 1999) <papid> P99-1004 </papid>which compared several well known measures on similar tasks and found these three to be superior to many others.</citsent>
<aftsection>
<nextsent>another reason for this choice is that there are different ideas underlying these measures: while the jac cards coefficient is binary measure, l1 and the skew divergence are probabilistic, the former being geometrically motivated and the latter being version of the information theoretic kull back leibler divergence (cf., lee 1999).<papid> P99-1004 </papid></nextsent>
<nextsent>5.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1679">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we offer this result on standard data as reference-point for evaluations of ordering in sentence realization.
</prevsent>
<prevsent>word and constituent order play crucial role in establishing the fluency and intelligibility of sentence.
</prevsent>
</prevsection>
<citsent citstr=" W01-0808 ">
in some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, aikawa et al (2001) <papid> W01-0808 </papid>and reiter and dale (2000)).</citsent>
<aftsection>
<nextsent>in contrast, the nitrogen (langkilde and knight, 1998<papid> W98-1426 </papid>a, 1998b) system employs word n-gram language model to choose among large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection.</nextsent>
<nextsent>nitro gens model does not take into consideration any non-surface linguistic features available during realization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1680">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word and constituent order play crucial role in establishing the fluency and intelligibility of sentence.
</prevsent>
<prevsent>in some systems, establishing order during the sentence realization stage of natural language generation has been accomplished by hand-crafted generation grammars in the past (see for example, aikawa et al (2001) <papid> W01-0808 </papid>and reiter and dale (2000)).</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
in contrast, the nitrogen (langkilde and knight, 1998<papid> W98-1426 </papid>a, 1998b) system employs word n-gram language model to choose among large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection.</citsent>
<aftsection>
<nextsent>nitro gens model does not take into consideration any non-surface linguistic features available during realization.
</nextsent>
<nextsent>the fergus system (bangalore and rambow, 2000) <papid> C00-1007 </papid>employs statistical tree model to select probable trees and word n-gram model to rank the string candidates generated from the best trees.</nextsent>
<nextsent>like nitrogen, the halogen system (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from packed forest by constraining the search first within the scope of each constituent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1682">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the nitrogen (langkilde and knight, 1998<papid> W98-1426 </papid>a, 1998b) system employs word n-gram language model to choose among large set of word sequence candidates which vary in constituent order, word order, lexical choice, and morphological inflection.</prevsent>
<prevsent>nitro gens model does not take into consideration any non-surface linguistic features available during realization.</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
the fergus system (bangalore and rambow, 2000) <papid> C00-1007 </papid>employs statistical tree model to select probable trees and word n-gram model to rank the string candidates generated from the best trees.</citsent>
<aftsection>
<nextsent>like nitrogen, the halogen system (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from packed forest by constraining the search first within the scope of each constituent.</nextsent>
<nextsent>our research is carried out within the amalgam broad coverage sentence realization system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1683">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nitro gens model does not take into consideration any non-surface linguistic features available during realization.
</prevsent>
<prevsent>the fergus system (bangalore and rambow, 2000) <papid> C00-1007 </papid>employs statistical tree model to select probable trees and word n-gram model to rank the string candidates generated from the best trees.</prevsent>
</prevsection>
<citsent citstr=" A00-2023 ">
like nitrogen, the halogen system (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from packed forest by constraining the search first within the scope of each constituent.</citsent>
<aftsection>
<nextsent>our research is carried out within the amalgam broad coverage sentence realization system.
</nextsent>
<nextsent>amalgam generates sentence strings from abstract predicate-argument structures (figure 1), using pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (corston-oliver et al, 2002; gamon et al, 2002<papid> P02-1004 </papid>a, 2002b; smets et al, 2003).<papid> E03-1006 </papid></nextsent>
<nextsent>amalgam has an explicit ordering stage that determines the order of constituents and their daughters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1684">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>like nitrogen, the halogen system (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from packed forest by constraining the search first within the scope of each constituent.</prevsent>
<prevsent>our research is carried out within the amalgam broad coverage sentence realization system.</prevsent>
</prevsection>
<citsent citstr=" P02-1004 ">
amalgam generates sentence strings from abstract predicate-argument structures (figure 1), using pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (corston-oliver et al, 2002; gamon et al, 2002<papid> P02-1004 </papid>a, 2002b; smets et al, 2003).<papid> E03-1006 </papid></citsent>
<aftsection>
<nextsent>amalgam has an explicit ordering stage that determines the order of constituents and their daughters.
</nextsent>
<nextsent>the input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or ranked list of such trees.
</nextsent>
<nextsent>for ordering, amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context.
</nextsent>
<nextsent>by separately establishing order within constituents, amalgam heavily constrains the possible alternatives in later stages of the realization process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1685">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>like nitrogen, the halogen system (langkilde, 2000; <papid> A00-2023 </papid>langkilde-geary, 2002a, 2002b) uses word n-grams, but it extracts the best-scoring surface realizations efficiently from packed forest by constraining the search first within the scope of each constituent.</prevsent>
<prevsent>our research is carried out within the amalgam broad coverage sentence realization system.</prevsent>
</prevsection>
<citsent citstr=" E03-1006 ">
amalgam generates sentence strings from abstract predicate-argument structures (figure 1), using pipeline of stages, many of which employ machine-learned models to predict where to perform specific linguistic operations based on the linguistic context (corston-oliver et al, 2002; gamon et al, 2002<papid> P02-1004 </papid>a, 2002b; smets et al, 2003).<papid> E03-1006 </papid></citsent>
<aftsection>
<nextsent>amalgam has an explicit ordering stage that determines the order of constituents and their daughters.
</nextsent>
<nextsent>the input for this stage is an unordered tree of constituents; the output is an ordered tree of constituents or ranked list of such trees.
</nextsent>
<nextsent>for ordering, amalgam leverages tree constituent structure and, importantly, features of those constituents and the surrounding context.
</nextsent>
<nextsent>by separately establishing order within constituents, amalgam heavily constrains the possible alternatives in later stages of the realization process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1687">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> evaluation on the penn treebank.  </section>
<citcontext>
<prevsection>
<prevsent>our goal in evaluating on penn tree bank (ptb) data is two-fold: (1) to enable comparison of amalgams performance with other systems operating on similar input, and (2) to measure amalgams capabilities on less domain-specific data than technical software manuals.
</prevsent>
<prevsent>we derive from the bracketed tree structures in the ptb using deterministic procedure an abstract representation we refer to as dependency structure input format (dsif), which is only loosely related to nlpwins abstract predicate argument structures.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the ptb to dsif transformation pipeline includes the following stages, inspired by langkilde-gearys (2002b) description: a. deserialize the tree b. label heads, according to charniaks head labeling rules (charniak, 2000) <papid> A00-2018 </papid>c. remove empty nodes and flatten any remaining empty non-terminals d. relabel heads to conform more closely to the head conventions of nlpwin e. label with logical roles, inferred from ptb functional roles f. flatten to maximal projections of heads (mph), except in the case of conjunctions g. flatten non-branching non-terminals h. perform dictionary look-up and morphological analysis i. introduce structure for material between paired delimiters and for any coordination not already represented in the ptb j. remove punctuation k. remove function words l. map the head of each maximal projection to dependency node, and map the heads modifiers to the first nodes dependents, thereby forming complete dependency tree.</citsent>
<aftsection>
<nextsent>to evaluate ordering performance alone, our data are obtained by performing all of the steps above except for (j) and (k).
</nextsent>
<nextsent>we employ only binary conditional ordering model, found in the previous section to be the best of the models considered.
</nextsent>
<nextsent>to train the order models, we use set of 10,000 sentences drawn from the standard ptb training set, namely sections 0221 from the wall street journal portion of the ptb (the full set contains approx.
</nextsent>
<nextsent>40,000 sentences).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1689">
<title id=" C04-1097.xml">linguistically informed statistical models of constituent structure for ordering in sentence realization </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>experiments by daum?
</prevsent>
<prevsent>et al(2002) and the parsing work of charniak (2000) <papid> A00-2018 </papid>and others indicate that further lexicalization may yield some additional improvements for ordering.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
however, the parsing results of klein &amp; manning (2003) <papid> P03-1054 </papid>involving un lexicalized grammars suggest that gains may be limited.</citsent>
<aftsection>
<nextsent>for comparison, we encourage implementers of other sentence realization systems to conduct order-only evaluations using ptb data.
</nextsent>
<nextsent>acknowledgements we wish to thank irene langkilde-geary and members of the msr nlp group for helpful discussions.
</nextsent>
<nextsent>thanks also go to the anonymous reviewers for helpful feedback.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1690">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.
</prevsent>
<prevsent>finally, novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
distributional similarity has been an active research area for more than decade (hindle, 1990), (<papid> P90-1034 </papid>ruge, 1992), (grefenstette, 1994), (lee, 1997), (lin, 1998), (<papid> P98-2127 </papid>dagan et al, 1999), (weeds and weir, 2003).<papid> W03-1011 </papid></citsent>
<aftsection>
<nextsent>inspired by harris distributional hypothesis (harris, 1968), similarity measures compare pair of weighted feature vectors that characterize two words.
</nextsent>
<nextsent>features typically correspond to other words that co-occur with the characterized word in the same context.
</nextsent>
<nextsent>it is then assumed that different words that occur within similar contexts are semantically similar.
</nextsent>
<nextsent>as it turns out, distributional similarity captures somewhat loose notion of semantic similarity (see table 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1692">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.
</prevsent>
<prevsent>finally, novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
distributional similarity has been an active research area for more than decade (hindle, 1990), (<papid> P90-1034 </papid>ruge, 1992), (grefenstette, 1994), (lee, 1997), (lin, 1998), (<papid> P98-2127 </papid>dagan et al, 1999), (weeds and weir, 2003).<papid> W03-1011 </papid></citsent>
<aftsection>
<nextsent>inspired by harris distributional hypothesis (harris, 1968), similarity measures compare pair of weighted feature vectors that characterize two words.
</nextsent>
<nextsent>features typically correspond to other words that co-occur with the characterized word in the same context.
</nextsent>
<nextsent>it is then assumed that different words that occur within similar contexts are semantically similar.
</nextsent>
<nextsent>as it turns out, distributional similarity captures somewhat loose notion of semantic similarity (see table 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1694">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results, proposing an objective measure for evaluating feature vector quality.
</prevsent>
<prevsent>finally, novel feature weighting and selection function is presented, which yields superior feature vectors and better word similarity performance.
</prevsent>
</prevsection>
<citsent citstr=" W03-1011 ">
distributional similarity has been an active research area for more than decade (hindle, 1990), (<papid> P90-1034 </papid>ruge, 1992), (grefenstette, 1994), (lee, 1997), (lin, 1998), (<papid> P98-2127 </papid>dagan et al, 1999), (weeds and weir, 2003).<papid> W03-1011 </papid></citsent>
<aftsection>
<nextsent>inspired by harris distributional hypothesis (harris, 1968), similarity measures compare pair of weighted feature vectors that characterize two words.
</nextsent>
<nextsent>features typically correspond to other words that co-occur with the characterized word in the same context.
</nextsent>
<nextsent>it is then assumed that different words that occur within similar contexts are semantically similar.
</nextsent>
<nextsent>as it turns out, distributional similarity captures somewhat loose notion of semantic similarity (see table 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1696">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> background and definitions.  </section>
<citcontext>
<prevsection>
<prevsent>in the distributional similarity scheme each word is represented by feature vector, where an entry in the vector corresponds to feature f. each feature represents another word (or term) with which co-occurs, and possibly specifies also the syntactic relation between the two words.
</prevsent>
<prevsent>the value of each entry is determined by some weight function weight(w,f), which quantifies the degree of statistical association between the feature and the corresponding word.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
typical feature weighting functions include the logarithm of the frequency of word-feature cooccurrence (ruge, 1992), and the conditional probability of the feature given the word (within prob abilistic-based measures) (pereira et al, 1993), (<papid> P93-1024 </papid>lee, 1997), (dagan et al, 1999).</citsent>
<aftsection>
<nextsent>probably the most widely used association weight function is (point-wise) mutual information (mi) (church et al., 1990), (hindle, 1990), (<papid> P90-1034 </papid>lin, 1998), (<papid> P98-2127 </papid>dagan, 2000), defined by: )()( ),(log),( 2 fpwp fwpfwmi = known weakness of mi is its tendency to assign high weights for rare features.</nextsent>
<nextsent>yet, similarity measures that utilize mi showed good perform ance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1703">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> background and definitions.  </section>
<citcontext>
<prevsection>
<prevsent>for example, given the word company?
</prevsent>
<prevsent>the feature  earnings_report, gen?  (geni tive) corresponds to the phrase companys earnings report?, and  profit, pcomp?  (preposi tional complement) corresponds to the profit of the company?.
</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
the syntactic relations are generated by the minipar dependency parser (lin, 1993).<papid> P93-1016 </papid></citsent>
<aftsection>
<nextsent>the arrows indicate the direction of the syntactic dependency: downward arrow indicates that the feature is the parent of the target word, and the upward arrow stands for the opposite.
</nextsent>
<nextsent>in our implementation we filtered out features with overall frequency lower than 10 in the corpus and with mi weights lower than 4.
</nextsent>
<nextsent>(in the tuning experiments the filtered version showed 10% improvement in precision over no feature filtering.)
</nextsent>
<nextsent>from now on we refer to this implementation as lin98.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1710">
<title id=" C04-1036.xml">feature vector quality and distributional similarity </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>however, we found that many word pairs from the reuters corpus that are clearly substitutable are not linked appropriately in wordnet.
</prevsent>
<prevsent>we therefore conducted manual evaluation based on the judgments of two human subjects.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
the judgment criterion follows common evaluations of paraphrase acquisition (lin and pantel, 2001), (barzilay and mckeown, 2001), <papid> P01-1008 </papid>and corresponds to the meaning-entailing substitutability criterion discussed in section 1.</citsent>
<aftsection>
<nextsent>two words are judged as substitutable (correct similarity) if there are some contexts in which one of the words can be substituted by the other, such that the meaning of the original word can be inferred from the new one.
</nextsent>
<nextsent>typically substitutability corresponds to certain onto logical relations.
</nextsent>
<nextsent>synonyms are substitutable in both directions.
</nextsent>
<nextsent>for example, worker and employee entail each other meanings, as in the context high salaried worker/employee?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1711">
<title id=" C04-1136.xml">significance tests for the evaluation of ranking methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the evaluation of extraction tools (e.g. in information retrieval) usually requires that both precision and recall are high, ranking methods often put greater weight on high precision, possibly at the price of missing considerable number of tps.
</prevsent>
<prevsent>moreover, when n-best lists of the same size are compared, precision and recall are fully equivalent.2 for these reasons, will concentrate on the precision ? here.as an example, consider the identification of collocations from text corpora.
</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
following the methodology described by evert and krenn (2001), <papid> P01-1025 </papid>german pp-verb combinations were extracted from chunk-parsed version of the frankfurter rundschau corpus.3 cooccurrence frequency threshold of 2namely, ? = ntp ? r/n, where ntp stands for the total number of tps in the candidate set.3the frankfurter rundschau corpus is german newspaper corpus, comprising ca.</citsent>
<aftsection>
<nextsent>40 million words of text.
</nextsent>
<nextsent>it is part of the eci multilingual corpus 1 distributed by elsnet.
</nextsent>
<nextsent>for this ? 30 was applied, resulting in candidate set of 5 102 pp-verb pairs.
</nextsent>
<nextsent>the candidates were then ranked according to the scores assigned by four association measures: the log-likelihood ratio g2 (dunning, 1993), <papid> J93-1003 </papid>pearsons chi-squared statistic x2 (manning and schutze, 1999, 169172), the t-scorestatistic (church et al, 1991), and mere cooccurrence frequency .4 tps were identified according to the definition of krenn (2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1712">
<title id=" C04-1136.xml">significance tests for the evaluation of ranking methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is part of the eci multilingual corpus 1 distributed by elsnet.
</prevsent>
<prevsent>for this ? 30 was applied, resulting in candidate set of 5 102 pp-verb pairs.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the candidates were then ranked according to the scores assigned by four association measures: the log-likelihood ratio g2 (dunning, 1993), <papid> J93-1003 </papid>pearsons chi-squared statistic x2 (manning and schutze, 1999, 169172), the t-scorestatistic (church et al, 1991), and mere cooccurrence frequency .4 tps were identified according to the definition of krenn (2000).</citsent>
<aftsection>
<nextsent>the graphs in figure 1 show the precision achieved by these measures, for ranging from 100 to 2 000 (lists with   100 were omitted because the graphs become highly unstable for small n).
</nextsent>
<nextsent>the baseline precision of 11.09% corresponds to random selection of candidates.
</nextsent>
<nextsent>0 500 1000 1500 2000 0 10 20 30 40 50 nbest list precision (%) baseline = 11.09% g2tx2ffigure 1: evaluation example: candidates for german pp-verb collocations are ranked by four different association measures.
</nextsent>
<nextsent>from figure 1, we can see that g2 and are the most useful ranking methods, being marginally better for ? 800 and g2 for ? 1 500.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1714">
<title id=" C04-1136.xml">significance tests for the evaluation of ranking methods </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the causes of such random variation include the source material from which the candidates are extracted (what if slightly different source had been used?), noise introduced by automatic pre-processing and extraction tools, and the uncertainty of human annotators manifested in varying degrees of inter-annotator agreement.most researchers understand the necessity of testing whether their results are statistically significant, but it is fairly unclear which tests are appropriate.for instance, krenn (2000) applies the standard 2test to her comparative evaluation of collocation extraction methods.
</prevsent>
<prevsent>she is aware, though, that this test assumes independent samples and is hardly suit able for different ranking methods applied to thesame candidate set: krenn and evert (2001) suggest several alternative tests for related samples.
</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
a wide range of exact and asymptotic tests as well as computationally intensive random isation tests (yeh, 2000) <papid> C00-2137 </papid>are available and add to the confusion about an appropriate choice.the aim of this paper is to formulate statistical model that interprets the evaluation of ranking methods as random experiment.</citsent>
<aftsection>
<nextsent>this model defines the degree to which evaluation results are affected by random variation, allowing us to derive appropriate significance tests.
</nextsent>
<nextsent>after formalising the evaluation procedure in section 2, recast the procedure as random experiment and make the underlying assumptions explicit (section 3.1).
</nextsent>
<nextsent>on the basis of this model, develop significance tests for the precision of single ranking method (section 3.2) and for the comparison of two ranking methods(section 3.3).
</nextsent>
<nextsent>the paper concludes with an empirical validation of the statistical model in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1715">
<title id=" C02-2011.xml">semantic case role detection for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>invesco in merger talks with aim management sayer verbal process receiver figure 1 ? example of verbal process since these main types (and some secondary ones) correspond to universal experiential modi, it is to be expected that they will have certain universal validity, i.e. that they are in some way or another present in all languages of the world.
</prevsent>
<prevsent>for our preliminary experiments, we use reduced version of the case role model proposed by halliday (1994, p. 106-175), as it is consistent, well-developed and relatively simple system, which makes it very suitable for testing the validity of our assumptions.
</prevsent>
</prevsection>
<citsent citstr=" W90-0108 ">
for actual applications, we will replace it by more elaborate variant, most likely bateman generalized upper model (bateman 1990; <papid> W90-0108 </papid>bateman et al in progress).</citsent>
<aftsection>
<nextsent>bateman model is finer-grained than halliday s; it is to large extent language-independent; and it has been specifically developed for implementation into nlp systems (see bateman et al in progress).
</nextsent>
<nextsent>given the framework outlined above, we consider case role detection to be standard classification task.
</nextsent>
<nextsent>in pattern classification one attempts to learn certain patterns or rules from classified examples and to use them for classifying previously unseen instances (hand 1997).
</nextsent>
<nextsent>in our case, class is concatenation of case roles that constitute one particular process (i.e. the deep structure figure) and the pattern itself is to be derived from the morphosyntactic and lexical properties corresponding to that process (its surface realisation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1716">
<title id=" C02-2011.xml">semantic case role detection for information extraction </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>historically, case role detection has its roots in frame-based approaches to ie (e.g. schank &amp; abelson 1977).
</prevsent>
<prevsent>the main problem here is that to build case frames one needs prior knowledge on which information exactly one wants to extract.
</prevsent>
</prevsection>
<citsent citstr=" W98-1106 ">
in recent years, different solutions have been offered to automatically generate those frames from annotated examples (e.g. riloff &amp; schmelzenbach 1998, <papid> W98-1106 </papid>soderland 1999) or by using added knowledge (e.g. harabagiu &amp; maiorano 2000).</citsent>
<aftsection>
<nextsent>many of those approaches were very successful but most of them have tendency to blend syntactic and semantic concepts and they still have to be trained on individual domains.
</nextsent>
<nextsent>some very interesting research on case frame detection has been done by gildea (gildea 2000, gildea 2001).
</nextsent>
<nextsent>he uses statistical methods to learn case frames from parsed examples from framenet (johnson et al 2001).
</nextsent>
<nextsent>conclusion there is definite need for case role analysis in ie and in natural language processing in general.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1717">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
to its left, another to its right, and prepositional phrase to the right of that.supertags convey such detailed syntactic subcategorization information that super tag disambiguation is referred to as almost parsing (bangalore and joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>standard sequence prediction models are highly effective for super tagging, including hidden markov models (bangalore and joshi, 1999; <papid> J99-2004 </papid>nielsen, 2002), maximum entropy markov models (clark, 2002; hockenmaier et al, 2004; clark and curran, 2007), and conditional random fields (blunsom and baldwin, 2006).<papid> W06-1620 </papid>the original motivation for supertags parse pre filtering for lexicalized grammars of bangalore and joshi (1999) <papid> J99-2004 </papid>has been realized to good effect:the super tagger of clark and curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy.</nextsent>
<nextsent>espinosa et al (2008) <papid> P08-1022 </papid>have shown that hyper tagging (predicting the super tag associated with logical form) can improve both speed and accuracy of wide-coverage sentence realization with ccg.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1719">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>to its left, another to its right, and prepositional phrase to the right of that.supertags convey such detailed syntactic subcategorization information that super tag disambiguation is referred to as almost parsing (bangalore and joshi, 1999).<papid> J99-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" W06-1620 ">
standard sequence prediction models are highly effective for super tagging, including hidden markov models (bangalore and joshi, 1999; <papid> J99-2004 </papid>nielsen, 2002), maximum entropy markov models (clark, 2002; hockenmaier et al, 2004; clark and curran, 2007), and conditional random fields (blunsom and baldwin, 2006).<papid> W06-1620 </papid>the original motivation for supertags parse pre filtering for lexicalized grammars of bangalore and joshi (1999) <papid> J99-2004 </papid>has been realized to good effect:the super tagger of clark and curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy.</citsent>
<aftsection>
<nextsent>espinosa et al (2008) <papid> P08-1022 </papid>have shown that hyper tagging (predicting the super tag associated with logical form) can improve both speed and accuracy of wide-coverage sentence realization with ccg.</nextsent>
<nextsent>supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (birch et al, 2007; <papid> W07-0702 </papid>hassan et al, 2007).<papid> P07-1037 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1722">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to its left, another to its right, and prepositional phrase to the right of that.supertags convey such detailed syntactic subcategorization information that super tag disambiguation is referred to as almost parsing (bangalore and joshi, 1999).<papid> J99-2004 </papid></prevsent>
<prevsent>standard sequence prediction models are highly effective for super tagging, including hidden markov models (bangalore and joshi, 1999; <papid> J99-2004 </papid>nielsen, 2002), maximum entropy markov models (clark, 2002; hockenmaier et al, 2004; clark and curran, 2007), and conditional random fields (blunsom and baldwin, 2006).<papid> W06-1620 </papid>the original motivation for supertags parse pre filtering for lexicalized grammars of bangalore and joshi (1999) <papid> J99-2004 </papid>has been realized to good effect:the super tagger of clark and curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy.</prevsent>
</prevsection>
<citsent citstr=" P08-1022 ">
espinosa et al (2008) <papid> P08-1022 </papid>have shown that hyper tagging (predicting the super tag associated with logical form) can improve both speed and accuracy of wide-coverage sentence realization with ccg.</citsent>
<aftsection>
<nextsent>supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (birch et al, 2007; <papid> W07-0702 </papid>hassan et al, 2007).<papid> P07-1037 </papid></nextsent>
<nextsent>super taggers typically relyon significant amount of carefully annotated sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1723">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>standard sequence prediction models are highly effective for super tagging, including hidden markov models (bangalore and joshi, 1999; <papid> J99-2004 </papid>nielsen, 2002), maximum entropy markov models (clark, 2002; hockenmaier et al, 2004; clark and curran, 2007), and conditional random fields (blunsom and baldwin, 2006).<papid> W06-1620 </papid>the original motivation for supertags parse pre filtering for lexicalized grammars of bangalore and joshi (1999) <papid> J99-2004 </papid>has been realized to good effect:the super tagger of clark and curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy.</prevsent>
<prevsent>espinosa et al (2008) <papid> P08-1022 </papid>have shown that hyper tagging (predicting the super tag associated with logical form) can improve both speed and accuracy of wide-coverage sentence realization with ccg.</prevsent>
</prevsection>
<citsent citstr=" W07-0702 ">
supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (birch et al, 2007; <papid> W07-0702 </papid>hassan et al, 2007).<papid> P07-1037 </papid></citsent>
<aftsection>
<nextsent>super taggers typically relyon significant amount of carefully annotated sentences.
</nextsent>
<nextsent>as with many problems, there is pressing need to find strategies for reducing the amount of supervision required for producing accurate super taggers, butas yet, no one has explored the use of weak supervision for the task.
</nextsent>
<nextsent>in particular, there are many dialog systems which relyon hand-crafted lexicons that both provide starting point for bootstrapping super tagger and which could benefit greatly from super tag pre-parse filter.
</nextsent>
<nextsent>for example, the dialog system used by kruijff et al (2007) uses hand 57 crafted ccg grammar for openccg (white and baldridge, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1724">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>standard sequence prediction models are highly effective for super tagging, including hidden markov models (bangalore and joshi, 1999; <papid> J99-2004 </papid>nielsen, 2002), maximum entropy markov models (clark, 2002; hockenmaier et al, 2004; clark and curran, 2007), and conditional random fields (blunsom and baldwin, 2006).<papid> W06-1620 </papid>the original motivation for supertags parse pre filtering for lexicalized grammars of bangalore and joshi (1999) <papid> J99-2004 </papid>has been realized to good effect:the super tagger of clark and curran (2007) provides staged n-best lists of multi-tags that dramatically improve parsing speed and coverage without much loss in accuracy.</prevsent>
<prevsent>espinosa et al (2008) <papid> P08-1022 </papid>have shown that hyper tagging (predicting the super tag associated with logical form) can improve both speed and accuracy of wide-coverage sentence realization with ccg.</prevsent>
</prevsection>
<citsent citstr=" P07-1037 ">
supertags have gained further relevance as they are increasingly used as features for other tasks, including machine translation (birch et al, 2007; <papid> W07-0702 </papid>hassan et al, 2007).<papid> P07-1037 </papid></citsent>
<aftsection>
<nextsent>super taggers typically relyon significant amount of carefully annotated sentences.
</nextsent>
<nextsent>as with many problems, there is pressing need to find strategies for reducing the amount of supervision required for producing accurate super taggers, butas yet, no one has explored the use of weak supervision for the task.
</nextsent>
<nextsent>in particular, there are many dialog systems which relyon hand-crafted lexicons that both provide starting point for bootstrapping super tagger and which could benefit greatly from super tag pre-parse filter.
</nextsent>
<nextsent>for example, the dialog system used by kruijff et al (2007) uses hand 57 crafted ccg grammar for openccg (white and baldridge, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1725">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> category transition distribution.  </section>
<citcontext>
<prevsection>
<prevsent>ccg analyses of sentences are built up from lexical categories combining to form derived categories, until an entire sentence is reduced to single derived category with corresponding dependencies.
</prevsent>
<prevsent>one of ccgs most interesting linguistic properties is it allows alternative constituents.
</prevsent>
</prevsection>
<citsent citstr=" P96-1011 ">
consider the derivations in figures 1 and 2, which show normal form derivation (eisner, 1996) <papid> P96-1011 </papid>and fully incremental derivation, respectively.</citsent>
<aftsection>
<nextsent>both produce the same dependencies, guaranteed by the semantic consistency of ccgs rules (steedman,2000).
</nextsent>
<nextsent>this property of ccg of supporting multiple derivations of the same analysis has been termed spurious ambiguity.
</nextsent>
<nextsent>however, the extra constituents are anything but spurious: they are implicated in range of ccg (along with other forms of categorial grammar) linguistic analyses, including coordination, long-distance extraction, intonation, and incremental processing.
</nextsent>
<nextsent>this all boils down to associativity: just as (1 + (4 + 2)) = ((1 + 4) + 2) = 7, ccg ensures that (ed?(sawted)) = ((edsaw)ted) = such multiple derivations arise when adjacent categories can combine through either application or composition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1726">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> category transition distribution.  </section>
<citcontext>
<prevsection>
<prevsent>for example, discriminative tagging models could include features that capture whether or not the current super tag can combine with the previous one and possibly via which ccg rule.
</prevsent>
<prevsent>here, showhow it can be used to provide non-uniform starting point for the transition distributions ? j|i in first-order hidden markov model.
</prevsent>
</prevsection>
<citsent citstr=" P05-1046 ">
this is similar to how grenager et al (2005) <papid> P05-1046 </papid>use diagonal initialization in an hmm for field segmentation to encourage the model to remain in the same state (and thus predict the same label for adjacent words).</citsent>
<aftsection>
<nextsent>for ccg super tagging, the initialization should discourage diagonal ization and establish preference for some transitions over others.
</nextsent>
<nextsent>there are many ways to define such starting point.
</nextsent>
<nextsent>the simplest would be to reserve small part of the mass spread uniformly over category pairs which cannot combine and then spread the rest of the mass uniformly over those which can.however, we can provide more refined distribution, ? j|i , by incorporating the lexical category distribution ? defined in the previous section to weight these transitions according to this further information.
</nextsent>
<nextsent>in similar manner to grenager et al (2005), <papid> P05-1046 </papid>define ? as follows: 3i make the standard assumption that type-raising is performed in the lexicon, so the possibility of combining these through type-raising plus composition is not available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1728">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>first, though extra context in theform of tritag transition distributions or other techniques can improve supervised pos tagging accuracy, the accuracy of bitag hmms is not far behind.
</prevsent>
<prevsent>the goal here is to investigate the relative gainsof using ccg-based information in weakly supervised hmm learning.
</prevsent>
</prevsection>
<citsent citstr=" C04-1080 ">
second, the expectation maximization algorithm for bitag hmms is efficient and has been shown to be quite effective for acquiring accurate pos taggers given only lexicon (tag dictionary) and certain favorable conditions (banko and moore, 2004).<papid> C04-1080 </papid></citsent>
<aftsection>
<nextsent>third, the models simplicity makes it straightforward to test the idea of ccg-initialization on tag transitions.dirichlet priors can be used tobias hmms toward more skewed distributions (goldwater and griffiths, 2007; <papid> P07-1094 </papid>johnson, 2007), <papid> D07-1031 </papid>which is especially useful in the weakly supervised setting considered here.</nextsent>
<nextsent>following johnson (2007), <papid> D07-1031 </papid>use vari ational bayes em (beal, 2003) during the m-step for the transition distribution: ? l+1 j|i = f(e[n i,j ] + ? ) f(e[n ] + |c | ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1729">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>the goal here is to investigate the relative gainsof using ccg-based information in weakly supervised hmm learning.
</prevsent>
<prevsent>second, the expectation maximization algorithm for bitag hmms is efficient and has been shown to be quite effective for acquiring accurate pos taggers given only lexicon (tag dictionary) and certain favorable conditions (banko and moore, 2004).<papid> C04-1080 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-1094 ">
third, the models simplicity makes it straightforward to test the idea of ccg-initialization on tag transitions.dirichlet priors can be used tobias hmms toward more skewed distributions (goldwater and griffiths, 2007; <papid> P07-1094 </papid>johnson, 2007), <papid> D07-1031 </papid>which is especially useful in the weakly supervised setting considered here.</citsent>
<aftsection>
<nextsent>following johnson (2007), <papid> D07-1031 </papid>use vari ational bayes em (beal, 2003) during the m-step for the transition distribution: ? l+1 j|i = f(e[n i,j ] + ? ) f(e[n ] + |c | ? ?</nextsent>
<nextsent>i ) (3) f(v) = exp(?(v)) (4) 60 ?(v) = { g(v ? 1 2 ) if   7 ?(v + 1) ? 1 o.w.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1730">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>the goal here is to investigate the relative gainsof using ccg-based information in weakly supervised hmm learning.
</prevsent>
<prevsent>second, the expectation maximization algorithm for bitag hmms is efficient and has been shown to be quite effective for acquiring accurate pos taggers given only lexicon (tag dictionary) and certain favorable conditions (banko and moore, 2004).<papid> C04-1080 </papid></prevsent>
</prevsection>
<citsent citstr=" D07-1031 ">
third, the models simplicity makes it straightforward to test the idea of ccg-initialization on tag transitions.dirichlet priors can be used tobias hmms toward more skewed distributions (goldwater and griffiths, 2007; <papid> P07-1094 </papid>johnson, 2007), <papid> D07-1031 </papid>which is especially useful in the weakly supervised setting considered here.</citsent>
<aftsection>
<nextsent>following johnson (2007), <papid> D07-1031 </papid>use vari ational bayes em (beal, 2003) during the m-step for the transition distribution: ? l+1 j|i = f(e[n i,j ] + ? ) f(e[n ] + |c | ? ?</nextsent>
<nextsent>i ) (3) f(v) = exp(?(v)) (4) 60 ?(v) = { g(v ? 1 2 ) if   7 ?(v + 1) ? 1 o.w.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1738">
<title id=" C08-1008.xml">weakly supervised super tagging with grammar informed initialization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a very interesting aspect of this work is that they explicitly model ambiguity classes to exploit commonality in the lexicon between different word forms, which could be even more useful for supertagging.
</prevsent>
<prevsent>in grammar development context, it is often the case that only some of the categories for word have been assigned.
</prevsent>
</prevsection>
<citsent citstr=" N06-1041 ">
this is the scenario considered by haghighi and klein (2006) <papid> N06-1041 </papid>for pos tag ging: how to construct an accurate tagger given set of tags and few example words for each of those tags.</citsent>
<aftsection>
<nextsent>they use distributional similarity of words to define features for tagging that effectively allow such prototype words to stand in for others.
</nextsent>
<nextsent>this idea could be used with my approach as well; the most obvious way would be to use prototype words to suggest extra categories (beyond the tag dictionary) for known words and reduced set of categories for unknown words.other work aims to do truly unsupervised learning of taggers, such as goldwater and griffiths (2007) <papid> P07-1094 </papid>and johnson (2007).<papid> D07-1031 </papid></nextsent>
<nextsent>no tag dictionaries are assumed, and the models are parametrized withdirichlet priors.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1741">
<title id=" C04-1020.xml">representing discourse coherence a corpus based analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse structure relations here refer to informational relations that hold between sentences or other non-overlapping segments in discourse monologue.
</prevsent>
<prevsent>that is, discourse structure relations reflect how the meaning conveyed by one discourse segment relates to the meaning conveyed by another discourse segment (cf.
</prevsent>
</prevsection>
<citsent citstr=" P99-1006 ">
hobbs, 1985; marcu, 2000; webber et al, 1999).<papid> P99-1006 </papid></citsent>
<aftsection>
<nextsent>accounts of discourse structure vary greatly with respect to how many discourse relations they assume, ranging from two (grosz &amp; sidner, 1986) <papid> J86-3001 </papid>to over 400 different coherence relations, reported in hovy and maier (1995).</nextsent>
<nextsent>however, hovy and maier (1995) argue that taxonomies with more relations represent sub types of taxonomies with fewer relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1744">
<title id=" C04-1020.xml">representing discourse coherence a corpus based analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, discourse structure relations reflect how the meaning conveyed by one discourse segment relates to the meaning conveyed by another discourse segment (cf.
</prevsent>
<prevsent>hobbs, 1985; marcu, 2000; webber et al, 1999).<papid> P99-1006 </papid></prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
accounts of discourse structure vary greatly with respect to how many discourse relations they assume, ranging from two (grosz &amp; sidner, 1986) <papid> J86-3001 </papid>to over 400 different coherence relations, reported in hovy and maier (1995).</citsent>
<aftsection>
<nextsent>however, hovy and maier (1995) argue that taxonomies with more relations represent sub types of taxonomies with fewer relations.
</nextsent>
<nextsent>this means that different taxonomies can be compatible with each other.
</nextsent>
<nextsent>we describe an account with small number of relations in order to achieve more generalizable representations of discourse structures; however, the number is not so small that informational structures that we are interested in are obscured.
</nextsent>
<nextsent>the next section will describe in detail the set of coherence relations we use, which are mostly based on hobbs (1985).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1745">
<title id=" C04-1020.xml">representing discourse coherence a corpus based analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in fact, major goal of this paper is to show that trees do not seem adequate to represent discourse structures.
</prevsent>
<prevsent>with coherence relations this section describes (1) how we define discourse segments, (2) which coherence relations we used to connect the discourse segments, and (3) how the annotation procedure worked.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
discourse segments can be defined as nonoverlapping spans of prosodic units (hirschberg &amp; nakatani, 1996), <papid> P96-1038 </papid>intentional units (grosz &amp; sidner, 1986), <papid> J86-3001 </papid>phrasal units (lascarides &amp; asher, 1993), or sentences (hobbs, 1985).</citsent>
<aftsection>
<nextsent>we adopted sentence unit-based definition of discourse segments.
</nextsent>
<nextsent>however, we also assume that content ful coordinating and subordinating conjunctions (cf.
</nextsent>
<nextsent>table 1) can delimit discourse segments.
</nextsent>
<nextsent>we assume set of coherence relations that is similar to that of hobbs (1985) and kehler (2002).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1747">
<title id=" C04-1020.xml">representing discourse coherence a corpus based analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for the 135 texts, the mean number of words was 545 (min.: 161; max.: 1409; median: 529), the mean number of discourse segments was 61 (min.: 6; max.: 143; median: 60).
</prevsent>
<prevsent>each text was independently annotated by two annotators.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
in order to determine inter-annotator agreement for the database of annotated texts, we computed kappa statistics (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>for all annotations of the 135 texts, the agreement was 88.45%, perchance agreement was 24.86%, and kappa was 84.63%.
</nextsent>
<nextsent>annotator agreement did not differ by text length (2 = 1.27;   0.75), arc length (2   1), or kind of coherence relation (2   1).
</nextsent>
<nextsent>3 data structures for representing coherence.
</nextsent>
<nextsent>relations most accounts of discourse coherence assume tree structures to represent coherence relations between discourse segments in text (carlson et al., 2002; corston-oliver, 1998; lascarides &amp; asher, 1993; long acre, 1983; grosz &amp; sidner, 1986; <papid> J86-3001 </papid>mann &amp; thompson, 1988; marcu, 2000; polanyi, 1988; van dijk &amp; kintsch, 1983; walker, 1998; webber et al, 1999).<papid> P99-1006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1755">
<title id=" C04-1020.xml">representing discourse coherence a corpus based analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because trees are neither descript ively adequate data structure for representing coherence structures nor easier to derive, we argue for less constrained graphs as data structure for representing coherence structures.
</prevsent>
<prevsent>such less constrained graphs would have the advantage of being able to adequately represent coherence structures in one single data structure (cf.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
skut et al., 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>furthermore, they are at least not harder to derive than (augmented) tree structures.
</nextsent>
<nextsent>the greater descriptive adequacy might in fact make them easier to derive.
</nextsent>
<nextsent>however, this is still an open issue and will have to be addressed in future research.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1756">
<title id=" C02-1051.xml">automatic linguistic analysis for language teachers the case of zeros </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>explanation.
</prevsent>
<prevsent>zd is part of the call program for jsl learners, zero checker, which supports reading comprehension and writing revision process with focus on zeros.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
thus, zd will also serve as pre-processing module for the models of resolving and generating zeros, created within the centering framework (e.g., grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1757">
<title id=" C08-1077.xml">detecting multiple facets of an event using graph based unsupervised methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we plan to have this comparison in our future work.to reduce the complexity of this task, candidate set of sub topics needs to be generated that cover the document collection.
</prevsent>
<prevsent>we choose to use key phrase detection algorithm to generate topic labels.
</prevsent>
</prevsection>
<citsent citstr=" W03-1028 ">
several key phrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (turney, 2000), (hulth, 2003) <papid> W03-1028 </papid>and tf-idf ((frank et al , 1999)).</citsent>
<aftsection>
<nextsent>our method uses language models and pointwise mutual information expressed as the kullback-leibler divergence.
</nextsent>
<nextsent>kullback-leibler divergence has been found to be an effective method of finding key phrases in text collections.
</nextsent>
<nextsent>but identification of key phrases is not enough to find topics in document.
</nextsent>
<nextsent>thekeyphrases identified may describe the entire collection, or aspects of the collection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1758">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic sentence compression can be broadly described as the task of creating grammatical summary of single sentence with minimal information loss.
</prevsent>
<prevsent>it has recently attracted much attention, in part because of its relevance to applications.
</prevsent>
</prevsection>
<citsent citstr=" A00-1043 ">
examples include the generation of subtitles from spoken transcripts (vandeghinste and pan, 2004), the display of text on small screens such as mobile phones or pdas (corston-oliver, 2001), and, notably, summarisation (jing, 2000; <papid> A00-1043 </papid>lin, 2003).<papid> W03-1101 </papid></citsent>
<aftsection>
<nextsent>most prior work has focused on specific instantiation of sentence compression, namely word deletion.
</nextsent>
<nextsent>given an input sentence of words, 1 , 2 . . .
</nextsent>
<nextsent>w , compression is formed by dropping any subset of these words (knight ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1759">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>automatic sentence compression can be broadly described as the task of creating grammatical summary of single sentence with minimal information loss.
</prevsent>
<prevsent>it has recently attracted much attention, in part because of its relevance to applications.
</prevsent>
</prevsection>
<citsent citstr=" W03-1101 ">
examples include the generation of subtitles from spoken transcripts (vandeghinste and pan, 2004), the display of text on small screens such as mobile phones or pdas (corston-oliver, 2001), and, notably, summarisation (jing, 2000; <papid> A00-1043 </papid>lin, 2003).<papid> W03-1101 </papid></citsent>
<aftsection>
<nextsent>most prior work has focused on specific instantiation of sentence compression, namely word deletion.
</nextsent>
<nextsent>given an input sentence of words, 1 , 2 . . .
</nextsent>
<nextsent>w , compression is formed by dropping any subset of these words (knight ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1760">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>and marcu, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P05-1036 ">
the simplification renders the task computationally feasible, allowing efficient decoding using dynamic program (knight andmarcu, 2002; turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006).<papid> E06-1038 </papid></citsent>
<aftsection>
<nextsent>furthermore, constraining the problem to word deletion affords substantial modeling flexibility.
</nextsent>
<nextsent>indeed, variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (knight and marcu, 2002; galley and mckeown, 2007;<papid> N07-1023 </papid>turner and charniak, 2005), <papid> P05-1036 </papid>to large-margin learning (mcdonald, 2006; <papid> E06-1038 </papid>cohn and lapata, 2007), <papid> D07-1008 </papid>and integer linear programming (clarke, 2008).</nextsent>
<nextsent>however, the simplification also renders the task somewhat artificial.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1761">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>and marcu, 2002).
</prevsent>
</prevsection>
<citsent citstr=" E06-1038 ">
the simplification renders the task computationally feasible, allowing efficient decoding using dynamic program (knight andmarcu, 2002; turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006).<papid> E06-1038 </papid></citsent>
<aftsection>
<nextsent>furthermore, constraining the problem to word deletion affords substantial modeling flexibility.
</nextsent>
<nextsent>indeed, variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (knight and marcu, 2002; galley and mckeown, 2007;<papid> N07-1023 </papid>turner and charniak, 2005), <papid> P05-1036 </papid>to large-margin learning (mcdonald, 2006; <papid> E06-1038 </papid>cohn and lapata, 2007), <papid> D07-1008 </papid>and integer linear programming (clarke, 2008).</nextsent>
<nextsent>however, the simplification also renders the task somewhat artificial.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1762">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the simplification renders the task computationally feasible, allowing efficient decoding using dynamic program (knight andmarcu, 2002; turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006).<papid> E06-1038 </papid></prevsent>
<prevsent>furthermore, constraining the problem to word deletion affords substantial modeling flexibility.</prevsent>
</prevsection>
<citsent citstr=" N07-1023 ">
indeed, variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (knight and marcu, 2002; galley and mckeown, 2007;<papid> N07-1023 </papid>turner and charniak, 2005), <papid> P05-1036 </papid>to large-margin learning (mcdonald, 2006; <papid> E06-1038 </papid>cohn and lapata, 2007), <papid> D07-1008 </papid>and integer linear programming (clarke, 2008).</citsent>
<aftsection>
<nextsent>however, the simplification also renders the task somewhat artificial.
</nextsent>
<nextsent>there are many rewrite operations that could compress sentence, besides deletion, including reordering, substitution, and insertion.
</nextsent>
<nextsent>in fact, professional abs tractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (jing, 2000).<papid> A00-1043 </papid></nextsent>
<nextsent>therefore, in this paper we consider sentence compression from more general perspective and generate abstracts rather than extracts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1765">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the simplification renders the task computationally feasible, allowing efficient decoding using dynamic program (knight andmarcu, 2002; turner and charniak, 2005; <papid> P05-1036 </papid>mcdonald, 2006).<papid> E06-1038 </papid></prevsent>
<prevsent>furthermore, constraining the problem to word deletion affords substantial modeling flexibility.</prevsent>
</prevsection>
<citsent citstr=" D07-1008 ">
indeed, variety of models have been successfully developed for this task ranging from in stantiations of the noisy-channel model (knight and marcu, 2002; galley and mckeown, 2007;<papid> N07-1023 </papid>turner and charniak, 2005), <papid> P05-1036 </papid>to large-margin learning (mcdonald, 2006; <papid> E06-1038 </papid>cohn and lapata, 2007), <papid> D07-1008 </papid>and integer linear programming (clarke, 2008).</citsent>
<aftsection>
<nextsent>however, the simplification also renders the task somewhat artificial.
</nextsent>
<nextsent>there are many rewrite operations that could compress sentence, besides deletion, including reordering, substitution, and insertion.
</nextsent>
<nextsent>in fact, professional abs tractors tend to use these operations to transform selected sentences from an article into the corresponding summary sentences (jing, 2000).<papid> A00-1043 </papid></nextsent>
<nextsent>therefore, in this paper we consider sentence compression from more general perspective and generate abstracts rather than extracts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1767">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our task is related to, but different from, paraphrase extraction (barzilay, 2003).
</prevsent>
<prevsent>we must not only have access to paraphrases (i.e., rewrite rules), but also be able to combine them in order to generate new text, while attempting to produce shorter resulting string.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
quirk et al (2004) <papid> W04-3219 </papid>present an end-to-end paraphrasing system inspired byphrase-based machine translation that can both acquire paraphrases and use them to generate new strings.</citsent>
<aftsection>
<nextsent>however, their model is limited to lexical substitution ? no reordering takes place ? and is 137 lacking the compression objective.once we move away from extractive compression we are faced with two problems.
</nextsent>
<nextsent>first, wemust find an appropriate training set for our abs tractive task.
</nextsent>
<nextsent>compression corpora are not naturally available and existing paraphrase corpora do not normally contain compressions.
</nextsent>
<nextsent>our second problem concerns the modeling task itself.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1768">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides obtaining useful data for modeling purposes, we also demonstrate that ab str active compression is meaningful task.
</prevsent>
<prevsent>we then present tree-to-tree transducer capable of transforming an input parse tree into compressed parse tree.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
our approach is based on synchronous tree substitution grammar (stsg, eisner (2003)),<papid> P03-2041 </papid>a formalism that can account for structural mismatches, and is trained discriminatively.</citsent>
<aftsection>
<nextsent>specifically, we generalise the model of cohn and lapata (2007) <papid> D07-1008 </papid>to our abs tractive task.</nextsent>
<nextsent>we present noveltree-to-tree grammar extraction method which acquires paraphrases from bilingual corpora and ensure coherent output by including ngram language model as feature.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1775">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> abs tractive compression corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we first examined whether the annotators compressed at similar level.
</prevsent>
<prevsent>the compression rate was 56% for one annotator and 54% for the other.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
2 we also assessed whether they agreed in their rewrites by measuring bleu (pap ineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>the inter-annotator bleu score was 23.79%, compared with the source agreement bleu of only 13.22%.
</nextsent>
<nextsent>both the compression rateand bleu score indicate that the task is well defined and the compress ions valid.
</nextsent>
<nextsent>the remaining 25 documents were compressed by single annotator to ensure consistency.
</nextsent>
<nextsent>all our experiments used the data from this annotator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1791">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> extensions.  </section>
<citcontext>
<prevsection>
<prevsent>some english text is translated to foreign language, and then translated back into english (bannard and callison-burch, 2005): p(e ? |e) = ? p(e ? |f)p(f |e) (4) where p(f |e) is the probability of translating an english string into foreign string and p(e ?|f) the probability of translating the same foreign string into some other english string ? .
</prevsent>
<prevsent>we.
</prevsent>
</prevsection>
<citsent citstr=" P05-1074 ">
thus obtain english-english translation probabilities p(e ? |e) by marginalizing out the foreign text.instead of using strings (bannard and callison burch, 2005), <papid> P05-1074 </papid>we use elementary trees on the english side, resulting in monolingual stsg.</citsent>
<aftsection>
<nextsent>we obtain the elementary trees and foreign strings using the gkhm algorithm (galley et al, 2004).
</nextsent>
<nextsent>this takes as input bilingual word-aligned corpus with trees on one side, and finds the minimal set of tree fragments and their corresponding strings which is consistent with the word alignment.
</nextsent>
<nextsent>this process is illustrated in figure 2 where the aligned pair on the left gives rise to the rules shown onthe right.
</nextsent>
<nextsent>note that the english rules and foreign strings shown include variable indices where they have been generalised.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1796">
<title id=" C08-1018.xml">sentence compression beyond word deletion </title>
<section> experimental design.  </section>
<citcontext>
<prevsection>
<prevsent>we give details on the corpora and grammars we used, model parameters and features, 6 the baseline used for comparison with our approach, and explain how our system output was evaluated.
</prevsent>
<prevsent>grammar extraction our grammar used rules extracted directly from our compression corpus (the training partition, 480 sentences) and bilingual corpus (see table 2 for examples).
</prevsent>
</prevsection>
<citsent citstr=" N06-1014 ">
the former corpus was word-aligned using the berkeley aligner (liang et al, 2006) <papid> N06-1014 </papid>initial ised with lexicon of word identity mappings, and parsed with bikels (2002) parser.</citsent>
<aftsection>
<nextsent>from this we extracted grammar rules following the technique described in cohn and lapata (2007).<papid> D07-1008 </papid></nextsent>
<nextsent>for the pivot grammar we use the french-english europarl v2 which contains approximately 688k sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1800">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the paper aims at deeper understanding of several well-known algorithms and proposes ways to optimize them.
</prevsent>
<prevsent>it describes and discusses factor sand strategies of factor interaction used in the algorithms.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
the factors used in the algorithms and the algorithms themselves are evaluated on german corpus annotated with syntactic and coreference information (negra) (skut et al, 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</nextsent>
<nextsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</nextsent>
<nextsent>some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></nextsent>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1801">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1804">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J99-3001 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1807">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1808">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1811">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1818">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1820">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
</prevsent>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></citsent>
<aftsection>
<nextsent>they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></nextsent>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1823">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, variety of approaches to pronoun resolution have been proposed.
</prevsent>
<prevsent>some of them are based on centering theory (strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 2001), <papid> J01-4003 </papid>others on machine learning (aone and bennett, 1995; ge et al, 1998;<papid> W98-1119 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
they supplement older heuristic approaches(hobbs, 1978; lappin and leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.
</nextsent>
<nextsent>appreciation of the new insights is quite hard.
</nextsent>
<nextsent>evaluation differs not only with regard to size and genre of corpora but also along the following lines.
</nextsent>
<nextsent>scope of application: some approaches only deal with personal and possessive pronouns (centering and heuristic), while others consider coreference links in general (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie,2002; <papid> P02-1014 </papid>yang et al, 2003).<papid> P03-1023 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1846">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>my thanks go to melvin wurster for help in annotation and to ciprian gerstenberger for discussion.
</prevsent>
<prevsent>quality of linguistic input: some proposals were evaluated on hand annotated (strube and hahn, 1999) <papid> J99-3001 </papid>or tree bank input (ge et al, 1998;<papid> W98-1119 </papid>tetreault, 2001).<papid> J01-4003 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
other proposals provide more realistic picture in that they work as backend to parser (lappin and leass, 1994) <papid> J94-4002 </papid>or noun chunker (mitkov, 1998; <papid> P98-2143 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002)).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>in evaluation of applications presupposing parsing, itis helpful to separate errors due to parsing from intrinsic errors.
</nextsent>
<nextsent>on the other hand, one would also like to gauge the end-to-end performance of system.
</nextsent>
<nextsent>thus we will provide performance figures forboth ideal (hand-annotated) input and realistic (au tomatically generated) input.
</nextsent>
<nextsent>language: most approaches were evaluated on english where large resources are available, both in terms of pre-annotated data (muc-6 and muc-7 data) and lexical information (wordnet).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1858">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> evaluation corpus.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 concludes.
</prevsent>
<prevsent>we chose as an evaluation base the negra treebank, which contains about 350,000 tokens of german newspaper text.
</prevsent>
</prevsection>
<citsent citstr=" P03-1015 ">
the same corpus was also processed with finite-state parser, performing at 80% dependency f-score (schiehlen, 2003).<papid> P03-1015 </papid>all personal pronouns (pper), possessive pronouns (pposat), and demonstrative pronouns (pds) in negra were annotated in format geared to the muc-7 guidelines (muc-7, 1997).</citsent>
<aftsection>
<nextsent>proper names were annotated automatically by named entity recognizer.
</nextsent>
<nextsent>in small portion of the corpus (6.7%), all coreference links were annotated.
</nextsent>
<nextsent>thusthe size of the annotated data (3,115 personal pronouns1 , 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 german pronouns in (strube and hahn, 1999), <papid> J99-3001 </papid>2,477 english pronouns in (ge et al, 1998), <papid> W98-1119 </papid>about 5,400 english coreferential expressions in (ng and cardie, 2002)).<papid> P02-1014 </papid>in the experiments, systems only looked for single np antecedents.</nextsent>
<nextsent>hence, propositional or predicative antecedents (8.4% of the pronouns annotated) and split antecedents (0.2%) were inaccessible, which reduced optimal success rate to 91.4%.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1871">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> factors in pronoun resolution.  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, they also provide hardfilter: the correct antecedent must fit into the environment of the pronoun (carbonell and brown, 1988).
</prevsent>
<prevsent>in general, however, the required knowledge sources are lacking, so they must be hand-coded and can only be applied in restricted domains (strube and hahn, 1999).<papid> J99-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1037 ">
selectional restrictions can also be modelled by collocational data extracted by aparser, which have, however, only very small impact on overall performance (kehler et al, 2004).<papid> N04-1037 </papid></citsent>
<aftsection>
<nextsent>we will neglect sortal constraints in this paper.
</nextsent>
<nextsent>3.2 preferences.
</nextsent>
<nextsent>preferences can be classified according to their requirements on linguistic processing.
</nextsent>
<nextsent>sentence re cency and surface order can be read directly off the surface.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1881">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> factors in pronoun resolution.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 underscores the virtues of sentence recency: in the most recent sentence with antecedents satisfying the filters, there are on average only 2.4 such antecedents.
</prevsent>
<prevsent>however, the benefit also comes at cost: the upper ceiling of performance is lowered to 82.0% in our corpus: in many cases an incorrect antecedent is found in more recent sentence.similarly, we can assess other strategies of sentence ordering that have been proposed in the literature.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
hard-core centering approaches only deal with the last sentence (brennan et al, 1987).<papid> P87-1022 </papid></citsent>
<aftsection>
<nextsent>in negra, these approaches can consequently have at most success rate of 44.2%.
</nextsent>
<nextsent>performance is particularly low with possessive pronouns which often only have antecedents in the current sentence.
</nextsent>
<nextsent>strube (1998)<papid> P98-2204 </papid>scentering approach (whose sentence ordering is designated as sr2 in table 2) also deals with and even prefers intra sentential anaphora, which raises the upper limit to more acceptable 80.2%.</nextsent>
<nextsent>strube and hahn (1999) <papid> J99-3001 </papid>extend the context to more than the last sentence, but switch preference order between the last and the current sentence so that an antecedent is determined in the last sentence, whenever possi ble.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1887">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> factors in pronoun resolution.  </section>
<citcontext>
<prevsection>
<prevsent>grammatical roles (gr).
</prevsent>
<prevsent>another important factor in pronoun resolution is the grammatical roleof the antecedent.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the role hierarchy used in centering (brennan et al, 1987; <papid> P87-1022 </papid>grosz et al, 1995)<papid> J95-2003 </papid>ranks subjects over direct objects over indirect objects over others.</citsent>
<aftsection>
<nextsent>lappin and leass (1994)<papid> J94-4002 </papid>provide more elaborate model which ranks np complements and np adjuncts lowest.</nextsent>
<nextsent>two other distinctions in their model express preference of rhematic2 over thematic arguments: existential subjects, which follow the verb, rank very high, between subjects and direct objects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B1898">
<title id=" C04-1074.xml">optimizing algorithms for pronoun resolution </title>
<section> factors in pronoun resolution.  </section>
<citcontext>
<prevsection>
<prevsent>in interaction, strong match performed best, so we adopt it.surface order (lr, rl).
</prevsent>
<prevsent>surface order is usually used to bring down the number of available antecedents to one, since it is the only factor that produces unique discourse referent.
</prevsent>
</prevsection>
<citsent citstr=" P99-1079 ">
there is less consensus on the preference order: (sentence-wise) left-to-right (hobbs, 1978; strube, 1998; <papid> P98-2204 </papid>strube and hahn, 1999; <papid> J99-3001 </papid>tetreault, 1999) <papid> P99-1079 </papid>or right-to-left (recency) (lappin and leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>furthermore, something has to be said about antecedents which embed other antecedents (e.g. conjoined nps and their conjuncts).
</nextsent>
<nextsent>we registered performance gains 2carbonell and brown (1988) also argue that clefted or fronted arguments should be preferred.
</nextsent>
<nextsent>(of up to 3%) by ranking embedding antecedents higher than embedded ones (tetreault, 2001).<papid> J01-4003 </papid></nextsent>
<nextsent>left-to-right order is often used as surrogate for grammatical role hierarchy in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2000">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>much research aimed at automatic discovery of verb classes (see section 2).verbnet (vn) (kipper et al, 2000; kipper schuler, 2005) is large scale, publicly available domain independent verb lexicon that builds on levin classes and extends them with new verbs, new classes, and additional information such as semantic roles and selectional restrictions.
</prevsent>
</prevsection>
<citsent citstr=" H05-1111 ">
vn classes were proven beneficial for semantic role labeling (srl) (swier and stevenson, 2005), <papid> H05-1111 </papid>semantic parsing (shi and mihalcea, 2005) and building conceptual graphs (hensman and dunnion, 2004).</citsent>
<aftsection>
<nextsent>levin-inspired classes have been used in several nlp tasks, such as machine translation (dorr, 1997) and document classification (klavans and kan, 1998).<papid> P98-1112 </papid></nextsent>
<nextsent>many applications that use vn need to map verb instances onto their vn classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2001">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much research aimed at automatic discovery of verb classes (see section 2).verbnet (vn) (kipper et al, 2000; kipper schuler, 2005) is large scale, publicly available domain independent verb lexicon that builds on levin classes and extends them with new verbs, new classes, and additional information such as semantic roles and selectional restrictions.
</prevsent>
<prevsent>vn classes were proven beneficial for semantic role labeling (srl) (swier and stevenson, 2005), <papid> H05-1111 </papid>semantic parsing (shi and mihalcea, 2005) and building conceptual graphs (hensman and dunnion, 2004).</prevsent>
</prevsection>
<citsent citstr=" P98-1112 ">
levin-inspired classes have been used in several nlp tasks, such as machine translation (dorr, 1997) and document classification (klavans and kan, 1998).<papid> P98-1112 </papid></citsent>
<aftsection>
<nextsent>many applications that use vn need to map verb instances onto their vn classes.
</nextsent>
<nextsent>however, verbs are polysemous with respect to vn classes.
</nextsent>
<nextsent>sem link (loper et al, 2007) is dataset that maps each verb instance in the wsj penn treebank to its vnclass.
</nextsent>
<nextsent>the mapping has been created using combination of automatic and manual methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2002">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our adaptation scenario is complete in the sense that the parser we use was also trained on different corpus (wsj).
</prevsent>
<prevsent>we also report experiments done using gold-standard (manually created) parses.
</prevsent>
</prevsection>
<citsent citstr=" J04-1003 ">
the most relevant previous works addressing verb instance class classification are (lapata and brew, 2004; <papid> J04-1003 </papid>li and brew, 2007; girju et al, 2005).the former two do not use verbnet and their experiments were narrower than ours, so we can not compare to their results.</citsent>
<aftsection>
<nextsent>the latter mapped to vn, but used preliminary highly restricted setup where most instances were monosemous.
</nextsent>
<nextsent>for completeness, we compared our method to theirs2, achieving similar results.we review related work in section 2, and discuss the task in section 3.
</nextsent>
<nextsent>section 4 introduces the model, section 5 describes the experimental setup, and section 6 presents our results.
</nextsent>
<nextsent>verbnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2003">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>vn is major electronic english verblexicon.
</prevsent>
<prevsent>it is organized in hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
vn is built on refinement of the levin classes, the intersec tive levin classes (dang et al, 1998),<papid> P98-1046 </papid>aimed at achieving more coherent classes both semantically and syntactically.</citsent>
<aftsection>
<nextsent>vn was also substantially extended (kipper et al, 2006) using the levin classes extension proposed in (korhonen and briscoe, 2004).<papid> W04-2606 </papid></nextsent>
<nextsent>vn today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to lemma withan ascribed class as type).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2004">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is organized in hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class.
</prevsent>
<prevsent>vn is built on refinement of the levin classes, the intersec tive levin classes (dang et al, 1998),<papid> P98-1046 </papid>aimed at achieving more coherent classes both semantically and syntactically.</prevsent>
</prevsection>
<citsent citstr=" W04-2606 ">
vn was also substantially extended (kipper et al, 2006) using the levin classes extension proposed in (korhonen and briscoe, 2004).<papid> W04-2606 </papid></citsent>
<aftsection>
<nextsent>vn today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to lemma withan ascribed class as type).
</nextsent>
<nextsent>of the 3626 lem mas, 912 are polysemous (i.e., appear in more than single class).
</nextsent>
<nextsent>vns significant coverage of the english verb lexicon is demonstrated by the 1our annotations will be made available to the community.
</nextsent>
<nextsent>2using the same sentences and instances, obtained from the authors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2006">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>each class contains rich semantic information, including semantic roles of the arguments augmented with selectional restrictions, and possible subcategorization frames consisting of syntactic description and semantic predicates with temporal information.
</prevsent>
<prevsent>vn thematic roles are relatively coarse, vs. the situation-specific framenet role system or theverb-specific propbank role system, enabling generalizations across wide semantic scope.
</prevsent>
</prevsection>
<citsent citstr=" N07-1069 ">
swier and stevenson (2005) <papid> H05-1111 </papid>and yi et al (2007) <papid> N07-1069 </papid>used vn for srl.</citsent>
<aftsection>
<nextsent>verb type classification.
</nextsent>
<nextsent>quite few works have addressed the issue of verb type classification and in particular classification to levin inspired?
</nextsent>
<nextsent>classes (e.g., (schulte im walde, 2000; merlo and stevenson, 2001)).<papid> J01-3003 </papid></nextsent>
<nextsent>such work is not comparable to ours, as it deals with verb type (sense) rather than verb token (instance) classification.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2007">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>verb type classification.
</prevsent>
<prevsent>quite few works have addressed the issue of verb type classification and in particular classification to levin inspired?
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
classes (e.g., (schulte im walde, 2000; merlo and stevenson, 2001)).<papid> J01-3003 </papid></citsent>
<aftsection>
<nextsent>such work is not comparable to ours, as it deals with verb type (sense) rather than verb token (instance) classification.
</nextsent>
<nextsent>verb token classification.
</nextsent>
<nextsent>lapata and brew (2004) <papid> J04-1003 </papid>dealt with classification to levin classes of polysemous verbs.</nextsent>
<nextsent>they established prior from the bnc in an unsupervised manner.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2009">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in order to create the data4, they used mapping between propbank role sets and vn classes, and took the instances in wsj sections 15-18,20,21 that were annotated by propbank and for which the roleset determines the vn class uniquely.
</prevsent>
<prevsent>this resulted in most instances being in fact monosemous.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
their 3propbank (palmer et al, 2005) <papid> J05-1004 </papid>is corpus annotation of the wsj sections of the penn treebank with semantic roles of each verbal proposition.</citsent>
<aftsection>
<nextsent>4semlink was not available then.
</nextsent>
<nextsent>10 experiment was conducted in wsj in-domain scenario, and in much narrower scope than in this paper.
</nextsent>
<nextsent>they had 870 (39 polysemous) unique verb lemmas, compared to 2091 (695 polysemous) in our in-domain scenario.
</nextsent>
<nextsent>they did not test their model in an adaptation scenario.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2011">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we compared our method to theirs for completeness and achieved similar results.
</prevsent>
<prevsent>semlink.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the semlink project (yi et al, 2007; <papid> N07-1069 </papid>loper et al, 2007) aims to create mapping of propbank, framenet (baker et al, 1998), <papid> P98-1013 </papid>wordnet (henceforth wn) and vn to one another, thus allowing these resources to synergize.</citsent>
<aftsection>
<nextsent>in addition,the project includes the most extensive token mapping of verbs to their vn classes available today.
</nextsent>
<nextsent>it covers all verbs in the wsj sections of the penn treebank within vn coverage (out of 113k verb instances, 97k have lemmas present in vn).
</nextsent>
<nextsent>polysemy is major issue in nlp.
</nextsent>
<nextsent>verbs are not an exception, resulting in single verb form (lemma)appearing in more than single class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2013">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> the learning model.  </section>
<citcontext>
<prevsection>
<prevsent>we first discuss the feature set and then the learning algorithm.features.
</prevsent>
<prevsent>our feature set heavily relies on syntactic annotation.
</prevsent>
</prevsection>
<citsent citstr=" C96-1055 ">
dorr and jones (1996) <papid> C96-1055 </papid>showed that perfect knowledge of the allowable syntactic frames for verb allows 98% accuracy in type assignment to levin classes.</citsent>
<aftsection>
<nextsent>this motivates the encoding of the syntactic structure of the sentence as features, since we have no access to all frames, only to the one appearing in the sentence.since some verbs may appear in the same syntactic frame in different vn classes, model relying on the syntactic frame alone would not be ableto disambiguate instances of these verbs when appearing in those frames.
</nextsent>
<nextsent>hence our features include lexical context words.
</nextsent>
<nextsent>the parse tree enables us to use words that appear in specific syntactic slots rather than in linear window around the verb.
</nextsent>
<nextsent>to this end, we use the headwords of the neighboring constituents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2014">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>we performed two parallel sets of experiments, one using manually created gold standard parse trees and one using parse trees created by state-of-the-art 9experiments on development data revealed that for verbs for which almost all of the training instances are mapped to the same vn class, it is most beneficial to select that class.
</prevsent>
<prevsent>thus, where more than 90% of the training instances of verbare mapped to the same class, our algorithm mapped the instances of the verb to that class regardless of the context.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
parser (charniak and johnson, 2005) <papid> P05-1022 </papid>note that this parser does not output function tags).</citsent>
<aftsection>
<nextsent>the parser was also trained on sections 02-21 and tunedon section 0010.
</nextsent>
<nextsent>consequently, our adaptation scenario is full adaptation situation in which both the parser and the verbnet training data are not in the test domain.
</nextsent>
<nextsent>note that generative parser adaptation results are known to be of much lower quality than in-domain results (lease and charniak, 2005).
</nextsent>
<nextsent>the quality of the discriminative parser we used did indeed decrease in our adaptation scenario (sec tion 7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2020">
<title id=" C08-1002.xml">a supervised algorithm for verb disambiguation into verbnet classes </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>this is an interesting direction for future work.in addition, we conducted some additional preliminary experiments in order to shed light on some aspects of the task.
</prevsent>
<prevsent>the experiments reported below were conducted on the development data, given gold standard parse trees.first, motivated by the close connection between wsd and our task (see section 3), we conducted an experiment to test the applicability ofusing wsd engine.
</prevsent>
</prevsection>
<citsent citstr=" P05-3014 ">
in addition to the experiments listed above, we also attempted to encode the output of modern wsd engine (the vbcollo cations model of sense learner 2.0 (mihalcea and csomai, 2005)), <papid> P05-3014 </papid>both by encoding the synset (ifexists) of the verb instance as feature, and by encoding each possible mapped class of the wsd engine output synset as feature.</citsent>
<aftsection>
<nextsent>there are 14 main scenario other is possible?
</nextsent>
<nextsent>(oip) scenario wsjwsj wsj genia wsjwsj wsj genia mf model mf model mf model mf model gold std total 93.68 96.42 69.09 73.16 88.6 92.78 46.41 52.46 er 43.35 13.17 36.67 11.29 poly.
</nextsent>
<nextsent>88.87 93.68 48.58 55.35 ? ?
</nextsent>
<nextsent>er 43.22 13.17 ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2022">
<title id=" C02-1101.xml">detecting errors in corpora using support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we apply the method to english and japanese pos-tagged corpora and achieve high precision in detecting errors.
</prevsent>
<prevsent>corpora are widely used in natural language processing today.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
for example, many statisticalpart-of-speech (pos) taggers have been developed and they use corpora as the training data to obtain statistical information or rules (brill, 1995; <papid> J95-4004 </papid>ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for natural language processing systems based on corpus, the quantity and quality of the corpus affect their performance.
</nextsent>
<nextsent>in general, corpora are annotated by hand, and therefore are error-prone.
</nextsent>
<nextsent>these errors are problematic for corpus-based systems.
</nextsent>
<nextsent>the errors become false training examples and deteriorate the performance of the systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2023">
<title id=" C02-1101.xml">detecting errors in corpora using support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we apply the method to english and japanese pos-tagged corpora and achieve high precision in detecting errors.
</prevsent>
<prevsent>corpora are widely used in natural language processing today.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
for example, many statisticalpart-of-speech (pos) taggers have been developed and they use corpora as the training data to obtain statistical information or rules (brill, 1995; <papid> J95-4004 </papid>ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>for natural language processing systems based on corpus, the quantity and quality of the corpus affect their performance.
</nextsent>
<nextsent>in general, corpora are annotated by hand, and therefore are error-prone.
</nextsent>
<nextsent>these errors are problematic for corpus-based systems.
</nextsent>
<nextsent>the errors become false training examples and deteriorate the performance of the systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2028">
<title id=" C02-1101.xml">detecting errors in corpora using support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, correcting the errors in corpus and improving its quality is important.
</prevsent>
<prevsent>however, to find and correct errors in corpora by hand is costly, since the size of corpora is usually very large.
</prevsent>
</prevsection>
<citsent citstr=" W99-0606 ">
hence, automatic detection of errors in corpora is necessary.one of the approaches for corpus error detection is use of machine learning techniques (abney et al, 1999; <papid> W99-0606 </papid>matsumoto and yamashita, 2000; ma et al, 2001).</citsent>
<aftsection>
<nextsent>these methods regard difficult elements for learning model (boosting or neural networks) to learn as corpus errors.abney et al (1999) <papid> W99-0606 </papid>studied corpus error detection using boosting.</nextsent>
<nextsent>boosting assigns weights to training examples, and the weights are large forthe examples that are difficult to classify.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2030">
<title id=" C02-1101.xml">detecting errors in corpora using support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these methods regard difficult elements for learning model (boosting or neural networks) to learn as corpus errors.abney et al (1999) <papid> W99-0606 </papid>studied corpus error detection using boosting.</prevsent>
<prevsent>boosting assigns weights to training examples, and the weights are large forthe examples that are difficult to classify.</prevsent>
</prevsection>
<citsent citstr=" A00-2020 ">
mislabeled examples caused by annotators tend to be difficult examples to classify and these authors conducted error detection of pos tags and ppattachment information in corpus by extracting examples with large weight.some probabilistic approaches for corpus error detection have also been studied (eskin,2000; <papid> A00-2020 </papid>murata et al, 2000).</citsent>
<aftsection>
<nextsent>eskin (2000) <papid> A00-2020 </papid>conducted corpus error detection using anomaly de tection.</nextsent>
<nextsent>he supposed that all the elements in acorpus are generated by mixture model consisting of two distributions, majority distribution (typically structured distribution) and an anomalous distribution (a uniform randomdistribution), and erroneous elements are generated by the anomalous distribution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2033">
<title id=" C02-1101.xml">detecting errors in corpora using support vector machines </title>
<section> corpus error detection using.  </section>
<citcontext>
<prevsection>
<prevsent>in the next subsection, we describe how to construct an svm model for pos tagging.
</prevsent>
<prevsent>2.2 revision learning for pos tagging.
</prevsent>
</prevsection>
<citsent citstr=" P02-1063 ">
we use revision learning method (nakagawa et al, 2002) <papid> P02-1063 </papid>for pos tagging with svms1.</citsent>
<aftsection>
<nextsent>this method creates training examples of svms with 1the well known one-versus-rest method (allwein et al., 2000) can be also used for pos tagging with svms,but it has large computational cost and cannot handle segmentation of words directly that is necessary for japanese morphological analysis.
</nextsent>
<nextsent>binary labels for each pos tag class using stochastic model (e.g. n-gram) as follows: each word in corpus becomes positive example of its pos tag class.
</nextsent>
<nextsent>we then build simple stochastic pos tagger based on n-gram (posbigram or trigram) model, and words in the corpus that the stochastic model failed to tag witha correct part-of-speech are collected as negative examples of the incorrect pos tag class.
</nextsent>
<nextsent>in such way, revision learning makes model of svms to revise outputs of the stochastic model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2037">
<title id=" C04-1072.xml">orange a method for evaluating automatic evaluation metrics for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a similar metric, nist, used by nist (nist 2002) in couple of machine translation evaluations in the past two years is based on bleu.
</prevsent>
<prevsent>the main idea of bleu is to measure the translation closeness between candidate translation and set of reference translations with numerical metric.
</prevsent>
</prevsection>
<citsent citstr=" C92-2067 ">
although the idea of using objective functions to automatically evaluate machine translation quality is not new (su et al 1992), <papid> C92-2067 </papid>the success of bleu prompts lot of interests in developing better automatic evaluation metrics.</citsent>
<aftsection>
<nextsent>for example, akiba et al (2001) proposed metric called red based on edit distances over set of multiple references.
</nextsent>
<nextsent>nieen et al (2000) calculated the length normalized edit distance, called word error rate (wer), between candidate and multiple reference translations.
</nextsent>
<nextsent>leusch et al (2003) proposed related measure called position independent word error rate (per) that did not consider word position, i.e. using bag-of-words instead.
</nextsent>
<nextsent>turian et al (2003) introduced general text matcher (gtm) based on accuracy measures such as recall, precision, and f-measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2038">
<title id=" C04-1072.xml">orange a method for evaluating automatic evaluation metrics for machine translation </title>
<section> orange.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, reference translations should be ranked higher than machine translations on average if good automatic evaluation metric is used.
</prevsent>
<prevsent>based on these assumptions, we propose new automatic evaluation method for evaluation of automatic machine translation metrics as follows: given source sentence, its machine translations, and its reference translations, we compute the average rank of the reference translations within the combined machine and reference translation list.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
for example, statistical machine translation system such as isis altemp smt system (och 2003) <papid> P03-1021 </papid>can generate list of n-best alternative translations given source sentence.</citsent>
<aftsection>
<nextsent>we compute the automatic scores for the n-best translations and their reference translations.
</nextsent>
<nextsent>we then rank these translations, calculate the average rank of the references in the n-best list, and compute the ratio of the average reference rank to the length of the n-best list.
</nextsent>
<nextsent>we call this ratio orange?
</nextsent>
<nextsent>(oracle1 ranking for gisting evaluation) and the smaller the ratio is, the better the automatic metric is. there are several advantages of the proposed orange evaluation method: ? no extra human involvement ? orange uses the existing human references but not human evaluations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2041">
<title id=" C02-1097.xml">word sense disambiguation using static and dynamic sense vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>co-occurring words within limited window-sized context support one sense among the semantically ambiguous ones of the word.
</prevsent>
<prevsent>the problem is to find the most effective patterns in order to capture the right sense.
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
it is true that they have similar context and co-occurrence information when words are used with the same sense (rigau, et al, 1997).<papid> P97-1007 </papid></citsent>
<aftsection>
<nextsent>it is also true that contextual words nearby an ambiguous word give more effective patterns or features than those far from it (chen, et al, 1998).
</nextsent>
<nextsent>in this paper, we represent each sense of word as vector in word space.
</nextsent>
<nextsent>first, contextual words in the training sense tagged data4 are represented as context vectors.
</nextsent>
<nextsent>then, ambiguous word in given context of wt?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2042">
<title id=" C02-1097.xml">word sense disambiguation using static and dynamic sense vectors </title>
<section> word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>they provide supporting vector for certain sense.
</prevsent>
<prevsent>contextual words nearby target word give more relevant information to decide its sense than those far from it.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
distance from target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense (yarowsky, 1995).<papid> P95-1026 </papid></citsent>
<aftsection>
<nextsent>each word in the training samples can be weighted by formula (1).
</nextsent>
<nextsent>let wij(tk) represent weighting function for term tk, which appears in the jth training sample for the ith sense, tfijk 5 pos, collocations, semantic word associations,.
</nextsent>
<nextsent>subcategorization information, semantic roles, selectional preferences and frequency of senses are useful for wsd (agirre et al, 2001).
</nextsent>
<nextsent>6 since, the length of context window was.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2043">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
role-semantic analysis automatically (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005; baker et al , 2007).<papid> W07-2018 </papid></citsent>
<aftsection>
<nextsent>it is widely conjectured that an increased srl accuracy will lead to improvements in certain nlp applications, especially template-filling systems.
</nextsent>
<nextsent>srl has also been used in prototypes of more advancedsemantics-based applications such as textual entailment recognition.
</nextsent>
<nextsent>it has previously been shown that srl systems need syntactic structure as input (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al , 2008).</nextsent>
<nextsent>an important consideration is then what information this input should represent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2044">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
role-semantic analysis automatically (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005; baker et al , 2007).<papid> W07-2018 </papid></citsent>
<aftsection>
<nextsent>it is widely conjectured that an increased srl accuracy will lead to improvements in certain nlp applications, especially template-filling systems.
</nextsent>
<nextsent>srl has also been used in prototypes of more advancedsemantics-based applications such as textual entailment recognition.
</nextsent>
<nextsent>it has previously been shown that srl systems need syntactic structure as input (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al , 2008).</nextsent>
<nextsent>an important consideration is then what information this input should represent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2046">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" W07-2018 ">
role-semantic analysis automatically (gildea and jurafsky, 2002; <papid> J02-3001 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005; baker et al , 2007).<papid> W07-2018 </papid></citsent>
<aftsection>
<nextsent>it is widely conjectured that an increased srl accuracy will lead to improvements in certain nlp applications, especially template-filling systems.
</nextsent>
<nextsent>srl has also been used in prototypes of more advancedsemantics-based applications such as textual entailment recognition.
</nextsent>
<nextsent>it has previously been shown that srl systems need syntactic structure as input (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al , 2008).</nextsent>
<nextsent>an important consideration is then what information this input should represent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2048">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is widely conjectured that an increased srl accuracy will lead to improvements in certain nlp applications, especially template-filling systems.
</prevsent>
<prevsent>srl has also been used in prototypes of more advancedsemantics-based applications such as textual entailment recognition.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
it has previously been shown that srl systems need syntactic structure as input (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al , 2008).</citsent>
<aftsection>
<nextsent>an important consideration is then what information this input should represent.
</nextsent>
<nextsent>by habit, most systems for automatic role-semantic analysis have used penn style constituents (marcus et al , 1993) <papid> J93-2004 </papid>produced by collins?</nextsent>
<nextsent>(1997) or charniaks (2000) parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2049">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it has previously been shown that srl systems need syntactic structure as input (gildea and palmer, 2002; <papid> P02-1031 </papid>punyakanok et al , 2008).</prevsent>
<prevsent>an important consideration is then what information this input should represent.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
by habit, most systems for automatic role-semantic analysis have used penn style constituents (marcus et al , 1993) <papid> J93-2004 </papid>produced by collins?</citsent>
<aftsection>
<nextsent>(1997) or charniaks (2000) parsers.
</nextsent>
<nextsent>the influence of the syntactic formalism on srlhas only been considered in few previous articles.
</nextsent>
<nextsent>for instance, gildea and hockenmaier (2003) <papid> W03-1008 </papid>reported that ccg-based parser gives improved results over the collins parser.dependency syntax has only received little attention for the srl task, despite surge of interest in dependency parsing during the last few years (buchholz and marsi, 2006).<papid> W06-2920 </papid></nextsent>
<nextsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2050">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1997) or charniaks (2000) parsers.
</prevsent>
<prevsent>the influence of the syntactic formalism on srlhas only been considered in few previous articles.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
for instance, gildea and hockenmaier (2003) <papid> W03-1008 </papid>reported that ccg-based parser gives improved results over the collins parser.dependency syntax has only received little attention for the srl task, despite surge of interest in dependency parsing during the last few years (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?
</nextsent>
<nextsent>et al  (2002) and hacioglu (2004).<papid> C04-1186 </papid></nextsent>
<nextsent>two studies that compared the respective performances ofconstituent-based and dependency-based srl systems (pradhan et al , 2005; <papid> P05-1072 </papid>swanson and gordon, 2006), <papid> P06-2104 </papid>both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by very wide mar gin.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2051">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1997) or charniaks (2000) parsers.
</prevsent>
<prevsent>the influence of the syntactic formalism on srlhas only been considered in few previous articles.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
for instance, gildea and hockenmaier (2003) <papid> W03-1008 </papid>reported that ccg-based parser gives improved results over the collins parser.dependency syntax has only received little attention for the srl task, despite surge of interest in dependency parsing during the last few years (buchholz and marsi, 2006).<papid> W06-2920 </papid></citsent>
<aftsection>
<nextsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?
</nextsent>
<nextsent>et al  (2002) and hacioglu (2004).<papid> C04-1186 </papid></nextsent>
<nextsent>two studies that compared the respective performances ofconstituent-based and dependency-based srl systems (pradhan et al , 2005; <papid> P05-1072 </papid>swanson and gordon, 2006), <papid> P06-2104 </papid>both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by very wide mar gin.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2052">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, gildea and hockenmaier (2003) <papid> W03-1008 </papid>reported that ccg-based parser gives improved results over the collins parser.dependency syntax has only received little attention for the srl task, despite surge of interest in dependency parsing during the last few years (buchholz and marsi, 2006).<papid> W06-2920 </papid></prevsent>
<prevsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?</prevsent>
</prevsection>
<citsent citstr=" C04-1186 ">
et al  (2002) and hacioglu (2004).<papid> C04-1186 </papid></citsent>
<aftsection>
<nextsent>two studies that compared the respective performances ofconstituent-based and dependency-based srl systems (pradhan et al , 2005; <papid> P05-1072 </papid>swanson and gordon, 2006), <papid> P06-2104 </papid>both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by very wide mar gin.</nextsent>
<nextsent>however, the figures reported in these studies can be misleading since the comparison involved 10-year-old rule-based dependency parser versus state-of-the-art statistical constituent parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2053">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?
</prevsent>
<prevsent>et al  (2002) and hacioglu (2004).<papid> C04-1186 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1072 ">
two studies that compared the respective performances ofconstituent-based and dependency-based srl systems (pradhan et al , 2005; <papid> P05-1072 </papid>swanson and gordon, 2006), <papid> P06-2104 </papid>both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by very wide mar gin.</citsent>
<aftsection>
<nextsent>however, the figures reported in these studies can be misleading since the comparison involved 10-year-old rule-based dependency parser versus state-of-the-art statistical constituent parser.
</nextsent>
<nextsent>the recent progress in statistical dependency parsing gives grounds for new evaluation.
</nextsent>
<nextsent>393in addition, there are number of linguistic motivations why dependency syntax could be beneficial in an srl context.
</nextsent>
<nextsent>first, complex linguistic phenomena such as wh-word extraction and topical ization can be transparently represented by allowing non projective dependency links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2054">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>early examples ofdependency-based srl systems, which used gold standard dependency treebanks, include abokrtsk?
</prevsent>
<prevsent>et al  (2002) and hacioglu (2004).<papid> C04-1186 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-2104 ">
two studies that compared the respective performances ofconstituent-based and dependency-based srl systems (pradhan et al , 2005; <papid> P05-1072 </papid>swanson and gordon, 2006), <papid> P06-2104 </papid>both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by very wide mar gin.</citsent>
<aftsection>
<nextsent>however, the figures reported in these studies can be misleading since the comparison involved 10-year-old rule-based dependency parser versus state-of-the-art statistical constituent parser.
</nextsent>
<nextsent>the recent progress in statistical dependency parsing gives grounds for new evaluation.
</nextsent>
<nextsent>393in addition, there are number of linguistic motivations why dependency syntax could be beneficial in an srl context.
</nextsent>
<nextsent>first, complex linguistic phenomena such as wh-word extraction and topical ization can be transparently represented by allowing non projective dependency links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2055">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these links also justify why dependency syntax is often considered superior for free-word-order lan guages; it is even very questionable whether the traditional constituent-based srl strategies are viable for such languages.
</prevsent>
<prevsent>second, grammatical function such as subject and object is an integral concept in dependency syntax.
</prevsent>
</prevsection>
<citsent citstr=" P83-1010 ">
this concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as absity (hirst, 1983).<papid> P83-1010 </papid></citsent>
<aftsection>
<nextsent>however, except from few tentative experiments (toutanova et al , 2005), <papid> P05-1073 </papid>grammatical function isnot explicitly used by current automatic srl systems, but instead emulated from constituent trees by features like the constituent position and the governing category.</nextsent>
<nextsent>more generally, these linguistic reasons have made number of linguists argue that dependency structures are more suit able for explaining the syntax-semantics interface (melcuk, 1988; hudson, 1984).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2056">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, grammatical function such as subject and object is an integral concept in dependency syntax.
</prevsent>
<prevsent>this concept is intuitive when reasoning about the link between syntax and semantics, and it has been used earlier in semantic interpreters such as absity (hirst, 1983).<papid> P83-1010 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
however, except from few tentative experiments (toutanova et al , 2005), <papid> P05-1073 </papid>grammatical function isnot explicitly used by current automatic srl systems, but instead emulated from constituent trees by features like the constituent position and the governing category.</citsent>
<aftsection>
<nextsent>more generally, these linguistic reasons have made number of linguists argue that dependency structures are more suit able for explaining the syntax-semantics interface (melcuk, 1988; hudson, 1984).
</nextsent>
<nextsent>in this work, we provide new evaluation ofthe influence of the syntactic representation on semantic role labeling in english.
</nextsent>
<nextsent>contrary to previously reported results, we show that dependency based systems are on par with constituent-based systems or perform nearly as well.
</nextsent>
<nextsent>furthermore,we show that semantic role classifiers using dependency parser learn faster than their constituent based counterparts and therefore need less training data to achieve similar performances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2058">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> statistical dependency parsing for.  </section>
<citcontext>
<prevsection>
<prevsent>english except for small-scale efforts, there is no dependency treebank of significant size for english.
</prevsent>
<prevsent>statistical dependency parsers of english must therefore relyon dependency structures automatically converted from constituent corpus such as the penn treebank (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
typical approaches to conversion of constituent structures into dependencies are based on hand constructed head percolation rules, an idea that hasits roots in lexicalized constituent parsing (magerman, 1994; collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>the head rules created by yamada and matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of english (nivre and scholz, 2004; <papid> C04-1010 </papid>mcdonald et al , 2005).<papid> P05-1012 </papid>recently, johansson and nugues (2007) extended the head percolation strategy to incorporatelong-distance links such as wh-movement and top icalization, and used the full set of grammatical function tags from penn in addition to number of inferred tags (in total 57 function tags).</nextsent>
<nextsent>a dependency parser based on this syntax was used in the best-performing system in the semeval-2007 task on frame-semantic structure extraction (baker etal., 2007), <papid> W07-2018 </papid>and the conversion method (in two different forms) was used for the english data in the conll shared tasks of 2007 and 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2060">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> statistical dependency parsing for.  </section>
<citcontext>
<prevsection>
<prevsent>statistical dependency parsers of english must therefore relyon dependency structures automatically converted from constituent corpus such as the penn treebank (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>typical approaches to conversion of constituent structures into dependencies are based on hand constructed head percolation rules, an idea that hasits roots in lexicalized constituent parsing (magerman, 1994; collins, 1997).<papid> P97-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
the head rules created by yamada and matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of english (nivre and scholz, 2004; <papid> C04-1010 </papid>mcdonald et al , 2005).<papid> P05-1012 </papid>recently, johansson and nugues (2007) extended the head percolation strategy to incorporatelong-distance links such as wh-movement and top icalization, and used the full set of grammatical function tags from penn in addition to number of inferred tags (in total 57 function tags).</citsent>
<aftsection>
<nextsent>a dependency parser based on this syntax was used in the best-performing system in the semeval-2007 task on frame-semantic structure extraction (baker etal., 2007), <papid> W07-2018 </papid>and the conversion method (in two different forms) was used for the english data in the conll shared tasks of 2007 and 2008.</nextsent>
<nextsent>with constituents and dependencies to study the influence of syntactic representation on srl performance, we developed framework that could be easily parametrized to process either constituent or dependency input1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2061">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> statistical dependency parsing for.  </section>
<citcontext>
<prevsection>
<prevsent>statistical dependency parsers of english must therefore relyon dependency structures automatically converted from constituent corpus such as the penn treebank (marcus et al , 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>typical approaches to conversion of constituent structures into dependencies are based on hand constructed head percolation rules, an idea that hasits roots in lexicalized constituent parsing (magerman, 1994; collins, 1997).<papid> P97-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
the head rules created by yamada and matsumoto (2003) have been used in almost all recent work on statistical dependency parsing of english (nivre and scholz, 2004; <papid> C04-1010 </papid>mcdonald et al , 2005).<papid> P05-1012 </papid>recently, johansson and nugues (2007) extended the head percolation strategy to incorporatelong-distance links such as wh-movement and top icalization, and used the full set of grammatical function tags from penn in addition to number of inferred tags (in total 57 function tags).</citsent>
<aftsection>
<nextsent>a dependency parser based on this syntax was used in the best-performing system in the semeval-2007 task on frame-semantic structure extraction (baker etal., 2007), <papid> W07-2018 </papid>and the conversion method (in two different forms) was used for the english data in the conll shared tasks of 2007 and 2008.</nextsent>
<nextsent>with constituents and dependencies to study the influence of syntactic representation on srl performance, we developed framework that could be easily parametrized to process either constituent or dependency input1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2064">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> automatic semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>with constituents and dependencies to study the influence of syntactic representation on srl performance, we developed framework that could be easily parametrized to process either constituent or dependency input1.
</prevsent>
<prevsent>this section describes its implementation.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
as the role-semantic paradigm, we used framenet (baker et al , 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>3.1 systems.
</nextsent>
<nextsent>we built srl systems based on six differentparsers.
</nextsent>
<nextsent>all parsers were trained on the penn tree bank, either directly for the constituent parsers or through the lth constituent-to-dependency converter (johansson and nugues, 2007).
</nextsent>
<nextsent>our systems are identified as follows: lth.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2067">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> automatic semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>mst.
</prevsent>
<prevsent>a dependency-based system using mst parser (mcdonald et al , 2005).<papid> P05-1012 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
c&amp;j.; constituent-based system using the reranking parser (the may 2006 version) by charniak and johnson (2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>charniak.
</nextsent>
<nextsent>a constituent-based system using charniaks parser (charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>collins.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2068">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> automatic semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>c&amp;j.; constituent-based system using the reranking parser (the may 2006 version) by charniak and johnson (2005).<papid> P05-1022 </papid></prevsent>
<prevsent>charniak.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
a constituent-based system using charniaks parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>collins.
</nextsent>
<nextsent>a constituent-based system using collins?
</nextsent>
<nextsent>parser (collins, 1997).<papid> P97-1003 </papid></nextsent>
<nextsent>1our implementation is available for download at http://nlp.cs.lth.se/fnlabeler.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2071">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> automatic semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>charniaks and collins?
</prevsent>
<prevsent>parsers are widely used constituent parsers for english, andthe c&j; parser is the best-performing freely available constituent parser at the time of writing according to published figures.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
charniaks parserand the c&j; parser come with built-in part-of speech tagger; all other systems used the stanford tagger (toutanova et al , 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>following gildea and jurafsky (2002), <papid> J02-3001 </papid>the srl problem is traditionally divided into two subtasks: identifying the arguments and labeling them with semantic roles.</nextsent>
<nextsent>although state-of-the-art srl systems use sophisticated statistical models to perform these two tasks jointly (e.g. toutanova etal., 2005, <papid> P05-1073 </papid>johansson and nugues, 2008), we implemented them as two independent support vector classifiers to be able to analyze the impact of syntactic representation on each task separately.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2074">
<title id=" C08-1050.xml">the effect of syntactic representation on semantic role labeling </title>
<section> automatic semantic role labeling.  </section>
<citcontext>
<prevsection>
<prevsent>in both cases, the identi argument argument features identification classification target lemma c,d c,d fes c,d c,d target pos c,d c,d voice c,d c,d position c,d c,d argword/pos c,d c,d leftword/pos c,d c,d rightword/pos c,d c,d parentword/pos c,d c-subcat c c-path c phrase type c govcat c d-subcat d d-path d childdepset d parenthasobj reltoparent function table 1: classifier features.
</prevsent>
<prevsent>the features used by the constituent-based and the dependency-based systems are marked and d, respectively.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
fic ation step was preceded by pruning stage that heuristic ally removes parse tree nodes unlikely to represent arguments (xue and palmer, 2004).<papid> W04-3212 </papid>to score the performance of the argument identifier, traditional evaluation procedures treat the identification as bracketing problem, meaning that the entities scored by the evaluation procedure are labeled snippets of text; however, it is questionable whether this is the proper way to evaluate task whose purpose is to find semantic relations between logical entities.</citsent>
<aftsection>
<nextsent>we believe that the same criticisms that have been leveled at the parseval metric for constituent structures are equally valid for the bracket-based evaluation ofsrl systems.
</nextsent>
<nextsent>the inappropriateness of the traditional metric has led to number of alternative metrics (litkowski, 2004; <papid> W04-0803 </papid>baker et al , 2007).<papid> W07-2018 </papid></nextsent>
<nextsent>we have stuck to the traditional bracket-basedscoring metric for compatibility with previous results, but since it represents the arguments as labeled spans, conversion step is needed when using dependencies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2080">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the specialized models of alternation do not perform as well,closer examination reveals alternation behavior represented implicitly in the generic models.
</prevsent>
<prevsent>recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora.
</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
mccarthy (2000), <papid> A00-2034 </papid>merlo and stevenson (2001), <papid> J01-3003 </papid>and schulte im walde (2000) have evaluated their sys tems?</citsent>
<aftsection>
<nextsent>accuracy against human judgments of verb classification, with the comprehensive verb classes of levin (1993) often serving as gold standard.
</nextsent>
<nextsent>another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999),<papid> P99-1014 </papid>without focusing specifically on alternation behavior.</nextsent>
<nextsent>we aim to bring these strands of research together with unified probabilistic model of verb argument structure incorporating alternation behavior.unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important pieceof the language understanding problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2081">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the specialized models of alternation do not perform as well,closer examination reveals alternation behavior represented implicitly in the generic models.
</prevsent>
<prevsent>recent research into verb-argument structure has has attempted to acquire the syntactic alternation behavior of verbs directly from large corpora.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
mccarthy (2000), <papid> A00-2034 </papid>merlo and stevenson (2001), <papid> J01-3003 </papid>and schulte im walde (2000) have evaluated their sys tems?</citsent>
<aftsection>
<nextsent>accuracy against human judgments of verb classification, with the comprehensive verb classes of levin (1993) often serving as gold standard.
</nextsent>
<nextsent>another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999),<papid> P99-1014 </papid>without focusing specifically on alternation behavior.</nextsent>
<nextsent>we aim to bring these strands of research together with unified probabilistic model of verb argument structure incorporating alternation behavior.unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important pieceof the language understanding problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2082">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mccarthy (2000), <papid> A00-2034 </papid>merlo and stevenson (2001), <papid> J01-3003 </papid>and schulte im walde (2000) have evaluated their sys tems?</prevsent>
<prevsent>accuracy against human judgments of verb classification, with the comprehensive verb classes of levin (1993) often serving as gold standard.</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999),<papid> P99-1014 </papid>without focusing specifically on alternation behavior.</citsent>
<aftsection>
<nextsent>we aim to bring these strands of research together with unified probabilistic model of verb argument structure incorporating alternation behavior.unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important pieceof the language understanding problem.
</nextsent>
<nextsent>learning the alternation behavior of verbs automatically from unannotated text would significantly reduce the amount of labor needed to create text understanding systems, whether that labor takes the form of writing lexical entries or of annotating semantic information to train statistical systems.our use of generative probabilistic models of argument structure also allows for language modeling applications independent of semantic interpretation.
</nextsent>
<nextsent>language models based on head-modifier lexical dependencies in syntactic trees have been shown to have lower perplexity than n-gram language model sand to reduce word-error rates for speech recognition (chelba and jelinek, 1999; roark, 2001).<papid> J01-2004 </papid></nextsent>
<nextsent>incorporating semantic classes and verb alternation behavior could improve such models?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2083">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>mccarthy (2000), <papid> A00-2034 </papid>merlo and stevenson (2001), <papid> J01-3003 </papid>and schulte im walde (2000) have evaluated their sys tems?</prevsent>
<prevsent>accuracy against human judgments of verb classification, with the comprehensive verb classes of levin (1993) often serving as gold standard.</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
another area of research has focused on automatic clustering algorithms for verbs and their arguments with the goal of finding groups of semantically related words (pereira et al, 1993; <papid> P93-1024 </papid>rooth et al, 1999),<papid> P99-1014 </papid>without focusing specifically on alternation behavior.</citsent>
<aftsection>
<nextsent>we aim to bring these strands of research together with unified probabilistic model of verb argument structure incorporating alternation behavior.unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important pieceof the language understanding problem.
</nextsent>
<nextsent>learning the alternation behavior of verbs automatically from unannotated text would significantly reduce the amount of labor needed to create text understanding systems, whether that labor takes the form of writing lexical entries or of annotating semantic information to train statistical systems.our use of generative probabilistic models of argument structure also allows for language modeling applications independent of semantic interpretation.
</nextsent>
<nextsent>language models based on head-modifier lexical dependencies in syntactic trees have been shown to have lower perplexity than n-gram language model sand to reduce word-error rates for speech recognition (chelba and jelinek, 1999; roark, 2001).<papid> J01-2004 </papid></nextsent>
<nextsent>incorporating semantic classes and verb alternation behavior could improve such models?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2084">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we aim to bring these strands of research together with unified probabilistic model of verb argument structure incorporating alternation behavior.unraveling the mapping between syntactic functions such as subject and object and semantic roles such as agent and patient is an important pieceof the language understanding problem.
</prevsent>
<prevsent>learning the alternation behavior of verbs automatically from unannotated text would significantly reduce the amount of labor needed to create text understanding systems, whether that labor takes the form of writing lexical entries or of annotating semantic information to train statistical systems.our use of generative probabilistic models of argument structure also allows for language modeling applications independent of semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
language models based on head-modifier lexical dependencies in syntactic trees have been shown to have lower perplexity than n-gram language model sand to reduce word-error rates for speech recognition (chelba and jelinek, 1999; roark, 2001).<papid> J01-2004 </papid></citsent>
<aftsection>
<nextsent>incorporating semantic classes and verb alternation behavior could improve such models?
</nextsent>
<nextsent>performance.
</nextsent>
<nextsent>automatically derived word clusters are used in the statistical parsers of charniak (1997) and magerman (1995).<papid> P95-1037 </papid></nextsent>
<nextsent>incorporating alternation behavior into such models might improve parsing results as well.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2085">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>incorporating semantic classes and verb alternation behavior could improve such models?
</prevsent>
<prevsent>performance.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
automatically derived word clusters are used in the statistical parsers of charniak (1997) and magerman (1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>incorporating alternation behavior into such models might improve parsing results as well.
</nextsent>
<nextsent>this paper focuses on evaluating probabilistic models of verb-argument structure in terms of how well they model unseen test data, as measured by perplexity.
</nextsent>
<nextsent>we will examine maximum likelihood bigram and trigram models, clustering models based on those of rooth et al (1999), <papid> P99-1014 </papid>as well as new probabilistic model designed to capture alternations in verb-argument structure.</nextsent>
<nextsent>automatic clustering of co-occurrences of verbs and their direct objects was first used to induce semantically related classes of both verbs and nouns (pereira et al, 1993).<papid> P93-1024 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2098">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>the verb lower might belong to different cluster because,although it appears with the same nouns, they appear as the direct object but not as the subject.
</prevsent>
<prevsent>the expectation maximization algorithm is used to train the model from the corpus, ite rating over an expectation step in which expected values for the two unobserved variables and are calculated foreach observation in the training data, and maximization step in which the parameter of each of the five distributions (c), (vjc), (sjc), (rjc; s), and (njn; c) are set to maximize the likelihood of the data given the expectations for and r.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
for our experiments we used version of the british national corpus parsed with the statistical parser of collins (1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>subject and direct object relations were extracted by searching for np nodes dominated by and vp nodes respectively.
</nextsent>
<nextsent>the headwords of the resulting subject and object nodes were found using the deterministic headword rules employed by the parsing model.
</nextsent>
<nextsent>the individual observations of our dataset are noun-verb pairs of threetypes: direct object, subject of verb with an object, and subject of verb without an object.
</nextsent>
<nextsent>as result, the subject and object relations of the same original sentence are considered independently by all of the models we examine.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2099">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>the individual observations of our dataset are noun-verb pairs of threetypes: direct object, subject of verb with an object, and subject of verb without an object.
</prevsent>
<prevsent>as result, the subject and object relations of the same original sentence are considered independently by all of the models we examine.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
direct object noun phrases were assigned the function tags of the treebank-2 annotation style (marcus et al, 1994) <papid> H94-1020 </papid>in order to distinguish noun phrases such as temporal adjuncts from true directobjects.</citsent>
<aftsection>
<nextsent>for example, in the sentence he ate yes terday?, yesterday would be assigned the temporal tag, and therefore not considered direct object for our purposes.
</nextsent>
<nextsent>similarly, in the sentence interest rates rose 2%?, 2% would be assigned the extent tag, and this instance of rise would be considered intransitive.function tags were assigned using simple probability model trained on the wall street journal data from the penn treebank, in technique similar to that of blaheta and charniak (2000).<papid> A00-2031 </papid></nextsent>
<nextsent>the model predicts the function tag conditioned on the verb and head noun of the noun phrase: (f jv; n) = ( ~ (f jv; n) (v; n) 2 1 2 ~ (f jv) + 1 2 ~ (f jn) otherwise where ranges over the function tags defined (mar cus et al, 1994), <papid> H94-1020 </papid>or the null tag.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2100">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> the data.  </section>
<citcontext>
<prevsection>
<prevsent>direct object noun phrases were assigned the function tags of the treebank-2 annotation style (marcus et al, 1994) <papid> H94-1020 </papid>in order to distinguish noun phrases such as temporal adjuncts from true directobjects.</prevsent>
<prevsent>for example, in the sentence he ate yes terday?, yesterday would be assigned the temporal tag, and therefore not considered direct object for our purposes.</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
similarly, in the sentence interest rates rose 2%?, 2% would be assigned the extent tag, and this instance of rise would be considered intransitive.function tags were assigned using simple probability model trained on the wall street journal data from the penn treebank, in technique similar to that of blaheta and charniak (2000).<papid> A00-2031 </papid></citsent>
<aftsection>
<nextsent>the model predicts the function tag conditioned on the verb and head noun of the noun phrase: (f jv; n) = ( ~ (f jv; n) (v; n) 2 1 2 ~ (f jv) + 1 2 ~ (f jn) otherwise where ranges over the function tags defined (mar cus et al, 1994), <papid> H94-1020 </papid>or the null tag.</nextsent>
<nextsent>only cases assigned the null tag by this model were considered true direct objects.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2103">
<title id=" C02-1132.xml">probabilistic models of verb argument structure </title>
<section> noun-slot aspect 2.66m 6.56m.  </section>
<citcontext>
<prevsection>
<prevsent>we have attempted to learn the mapping from syntactic position to semantic role in an unsupervised manner, and have evaluated the results in terms of our systems?
</prevsent>
<prevsent>success as language model for unseen data.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
the models designed to explicit represent verb alternation behavior did not perform as well by this metric as other, simpler probability models.a perspective on this work can be gained by comparison with attempts at unsupervised learning ofother natural language phenomena including part of-speech tagging (merialdo, 1994) <papid> J94-2001 </papid>and syntactic dependencies (carroll and charniak, 1992; paskin, 2001).</citsent>
<aftsection>
<nextsent>while models trained using the expectation maximization algorithm do well at fitting the data,the results may not correspond to the human analyses they were intended to learn.
</nextsent>
<nextsent>language does not exist in the abstract, but conveys information about the world, and the ultimate goal of grammar induction is not just to model strings but to extract thisinformation.
</nextsent>
<nextsent>this suggests that although the probability models constrained to represent verb alternation behavior did not achieve the best perplexity results, they may be useful as part of an understanding system which assigns semantic roles to arguments.
</nextsent>
<nextsent>the implicit representation of alternation behavior in our generic clustering model also suggests using its clusters to initialize more complex model capable of assigning semantic roles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2104">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the result is an accurate wide-coverage ccg parser which is an order of magnitude faster than comparable systems for other linguistically motivated formalisms.
</prevsent>
<prevsent>lexicalised grammar formalisms such as lexicalized tree adjoining grammar (ltag) and com bina tory categorial grammar (ccg) assign one or more syntactic structures to each word in sentence which are then manipulated by the parser.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
super tag ging was introduced for ltag as way of increasing parsing efficiency by reducing the number of structures assigned to each word (bangalore and joshi, 1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>super tagging has more recently been applied to ccg (clark, 2002; curran and clark, 2003).<papid> E03-1071 </papid>super tagging accuracy is relatively high for manually constructed ltags (bangalore and joshi,1999).<papid> J99-2004 </papid></nextsent>
<nextsent>however, for ltags extracted automatically from the penn treebank, performance is much lower (chen et al, 1999; chen et al, 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2105">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lexicalised grammar formalisms such as lexicalized tree adjoining grammar (ltag) and com bina tory categorial grammar (ccg) assign one or more syntactic structures to each word in sentence which are then manipulated by the parser.
</prevsent>
<prevsent>super tag ging was introduced for ltag as way of increasing parsing efficiency by reducing the number of structures assigned to each word (bangalore and joshi, 1999).<papid> J99-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" E03-1071 ">
super tagging has more recently been applied to ccg (clark, 2002; curran and clark, 2003).<papid> E03-1071 </papid>super tagging accuracy is relatively high for manually constructed ltags (bangalore and joshi,1999).<papid> J99-2004 </papid></citsent>
<aftsection>
<nextsent>however, for ltags extracted automatically from the penn treebank, performance is much lower (chen et al, 1999; chen et al, 2002).
</nextsent>
<nextsent>in fact, performance for such grammars is below that needed for successful integration into full parser (sarkar et al, 2000).
</nextsent>
<nextsent>in this paper we demonstrate that ccg super tagging accuracy is not only sufficient for accurate and robust parsing using an automatically extracted grammar, but also offers several practical advantages.
</nextsent>
<nextsent>our wide-coverage ccg parser uses log-linear model to select an analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2107">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> ccg super tagging.  </section>
<citcontext>
<prevsection>
<prevsent>our results confirm that wide-coverage ccg parsing is feasible for many large-scale nlp tasks.
</prevsent>
<prevsent>parsing using ccg can be viewed as two-stage process: first assign lexical categories to the wordsin the sentence, and then combine the categories together using ccgs combinatory rules.1 the first stage can be accomplished by simply assigning to each word all categories from the words entry in the lexicon (hockenmaier, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P02-1042 ">
1see steedman (2000) for an introduction to ccg, and see clark et al (2002) <papid> P02-1042 </papid>and hockenmaier (2003) for an introduction to wide-coverage parsing using ccg.</citsent>
<aftsection>
<nextsent>the wsj is publication that enjoy reading np/n (s[dcl]\np)/np np/n (np\np)/(s[dcl]/np) np (s[dcl]\np)/(s[ng]\np) (s[ng]\np)/np figure 1: example sentence with ccg lexical categories frequency # cat types # cat tokens in # sentences in 2-21 # cat tokens in # sentences in 00 cut-off 2-21 not in cat set with missing cat 00 not in cat set with missing cat 1 1 225 0 0 12 (0.03%) 12 (0.6%) 10 409 1 933 (0.2%) 1 712 (4.3%) 79 (0.2%) 69 (3.6%) table 1: statistics for the lexical category setan alternative is to use statistical tagging approach to assign one or more categories.
</nextsent>
<nextsent>a statistical model can be used to determine the most likely categories given the words context.
</nextsent>
<nextsent>the advantage of this super tagging approach is that the number of categories assigned to each word can be reduced, with correspondingly massive reduction in the number of derivations.
</nextsent>
<nextsent>bangalore and joshi (1999) <papid> J99-2004 </papid>use standard markov model tagger to assign ltag elementary trees to words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2114">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>to give one example, the number of categories in the tag dictionarys entry for the wordis is 45 (only considering categories which have appeared at least 10 times in the training data).
</prevsent>
<prevsent>however, in the sentence mr. vinken is chairman of elsevier n.v., the dutch publishing group., the super tag ger correctly assigns 1 category to is for ? = 0.1, and 3 categories for ? = 0.01.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
the parser is described in detail in clark and curran (2004).<papid> P04-1014 </papid></citsent>
<aftsection>
<nextsent>it takes pos tagged sentences as input with each word assigned set of lexical categories.
</nextsent>
<nextsent>a packed chart is used to efficiently represent all of the possible analyses for sentence, and the cky chart parsing algorithm described in steedman (2000) is used to build the chart.
</nextsent>
<nextsent>clark and curran (2004) <papid> P04-1014 </papid>evaluate number of log-linear parsing models for ccg.</nextsent>
<nextsent>in this paper weuse the normal-form model, which defines probabilities with the conditional log-linear form in (1), where is derivation and is sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2140">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>calculation of these values requires all derivations for each sentence in the training data.
</prevsent>
<prevsent>in clark and curran (2004) <papid> P04-1014 </papid>we describe efficient methods for performing the calculations using packed charts.</prevsent>
</prevsection>
<citsent citstr=" W03-1013 ">
however, very large amount of memory is still needed to store the packed charts for the complete training data even though the representation is very compact; in clark and curran (2003) <papid> W03-1013 </papid>we report memory usage of 30 gb.</citsent>
<aftsection>
<nextsent>to handle this we have developed parallel implementation of the estimation algorithm which runs on beowulf cluster.
</nextsent>
<nextsent>the need for large high-performance computing resources is disadvantage of our earlier approach.in the next section we show how use of the super tag ger, combined with normal-form constraints on the derivations, can significantly reduce the memory requirements for the model estimation.
</nextsent>
<nextsent>since the training data contains the correct lexical categories, we ensure the correct category is assigned to each word when generating the packed charts for model estimation.
</nextsent>
<nextsent>whilst training the parser, the super tagger can be thought of as supplying number of plausible but incorrect categories for each word; these, together with the correct categories, determine the parts of the parse space that are used in the estimation process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2141">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> generating parser training data.  </section>
<citcontext>
<prevsection>
<prevsent>as well as the super tagger, we use two additional strategies for reducing the derivation space.
</prevsent>
<prevsent>the first, following hockenmaier (2003), is to only allow categories to combine if the combination hasbeen seen in sections 2-21 of ccgbank.
</prevsent>
</prevsection>
<citsent citstr=" P96-1011 ">
for example, np/np could combine with np/np according to ccgs combinatory rules (by forward composi tion), but since this particular combination does not appear in ccgbank the parser does not allow it.the second strategy is to use eisners normal form constraints (eisner, 1996).<papid> P96-1011 </papid></citsent>
<aftsection>
<nextsent>the constraints supertagging/parsing usage constraints disk memory ? = 0.01 ? 0.05 ? 0.1 17 gb 31 gb ccgbank constraints 13 gb 23 gb eisner constraints 9 gb 16 gb ? = 0.05 ? 0.1 2 gb 4 gb table 3: space requirements for model training data prevent any constituent which is the result of forward (backward) composition serving as the primary functor in another forward (backward) composition or forward (backward) application.
</nextsent>
<nextsent>eisner only deals with grammar without type-raising,and so the constraints do not guarantee normal form parse when using grammar extracted from ccgbank.
</nextsent>
<nextsent>however, the constraints are still useful in restricting the derivation space.
</nextsent>
<nextsent>as far as we are aware, this is the first demonstration of the utility of such constraints for wide-coverage ccg parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2149">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> parsing unseen data.  </section>
<citcontext>
<prevsection>
<prevsent>in this section we describe our approach to tightly integrating the super tagger and parser for parsing unseen data.
</prevsent>
<prevsent>our previous approach to parsing unseen data (clark et al, 2002; <papid> P02-1042 </papid>clark and curran, 2003) <papid> W03-1013 </papid>wasto use the least restrictive setting of the super tagger which still allows reasonable compromise between speed and accuracy.</prevsent>
</prevsection>
<citsent citstr=" C00-1085 ">
our philosophy was to give the parser the greatest possibility of finding the correct parse, by giving it as many categories as possible, while still retaining reasonable efficiency.4another possible solution would be to use sampling methods, e.g. osborne (2000).<papid> C00-1085 </papid></citsent>
<aftsection>
<nextsent>supertagging/parsing time sents words constraints sec /sec /sec ? = 0.01?
</nextsent>
<nextsent>0.1 3 523 0.7 16 ccgbank constraints 1 181 2.0 46 eisner constraints 995 2.4 55 ? = 0.1?
</nextsent>
<nextsent>0.01k=100 608 3.9 90 ccgbank constraints 124 19.4 440 eisner constraints 100 24.0 546 parser beam 67 35.8 814 94% coverage 49 49.0 1 114 parser beam 46 52.2 1 186 oracle 18 133.4 3 031 table 4: parse times for section 23 the problem with this approach is that, for some sentences, the number of categories in the chart still gets extremely large and so parsing is unacceptably slow.
</nextsent>
<nextsent>hence we applied limit to the number of categories in the chart, as in the previous section,and reverted to more restrictive setting of the su per tagger if the limit was exceeded.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2160">
<title id=" C04-1041.xml">the importance of super tagging for wide coverage ccg parsing </title>
<section> parsing unseen data.  </section>
<citcontext>
<prevsection>
<prevsent>did find that ltag super tagging increased parsing speed, but at significant cost in coverage: only 1,324 sentences out of test set of 2,250 received parse.
</prevsent>
<prevsent>the parse times reported are also not as good as those reported here: the time taken to parse the 2,250 test sentences was over 5 hours.5multiplying by an estimate of the outside score may im prove the efficacy of the beam.
</prevsent>
</prevsection>
<citsent citstr=" N04-1013 ">
kaplan et al (2004) <papid> N04-1013 </papid>report high parsing speeds for deep parsing system which uses an lfg gram mar: 1.9 sentences per second for 560 sentences from section 23 of the penn treebank.</citsent>
<aftsection>
<nextsent>they also report speeds for the publicly available collins parser (collins, 1999): 2.8 sentences per second for the same set.
</nextsent>
<nextsent>the best speeds we have reported for the ccg parser are an order of magnitude faster.
</nextsent>
<nextsent>this paper has shown that by tightly integrating super tagger with ccg parser, very fast parse times can be achieved for penn treebank wsj text.
</nextsent>
<nextsent>as far as we are aware, the times reported here are an orderof magnitude faster than any reported for comparable systems using linguistically motivated grammar formalisms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2161">
<title id=" C02-1027.xml">shallow language processing architecture for bulgarian </title>
<section> lingua - an architecture for.  </section>
<citcontext>
<prevsection>
<prevsent>language processing in bulgarian lingua is text processing framework for bulgarian which automatically performs token isation, sentence splitting, part-of-speech tagging, parsing, clause segmentation, section heading identification and resolution for third person personal pronouns (figure 1).
</prevsent>
<prevsent>all modules of lingua are original and purpose built, except for the module for morphological analysis which uses krushkovs morphological analyser bulmorph (krushkov, 1997).
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
the anaphora re solver is an adaptation for bulgarian of mitkovs knowledge-poor pronoun resolution approach (mitkov, 1998).<papid> P98-2143 </papid></citsent>
<aftsection>
<nextsent>lingua was used in number of projects covering automatic text abridging, word semantic extraction (totkov and tanev, 1999) and term extraction.
</nextsent>
<nextsent>the following sections outline the basic language processing functions, provided by the language engine.
</nextsent>
<nextsent>2.1 text segmentation: tokenisation,.
</nextsent>
<nextsent>sentence splitting and paragraph identification the first stage of every text processing task isthe segmentation of text in terms of tokens, sentences and paragraphs.lingua performs text segmentation by operating within an input window of 30 tokens, applying rule-based algorithm for token synthesis, sentence splitting and paragraph identification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2163">
<title id=" C02-1027.xml">shallow language processing architecture for bulgarian </title>
<section> lingua - an architecture for.  </section>
<citcontext>
<prevsection>
<prevsent>for other languages such as turkish this ratio is about 1,9 and for certain english corpora 2,0 1.we used 33 hand-crafted rules for disambiguation.
</prevsent>
<prevsent>since large tagged corpora in bulgarian are not widely available, the development of corpus-based probabilistic tagger was an unrealistic goal for us.
</prevsent>
</prevsection>
<citsent citstr=" E95-1022 ">
however, as some studies suggest (voutilainen, 1995), <papid> E95-1022 </papid>the precision ofrule-based taggers may exceed that of the probabilistic ones.</citsent>
<aftsection>
<nextsent>2.3 parsing.
</nextsent>
<nextsent>seeking robust flexible solution for parsing we implemented two alternative approaches in lingua: fast-working np extractor and more general parser, which works more slowly, but delivers better results both inaccuracy andcoverage.
</nextsent>
<nextsent>as no syntactically annotated bulgarian corpora were available to us, using statistical data to implement probabilistic algorithm was not an option.
</nextsent>
<nextsent>the np extraction algorithm is capable of analysing nested nps, nps which contain left 1kemal oflazer, personal communication modifiers, prepositional phrases and coordinating phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2170">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>low parser into useful guidance to the deep parser.
</prevsent>
<prevsent>we were interested in seeing if we could take shallow parser off the shelf, namely the collins parser, and use its output fairly directly to improve the performance of the trips parser.
</prevsent>
</prevsection>
<citsent citstr=" P99-1010 ">
it has been reported that stochastic parsers degrade in performance on domains different than what they weretrained on (hwa, 1999; <papid> P99-1010 </papid>gildea, 2001), <papid> W01-0521 </papid>so therere ally was an issue whether the output would be good enough.</citsent>
<aftsection>
<nextsent>in particular, we are taking the collins parser trained on the wall street journal and applying it unchanged to spontaneous human-human dialog in an emergency rescue task domain.
</nextsent>
<nextsent>we have found that there are islands of reliability in the results from the collins parser that can be used to substantially improve the performance of the trips parser.the remainder of the paper is organized as follows.
</nextsent>
<nextsent>section 2.1 provides background on the monroe corpus, set of task-oriented dialogs that is the basis for the parser evaluations.
</nextsent>
<nextsent>in section 2.2 we describe the trips parser and the representation it produces for reasoning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2171">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>low parser into useful guidance to the deep parser.
</prevsent>
<prevsent>we were interested in seeing if we could take shallow parser off the shelf, namely the collins parser, and use its output fairly directly to improve the performance of the trips parser.
</prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
it has been reported that stochastic parsers degrade in performance on domains different than what they weretrained on (hwa, 1999; <papid> P99-1010 </papid>gildea, 2001), <papid> W01-0521 </papid>so therere ally was an issue whether the output would be good enough.</citsent>
<aftsection>
<nextsent>in particular, we are taking the collins parser trained on the wall street journal and applying it unchanged to spontaneous human-human dialog in an emergency rescue task domain.
</nextsent>
<nextsent>we have found that there are islands of reliability in the results from the collins parser that can be used to substantially improve the performance of the trips parser.the remainder of the paper is organized as follows.
</nextsent>
<nextsent>section 2.1 provides background on the monroe corpus, set of task-oriented dialogs that is the basis for the parser evaluations.
</nextsent>
<nextsent>in section 2.2 we describe the trips parser and the representation it produces for reasoning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2172">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the grammatical formalism and parsing framework is essentially lexicalized version of the formalism described in (allen, 1995).
</prevsent>
<prevsent>it is gpsg/hpsg(pollard and sag, 1994) inspired unification grammar of approximately 1300 rules with rich model of semantic features (dzikovska, 2004).
</prevsent>
</prevsection>
<citsent citstr=" W04-0214 ">
the parser2parseable utterances exclude utterances that are incomplete or ungrammatical (see (tetreault et al, 2004).)<papid> W04-0214 </papid>is an agenda-driven best-first chart parser that supports experimentation with different parsing strategies, although in practice we almost always use straightforward bi-directional bottom-up algorithm.</citsent>
<aftsection>
<nextsent>as an illustration of its flexibility, the modifications required to perform this experiment required adding only one function of ten lines of code.
</nextsent>
<nextsent>the grammar used for these experiments is the same trips grammar used in all our applications, and the rules have hand-tuned weights.
</nextsent>
<nextsent>the weights of newly derived constituents are computed exactly as in pcfg algorithm, the only difference being that the weights dont necessarily add to 1 and so are not pro babil ities.3 the trips parser does not use maximum entropy model (cf.
</nextsent>
<nextsent>the xle system (kaplan et al, 2004)) <papid> N04-1013 </papid>because there is insufficient training data andit is as yet unclear how such as model would perform at the detailed level of semantic representation produced by the trips parser (see figure 2 and discussion below).the rules, lexicon, and semantic ontology are independent of any specific domain but tailored to human-computer practical dialog.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2173">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the grammar used for these experiments is the same trips grammar used in all our applications, and the rules have hand-tuned weights.
</prevsent>
<prevsent>the weights of newly derived constituents are computed exactly as in pcfg algorithm, the only difference being that the weights dont necessarily add to 1 and so are not pro babil ities.3 the trips parser does not use maximum entropy model (cf.
</prevsent>
</prevsection>
<citsent citstr=" N04-1013 ">
the xle system (kaplan et al, 2004)) <papid> N04-1013 </papid>because there is insufficient training data andit is as yet unclear how such as model would perform at the detailed level of semantic representation produced by the trips parser (see figure 2 and discussion below).the rules, lexicon, and semantic ontology are independent of any specific domain but tailored to human-computer practical dialog.</citsent>
<aftsection>
<nextsent>the grammar is fairly extensive in coverage (and still growing),and has quite good coverage of corpus of human human dialogs in the monroe domain, an emergency management domain (swift et al, 2004).
</nextsent>
<nextsent>the3we have version of the grammar that uses non lexicalized pcfg model, but it was not used here as it does not perform as well.
</nextsent>
<nextsent>thus we are using our best model, making it the most challenging to show improvement.
</nextsent>
<nextsent>sa_tell lf::fill-container :content (set-of lf::fruit) :theme :goal the (set-of lf::fruit):subset :of the lf::land-vehicle lf::weight-unit pound :quantity lf::qmodifier min :quan 300:is lf::number :mods (speechact v38109 sa_tell :content v37618) (f v37618 (lf::fill-container load) :goal v37800 :theme v38041 :tma ((tense past) (passive +))) (the v37800 (lf::land-vehicle truck)) (a v38041 (set-of (lf::fruit orange)) :quantity v37526 :subset v37539) (quantity-term v37526 (lf::weight-unit pound) :quan v37479) (quantity-term v37479 lf::number :mods (v38268)) (f v38268 (lf::qmodifier min) :of v37479 :is v37523) (quantity-term v37523 lf::number :value 300) (the v37539 (set-of (lf::fruit orange))) figure 2: parser logical form (together with graphical approximation of the semantic content) for at least three hundred pounds of the oranges were put in the truck.system is inactive use in our spoken dialog understanding work in several different domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2174">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>as one way to control ambiguity, the grammar makes use of selectional restrictions.
</prevsent>
<prevsent>our semantic model utilizes two related mechanisms: first, an ontology of the predicates that are used to create the logical forms, and second, vector of semantic features associated with these predicates that are used for selectional restrictions.
</prevsent>
</prevsection>
<citsent citstr=" A00-2008 ">
the grammar computes flattened and un scoped logical form using reified events (see also(copestake et al, 1997) for flat semantic represen tation), with many of its word senses derived from framenet frames (johnson and fillmore, 2000) <papid> A00-2008 </papid>and semantic roles (fillmore, 1968).</citsent>
<aftsection>
<nextsent>an example of the logical form representation produced by the parser is shown in figure 2, in both dependency graph (upper) and the actual parser output (lower).4 4term constructors appearing at the left most edge of terms in the parser output are (relation), (indefinite entity),the (definite entity) and quantity-term (numeric ex pressions).
</nextsent>
<nextsent>as pilot experiment, we evaluated the performance of the collins parser on single dialog of 167 sentences from the monroe corpus, dialog 3.
</nextsent>
<nextsent>we extracted context-free grammar backbones from our trips gold standard parses to score the collins?
</nextsent>
<nextsent>output against.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2175">
<title id=" C04-1055.xml">skeletons in the parser using a shallow parser to improve deep parsing </title>
<section> collins parser evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>output against.
</prevsent>
<prevsent>the evaluation was complicated by difference in tree formats, illustrated in figure 3.the two parsers use different (though closely related) set of syntactic categories.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the trips structure generally has more levels of structure (roughly corresponding to levels in x-bar theory) than the penn treebank analyses (marcus et al, 1993), <papid> J93-2004 </papid>in particular for base noun phrases.</citsent>
<aftsection>
<nextsent>we converted the trips category labels to their nearest equivalent in penn treebank inventory before scoring the collins parser in terms of labeled precision and recall of constituents, the standard measures in the statistical parsing community.
</nextsent>
<nextsent>overall recall was 32%, while precision was 64%.
</nextsent>
<nextsent>while we expect the collins parser to have low recall (it generates fewer constituents overall), thelow precision indicates that simply relabeling constituents on one-for-one basis is not sufficient to resolve the differences in the two formalisms.
</nextsent>
<nextsent>precision and recall broken down by constituent type is shown in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2176">
<title id=" C08-1065.xml">authorship attribution and verification with many authors and limited data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, research has started to focus on authorship attribution on larger sets of authors: 8 (van halteren, 2005), 20 (arga monet al, 2003), 114 (madigan et al, 2005), or up 513 to thousands of authors (koppel et al, 2006) (see section 5).
</prevsent>
<prevsent>a second problem in traditional studies are the unrealistic sizes of training data, which also makes the task considerably easier.
</prevsent>
</prevsection>
<citsent citstr=" C04-1088 ">
researchers tend to use over 10,000 words per author (argamon et al, 2007; burrows, 2007; gamon, 2004; <papid> C04-1088 </papid>hirst and feiguina, 2007; madigan et al, 2005; stamatatos,2007), which is regarded to be reliable minimum for an authorial set?</citsent>
<aftsection>
<nextsent>(burrows, 2007).
</nextsent>
<nextsent>whenno long texts are available, for example in poems (coyotl-morales et al, 2006) or student essays (van halteren, 2005), large number of short texts is selected for training for each author.
</nextsent>
<nextsent>one of the few studies focusing on small texts is feiguina and hirst (2007), but they select hundreds of these short texts (here 100, 200 or 500 words).
</nextsent>
<nextsent>the accuracy of any of these studies with unrealistic sizes of training data is overestimated when compared to realistic situations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2177">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the higher performance of our method can be attributed to the enormity of the web data used and the employment of the em algorithm.
</prevsent>
<prevsent>2.1 translation with non-parallel.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
corpora straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., brown et al  1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>parallel corpora are, however, difficult to obtain in practice.
</nextsent>
<nextsent>to deal with this difficulty, number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000).</nextsent>
<nextsent>within these methods, it is usually assumed that number of translation candidates for word or phrase are given (or can be easily collected) and the problem is focused on translation selection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2178">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>corpora straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., brown et al  1993).<papid> J93-2003 </papid></prevsent>
<prevsent>parallel corpora are, however, difficult to obtain in practice.</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
to deal with this difficulty, number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000).</citsent>
<aftsection>
<nextsent>within these methods, it is usually assumed that number of translation candidates for word or phrase are given (or can be easily collected) and the problem is focused on translation selection.
</nextsent>
<nextsent>all of the proposed methods manage to find out the translation(s) of given word or phrase, on the basis of the linguistic phenomenon that the contexts of translation tend to be similar to the contexts of the given word or phrase.
</nextsent>
<nextsent>fung and yee (1998), <papid> P98-1069 </papid>for example, proposed to represent the contexts of word or phrase with real-valued vector (e.g., tf-idf vector), in which one element corresponds to one word in the contexts.</nextsent>
<nextsent>in translation selection, they select the translation candidates whose context vectors are the closest to that of the given word or phrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2179">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>corpora straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., brown et al  1993).<papid> J93-2003 </papid></prevsent>
<prevsent>parallel corpora are, however, difficult to obtain in practice.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
to deal with this difficulty, number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1999; <papid> P99-1067 </papid>diab and finch, 2000).</citsent>
<aftsection>
<nextsent>within these methods, it is usually assumed that number of translation candidates for word or phrase are given (or can be easily collected) and the problem is focused on translation selection.
</nextsent>
<nextsent>all of the proposed methods manage to find out the translation(s) of given word or phrase, on the basis of the linguistic phenomenon that the contexts of translation tend to be similar to the contexts of the given word or phrase.
</nextsent>
<nextsent>fung and yee (1998), <papid> P98-1069 </papid>for example, proposed to represent the contexts of word or phrase with real-valued vector (e.g., tf-idf vector), in which one element corresponds to one word in the contexts.</nextsent>
<nextsent>in translation selection, they select the translation candidates whose context vectors are the closest to that of the given word or phrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2181">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a straightforward extension of fung and yees assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping).
</prevsent>
<prevsent>this approach, however, has drawback of losing information in vector transformation, as will be described.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
for other methods using non-parallel corpora, see also (tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kikui, 1999, <papid> W99-0905 </papid>koehn and kevin 2000; sumita 2000; <papid> P00-1054 </papid>nakagawa 2001; gao et al  2001).</citsent>
<aftsection>
<nextsent>2.2 translation using web data.
</nextsent>
<nextsent>web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data).
</nextsent>
<nextsent>recently, new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., brill et al  2001).
</nextsent>
<nextsent>nagata et al (2001), <papid> W01-1413 </papid>for example, proposed to collect partial parallel corpus data on the web to create translation dictionary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2182">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a straightforward extension of fung and yees assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping).
</prevsent>
<prevsent>this approach, however, has drawback of losing information in vector transformation, as will be described.
</prevsent>
</prevsection>
<citsent citstr=" W99-0905 ">
for other methods using non-parallel corpora, see also (tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kikui, 1999, <papid> W99-0905 </papid>koehn and kevin 2000; sumita 2000; <papid> P00-1054 </papid>nakagawa 2001; gao et al  2001).</citsent>
<aftsection>
<nextsent>2.2 translation using web data.
</nextsent>
<nextsent>web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data).
</nextsent>
<nextsent>recently, new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., brill et al  2001).
</nextsent>
<nextsent>nagata et al (2001), <papid> W01-1413 </papid>for example, proposed to collect partial parallel corpus data on the web to create translation dictionary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2183">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a straightforward extension of fung and yees assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping).
</prevsent>
<prevsent>this approach, however, has drawback of losing information in vector transformation, as will be described.
</prevsent>
</prevsection>
<citsent citstr=" P00-1054 ">
for other methods using non-parallel corpora, see also (tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kikui, 1999, <papid> W99-0905 </papid>koehn and kevin 2000; sumita 2000; <papid> P00-1054 </papid>nakagawa 2001; gao et al  2001).</citsent>
<aftsection>
<nextsent>2.2 translation using web data.
</nextsent>
<nextsent>web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data).
</nextsent>
<nextsent>recently, new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., brill et al  2001).
</nextsent>
<nextsent>nagata et al (2001), <papid> W01-1413 </papid>for example, proposed to collect partial parallel corpus data on the web to create translation dictionary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2184">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data).
</prevsent>
<prevsent>recently, new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., brill et al  2001).
</prevsent>
</prevsection>
<citsent citstr=" W01-1413 ">
nagata et al (2001), <papid> W01-1413 </papid>for example, proposed to collect partial parallel corpus data on the web to create translation dictionary.</citsent>
<aftsection>
<nextsent>they observed that there are many partial parallel corpora between english and japanese on the web, and most typically english translations of japanese terms (words or phrases) are parenthesized and inserted immediately after the japanese terms in documents written in japanese.
</nextsent>
<nextsent>our method for base np translation comprises of two steps: translation candidate collection and translation selection.
</nextsent>
<nextsent>in translation candidate collection, we look for translation candidates of given base np.
</nextsent>
<nextsent>in translation selection, we find out possible translation(s) from the translation candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2185">
<title id=" C02-1011.xml">base noun phrase translation using web data and the em algorithm </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we will denote this strategy back-off?.
</prevsent>
<prevsent>we conducted experiments on translation of the base nps from english to chinese.
</prevsent>
</prevsection>
<citsent citstr=" P00-1015 ">
we extracted base nps (noun-noun pairs) from the encarta 1 english corpus using the tool developed by xun et al (2000).<papid> P00-1015 </papid></citsent>
<aftsection>
<nextsent>there were about 1 http://encarta.msn.com/default.asp 3000 base nps extracted.
</nextsent>
<nextsent>in the experiments, we.
</nextsent>
<nextsent>used the hit english-chinese word translation dictionary2 . the dictionary contains about 76000 chinese words, 60000 english words, and 118000 translation links.
</nextsent>
<nextsent>as web search engine, we used google (http://www.google.com).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2186">
<title id=" C02-1160.xml">modular mt with a learned bilingual dictionary rapid deployment of a new language pair </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>commercial systems and other large-scale systems have traditionally relied heavily on the knowledge encoded in their bilingual dictionaries.
</prevsent>
<prevsent>gerber &amp; yang (1997) clearly state that sys trans translation capabilities are dependent on large, carefully encoded, high quality dictionaries?.
</prevsent>
</prevsection>
<citsent citstr=" W01-1411 ">
with the advent of bi texts, efforts to derive bilingual lexicons have led to substantial research (melamed 1996, moore 2001 <papid> W01-1411 </papid>for discussion), including resources for semi-automatic creation of bilingual lexica such as sable (melamed 1997), used for instance in palmer et al (1998).</citsent>
<aftsection>
<nextsent>statistical mt systems have relied on bi-texts to automatically create word-alignments; in many statistical mt systems however, the authors state that use of conventional bilingual dictionary enhances the performance of the system (al-onaizan et al 1999, koehn &amp; knight 2001).<papid> W01-0504 </papid></nextsent>
<nextsent>we find then, that inspite of the movement to create bilingual dictionaries automatically, there is still heavy reliance on hand-crafted and hand-edited resources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2187">
<title id=" C02-1160.xml">modular mt with a learned bilingual dictionary rapid deployment of a new language pair </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>gerber &amp; yang (1997) clearly state that sys trans translation capabilities are dependent on large, carefully encoded, high quality dictionaries?.
</prevsent>
<prevsent>with the advent of bi texts, efforts to derive bilingual lexicons have led to substantial research (melamed 1996, moore 2001 <papid> W01-1411 </papid>for discussion), including resources for semi-automatic creation of bilingual lexica such as sable (melamed 1997), used for instance in palmer et al (1998).</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
statistical mt systems have relied on bi-texts to automatically create word-alignments; in many statistical mt systems however, the authors state that use of conventional bilingual dictionary enhances the performance of the system (al-onaizan et al 1999, koehn &amp; knight 2001).<papid> W01-0504 </papid></citsent>
<aftsection>
<nextsent>we find then, that inspite of the movement to create bilingual dictionaries automatically, there is still heavy reliance on hand-crafted and hand-edited resources.
</nextsent>
<nextsent>we found no full-scale mt system that relied only on learned bilingual dictionaries and certainly none that was found better in performance for doing so.
</nextsent>
<nextsent>rapid deployment of new language pair has been one of the strong features of statistical mt systems.
</nextsent>
<nextsent>for example, mt in day?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2188">
<title id=" C02-1160.xml">modular mt with a learned bilingual dictionary rapid deployment of a new language pair </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>the transfer component, described in detail in menezes (2001), consists of high-quality transfer patterns automatically acquired from sentence-aligned bilingual corpora.
</prevsent>
<prevsent>the innovation of this work is the use of an unedited, automatically created dictionary which contains translation pairs and parts of speech, without any use of broad domain, general purpose hand-crafted dictionary resource.
</prevsent>
</prevsection>
<citsent citstr=" W01-1402 ">
the architecture of the mt system as described elsewhere (richardson et al 2001) <papid> W01-1402 </papid>used both traditional bilingual dictionary and an automatically derived word-association file at training time, but it used only the traditional bilingual dictionary at runtime.</citsent>
<aftsection>
<nextsent>we refer to this below as the hanc system, because it uses hand-crafted dictionary2.
</nextsent>
<nextsent>we changed this sothat learned dictionary consisting of word associations (moore 2001) <papid> W01-1411 </papid>with parts of speech and function word only bilingual dictionary (prepositions, conjunctions and pronouns) replaces the previous combination both at training and at runtime3.</nextsent>
<nextsent>we refer to this as the 1 in both french-english and french-spanish, we use.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2191">
<title id=" C08-1060.xml">reading the markets forecasting public opinion of political candidates by news analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the same is true of politics, where candidates performance is impacted by media influenced public perception.
</prevsent>
<prevsent>computational linguistics can discover such signals in the news.
</prevsent>
</prevsection>
<citsent citstr=" P07-1124 ">
for example, de vitt and ahmad (2007) <papid> P07-1124 </papid>gave computable metric of polarity in financial news text consistent with human judgments.</citsent>
<aftsection>
<nextsent>koppel and shtrimberg (2004)used daily news analysis to predict financial market performance, though predictions could not be used for future investment decisions.
</nextsent>
<nextsent>recently, ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.a study conducted of the 2007 french presidential election showed correlation between the frequency of candidates name in the news and electoral success (veronis, 2007).this work forecasts day-to-day changes in public perception of political candidates from daily news.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2192">
<title id=" C08-1060.xml">reading the markets forecasting public opinion of political candidates by news analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>successive news days are compared to determine the novel component of each days news resulting in features for machine learning system.
</prevsent>
<prevsent>a combination system uses this information as well as predictions from internal market forces to model prediction markets better than several baselines.
</prevsent>
</prevsection>
<citsent citstr=" D07-1113 ">
results show that news articles can be mined to predict changes in public opinion.opinion forecasting differs from that of opinion analysis, such as extracting opinions, evaluating sentiment, and extracting predictions (kim and hovy, 2007).<papid> D07-1113 </papid></citsent>
<aftsection>
<nextsent>contrary to these tasks, our system receives objective news, not subjective opinions, and learns what events will impact public opinion.
</nextsent>
<nextsent>for example, oil prices rose?
</nextsent>
<nextsent>is fact but will likely shape opinions.
</nextsent>
<nextsent>this work analyzes news(cause) to predict future opinions (effect).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2195">
<title id=" C08-1060.xml">reading the markets forecasting public opinion of political candidates by news analysis </title>
<section> external information: news.  </section>
<citcontext>
<prevsection>
<prevsent>3effec tive features relyon the proper identification of the subject and object of defeated.?
</prevsent>
<prevsent>longer n-grams, which would be very sparse, would succeed for the first two sentences but not the third.to capture these interactions, features were extracted from dependency parses of the news articles.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
sentences were part of speech tagged(toutanova et al, 2003), <papid> N03-1033 </papid>parsed with dependency parser and labeled with grammatical function labels (mcdonald et al, 2006).<papid> W06-2932 </papid></citsent>
<aftsection>
<nextsent>the resulting parses encode dependencies for each sentence,where word relationships are expressed as parent child links.
</nextsent>
<nextsent>the parse for the third sentence above indicates that kerry?
</nextsent>
<nextsent>is the subject of defeated,?
</nextsent>
<nextsent>and bush?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2196">
<title id=" C08-1060.xml">reading the markets forecasting public opinion of political candidates by news analysis </title>
<section> external information: news.  </section>
<citcontext>
<prevsection>
<prevsent>3effec tive features relyon the proper identification of the subject and object of defeated.?
</prevsent>
<prevsent>longer n-grams, which would be very sparse, would succeed for the first two sentences but not the third.to capture these interactions, features were extracted from dependency parses of the news articles.
</prevsent>
</prevsection>
<citsent citstr=" W06-2932 ">
sentences were part of speech tagged(toutanova et al, 2003), <papid> N03-1033 </papid>parsed with dependency parser and labeled with grammatical function labels (mcdonald et al, 2006).<papid> W06-2932 </papid></citsent>
<aftsection>
<nextsent>the resulting parses encode dependencies for each sentence,where word relationships are expressed as parent child links.
</nextsent>
<nextsent>the parse for the third sentence above indicates that kerry?
</nextsent>
<nextsent>is the subject of defeated,?
</nextsent>
<nextsent>and bush?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2199">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>compared to relation ally richer, e.g., wordnet based, interlinguas as applied for cross-language information retrieval (clir) (gonzalo et al, 1999; ruiz et al, 1999), we use rather limited set of semantic relations and pursue more restrictive approach to synonymy.
</prevsent>
<prevsent>in particular, we restrict ourselves to the specific sublanguage used in the context of the medical domain.
</prevsent>
</prevsection>
<citsent citstr=" W02-0309 ">
our claim that this inter lingual approach is useful for the purpose ofcross-lingual text retrieval and categorization has already been experimentally supported (schulz et al, 2002; <papid> W02-0309 </papid>marko?</citsent>
<aftsection>
<nextsent>et al, 2003).the quality of cross-lingual indexing fundamentally depends on the underlying lexicon and thesaurus.
</nextsent>
<nextsent>its manual construction and maintenance is costly and error-prone.
</nextsent>
<nextsent>therefore, machine supported lexical acquisition techniques increasingly deserve attention.
</nextsent>
<nextsent>whereas in the medical domain parallel corpora are only available for limited number of language pairs, unrelated (i.e., non parallel, non-aligned) corpora might provide sufficient evidence for cognate identification, at least in languages which are closely related.in this paper, we present the results of such an experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2201">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the basic idea that underlies this approach is that subword that appears in certain context should have (true posi tive) cognate that occurs in similar context, at least when (very) large corpora are taken into account.
</prevsent>
<prevsent>cognate similarity can then be measured in terms of context vector comparison (cf.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
also rapp (1999) <papid> P99-1067 </papid>or koehn and knight (2002)).<papid> W02-0902 </papid>we therefore processed the portuguese corpus using the morpho-semantic normalization routines as discussed in section 2.</citsent>
<aftsection>
<nextsent>in the next step, we created context vector for each mid, the components of which contained the relative frequencies ofco-occurring mids in local window of four subsequent, yet unordered mid units (a size also endorsed by rapp (1999)).<papid> P99-1067 </papid></nextsent>
<nextsent>in order to compute the context vector for each spanish subword candidate, we then constructed seed lexicon with all the automatically created spanish subword candidates, together with the listof spanish affixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2204">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the basic idea that underlies this approach is that subword that appears in certain context should have (true posi tive) cognate that occurs in similar context, at least when (very) large corpora are taken into account.
</prevsent>
<prevsent>cognate similarity can then be measured in terms of context vector comparison (cf.
</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
also rapp (1999) <papid> P99-1067 </papid>or koehn and knight (2002)).<papid> W02-0902 </papid>we therefore processed the portuguese corpus using the morpho-semantic normalization routines as discussed in section 2.</citsent>
<aftsection>
<nextsent>in the next step, we created context vector for each mid, the components of which contained the relative frequencies ofco-occurring mids in local window of four subsequent, yet unordered mid units (a size also endorsed by rapp (1999)).<papid> P99-1067 </papid></nextsent>
<nextsent>in order to compute the context vector for each spanish subword candidate, we then constructed seed lexicon with all the automatically created spanish subword candidates, together with the listof spanish affixes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2210">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, it is upto the lexicon engineer to determine the level of pre selection in these three dimensions.
</prevsent>
<prevsent>we also conclude from our experiments that much larger corpus is needed in order to collect reasonable context evidence for the infrequent mids, in particular.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the rise of the empirical paradigm in the field of machine translation is, to large degree, due to the wide-spread availability of parallel corpora (brownet al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>they also constitute an important resource for the automated acquisition of translational lexicons (turcato, 1998).<papid> P98-2212 </papid></nextsent>
<nextsent>unfortunately, the limited availability of parallel corpora (e.g., the canadian hansard corpus of english and french parliament debates) restricts this method to few language pairs, mostly focused on specific sublanguages (e.g. politics, legislation, economy).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2211">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we also conclude from our experiments that much larger corpus is needed in order to collect reasonable context evidence for the infrequent mids, in particular.
</prevsent>
<prevsent>the rise of the empirical paradigm in the field of machine translation is, to large degree, due to the wide-spread availability of parallel corpora (brownet al, 1990).<papid> J90-2002 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2212 ">
they also constitute an important resource for the automated acquisition of translational lexicons (turcato, 1998).<papid> P98-2212 </papid></citsent>
<aftsection>
<nextsent>unfortunately, the limited availability of parallel corpora (e.g., the canadian hansard corpus of english and french parliament debates) restricts this method to few language pairs, mostly focused on specific sublanguages (e.g. politics, legislation, economy).
</nextsent>
<nextsent>neither exist such parallel corpora for the medical sublanguage, norfor the particular language pair, spanish and portuguese, we focus on in this work.
</nextsent>
<nextsent>the acquisition of unrelated, albeit comparable corpora is much easier.
</nextsent>
<nextsent>rapp (1999) <papid> P99-1067 </papid>used unrelated parallel corpora in order to learn english and german word-to-word translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2216">
<title id=" C04-1117.xml">cognate mapping  a heuristic strategy for the semi supervised acquisition of a spanish lexicon from a portuguese seed lexicon </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>his approach is based on similarity measures and context clues, using seed lexicon of trusted translations.
</prevsent>
<prevsent>koehn and knight (2002) <papid> W02-0902 </papid>derived such seed lexicon from german-english cognates which were selected by using string similarity criteria.</prevsent>
</prevsection>
<citsent citstr=" E03-1023 ">
an additional boost can be achieved by retrieving content-related document pairs using clir techniques (utsuro et al,2003).<papid> E03-1023 </papid></citsent>
<aftsection>
<nextsent>an alternative generative approach is proposed by barker and sutcliffe (2000) who created polish cognate candidates out of an english word list using set of string mapping rules.pirkola et al (2003) used aligned translation dictionaries as source data.
</nextsent>
<nextsent>based on that, they create dan algorithm to automatically generate transformation rules from five different languages to english, including spanish.
</nextsent>
<nextsent>applying two-step technique (translation rules and fuzzy n-gram matching), they achieved 81.1% of average precision in spanish to-english context covering biomedical words only.however, their evaluation metrics considerably differed from ours, since they considered multiple hypotheses.
</nextsent>
<nextsent>our work differs from these precursors in manyways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2217">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> paradigmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>to determine the words most similar to given word, its co-occurrence vector is compared to the co-occurrence vectors of all other words using one of the standard similarity measures, for example, the cosine coefficient.
</prevsent>
<prevsent>those words that obtain the best values are considered to be most similar.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
practical implementations of algorithms based on this principle have led to excellent results as documented in papers by ruge (1992), grefenstette (1994), agarwal (1995), landauer &amp; dumais (1997), schtze (1997), and lin (1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>2.1 human data.
</nextsent>
<nextsent>in this section we relate the results of our version of such an algorithm to similarity estimates obtained by human subjects.
</nextsent>
<nextsent>fortunately, we did not need to conduct our own experiment to obtain the humans similarity estimates.
</nextsent>
<nextsent>instead, such data was kindly provided by thomas k. landauer, who had taken it from the synonym portion of the test of english as foreign language (toefl).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2218">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> paradigmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>we also decided to lemmatize the corpus as well as the test data.
</prevsent>
<prevsent>this not only reduces the sparse-data problem but also significantly reduces the size of the co-occurrence matrix to be computed.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
more details on these two steps of corpus preprocessing can be found in rapp (1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>2.3 co-occurrence counting.
</nextsent>
<nextsent>for counting word co-occurrences, as in most other studies fixed window size is chosen and it is determined how often each pair of words occurs within text window of this size.
</nextsent>
<nextsent>choosing window size usually means trade-off between two parameters: specificity versus the sparse-data problem.
</nextsent>
<nextsent>the smaller the window, the stronger the associative relation between the words inside the window, but the more severe the sparse data problem (see figure 1 in section 3.2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2220">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> paradigmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>however, we do not deny the technical value of the method.
</prevsent>
<prevsent>the one-time effort of the dimensionality reduction may be well spent in practical system because all subsequent vector comparisons will be speeded up considerably with shorter vectors.
</prevsent>
</prevsection>
<citsent citstr=" W93-0113 ">
let us now compare our results to those obtained using shallow parsing, as previously done by grefenstette (1993).<papid> W93-0113 </papid></citsent>
<aftsection>
<nextsent>the view here is that the win dow-based method may work to some extent, but that many of the word co-occurrences in window are just incidental and add noise to the significant word pairs.
</nextsent>
<nextsent>a simple method to reduce this problem could be to introduce threshold for the minimum number of co-occurrences; more sophisticated method is the use of (shallow) parser.
</nextsent>
<nextsent>ruge (1992), who was the first to introduce this method, claims that only head-modifier relations, as known from dependency grammar, should be considered.
</nextsent>
<nextsent>for example, if we consider the sentence peter drives the blue car?, then we should not count the co-occurrence of peter and blue, because blue is neither head nor modifier of peter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2221">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> paradigmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>although the shallow parsing could not improve the results in this case, we nevertheless should point out its virtues: it improves efficiency since it leads to sparser matrices.
</prevsent>
<prevsent>it also seems to be able to separate the relevant from the irrelevant co-occurrences.
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
third, it may be useful for determining the type of relationship between words (e.g., synonymy, antonymy, meronymy, hyponymy, etc., see berland &amp; charniak, 1999).<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>although this is not within the scope of this paper, it is very relevant for related tasks, for example, the automatic generation of thesauri.
</nextsent>
<nextsent>syntagmatic associations are words that frequently occur together.
</nextsent>
<nextsent>therefore, an obvious approach to extract them from corpora is to look for word pairs whose co-occurrence is significantly larger than chance.
</nextsent>
<nextsent>to test for significance, the standard chi square test can be used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2222">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> syntagmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, an obvious approach to extract them from corpora is to look for word pairs whose co-occurrence is significantly larger than chance.
</prevsent>
<prevsent>to test for significance, the standard chi square test can be used.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
however, dunning (1993) <papid> J93-1003 </papid>pointed out that for the purpose of corpus statistics, where the sparseness of data is an important issue, it is better to use the log-likelihood ratio.</citsent>
<aftsection>
<nextsent>it would then be assumed that the strongest syntagmatic association to word would be that other word that gets the highest log-likelihood score.
</nextsent>
<nextsent>please note that this method is computationally far more efficient than the computation of paradigmatic associations.
</nextsent>
<nextsent>for the computation of the syn tag matic associations to stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary.
</nextsent>
<nextsent>the computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2223">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> syntagmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>for the computation of the syn tag matic associations to stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary.
</prevsent>
<prevsent>the computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
algorithms for the computation of first-order associations have been used in lexico graphy for the extraction of collocations (smadja, 1993) <papid> J93-1007 </papid>and in cognitive psychology for the simulation of associative learning (wettler &amp; rapp, 1993).<papid> W93-0310 </papid></citsent>
<aftsection>
<nextsent>3.1 association norms.
</nextsent>
<nextsent>as we did with the paradigmatic associations, we would like to compare the results of our simulation to human performance.
</nextsent>
<nextsent>however, it is difficult to say what kind of experiment should be conducted to obtain human data.
</nextsent>
<nextsent>as with the paradigmatic associations, we decided not to conduct our own experiment but to use the edinburgh associative thesaurus (eat), large collection of association norms, as compiled by kiss et al (1973).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2224">
<title id=" C02-1007.xml">the computation of word associations comparing syntagmatic and paradigmatic approaches </title>
<section> syntagmatic associations.  </section>
<citcontext>
<prevsection>
<prevsent>for the computation of the syn tag matic associations to stimulus word only the vector of this single word has to be considered, whereas for the computation of paradigmatic associations the vector of the stimulus word has to be compared to the vectors of all other words in the vocabulary.
</prevsent>
<prevsent>the computation of syntagmatic associations is said to be of first-order type, whereas the computation of paradigmatic associations is of second-order type.
</prevsent>
</prevsection>
<citsent citstr=" W93-0310 ">
algorithms for the computation of first-order associations have been used in lexico graphy for the extraction of collocations (smadja, 1993) <papid> J93-1007 </papid>and in cognitive psychology for the simulation of associative learning (wettler &amp; rapp, 1993).<papid> W93-0310 </papid></citsent>
<aftsection>
<nextsent>3.1 association norms.
</nextsent>
<nextsent>as we did with the paradigmatic associations, we would like to compare the results of our simulation to human performance.
</nextsent>
<nextsent>however, it is difficult to say what kind of experiment should be conducted to obtain human data.
</nextsent>
<nextsent>as with the paradigmatic associations, we decided not to conduct our own experiment but to use the edinburgh associative thesaurus (eat), large collection of association norms, as compiled by kiss et al (1973).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2226">
<title id=" C04-1093.xml">summarizing encyclopedic term descriptions on the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the use of existing search engines is associated with the following problems: (a) search engines often retrieve extraneous pages not describing submitted term, (b) even if desired pages are retrieved, user has to identify page fragments describing the term,(c) word senses are not distinguished for polyse mous terms, such as hub (device and center)?, (d) descriptions in multiple pages are independent and do not comprise condensed and coherent text as in existing encyclopedias.
</prevsent>
<prevsent>the authors of this paper have been resolving these problems progressively.
</prevsent>
</prevsection>
<citsent citstr=" P00-1062 ">
for problems (a) and(b), fujii and ishikawa (2000) <papid> P00-1062 </papid>proposed an automatic method to extract term descriptions from the web.</citsent>
<aftsection>
<nextsent>for problem (c), fujii and ishikawa (2001) improved the previous method, so that the multiple descriptions extracted for single term are categorized into domains and consequently word senses are distinguished.using these methods, we have compiled an encyclopedic corpus for approximately 600,000 japaneseterms.
</nextsent>
<nextsent>we have also built web site called cy clone1 to utilize this corpus, in which one or moreparagraph-style descriptions extracted from different pages can be retrieved in response to user input.
</nextsent>
<nextsent>in figure 1, three paragraphs describing xml?
</nextsent>
<nextsent>are presented with the titles of their source pages.however, the above-mentioned problem (d) remains unresolved and this is exactly what we intend to address in this paper.in hand-crafted encyclopedias, single term is described concisely from different viewpoints?, such as the definition, exemplification, and purpose.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2227">
<title id=" C04-1093.xml">summarizing encyclopedic term descriptions on the web </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>to the best of our knowledge, no attempt has been made to intend similar purposes.
</prevsent>
<prevsent>our research is related to question answering (qa).
</prevsent>
</prevsection>
<citsent citstr=" N03-2037 ">
for example, in trec qa track, definition questions are intended to provide user with the definition of target item or person (voorhees, 2003).<papid> N03-2037 </papid></citsent>
<aftsection>
<nextsent>however, while the expected answer for trec question is short definition sentences as in dictionary, we intend to produce an encyclopedic text describing target term from multiple viewpoints.the summarization method proposed in this paper is related to multi-document summarization (mds) (mani, 2001; radev and mckeown, 1998; <papid> J98-3005 </papid>schiffman et al, 2001).<papid> P01-1059 </papid></nextsent>
<nextsent>the novelty of our research is that we applied mds to producing condensed term description from unorganized web pages, while existing mds methods used newspaper articles to produce an outline of an event and biography of specific person.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2228">
<title id=" C04-1093.xml">summarizing encyclopedic term descriptions on the web </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>our research is related to question answering (qa).
</prevsent>
<prevsent>for example, in trec qa track, definition questions are intended to provide user with the definition of target item or person (voorhees, 2003).<papid> N03-2037 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
however, while the expected answer for trec question is short definition sentences as in dictionary, we intend to produce an encyclopedic text describing target term from multiple viewpoints.the summarization method proposed in this paper is related to multi-document summarization (mds) (mani, 2001; radev and mckeown, 1998; <papid> J98-3005 </papid>schiffman et al, 2001).<papid> P01-1059 </papid></citsent>
<aftsection>
<nextsent>the novelty of our research is that we applied mds to producing condensed term description from unorganized web pages, while existing mds methods used newspaper articles to produce an outline of an event and biography of specific person.
</nextsent>
<nextsent>we also proposed the concept of viewpoint for mds purposes.
</nextsent>
<nextsent>while we targeted japanese technical terms in the computer domain, our method can also be applied to other types of terms in different languages, without modifying the model.
</nextsent>
<nextsent>however, set of viewpoint sand patterns typically used to describe each view point need to be modified or replaced depending the application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2229">
<title id=" C04-1093.xml">summarizing encyclopedic term descriptions on the web </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>our research is related to question answering (qa).
</prevsent>
<prevsent>for example, in trec qa track, definition questions are intended to provide user with the definition of target item or person (voorhees, 2003).<papid> N03-2037 </papid></prevsent>
</prevsection>
<citsent citstr=" P01-1059 ">
however, while the expected answer for trec question is short definition sentences as in dictionary, we intend to produce an encyclopedic text describing target term from multiple viewpoints.the summarization method proposed in this paper is related to multi-document summarization (mds) (mani, 2001; radev and mckeown, 1998; <papid> J98-3005 </papid>schiffman et al, 2001).<papid> P01-1059 </papid></citsent>
<aftsection>
<nextsent>the novelty of our research is that we applied mds to producing condensed term description from unorganized web pages, while existing mds methods used newspaper articles to produce an outline of an event and biography of specific person.
</nextsent>
<nextsent>we also proposed the concept of viewpoint for mds purposes.
</nextsent>
<nextsent>while we targeted japanese technical terms in the computer domain, our method can also be applied to other types of terms in different languages, without modifying the model.
</nextsent>
<nextsent>however, set of viewpoint sand patterns typically used to describe each view point need to be modified or replaced depending the application.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2230">
<title id=" C08-1045.xml">non compositional language model and pattern dictionary development for japanese compound and complex sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this dictionary achieved syntactic coverage of 98% and semantic coverage of 78%.
</prevsent>
<prevsent>itwill substantially improve translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
a wide variety of machine translation (mt) methods are being studied(nagao, 1996; brown et al, 1990; <papid> J90-2002 </papid>vogel et al, 2003), but to obtain high-qualitytranslations between languages belonging to different families that are alien each other is difficult.</citsent>
<aftsection>
<nextsent>most practical systems still employ transfer method based on compositional semantics.
</nextsent>
<nextsent>aproblem with this method is that it produces translations by separating the syntactic structure from meaning, and is thus liable to lose the meaning of the source text.
</nextsent>
<nextsent>c ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2231">
<title id=" C08-1045.xml">non compositional language model and pattern dictionary development for japanese compound and complex sentences </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>better translation quality can be expected from pattern-based mt and example-based mt where the syntactic structure and semantics are handledtogether.
</prevsent>
<prevsent>however, pattern-based mt require immense pattern dictionaries that are difficult to de velop(jung et al, 1999; uchino et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" C92-4203 ">
meanwhile, example-based mt(nagao, 1984;sato, 1992; <papid> C92-4203 </papid>brown, 1999) obtains translation results by substituting semantically similar elements in structurally matching translation examples, so pre-prepared pattern dictionary is not needed.</citsent>
<aftsection>
<nextsent>however, the capability to substitute constituent in an example changes from one example to thenext, and to automate this judgement is impossible.
</nextsent>
<nextsent>this problem could be addressed by manually tagging each example beforehand to specify which constituents can be substituted, but the resulting method would be just another pattern-based translation method.attention has been focused on the use of cognitive grammar(langacker, 1987) and construction grammar(fillmore, 1988) in the search to find methods that might help to resolve this problem.however, the standards for determining the structural meaning units and the granularity needed for meaning analysis have not been clarified.
</nextsent>
<nextsent>as method in which the syntactic structure and meaning are dealt with as an integral whole, sentence pattern (sp)-dictionary called a-japanese lexicon has already been developed for japanese simple sentences(ikehara et al, 1997).
</nextsent>
<nextsent>this dictionary includes 14,800 valency patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2232">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is used generally forwhat is more strictly referred to by the term tactical genera tion?
</prevsent>
<prevsent>or surface realisation?.
</prevsent>
</prevsection>
<citsent citstr=" N07-1021 ">
1999), openccg (white, 2004) and xle (crouch et al, 2007), or created semi-automatically (belz,2007), <papid> N07-1021 </papid>or fully automatically extracted from annotated corpora, like the hpsg (nakanishi et al., 2005), <papid> W05-1510 </papid>lfg (cahill and van genabith, 2006; <papid> P06-1130 </papid>hogan et al, 2007) <papid> D07-1028 </papid>and ccg (white et al, 2007) resources derived from the penn-ii treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</nextsent>
<nextsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</nextsent>
<nextsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</nextsent>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2233">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is used generally forwhat is more strictly referred to by the term tactical genera tion?
</prevsent>
<prevsent>or surface realisation?.
</prevsent>
</prevsection>
<citsent citstr=" W05-1510 ">
1999), openccg (white, 2004) and xle (crouch et al, 2007), or created semi-automatically (belz,2007), <papid> N07-1021 </papid>or fully automatically extracted from annotated corpora, like the hpsg (nakanishi et al., 2005), <papid> W05-1510 </papid>lfg (cahill and van genabith, 2006; <papid> P06-1130 </papid>hogan et al, 2007) <papid> D07-1028 </papid>and ccg (white et al, 2007) resources derived from the penn-ii treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</nextsent>
<nextsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</nextsent>
<nextsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</nextsent>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2234">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is used generally forwhat is more strictly referred to by the term tactical genera tion?
</prevsent>
<prevsent>or surface realisation?.
</prevsent>
</prevsection>
<citsent citstr=" P06-1130 ">
1999), openccg (white, 2004) and xle (crouch et al, 2007), or created semi-automatically (belz,2007), <papid> N07-1021 </papid>or fully automatically extracted from annotated corpora, like the hpsg (nakanishi et al., 2005), <papid> W05-1510 </papid>lfg (cahill and van genabith, 2006; <papid> P06-1130 </papid>hogan et al, 2007) <papid> D07-1028 </papid>and ccg (white et al, 2007) resources derived from the penn-ii treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</nextsent>
<nextsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</nextsent>
<nextsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</nextsent>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2235">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is used generally forwhat is more strictly referred to by the term tactical genera tion?
</prevsent>
<prevsent>or surface realisation?.
</prevsent>
</prevsection>
<citsent citstr=" D07-1028 ">
1999), openccg (white, 2004) and xle (crouch et al, 2007), or created semi-automatically (belz,2007), <papid> N07-1021 </papid>or fully automatically extracted from annotated corpora, like the hpsg (nakanishi et al., 2005), <papid> W05-1510 </papid>lfg (cahill and van genabith, 2006; <papid> P06-1130 </papid>hogan et al, 2007) <papid> D07-1028 </papid>and ccg (white et al, 2007) resources derived from the penn-ii treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</nextsent>
<nextsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</nextsent>
<nextsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</nextsent>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2236">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is used generally forwhat is more strictly referred to by the term tactical genera tion?
</prevsent>
<prevsent>or surface realisation?.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
1999), openccg (white, 2004) and xle (crouch et al, 2007), or created semi-automatically (belz,2007), <papid> N07-1021 </papid>or fully automatically extracted from annotated corpora, like the hpsg (nakanishi et al., 2005), <papid> W05-1510 </papid>lfg (cahill and van genabith, 2006; <papid> P06-1130 </papid>hogan et al, 2007) <papid> D07-1028 </papid>and ccg (white et al, 2007) resources derived from the penn-ii treebank (ptb) (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</nextsent>
<nextsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</nextsent>
<nextsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</nextsent>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2237">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</prevsent>
<prevsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</citsent>
<aftsection>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
<nextsent>an exception is ratnaparkhi (2000), <papid> A00-2026 </papid>who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from semantic representation of attribute-value pairs, restricted to an air travel domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2238">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over the last decade, probabilistic models have become widely used in the field of natural language generation (nlg), often in the form of realisation ranker in two-stage generation architecture.
</prevsent>
<prevsent>the two-stage methodology is characterised by separation between generation and selection, in which rule-based methods are used to generate aspace of possible paraphrases, and statistical methods are used to select the most likely realisation from the space.
</prevsent>
</prevsection>
<citsent citstr=" A00-2023 ">
by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</citsent>
<aftsection>
<nextsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</nextsent>
<nextsent>an exception is ratnaparkhi (2000), <papid> A00-2026 </papid>who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from semantic representation of attribute-value pairs, restricted to an air travel domain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2240">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by and large, two statistical models are used in the rankers to choose output strings: ? n-gram language models over different units,such as word-level bigram/trigram models (bangalore and rambow, 2000; <papid> C00-1007 </papid>langkilde, 2000), <papid> A00-2023 </papid>or factored language models integrated with syntactic tags (white et al, 2007).</prevsent>
<prevsent>log-linear models with different syntactic and semantic features (velldal and oepen, 2005; nakanishi et al, 2005; <papid> W05-1510 </papid>cahill et al, 2007).to date, however, probabilistic models learning direct mappings from generation input to surface strings, without the effort to construct grammar, have rarely been explored.</prevsent>
</prevsection>
<citsent citstr=" A00-2026 ">
an exception is ratnaparkhi (2000), <papid> A00-2026 </papid>who presents maximum entropy models to learn attribute ordering and lexical choice for sentence generation from semantic representation of attribute-value pairs, restricted to an air travel domain.</citsent>
<aftsection>
<nextsent>297 snp prp we vp vbp believe pp in in np np dt the nn law pp in of np nns averages 1 ? ?
</nextsent>
<nextsent>pred believe?
</nextsent>
<nextsent>tense pres subj 2 ? ?
</nextsent>
<nextsent>pred pro?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2241">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> lfg-based generation.  </section>
<citcontext>
<prevsection>
<prevsent>modifiers like adj(unct), coord(inate)are not sub categorised for by the predicate, and can occur any number of times in local f-structure.
</prevsent>
<prevsent>atomic-valued features describe linguistic properties of the predicate, such as tense, aspect, mood, pers, num, case etc. 2.2 generation from f-structures.
</prevsent>
</prevsection>
<citsent citstr=" C00-1062 ">
work on generation in lfg generally assumes that the generation task is to determine the set of strings of the language that corresponds to specified structure, given particular grammar (kaplan and wedekind, 2000).<papid> C00-1062 </papid></citsent>
<aftsection>
<nextsent>previous work on generation 2f-structures can be also interpreted as quasi-logical forms (van genabith and crouch, 1996), which more closely resemble inputs used by some other generators.
</nextsent>
<nextsent>298 within lfg includes the xle,3 cahill and van genabith (2006)<papid> P06-1130 </papid>hogan et al (2007) <papid> D07-1028 </papid>and cahill etal.</nextsent>
<nextsent>(2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2249">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> lfg-based generation.  </section>
<citcontext>
<prevsection>
<prevsent>the xle generates sentences from fstructures according to parallel handcrafted grammars for english, french, german, norwegian, japanese, and urdu.
</prevsent>
<prevsent>based on the german xle resources, cahill et al (2007) describe two-stage,log-linear generation model.
</prevsent>
</prevsection>
<citsent citstr=" P04-1041 ">
cahill and van genabith (2006) <papid> P06-1130 </papid>and hogan et al (2007) <papid> D07-1028 </papid>present chart generator using wide-coverage pcfg-based lfg approximations automatically acquired from treebanks (cahill et al, 2004).<papid> P04-1041 </papid></citsent>
<aftsection>
<nextsent>basic idea traditional lfg generation models can be regarded as the reverse process of parsing, and use bi-directional f-structure-annotated cfg rules.
</nextsent>
<nextsent>in sense, the generation process is driven byan input dependency (or f-structure) representation, but proceeds through the detour?
</nextsent>
<nextsent>of using dependency-annotated cfg (or pcfg) grammars and chart-based generators.
</nextsent>
<nextsent>in this paper,we develop simple n-gram and dependency based, wide-coverage, robust, probabilistic generation model, which cuts out the middle-man from previous approaches: the cfg-component.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2251">
<title id=" C08-1038.xml">dependency based ngram models for general purpose sentence realisation </title>
<section> experiments and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>type experiment in (langkilde, 2002).
</prevsent>
<prevsent>5.2 experimental results.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
following (langkilde, 2002) and other workon general-purpose generators, bleu score (pa pineni et al, 2002), <papid> P02-1040 </papid>average nist simple string accuracy (ssa) and percentage of exactly matched sentences are adopted as evaluation metrics.</citsent>
<aftsection>
<nextsent>as our system guarantees that all input fstructures can generate complete sentence, special coverage-dependent evaluation (as has been adopted in most grammar-based generation sys tems) is not necessary in our experiments.
</nextsent>
<nextsent>experiments are carried out on an intel pentium4 server, with 3.80ghz cpu and 3gb memory.
</nextsent>
<nextsent>it takes less than 2 minutes to generate all 2,416 sentences (with average sentence length of 21 words) of wsj section 23 (average 0.05 sec persentence), and approximately 4 minutes to generate 1,708 sentences (with average sentence length of 30 words) of ctb test data (average 0.14 secper sentence), using 4-gram models in all experiments.
</nextsent>
<nextsent>our evaluation results for english and chinese data are shown in tables 4 and 5, respectively.different n-gram models perform nearly consistently in all the experiments on both english and chinese data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2254">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency-based syntactic parsing has become increasingly popular in computational linguistics in recent years.
</prevsent>
<prevsent>one of the reasons for the growing interest is apparently the belief that dependency based representations should be more suitable for languages that exhibit free or flexible word order and where most of the clues to syntactic structure are found in lexical and morphological features, rather than in syntactic categories and word order configurations.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
some support for this view can be found in the results from the conll shared tasks on dependency parsing in 2006 and 2007, where variety of data-driven methods for dependency parsing have been applied with encouraging result sto languages of great typo logical diversity (buch holz and marsi, 2006; <papid> W06-2920 </papid>nivre et al, 2007<papid> D07-1096 </papid>a).</citsent>
<aftsection>
<nextsent>however, there are still important differences in parsing accuracy for different language types.
</nextsent>
<nextsent>for ? joakim nivre, igor m. boguslavsky, and leonidl.
</nextsent>
<nextsent>iomdin, 2008.
</nextsent>
<nextsent>licensed under the creative commons attribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2255">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency-based syntactic parsing has become increasingly popular in computational linguistics in recent years.
</prevsent>
<prevsent>one of the reasons for the growing interest is apparently the belief that dependency based representations should be more suitable for languages that exhibit free or flexible word order and where most of the clues to syntactic structure are found in lexical and morphological features, rather than in syntactic categories and word order configurations.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
some support for this view can be found in the results from the conll shared tasks on dependency parsing in 2006 and 2007, where variety of data-driven methods for dependency parsing have been applied with encouraging result sto languages of great typo logical diversity (buch holz and marsi, 2006; <papid> W06-2920 </papid>nivre et al, 2007<papid> D07-1096 </papid>a).</citsent>
<aftsection>
<nextsent>however, there are still important differences in parsing accuracy for different language types.
</nextsent>
<nextsent>for ? joakim nivre, igor m. boguslavsky, and leonidl.
</nextsent>
<nextsent>iomdin, 2008.
</nextsent>
<nextsent>licensed under the creative commons attribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2260">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a complicating factor in this kind of comparison is the fact that the syntactic annotation in treebanks varies across languages, in such way that it is very difficult to tease apart the impact on parsing accuracy of linguistic structure, on the one hand, and linguistic annotation, on the other.
</prevsent>
<prevsent>it is also worth noting that the majority of the datasets used in the conll shared tasks arenot derived from treebanks with genuine dependency annotation, but have been obtained through conversion from other kinds of annotation.
</prevsent>
</prevsection>
<citsent citstr=" C00-2143 ">
andthe datasets that do come with original dependency annotation are generally fairly small, with less than 100,000 words available for training, the notable exception of course being the prague dependency treebank of czech (hajic et al, 2001), which is one of the largest and most widely used treebanks in the field.this paper contributes to the growing literature on dependency parsing for typo logically diverse languages by presenting the first results on parsing the russian treebank syntagrus (bo guslavsky et al, 2000; <papid> C00-2143 </papid>boguslavsky et al, 2002).</citsent>
<aftsection>
<nextsent>there are several factors that make this treebank an interesting resource in this context.
</nextsent>
<nextsent>first of all, it contains genuine dependency annotation,theoretically grounded in the long tradition of dependency grammar for slavic languages, represented by the work of tesni`ere (1959) andmelcuk (1988), among others.
</nextsent>
<nextsent>secondly, with close to 641 500,000 tokens, the treebank is larger than most other available dependency treebanks and providesa good basis for experimental investigations using data-driven methods.
</nextsent>
<nextsent>thirdly, the russian language, which has not been included in previous experimental evaluations such as the conll shared tasks, is richly inflected language with free word order and thus representative of the class of languages that tend to pose problems for the currently available parsing models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2274">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>the etap-3 morphological analyzer uses the dictionary to produce morphological annotation of words belonging to the corpus, including lemma, part-of-speech tag and additional morphological features dependent on the part of speech: animacy, gender, number,case, degree of comparison, short form (of adjectives and participles), representation (of verbs), aspect, tense, mood, person, voice, composite form, and attenuation.
</prevsent>
<prevsent>statistics for the version of syntagrus used for the experiments described in this paper are as follows: ? 32,242 sentences, belonging to the fiction genre (9.8%), texts of online news (12.4%), newspaper and journal articles (77.8%); ? 461,297 tokens, including expressions with non-alphabetical symbols (e.g., 10, 1.200, $333, +70c, #) but excluding punctuation; ? 31,683 distinct word types, of which 635 with frequency greater than 100, 5041 greater than 10, and 18231 greater than 1; ? 3,414 sentences (10.3%) with non-projective 643 pos dep mor lem lex top + + + + + top1 + head(top) + + ldep(top) + rdep(top) + next + + + + next+1 + + + + next+2 + next+3 + ldep(next) + table 1: history-based features (top = token on top of stack; next = next token in input buffer;head(w) = head of w; ldep(w) = left most dependent of w; rdep(w) = left most dependent of w).dependencies and 3,934 non-projective dependency arcs in total; ? 478 sentences (1.5%) containing phantom nodes and 631 phantom nodes in total.
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
malt parser (nivre et al, 2007<papid> D07-1096 </papid>b) is language independent system for data-driven dependency parsing, based on transition-based parsing model (mcdonald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>more precisely, the approach is based on four essential components: ? transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (nivre, 2003).
</nextsent>
<nextsent>history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></nextsent>
<nextsent>discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2276">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser (nivre et al, 2007<papid> D07-1096 </papid>b) is language independent system for data-driven dependency parsing, based on transition-based parsing model (mcdonald and nivre, 2007).<papid> D07-1013 </papid></prevsent>
<prevsent>more precisely, the approach is based on four essential components: ? transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (nivre, 2003).</prevsent>
</prevsection>
<citsent citstr=" H92-1026 ">
history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</nextsent>
<nextsent>pseudo-projective parsing for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2277">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser (nivre et al, 2007<papid> D07-1096 </papid>b) is language independent system for data-driven dependency parsing, based on transition-based parsing model (mcdonald and nivre, 2007).<papid> D07-1013 </papid></prevsent>
<prevsent>more precisely, the approach is based on four essential components: ? transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (nivre, 2003).</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</nextsent>
<nextsent>pseudo-projective parsing for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2278">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>malt parser (nivre et al, 2007<papid> D07-1096 </papid>b) is language independent system for data-driven dependency parsing, based on transition-based parsing model (mcdonald and nivre, 2007).<papid> D07-1013 </papid></prevsent>
<prevsent>more precisely, the approach is based on four essential components: ? transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (nivre, 2003).</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></citsent>
<aftsection>
<nextsent>discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</nextsent>
<nextsent>pseudo-projective parsing for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2279">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>more precisely, the approach is based on four essential components: ? transition-based deterministic algorithm for building labeled projective dependency graphs in linear time (nivre, 2003).
</prevsent>
<prevsent>history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</citsent>
<aftsection>
<nextsent>pseudo-projective parsing for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid></nextsent>
<nextsent>in the following subsections, we briefly describe each of these four components in turn.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2280">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> malt parser.  </section>
<citcontext>
<prevsection>
<prevsent>history-based feature models for predicting the next parser action (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997).?<papid> W97-0301 </papid></prevsent>
<prevsent>discriminative classifiers for mapping histories to parser actions (kudo and matsumoto, 2002; <papid> W02-2016 </papid>yamada and matsumoto, 2003).?</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
pseudo-projective parsing for recovering non projective structures (nivre and nilsson, 2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>in the following subsections, we briefly describe each of these four components in turn.
</nextsent>
<nextsent>3.1 parsing algorithm.
</nextsent>
<nextsent>the parser uses the deterministic algorithm for labeled dependency parsing first proposed by nivre (2003).
</nextsent>
<nextsent>the algorithm builds labeled dependency graph in one left-to-right pass over the input, using stack to store partially processed tokens and adding arcs using four elementary actions (where top is the token on top of the stack and next is the next token): ? shift: push next onto the stack.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2288">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in order to correctly analyse these tokens and their dependencies when parsing raw text, they would have to be recovered in pre-processing phase along the lines of dienes 3the precision is the percentage of non-projective dependencies predicted by the parser that were correct, while the recall is the percentage of true non-projective dependencies that were correctly predicted by the parser.
</prevsent>
<prevsent>646 and dubey (2003).summing up, the main result of the experimental evaluation is that both morphological and lexical features are crucial for attaining high accuracy when training and evaluating on the representations found in the syntagrus treebank of russian.
</prevsent>
</prevsection>
<citsent citstr=" H05-1100 ">
with regard to morphological features this is in line with number of recent studies showing the importance of morphology for parsing languages with less rigid word order, including work on spanish (cowan and collins, 2005), <papid> H05-1100 </papid>hebrew (tsarfaty, 2006; <papid> P06-3009 </papid>tsarfaty and simaan, 2007), turkish (eryigit et al, 2006), and swedish (vrelid and nivre, 2007).</citsent>
<aftsection>
<nextsent>with regard to lexical features, the situation ismore complex in that there are number of studies questioning the usefulness of lexical features in statistical parsing and arguing that equivalent or better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminal work of klein and manning (2003), <papid> P03-1054 </papid>or using hidden variables and unsupervised learning, as in the more recent work by petrov et al (2006), <papid> P06-1055 </papid>amongothers.</nextsent>
<nextsent>there are even studies showing that lexicalization can be harmful when parsing richly inflected languages like german (dubey and keller, 2003) <papid> P03-1013 </papid>and turkish (eryigit and oflazer, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2289">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in order to correctly analyse these tokens and their dependencies when parsing raw text, they would have to be recovered in pre-processing phase along the lines of dienes 3the precision is the percentage of non-projective dependencies predicted by the parser that were correct, while the recall is the percentage of true non-projective dependencies that were correctly predicted by the parser.
</prevsent>
<prevsent>646 and dubey (2003).summing up, the main result of the experimental evaluation is that both morphological and lexical features are crucial for attaining high accuracy when training and evaluating on the representations found in the syntagrus treebank of russian.
</prevsent>
</prevsection>
<citsent citstr=" P06-3009 ">
with regard to morphological features this is in line with number of recent studies showing the importance of morphology for parsing languages with less rigid word order, including work on spanish (cowan and collins, 2005), <papid> H05-1100 </papid>hebrew (tsarfaty, 2006; <papid> P06-3009 </papid>tsarfaty and simaan, 2007), turkish (eryigit et al, 2006), and swedish (vrelid and nivre, 2007).</citsent>
<aftsection>
<nextsent>with regard to lexical features, the situation ismore complex in that there are number of studies questioning the usefulness of lexical features in statistical parsing and arguing that equivalent or better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminal work of klein and manning (2003), <papid> P03-1054 </papid>or using hidden variables and unsupervised learning, as in the more recent work by petrov et al (2006), <papid> P06-1055 </papid>amongothers.</nextsent>
<nextsent>there are even studies showing that lexicalization can be harmful when parsing richly inflected languages like german (dubey and keller, 2003) <papid> P03-1013 </papid>and turkish (eryigit and oflazer, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2290">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>646 and dubey (2003).summing up, the main result of the experimental evaluation is that both morphological and lexical features are crucial for attaining high accuracy when training and evaluating on the representations found in the syntagrus treebank of russian.
</prevsent>
<prevsent>with regard to morphological features this is in line with number of recent studies showing the importance of morphology for parsing languages with less rigid word order, including work on spanish (cowan and collins, 2005), <papid> H05-1100 </papid>hebrew (tsarfaty, 2006; <papid> P06-3009 </papid>tsarfaty and simaan, 2007), turkish (eryigit et al, 2006), and swedish (vrelid and nivre, 2007).</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
with regard to lexical features, the situation ismore complex in that there are number of studies questioning the usefulness of lexical features in statistical parsing and arguing that equivalent or better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminal work of klein and manning (2003), <papid> P03-1054 </papid>or using hidden variables and unsupervised learning, as in the more recent work by petrov et al (2006), <papid> P06-1055 </papid>amongothers.</citsent>
<aftsection>
<nextsent>there are even studies showing that lexicalization can be harmful when parsing richly inflected languages like german (dubey and keller, 2003) <papid> P03-1013 </papid>and turkish (eryigit and oflazer, 2006).</nextsent>
<nextsent>however, it is worth noting that most of these results have been obtained either for models ofconstituency-based parsing or for models of dependency parsing suffering from sparse data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2291">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>646 and dubey (2003).summing up, the main result of the experimental evaluation is that both morphological and lexical features are crucial for attaining high accuracy when training and evaluating on the representations found in the syntagrus treebank of russian.
</prevsent>
<prevsent>with regard to morphological features this is in line with number of recent studies showing the importance of morphology for parsing languages with less rigid word order, including work on spanish (cowan and collins, 2005), <papid> H05-1100 </papid>hebrew (tsarfaty, 2006; <papid> P06-3009 </papid>tsarfaty and simaan, 2007), turkish (eryigit et al, 2006), and swedish (vrelid and nivre, 2007).</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
with regard to lexical features, the situation ismore complex in that there are number of studies questioning the usefulness of lexical features in statistical parsing and arguing that equivalent or better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminal work of klein and manning (2003), <papid> P03-1054 </papid>or using hidden variables and unsupervised learning, as in the more recent work by petrov et al (2006), <papid> P06-1055 </papid>amongothers.</citsent>
<aftsection>
<nextsent>there are even studies showing that lexicalization can be harmful when parsing richly inflected languages like german (dubey and keller, 2003) <papid> P03-1013 </papid>and turkish (eryigit and oflazer, 2006).</nextsent>
<nextsent>however, it is worth noting that most of these results have been obtained either for models ofconstituency-based parsing or for models of dependency parsing suffering from sparse data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2292">
<title id=" C08-1081.xml">parsing the syntagrus treebank of russian </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>with regard to morphological features this is in line with number of recent studies showing the importance of morphology for parsing languages with less rigid word order, including work on spanish (cowan and collins, 2005), <papid> H05-1100 </papid>hebrew (tsarfaty, 2006; <papid> P06-3009 </papid>tsarfaty and simaan, 2007), turkish (eryigit et al, 2006), and swedish (vrelid and nivre, 2007).</prevsent>
<prevsent>with regard to lexical features, the situation ismore complex in that there are number of studies questioning the usefulness of lexical features in statistical parsing and arguing that equivalent or better results can be achieved with unlexical ized models provided that linguistic categories can be split flexibly into more fine-grained categories, either using hand-crafted splits, as in the seminal work of klein and manning (2003), <papid> P03-1054 </papid>or using hidden variables and unsupervised learning, as in the more recent work by petrov et al (2006), <papid> P06-1055 </papid>amongothers.</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
there are even studies showing that lexicalization can be harmful when parsing richly inflected languages like german (dubey and keller, 2003) <papid> P03-1013 </papid>and turkish (eryigit and oflazer, 2006).</citsent>
<aftsection>
<nextsent>however, it is worth noting that most of these results have been obtained either for models ofconstituency-based parsing or for models of dependency parsing suffering from sparse data.
</nextsent>
<nextsent>4 in the experiments presented here, we have used transition-based model for dependency parsing that has much fewer parameters than state-of-the art probabilistic models for constituency parsing.
</nextsent>
<nextsent>moreover, we have been able to use relatively large training set, thereby minimizing the effect of sparseness for lexical features.
</nextsent>
<nextsent>we therefore conjecture that the beneficial effect of lexical features on parsing accuracy will generalize to other richly inflected languages when similar conditions hold.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2293">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>bouma, van noord and malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection.
</prevsent>
<prevsent>lin (1998),srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against gold standard?
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
relations, rather than matching (labelled) phrase structure bracketings.research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (briscoe and carroll, 1997; <papid> A97-1052 </papid>mccarthy, 2000), <papid> A00-2034 </papid>word classes for disambiguation (clark and weir, 2001), <papid> N01-1013 </papid>and collocations (lin 1999), <papid> P99-1041 </papid>has used grammatical relation/head/dependent tuples.</citsent>
<aftsection>
<nextsent>such 1a previous version of this paper was presented at iwpt01; this version contains new experiments and results.tuples also constitute convenient intermediate representation in applications such as information extraction (palmer et al , 1993; yeh, 2000), <papid> P00-1017 </papid>and document retrieval on the web (grefenstette, 1997).</nextsent>
<nextsent>a variety of different approaches have been taken for robust extraction of relation/head/dependent tu ples, or grammatical relations, from unrestricted text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2294">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>bouma, van noord and malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection.
</prevsent>
<prevsent>lin (1998),srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against gold standard?
</prevsent>
</prevsection>
<citsent citstr=" A00-2034 ">
relations, rather than matching (labelled) phrase structure bracketings.research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (briscoe and carroll, 1997; <papid> A97-1052 </papid>mccarthy, 2000), <papid> A00-2034 </papid>word classes for disambiguation (clark and weir, 2001), <papid> N01-1013 </papid>and collocations (lin 1999), <papid> P99-1041 </papid>has used grammatical relation/head/dependent tuples.</citsent>
<aftsection>
<nextsent>such 1a previous version of this paper was presented at iwpt01; this version contains new experiments and results.tuples also constitute convenient intermediate representation in applications such as information extraction (palmer et al , 1993; yeh, 2000), <papid> P00-1017 </papid>and document retrieval on the web (grefenstette, 1997).</nextsent>
<nextsent>a variety of different approaches have been taken for robust extraction of relation/head/dependent tu ples, or grammatical relations, from unrestricted text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2295">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>bouma, van noord and malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection.
</prevsent>
<prevsent>lin (1998),srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against gold standard?
</prevsent>
</prevsection>
<citsent citstr=" N01-1013 ">
relations, rather than matching (labelled) phrase structure bracketings.research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (briscoe and carroll, 1997; <papid> A97-1052 </papid>mccarthy, 2000), <papid> A00-2034 </papid>word classes for disambiguation (clark and weir, 2001), <papid> N01-1013 </papid>and collocations (lin 1999), <papid> P99-1041 </papid>has used grammatical relation/head/dependent tuples.</citsent>
<aftsection>
<nextsent>such 1a previous version of this paper was presented at iwpt01; this version contains new experiments and results.tuples also constitute convenient intermediate representation in applications such as information extraction (palmer et al , 1993; yeh, 2000), <papid> P00-1017 </papid>and document retrieval on the web (grefenstette, 1997).</nextsent>
<nextsent>a variety of different approaches have been taken for robust extraction of relation/head/dependent tu ples, or grammatical relations, from unrestricted text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2296">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>bouma, van noord and malouf (2001) create dependency treebanks semi-automatically in order to induce dependency-based statistical models for parse selection.
</prevsent>
<prevsent>lin (1998),srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against gold standard?
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
relations, rather than matching (labelled) phrase structure bracketings.research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (briscoe and carroll, 1997; <papid> A97-1052 </papid>mccarthy, 2000), <papid> A00-2034 </papid>word classes for disambiguation (clark and weir, 2001), <papid> N01-1013 </papid>and collocations (lin 1999), <papid> P99-1041 </papid>has used grammatical relation/head/dependent tuples.</citsent>
<aftsection>
<nextsent>such 1a previous version of this paper was presented at iwpt01; this version contains new experiments and results.tuples also constitute convenient intermediate representation in applications such as information extraction (palmer et al , 1993; yeh, 2000), <papid> P00-1017 </papid>and document retrieval on the web (grefenstette, 1997).</nextsent>
<nextsent>a variety of different approaches have been taken for robust extraction of relation/head/dependent tu ples, or grammatical relations, from unrestricted text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2297">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>lin (1998),srinivas (2000) and others have evaluated the accuracy of both phrase structure-based and dependency parsers by matching head-dependent relations against gold standard?
</prevsent>
<prevsent>relations, rather than matching (labelled) phrase structure bracketings.research on unsupervised acquisition of lexical information from corpora, such as argument structure of predicates (briscoe and carroll, 1997; <papid> A97-1052 </papid>mccarthy, 2000), <papid> A00-2034 </papid>word classes for disambiguation (clark and weir, 2001), <papid> N01-1013 </papid>and collocations (lin 1999), <papid> P99-1041 </papid>has used grammatical relation/head/dependent tuples.</prevsent>
</prevsection>
<citsent citstr=" P00-1017 ">
such 1a previous version of this paper was presented at iwpt01; this version contains new experiments and results.tuples also constitute convenient intermediate representation in applications such as information extraction (palmer et al , 1993; yeh, 2000), <papid> P00-1017 </papid>and document retrieval on the web (grefenstette, 1997).</citsent>
<aftsection>
<nextsent>a variety of different approaches have been taken for robust extraction of relation/head/dependent tu ples, or grammatical relations, from unrestricted text.
</nextsent>
<nextsent>dependency parsing is natural technique to use, and there has been some work in that areaon robust analysis and disambiguation (e.g. lafferty, sleator and temperley, 1992; srinivas, 2000).
</nextsent>
<nextsent>finite-state approaches (e.g. karlsson et al , 1995; at-mokhtar and chanod, 1997; grefenstette, 1998)have used hand-coded transducers to recognise linear configurations of words and part of speech labels associated with, for example, subject/object verb relationships.
</nextsent>
<nextsent>an intermediate step may be to mark nominal, verbal etc. chunks?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2298">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>statistical finite-state approaches have also been used: brants, skut and krenn (1997) train cascade of hidden markov models to tag words with their grammatical functions.
</prevsent>
<prevsent>approaches based on memory based learning have also used chunking as afirst stage, before assigning grammatical relation labels to heads of chunks (argamon, dagan and krymolowski, 1998; buchholz, veenstra and daelemans, 1999).
</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
blaheta and charniak (2000) <papid> A00-2031 </papid>assume richer input representation consisting of labelled trees produced by treebank grammar parser, and use the treebank again to train further procedure that assigns grammatical function tags to syntactic constituents in the trees.</citsent>
<aftsection>
<nextsent>alternatively, handwritten grammar can be used that produces shal low?
</nextsent>
<nextsent>and perhaps partial phrase structure analyses from which grammatical relations are extracted (e.g. carroll, minnen and briscoe, 1998; lin, 1998).recently, schmid and rooth (2001) <papid> P01-1060 </papid>have described an algorithm for computing expected governor labels for terminal words in labelled headed parse trees produced by probabilistic context-free grammar.</nextsent>
<nextsent>a governor label (implicitly) encodes agrammatical relation type (such as subject or ob ject) and governing lexical head.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2300">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> introduction1.  </section>
<citcontext>
<prevsection>
<prevsent>blaheta and charniak (2000) <papid> A00-2031 </papid>assume richer input representation consisting of labelled trees produced by treebank grammar parser, and use the treebank again to train further procedure that assigns grammatical function tags to syntactic constituents in the trees.</prevsent>
<prevsent>alternatively, handwritten grammar can be used that produces shal low?</prevsent>
</prevsection>
<citsent citstr=" P01-1060 ">
and perhaps partial phrase structure analyses from which grammatical relations are extracted (e.g. carroll, minnen and briscoe, 1998; lin, 1998).recently, schmid and rooth (2001) <papid> P01-1060 </papid>have described an algorithm for computing expected governor labels for terminal words in labelled headed parse trees produced by probabilistic context-free grammar.</citsent>
<aftsection>
<nextsent>a governor label (implicitly) encodes agrammatical relation type (such as subject or ob ject) and governing lexical head.
</nextsent>
<nextsent>the labels are expected in the sense that each is weighted by the sum of the probabilities of the trees giving rise to it, and are computed efficiently by processing the entire parse forest rather than individual trees.
</nextsent>
<nextsent>the set of terminal/relation/governing-head tuples willnot typically constitute globally coherent analysis, but may be useful for interfacing to applications that primarily accumulate fragments of grammatical information from text (such as for instance information extraction, or systems that acquire lexical data from corpora).
</nextsent>
<nextsent>the approach is not so suit able for applications that need to interpret complete and consistent sentence structures (such as the analysis phase of transfer-based machine translation).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2301">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> the analysis system.  </section>
<citcontext>
<prevsection>
<prevsent>inthat verbal argument structure is disambiguated lex ically, but the rest of the disambiguation is purely structural.
</prevsent>
<prevsent>the coverage of the gram marthe proportion of sentences for which at least one complete spanning analysis is foundis around 80% when applied to the susanne corpus (sampson, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P99-1061 ">
in addition,the system is able to perform parse failure recovery, finding the highest scoring sequence of phrasal fragments (following the approach of kiefer et al , 1999), <papid> P99-1061 </papid>and the system has produced at least partial analyses for over 98% of the sentences in the written part of the british national corpus.the parsing system reads off grammatical relation tuples (grs) from the constituent structure tree that is returned from the disambiguation phase.</citsent>
<aftsection>
<nextsent>information is used about which grammar rules introduce subjects, complements, and modifiers, and which daughter(s) is/are the head(s), and which the dependents.
</nextsent>
<nextsent>in carroll et al evaluation the system achieves gr accuracy that is comparable to published results for other systems: extraction of nonclausal subject relations with 83% precision, compared with grefenstettes (1998) figure of 80%; and overall f-score2 of un labelled head-dependent pairsof 80%, as opposed to lins (1998) 83%3 and srini vass (2000) 84% (this with respect only to binary relations, and omitting the analysis of control rela tionships).
</nextsent>
<nextsent>blaheta and charniak (2000) <papid> A00-2031 </papid>report an f-score of 87% for assigning grammatical function tags to constituents, but the task, and therefore the scoring method, is rather different.for the work reported in this paper we have extended carroll et al basic system, implement inga version of schmid and rooths expected governor technique (see section 1 above) but adapted for unification-based grammar and gr-based analyses.</nextsent>
<nextsent>each sentence is analysed as set of weighted grs where the weight associated with each grammatical relation is computed as the sum of the probabilities of the parses that relation was derived from, divided by the sum of the probabilities of all parses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2304">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>an application using grammatical relation analyses might be interested only in grs that the parser is fairly confident of being correct.
</prevsent>
<prevsent>for instance, in unsupervised acquisition of lexical information (such as subcategorisation frames for verbs) from text, the usual methodology is to (partially) analyse the text, retaining only reliable hypotheses which are then filtered based on the amount of evidence for them over the corpus as whole.
</prevsent>
</prevsection>
<citsent citstr=" J93-2002 ">
thus, brent (1993) <papid> J93-2002 </papid>only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) sothere can be no other attachment possibilities.</citsent>
<aftsection>
<nextsent>in recent work on unsupervised learning of prepositional phrase disambiguation, pantel and lin (2000) <papid> P00-1014 </papid>derive training instances only from relevant data appearing in syntactic contexts that are guaranteed to be unam biguous.</nextsent>
<nextsent>in our system, the weights on grs indicate how certain the parser is of the associated relations being correct.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2305">
<title id=" C02-1013.xml">high precision extraction of grammatical relations </title>
<section> empirical results.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, in unsupervised acquisition of lexical information (such as subcategorisation frames for verbs) from text, the usual methodology is to (partially) analyse the text, retaining only reliable hypotheses which are then filtered based on the amount of evidence for them over the corpus as whole.
</prevsent>
<prevsent>thus, brent (1993) <papid> J93-2002 </papid>only creates hypotheses on the basis of instances of verb frames that are reliably and unambiguously cued by closed class items (such as pronouns) sothere can be no other attachment possibilities.</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
in recent work on unsupervised learning of prepositional phrase disambiguation, pantel and lin (2000) <papid> P00-1014 </papid>derive training instances only from relevant data appearing in syntactic contexts that are guaranteed to be unam biguous.</citsent>
<aftsection>
<nextsent>in our system, the weights on grs indicate how certain the parser is of the associated relations being correct.
</nextsent>
<nextsent>we therefore investigated whether more highly weighted grs are in fact more likely5ignoring the weights on grs, standard (unweighted) evaluation results for all parses are: precision 36.65%, recall 89.42% and f-score 51.99.
</nextsent>
<nextsent>1.0 aux( , continue, will) 0.4490 iobj(on, place, tax-payers) 1.0 detmod( , burden, a) 0.3276 ncmod(on, burden, tax-payers) 1.0 dobj(do, this, ) 0.2138 ncmod(on, place, tax-payers) 1.0 dobj(place, burden, ) 0.0250 xmod(to, continue, place) 1.0 ncmod( , burden, disproportionate) 0.0242 ncmod( , fulton, tax-payers) 1.0 ncsubj(continue, failure, ) 0.0086 obj2(place, tax-payers) 1.0 ncsubj(place, failure, ) 0.0086 ncmod(on, burden, fulton) 1.0 xcomp(to, failure, do) 0.0020 mod( , continue, place) 0.9730 clausal(continue, place) 0.0010 ncmod(on, continue, tax-payers) 0.9673 ncmod( , tax-payers, fulton) figure 1: weighted grs for the sentence failure to do this will continue to place disproportionate burden on fulton taxpayers.
</nextsent>
<nextsent>100 75 50 recall (%) 50 75 100 precision (%) y z threshold=0{ { { | threshold=1 } } } }e~ figure 2: weighted gr accuracy as the threshold is varied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2306">
<title id=" C08-1072.xml">a syntactic time series model for parsing fluent and dis fluent speech </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>this paper describes an incremental approach to parsing transcribed spontaneous speech containing disfluencies with hierarchical hidden markov model (hhmm).
</prevsent>
</prevsection>
<citsent citstr=" P08-2027 ">
this model makes use of the right-cornertransform, which has been shown to in crease non-incremental parsing accuracy on transcribed spontaneous speech (millerand schuler, 2008), <papid> P08-2027 </papid>using trees transformed in this manner to train the hhmm parser.</citsent>
<aftsection>
<nextsent>not only do the representations used in this model align with structure in speech repairs, but as an hmm-like time series model, it can be directly integrated into conventional speech recognition systems run on continuous streams of audio.a system implementing this model is evaluated on the standard task of parsing the switchboard corpus, and achieves an improvement over the standard baseline probabilistic cyk parser.
</nextsent>
<nextsent>dis fluency is one obstacle preventing speech recognition systems from being able to recognize spontaneous speech.
</nextsent>
<nextsent>perhaps the most challenging aspect of dis fluency recognition is the phenomenon of speech repair, which involves speaker realizing mistake, cutting off the flowof speech, and then continuing on, possibly retracing and replacing part of the utterance to thatpoint.
</nextsent>
<nextsent>this paper will describe system which applies syntactic model of speech repair to timethe authors would like to thank the anonymous reviewers for their input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2307">
<title id=" C08-1072.xml">a syntactic time series model for parsing fluent and dis fluent speech </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>recent work has also looked at the possible contribution of higher-level cues, including syntactic structure, in the detection of speech repair.
</prevsent>
<prevsent>some of this work is inspired by levelts (1983) investigation ofthe syntactic and semantic variables in speech repairs, particularly his well-formedness rule, which states that the reparandum and alteration of repair typically have the same consitituent label, similar to coordination.
</prevsent>
</prevsection>
<citsent citstr=" P06-1021 ">
hale et al (2006) <papid> P06-1021 </papid>use levelts well-formednessrule to justify an annotation scheme where unfinished categories (marked x-unf) have the unf label appended to all ancestral category labels all the way up to the top-most constituent beneath an edited label (edited labels denot inga reparandum).</citsent>
<aftsection>
<nextsent>they reason that this should prevent grammar rules of finished constituents in the corpus from corrupting the grammar of unfinished constituents.
</nextsent>
<nextsent>while this annotation proves helpful, it also leads to the unfortunate result that large reparandum requires several special repair rules to be applied, even though the actual error is only 569 happening at one point.
</nextsent>
<nextsent>intuitively, though, it seems that an error is only occurring at the end of the reparandum, and that until that point only fluent grammar rules are being applied by the speaker.
</nextsent>
<nextsent>this intuition has also been confirmed by empirical studies (nakatani and hirschberg, 1994), which show that there is no obvious error signal in speech up until the moment of interruption.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2310">
<title id=" C08-1072.xml">a syntactic time series model for parsing fluent and dis fluent speech </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>intuitively, though, it seems that an error is only occurring at the end of the reparandum, and that until that point only fluent grammar rules are being applied by the speaker.
</prevsent>
<prevsent>this intuition has also been confirmed by empirical studies (nakatani and hirschberg, 1994), which show that there is no obvious error signal in speech up until the moment of interruption.
</prevsent>
</prevsection>
<citsent citstr=" P04-1005 ">
although speakers may retrace muchof the reparandum for clarity or other reasons, ideally the reparandum contains nothing but standard grammatical rules until the speech is interrupted.1another recent approach to this problem (john son and charniak, 2004) <papid> P04-1005 </papid>uses tree-adjoininggrammar (tag) approach to define mapping between source sentence possibly containing repair, and target, fluent sentence.</citsent>
<aftsection>
<nextsent>the use of the tag channel model is justified by the putative crossing dependencies seen in repairs like . . .
</nextsent>
<nextsent>a flight to boston, uh, mean, to denver.
</nextsent>
<nextsent>where there is repetition from the reparandum to there pair.
</nextsent>
<nextsent>essentially, this model is building up the reparandum and alteration in tandem, based onthese crossing dependencies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2313">
<title id=" C08-1072.xml">a syntactic time series model for parsing fluent and dis fluent speech </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 right-corner transform.
</prevsent>
<prevsent>the right-corner transform rewrites syntax trees, turning all right branching structure into left branching structure, and leaving left branching structure as is. as result, constituent structure can be explicitly built from the bottom up during incremental recognition.
</prevsent>
</prevsection>
<citsent citstr=" P98-1101 ">
this arrangement is well-suited to recognition of speech with errors, because it allows constituent structure to be built up using fluent speech rules until the moment of interruption, at which point special repair rule may be applied.before transforming the trees in the grammar into right-corner trees, trees are binarized in the same manner as johnson (1998<papid> P98-1101 </papid>b) and klein and manning (2003).<papid> P03-1054 </papid>2 binarized trees are then transformed into right-corner trees using transform rules similar to those described by johnson (1998<papid> P98-1101 </papid>a).</citsent>
<aftsection>
<nextsent>in this transform, all right branching sequences in each tree are transformed into left branching sequences of symbols of the form 1 /a 2, denoting an incomplete instance of category 1 lacking an instance of category 2 to the right.3 rewrite rules for the right-corner transform are shown below, first flattening right-branching struc ture: 1 ? 1 2 ? 2 3 3 ? 1 1 /a 2 ? 1 2 /a 3 ? 2 3 3 1 ? 1 2 2 /a 3 ? 2 . . .
</nextsent>
<nextsent>a 1 1 /a 2 ? 1 2 /a 3 ? 2 . . .
</nextsent>
<nextsent>then replacing it with left-branching structure: 1 1 /a 2 :?
</nextsent>
<nextsent>1 2 /a 3 ? 2 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2317">
<title id=" C08-1072.xml">a syntactic time series model for parsing fluent and dis fluent speech </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 right-corner transform.
</prevsent>
<prevsent>the right-corner transform rewrites syntax trees, turning all right branching structure into left branching structure, and leaving left branching structure as is. as result, constituent structure can be explicitly built from the bottom up during incremental recognition.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
this arrangement is well-suited to recognition of speech with errors, because it allows constituent structure to be built up using fluent speech rules until the moment of interruption, at which point special repair rule may be applied.before transforming the trees in the grammar into right-corner trees, trees are binarized in the same manner as johnson (1998<papid> P98-1101 </papid>b) and klein and manning (2003).<papid> P03-1054 </papid>2 binarized trees are then transformed into right-corner trees using transform rules similar to those described by johnson (1998<papid> P98-1101 </papid>a).</citsent>
<aftsection>
<nextsent>in this transform, all right branching sequences in each tree are transformed into left branching sequences of symbols of the form 1 /a 2, denoting an incomplete instance of category 1 lacking an instance of category 2 to the right.3 rewrite rules for the right-corner transform are shown below, first flattening right-branching struc ture: 1 ? 1 2 ? 2 3 3 ? 1 1 /a 2 ? 1 2 /a 3 ? 2 3 3 1 ? 1 2 2 /a 3 ? 2 . . .
</nextsent>
<nextsent>a 1 1 /a 2 ? 1 2 /a 3 ? 2 . . .
</nextsent>
<nextsent>then replacing it with left-branching structure: 1 1 /a 2 :?
</nextsent>
<nextsent>1 2 /a 3 ? 2 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2334">
<title id=" C08-1004.xml">an improved hierarchical bayesian model of language for document classification </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several authors, among them (church and gale,1995; katz, 1996), observe empirically such models are not always accurate predictors of actual word behaviour.
</prevsent>
<prevsent>this moves them to suggest distributions for word counts where the underlying probability varies between documents; thus the expected behaviour of word in new document isa combination of predictions for all possible probabilities.
</prevsent>
</prevsection>
<citsent citstr=" P03-1037 ">
other authors (jansche, 2003; <papid> P03-1037 </papid>eyhera mendy et al, 2003; lowe, 1999) use these same ideas to classify documents on the basis of subsets of vocabulary, in the first and third cases with encouraging results using small subsets (in the second case, the performance of the model is shown to be poor compared to the multinomial).</citsent>
<aftsection>
<nextsent>when one moves to consider counts of all words in some vocabulary, the proper distribution of the whole vector of word counts is multinomial.
</nextsent>
<nextsent>(madsen et al, 2005) apply the same idea as forthe single word (binomial) case to the multinomial, using the most convenient form of distribution to represent the way the vector of multino mial probabilities varies between documents, and report encouraging results compared to the simple multinomial.
</nextsent>
<nextsent>however, we show that the use of the most mathematically convenient distribution to describe the way the vector of probabilities varies entails some unwarranted and undesirable assumptions.
</nextsent>
<nextsent>this paper will first describe those assumptions, and then describe an approximate technique for overcoming the assumptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2346">
<title id=" C08-1079.xml">exploring domain differences for the design of a pronoun resolution system for biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contrary to their work, in this work we made use of genia, large co-reference annotated corpus for the bio domain, containing 1999 medline abstracts..
</prevsent>
<prevsent>while there are quite few works on this task for the bio-medical domain, for other domains, and especially for the news domain, myriad of works on pronoun resolution has been carried out by the nlp researchers (mitkov, 2002).
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
since soon (soon et al, 2001) <papid> J01-4004 </papid>started the trend of using the machine learning approach by using binary classifier in pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (haghighi and klein, 2007).<papid> P07-1107 </papid></citsent>
<aftsection>
<nextsent>such methods were claimed to be comparable with traditional methods.
</nextsent>
<nextsent>however, the problems caused by domain differences, which strongly affect deep-semantics related task like pronoun resolution, have not yet been studied well enough.
</nextsent>
<nextsent>in order to recognize the important factors in 625building an effective machine learning-based pronoun resolution system, and in particular for thebio-domain, we have built machine learning based pronoun re solver and observed the contributions of different features in the pronoun resolution process.
</nextsent>
<nextsent>in our experiments for the news domain, we used the muc-7 and ace corpora, and for the biomedical domain, we employed the genia coreference corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2347">
<title id=" C08-1079.xml">exploring domain differences for the design of a pronoun resolution system for biomedical text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>contrary to their work, in this work we made use of genia, large co-reference annotated corpus for the bio domain, containing 1999 medline abstracts..
</prevsent>
<prevsent>while there are quite few works on this task for the bio-medical domain, for other domains, and especially for the news domain, myriad of works on pronoun resolution has been carried out by the nlp researchers (mitkov, 2002).
</prevsent>
</prevsection>
<citsent citstr=" P07-1107 ">
since soon (soon et al, 2001) <papid> J01-4004 </papid>started the trend of using the machine learning approach by using binary classifier in pairwise manner for solving co-reference resolution problem, many machine learning-based systems have been built, using both supervised and, unsupervised learning methods (haghighi and klein, 2007).<papid> P07-1107 </papid></citsent>
<aftsection>
<nextsent>such methods were claimed to be comparable with traditional methods.
</nextsent>
<nextsent>however, the problems caused by domain differences, which strongly affect deep-semantics related task like pronoun resolution, have not yet been studied well enough.
</nextsent>
<nextsent>in order to recognize the important factors in 625building an effective machine learning-based pronoun resolution system, and in particular for thebio-domain, we have built machine learning based pronoun re solver and observed the contributions of different features in the pronoun resolution process.
</nextsent>
<nextsent>in our experiments for the news domain, we used the muc-7 and ace corpora, and for the biomedical domain, we employed the genia coreference corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2348">
<title id=" C08-1079.xml">exploring domain differences for the design of a pronoun resolution system for biomedical text </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>this will be shown in our experiments, and analysis of the experimental results will be given in the following section.
</prevsent>
<prevsent>3.1 pronoun resolution model.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we built machine learning based pronoun resolution engine using maximum entropy ranker model (berger et al, 1996), <papid> J96-1002 </papid>similar with denis and bald ridges model (denis and baldridge, 2007).</citsent>
<aftsection>
<nextsent>for every anaphoric pronoun ?, the ranker selects the most likely antecedent candidate ?, from set of candidate markables.
</nextsent>
<nextsent>p (?
</nextsent>
<nextsent>j |?)
</nextsent>
<nextsent>= exp ( ? i=1 ? f (?, ? )) ? exp ( ? i=1 ? f (?, ? )) (1)we constructed the training examples in the following way: for each gold anaphora link in the training corpus, we created positive instance, and negative training instances are created by pairing the pronoun with all of the other markables appearing in window of preceding sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2350">
<title id=" C08-1079.xml">exploring domain differences for the design of a pronoun resolution system for biomedical text </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we further conducted small test by excluding this combination from the netype feature group, but the success rate remained unchanged from the baselineresult.
</prevsent>
<prevsent>this signifies that this combination contributed the most to the above increase.the combination of netype and semw features exploits the co-ocurrence of the semantic type of the candidate antecedent and the context word, which appears in some relationship with thepronoun.
</prevsent>
</prevsection>
<citsent citstr=" P06-1005 ">
this combination feature uses the information similar to the semantic compatibility features proposed by yang (yang et al, 2005) and bergsma (bergsma and lin, 2006).<papid> P06-1005 </papid></citsent>
<aftsection>
<nextsent>depending on the pronoun type, the feature extractor decides which relationship is used.
</nextsent>
<nextsent>for example, there solver successfully recognizes the antecedent ofthe pronoun its in this discourse: hsf3 is con stitutively expressed in the erythroblast cell linehd6 , the lymph oblast cell line msb , and embryo fibroblasts , and yet its dna-binding activity is induced only upon exposure of hd6 cells toheat shock ,?
</nextsent>
<nextsent>because hsf3 was detected as protein entity, which has strong association with the governing head noun activity of the pronoun.
</nextsent>
<nextsent>another example is the correct anaphora link between it?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2351">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" N06-1020 ">
self-training has been shown capable of improving on state-of-the-art parser performance (mcclosky et al, 2006) <papid> N06-1020 </papid>despite the conventional wisdom on the matter and several studies to the contrary (charniak, 1997; steedman et al, 2003).</citsent>
<aftsection>
<nextsent>however, ithas remained unclear when and why self training is helpful.
</nextsent>
<nextsent>in this paper, we test four hypotheses (namely, presence of phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words).
</nextsent>
<nextsent>from these experiments, we gain better understanding of why self-training works forparsing.
</nextsent>
<nextsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2352">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these experiments, we gain better understanding of why self-training works forparsing.
</prevsent>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
</prevsection>
<citsent citstr=" E03-1005 ">
supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></citsent>
<aftsection>
<nextsent>these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2353">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these experiments, we gain better understanding of why self-training works forparsing.
</prevsent>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></citsent>
<aftsection>
<nextsent>these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2354">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these experiments, we gain better understanding of why self-training works forparsing.
</prevsent>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></citsent>
<aftsection>
<nextsent>these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2355">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these experiments, we gain better understanding of why self-training works forparsing.
</prevsent>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
</prevsection>
<citsent citstr=" P06-1055 ">
supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></citsent>
<aftsection>
<nextsent>these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2356">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>from these experiments, we gain better understanding of why self-training works forparsing.
</prevsent>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
</prevsection>
<citsent citstr=" P07-1080 ">
supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></citsent>
<aftsection>
<nextsent>these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2357">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since improvements from self training are correlated with unknown bi grams and biheads but not unknown words,the benefit of self-training appears most influenced by seeing known words in new combinations.
</prevsent>
<prevsent>supervised statistical parsers attempt to capture patterns of syntactic structure from labeled set of examples for the purpose of annotating new sentences with their structure (bod, 2003; <papid> E03-1005 </papid>charniak and johnson, 2005; <papid> P05-1022 </papid>collins and koo, 2005; <papid> J05-1003 </papid>petrov et al, 2006; <papid> P06-1055 </papid>titov and henderson, 2007).<papid> P07-1080 </papid></prevsent>
</prevsection>
<citsent citstr=" N07-1070 ">
these annotations can be used by various higher-level applications such as semantic role labeling (pradhan et al, 2007) <papid> N07-1070 </papid>and machine translation (yamada and knight, 2001).</citsent>
<aftsection>
<nextsent>c ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>however, labeled training data is expensive to annotate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2363">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>we present and test four hypotheses of why self-training helps in section 4 and conclude with discussion and future work in section 5.
</prevsent>
<prevsent>to our knowledge, the first reported uses of self training for parsing are by charniak (1997).
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
he used his parser trained on the wall street journal(wsj, mitch marcus et al (1993)) <papid> J93-2004 </papid>to parse 30 million words of un parsed wsj text.</citsent>
<aftsection>
<nextsent>he then trained 561 self-trained model from the combination of the newly parsed text with wsj training data.
</nextsent>
<nextsent>how ever, the self-trained model did not improve on the original model.
</nextsent>
<nextsent>self-training and co-training were subsequently investigated in the 2002 clsp summer work shop at johns hopkins university (steedman etal., 2003).
</nextsent>
<nextsent>they considered several different parameter settings, but in all cases, the number of sentences parsed per iteration of self-training was relatively small (30 sentences).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2364">
<title id=" C08-1071.xml">when is self training effective for parsing </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the largest seed size (amount of labeled training data) they usedwas 10,000 sentences from wsj, though many experiments used only 500 or 1,000 sentences.
</prevsent>
<prevsent>they found that under these parameters, self-training did not yield significant gain.
</prevsent>
</prevsection>
<citsent citstr=" P07-1078 ">
reichart and rappoport (2007) <papid> P07-1078 </papid>showed that one can self-train with only generative parser if the seed size is small.</citsent>
<aftsection>
<nextsent>the conditions are similar to steedman et al (2003), but only one iteration of self-training is performed (i.e. all unlabeled data is labeled at once).1 in this scenario, unknown words(words seen in the unlabeled data but not in train ing) were useful predictor of when self-training improves performance.
</nextsent>
<nextsent>mcclosky et al (2006) <papid> N06-1020 </papid>showed that self-training improves parsing accuracy when the two-stage charniak and johnson (2005) <papid> P05-1022 </papid>reranking parser is used.</nextsent>
<nextsent>using both stages (a generative parser and discriminative reranker) to label the unlabeled dataset is necessary to improve performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2411">
<title id=" C08-1063.xml">understanding and summarizing answers in community based question answering services </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>broder (2002) proposed that search queries can be classified into three categories, i.e. navigational, informational, and transactional.
</prevsent>
<prevsent>ross and levinson (2004) suggested more elaborated taxonomy with five more subcategories for informational queries and four more subcategories for resource (transactional) queries.
</prevsent>
</prevsection>
<citsent citstr=" H01-1069 ">
in open-domain question answering research that automatic systems are required to extract exact answers from text database given set of factoid questions (voorhees and m. ellen, 2003), all top performing systems had incorporated question taxonomies (hovy et al, 2001; <papid> H01-1069 </papid>moldovan et al, 2000; <papid> P00-1071 </papid>lytinen et al, 2002; jijkoun et al, 2005).</citsent>
<aftsection>
<nextsent>based on the past experiences from the annual nist trec question and answering track 4 (trec qa track), an international forum dedicating to evaluate and compare different open-domain question answering systems, we conjecture that cqa question taxonomy would help us determine what type of best answer is expected given question type.
</nextsent>
<nextsent>automatic summarization of cqa answers is one of the main focuses of this paper.
</nextsent>
<nextsent>we propose that summarization techniques (hovy and lin, 1999; lin and hovy, 2002) <papid> P02-1058 </papid>can be used to create cqa answer summaries for different question types.</nextsent>
<nextsent>creating an answer summary given question and its answers can be seen as multi document summarization task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2412">
<title id=" C08-1063.xml">understanding and summarizing answers in community based question answering services </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>broder (2002) proposed that search queries can be classified into three categories, i.e. navigational, informational, and transactional.
</prevsent>
<prevsent>ross and levinson (2004) suggested more elaborated taxonomy with five more subcategories for informational queries and four more subcategories for resource (transactional) queries.
</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
in open-domain question answering research that automatic systems are required to extract exact answers from text database given set of factoid questions (voorhees and m. ellen, 2003), all top performing systems had incorporated question taxonomies (hovy et al, 2001; <papid> H01-1069 </papid>moldovan et al, 2000; <papid> P00-1071 </papid>lytinen et al, 2002; jijkoun et al, 2005).</citsent>
<aftsection>
<nextsent>based on the past experiences from the annual nist trec question and answering track 4 (trec qa track), an international forum dedicating to evaluate and compare different open-domain question answering systems, we conjecture that cqa question taxonomy would help us determine what type of best answer is expected given question type.
</nextsent>
<nextsent>automatic summarization of cqa answers is one of the main focuses of this paper.
</nextsent>
<nextsent>we propose that summarization techniques (hovy and lin, 1999; lin and hovy, 2002) <papid> P02-1058 </papid>can be used to create cqa answer summaries for different question types.</nextsent>
<nextsent>creating an answer summary given question and its answers can be seen as multi document summarization task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2413">
<title id=" C08-1063.xml">understanding and summarizing answers in community based question answering services </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>based on the past experiences from the annual nist trec question and answering track 4 (trec qa track), an international forum dedicating to evaluate and compare different open-domain question answering systems, we conjecture that cqa question taxonomy would help us determine what type of best answer is expected given question type.
</prevsent>
<prevsent>automatic summarization of cqa answers is one of the main focuses of this paper.
</prevsent>
</prevsection>
<citsent citstr=" P02-1058 ">
we propose that summarization techniques (hovy and lin, 1999; lin and hovy, 2002) <papid> P02-1058 </papid>can be used to create cqa answer summaries for different question types.</citsent>
<aftsection>
<nextsent>creating an answer summary given question and its answers can be seen as multi document summarization task.
</nextsent>
<nextsent>we simply replace documents with answers and apply these techniques to generate the answer summary.
</nextsent>
<nextsent>the task has been one of the main tasks the document understanding conference5 since 2004.
</nextsent>
<nextsent>to study how to exploit the best answers of cqa, we need to first analyze cqa answers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2415">
<title id=" C08-1063.xml">understanding and summarizing answers in community based question answering services </title>
<section> question-type oriented answer.  </section>
<citcontext>
<prevsection>
<prevsent>compared with manually-summarized answer (msa), asa contains most information of msa while retains similar length with ba and msa.
</prevsent>
<prevsent>5.2 opinion questions.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
algorithm: for opinion questions, comprehensive investigation of this topic would be beyond the scope of this paper since this is still field 6 http://opennlp.sourceforge.net 501 under active development (wiebe et al, 2003; kim and hovy, 2004).<papid> C04-1200 </papid></citsent>
<aftsection>
<nextsent>we build simple yet novel opinion-focused answer summarizer which provides global view of all answers.
</nextsent>
<nextsent>we divide opinion questions into two subcategories.
</nextsent>
<nextsent>one is sentiment-oriented question that asks the sentiment about something, for example, what do you think of ??.
</nextsent>
<nextsent>the other is list-oriented question that intends to get list of answers and see what item is the most popular.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2416">
<title id=" C02-1024.xml">interleaved semantic interpretation in environment based parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P01-1061 ">
this paper extends polynomial-time parsing algorithm that resolves structural ambiguity in input sentences by calculating and comparing the deno tations of rival constituents, given some model of the application environment (schuler, 2001).<papid> P01-1061 </papid></citsent>
<aftsection>
<nextsent>the algorithm is extended to incorporate full set of logical operators, including quanti ers and conjunctions, into this calculation without increasing the complexity of the overall algorithm beyond polynomial time, both in terms of the length of the in put and the number of entities in the environment model.
</nextsent>
<nextsent>the development of speaker-independent mixed initiative speech interfaces, in which users not only answer questions but also ask questions and give instructions, is currently limited by the inadequacy of existing corpus-based disambiguation techniques.this paper explores the use of semantic and pragmatic information, in the form of the entities and relations in the interfaced application run-time environment, as an additional source of information to guide disambiguation.in particular, this paper extends an existing parsing algorithm that calculates and compares the de notations of rival parse tree constituents in order to resolve structural ambiguity in input sentences(schuler, 2001).<papid> P01-1061 </papid></nextsent>
<nextsent>the algorithm is extended to incorporate full set of logical operators into this calculation so as to improve the accuracy of the resulting denot ations { and thereby improve the accuracy of parsing { without increasing the complexity of the overall algorithm beyond polynomial time (both in terms of the length of the input and the number of entities in the environment model).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2423">
<title id=" C02-1024.xml">interleaved semantic interpretation in environment based parsing </title>
<section> basic algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>2since the indices in every rule antecedent constituents 1 : : : k each cover smaller spans than those in the consequent x, the algorithm will not enter into an in nite recursion; and since there are only 2jn dierent values of x, and only 2n dier ent rules that could prove any consequent (two rule forms for = and n, each with dierent values of k), the algorithm runs in polynomial time: o(n 3 jn j).
</prevsent>
<prevsent>the resulting chart can then be annotated with back pointers to produce polynomial-sized shared forest 1 following shieber et al (1995).
</prevsent>
</prevsection>
<citsent citstr=" J99-4004 ">
2 following goodman (1999).<papid> J99-4004 </papid>representation of all possible grammatical trees (bil lot and lang, 1989).</citsent>
<aftsection>
<nextsent>traditional corpus-based parsers select preferred trees from such forests by calculating viterbi scores for each proposed constituent, according to there cursive function: v (x) = max 1 ::: k s:t: 1 ::: k k i=1 v (a ) !
</nextsent>
<nextsent>p(a 1 ::: k x) these scores can be calculated in polynomial time, using the same dynamic programming algorithm asthat described for parsing.
</nextsent>
<nextsent>a tree can then be selected, from the top down, by expanding the highest scoring rule application for each constituent.
</nextsent>
<nextsent>the environment-based parser described here uses similar mechanism to select preferred trees, but the scores are based on the presence or absence of entities in the denot ation (interpretation) of each proposed constituent: 3 d (x) = max 1 ::: k s:t: 1 ::: k k i=1 d (a ) ! + ( 1 if d(x) 6=; 0 otherwise where the denot ation d(x) of proposed constituent is calculated using another recursive function: d(x) = [ 1 ::: k s:t: 1 ::: k  on i=1 d(a ) ! on ( r(x) if = 0 fhig otherwise in which r(x) is lexical relation de ned for each axiom of category equal to some subset of  worst-case denot ation ( ), as de ned above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2424">
<title id=" C02-1024.xml">interleaved semantic interpretation in environment based parsing </title>
<section> basic algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>a tree can then be selected, from the top down, by expanding the highest scoring rule application for each constituent.
</prevsent>
<prevsent>the environment-based parser described here uses similar mechanism to select preferred trees, but the scores are based on the presence or absence of entities in the denot ation (interpretation) of each proposed constituent: 3 d (x) = max 1 ::: k s:t: 1 ::: k k i=1 d (a ) ! + ( 1 if d(x) 6=; 0 otherwise where the denot ation d(x) of proposed constituent is calculated using another recursive function: d(x) = [ 1 ::: k s:t: 1 ::: k  on i=1 d(a ) ! on ( r(x) if = 0 fhig otherwise in which r(x) is lexical relation de ned for each axiom of category equal to some subset of  worst-case denot ation ( ), as de ned above.
</prevsent>
</prevsection>
<citsent citstr=" P94-1016 ">
4 the operator on is natural (relational) join on the elds of its operands: aonb = fhe 1 :::e max(a;b) j he 1 :::e i2a; he 1 :::e i2bg where a;  0; and  is projection that removes the rst element of the result (corresponding the most recently discharged argument of the head or functor category): a = fhe 2 :::e i he 1 :::e i2agthis inter leaving of semantic evaluation and parsing for the purpose of disambiguation has much in common with that of dowding et al (1994), <papid> P94-1016 </papid>except 3here, the score is simply equal to the number of non empty constituents in an analysis, but other metrics are pos sible.</citsent>
<aftsection>
<nextsent>4 so lexical relation for the constituent `lemon  of category np would contain all and only the lemons inthe environment, and lexical relation for the constituent `falling  of category snnp would contain mapping from every entity in the environment to some truth value (true if that entity is falling, false otherwise): e.g. fhlemon 1 ; truei; hlemon 2 ; falsei; : : : g. np[lemon] fl 1 ; 2 ; 3 ; 4 p:npnnp/np[in] fhb 1 ; hl 1 ; 1 ii; hm 1 ; hl 2 ; 2 iig np[bin] fb 1 ; 2 p:npnnp/np[by] fhm 1 ; hb 1 ; 1 ii; hm 2 ; hb 2 ; 2 iig np[machine] fm 1 ;m 2 ;m 3 pp:npnnp[in] fhl 1 ; 1 ig pp:npnnp[by] fhb 1 ; 1 i; hb 2 ; 2 ig np[lemon] fl 1 np[bin] fb 1 ; 2 pp:npnnp[in] fhl 1 ; 1 ig np[lemon] fl 1 [ ; figure 1: denotation-annotated forest for `lemon in bin by machine. that in this case, constituents are not only semantically type-checked, but are also fully interpreted each time they are proposed.
</nextsent>
<nextsent>figure 1 shows sample denotation-annotated forest for the phrase `the lemon in the bin by the machine , using the lexicalized grammar: lemon, bin, machine : np the : np=np in, by : npnnp=np in which the denot ation of each constituent (the set in each rectangle) is calculated using join on the denot ations of each pair of constituents that combine to produce it.
</nextsent>
<nextsent>in this example, the right-branchingtree would be preferred because the denot ation resulting from the composition at the root of the other tree would be empty.
</nextsent>
<nextsent>since this use of the join operation is linear on the sum of the cardinal ities of its ope rands, and since the denot ations of the categories in grammar are bounded in cardinality by o(jej ) where is the maximum valency of the categories in g, the total complexity of the above algorithm can be shown to be o(n 3 jej ): polynomial not only on the length of the input n, but also on the size of the environment (schuler, 2001).<papid> P01-1061 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2428">
<title id=" C02-1024.xml">interleaved semantic interpretation in environment based parsing </title>
<section> extended algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>np =np  x=np q
</prevsent>
<prevsent>np nnp q
</prevsent>
</prevsection>
<citsent citstr=" J83-2002 ">
np =np  the lexical entry for quanti ercan be split in this 7 dahl and mccord (1983) <papid> J83-2002 </papid>propose similar duplication mechanism to produce appropriate semantic representations for np and other conjunctions, but for dierent reasons.</citsent>
<aftsection>
<nextsent>8 e.g. for the word `and : fh:::true; :::true; :::truei; h::true; ::false; ::falsei; h::false; ::true; ::falsei; h::false; ::false; ::falseigway into number of components, the last (or low est) of which is not duplicated in conjunction while others may or may not be.
</nextsent>
<nextsent>these include component for the quanti er np =np (which will ultimately also contain noun phrase restrictor of category np ), component for restrictor pps and relative clauses of category np nnp that are attached above the quanti er and duplicated in the conjunction, and component for the body (a verb or verb phrase or other predicate) of category xnnp or x=np . the sub script species one of nite.
</nextsent>
<nextsent>set of quanti ers, and the sub script  indicates an unquanti ed np.
</nextsent>
<nextsent>the deductive parser presented in section 2 cannow be extended by incorporating sequences of recognized and unrecognized components into the constituent chart items.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2429">
<title id=" C08-1059.xml">stopping criteria for active learning of named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>schutze et al (2006) studied lewis-based performance estimation method in binary text classification setting.
</prevsent>
<prevsent>they attribute difficulties in estimating recall to missed cluster effect?, meaning that the active sampling procedure is failing to select some clusters of relevant training examples inthe pool that are too dissimilar to the relevant examples already known.
</prevsent>
</prevsection>
<citsent citstr=" P04-1075 ">
diversity measures as proposed by (shen et al, 2004) <papid> P04-1075 </papid>might help in mitigating this effect, but our experiments show that there are fundamental differences between text classification and ner.</citsent>
<aftsection>
<nextsent>since missed clusters of relevant examples in the training data would eventually be used as we exhaustively label the entire pool, we should see improvements in recall when the missed clusters get used.
</nextsent>
<nextsent>instead, we observed in section2.1, that there are no further performance gains after certain portion of the pool is labeled.
</nextsent>
<nextsent>thus, all examples that the classifier can make use of must have been taken into account, and there appear to be no missed clusters.tomanek et al (2007) <papid> D07-1051 </papid>present stopping criterion for query-by-committee-based al that is based on the rate of disagreement of the classifier sin the committee.</nextsent>
<nextsent>while our uncertainty convergence criterion can only be applied to uncertainty sampling, the performance convergence criterion can be used in committee-based setting.li and sethi (2006) estimate the conditional error as measure of uncertainty in selection (insteadof using it for stopping as we do), using variable bin histogram for improving the error estimates.they do not evaluate the quality of the probability estimates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2430">
<title id=" C08-1059.xml">stopping criteria for active learning of named entity recognition </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since missed clusters of relevant examples in the training data would eventually be used as we exhaustively label the entire pool, we should see improvements in recall when the missed clusters get used.
</prevsent>
<prevsent>instead, we observed in section2.1, that there are no further performance gains after certain portion of the pool is labeled.
</prevsent>
</prevsection>
<citsent citstr=" D07-1051 ">
thus, all examples that the classifier can make use of must have been taken into account, and there appear to be no missed clusters.tomanek et al (2007) <papid> D07-1051 </papid>present stopping criterion for query-by-committee-based al that is based on the rate of disagreement of the classifier sin the committee.</citsent>
<aftsection>
<nextsent>while our uncertainty convergence criterion can only be applied to uncertainty sampling, the performance convergence criterion can be used in committee-based setting.li and sethi (2006) estimate the conditional error as measure of uncertainty in selection (insteadof using it for stopping as we do), using variable bin histogram for improving the error estimates.they do not evaluate the quality of the probability estimates.
</nextsent>
<nextsent>as with our stopping criterion, we expect this selection criterion to be the more effective the more accurate the probability estimatesare.
</nextsent>
<nextsent>we therefore believe that our method of improving probability estimates based on loo bins could improve their selection criterion.
</nextsent>
<nextsent>471 stop crit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2431">
<title id=" C04-1112.xml">a lemma based approach to a maximum entropy word sense disambiguation system for dutch </title>
<section> introduction: wsd for dutch.  </section>
<citcontext>
<prevsection>
<prevsent>(hendrickx et al, 2002; hoste et al, 2002).
</prevsent>
<prevsent>in the system presented here, the classifiers built foreach ambiguous word are based on its lemma instead.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
lemmatization allows for more compact and generalizable data by clustering all inflected forms of an ambiguous word together, an effect already commented on by yarowsky (1994).<papid> P94-1013 </papid></citsent>
<aftsection>
<nextsent>the more inflection in language, the more lemmatization will help to compress and generalize the data.
</nextsent>
<nextsent>in the case of our wsd system this means that less classifier shave to be built therefore adding up the training material available to the algorithm for each ambiguous wordform.
</nextsent>
<nextsent>accuracy is expected to increase for the lemma-based model in comparison to the word form model.
</nextsent>
<nextsent>the paper is structured as follows: first, we will present the dictionary-based lemmatizer for dutch which was used to lemmatize the data, followed by detailed explanation of the lemma-based approach adopted in our wsd system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2433">
<title id=" C04-1112.xml">a lemma based approach to a maximum entropy word sense disambiguation system for dutch </title>
<section> maximum entropy word sense.  </section>
<citcontext>
<prevsection>
<prevsent>several problems in nlp have lent themselves to solutions using statistical language processing techniques.
</prevsent>
<prevsent>many of these problems can be viewed as classification task in which linguistic classes have to be predicted given context.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the statistical classifier used in the experiments reported in this paper is maximum entropy classifier (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1997<papid> W97-0301 </papid>b).</citsent>
<aftsection>
<nextsent>maximum entropy is general technique for estimating probability distributions from data.
</nextsent>
<nextsent>a probability distribution is derived from set of events based on the computable qualities (characteristics) of these events.
</nextsent>
<nextsent>the characteristics are called features, and the events are sets of feature values.
</nextsent>
<nextsent>if nothing about the data is known, estimating aprobability distribution using the principle of maximum entropy involves selecting the most uniform distribution where all events have equal probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2435">
<title id=" C04-1112.xml">a lemma based approach to a maximum entropy word sense disambiguation system for dutch </title>
<section> maximum entropy word sense.  </section>
<citcontext>
<prevsection>
<prevsent>several problems in nlp have lent themselves to solutions using statistical language processing techniques.
</prevsent>
<prevsent>many of these problems can be viewed as classification task in which linguistic classes have to be predicted given context.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
the statistical classifier used in the experiments reported in this paper is maximum entropy classifier (berger et al, 1996; <papid> J96-1002 </papid>ratnaparkhi, 1997<papid> W97-0301 </papid>b).</citsent>
<aftsection>
<nextsent>maximum entropy is general technique for estimating probability distributions from data.
</nextsent>
<nextsent>a probability distribution is derived from set of events based on the computable qualities (characteristics) of these events.
</nextsent>
<nextsent>the characteristics are called features, and the events are sets of feature values.
</nextsent>
<nextsent>if nothing about the data is known, estimating aprobability distribution using the principle of maximum entropy involves selecting the most uniform distribution where all events have equal probability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2441">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we successfully integrate the maxent-based rule selection models into the state-of-the-art syntax-based smtmodel.
</prevsent>
<prevsent>experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art smt system.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
the syntax-based statistical machine translation (smt) models (chiang, 2005; <papid> P05-1033 </papid>liu et al, 2006; <papid> P06-1077 </papid>galley et al, 2006; <papid> P06-1121 </papid>huang et al, 2006) <papid> W06-3601 </papid>use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings.generally, translation rule consists of left-hand side (lhs) 1and right-hand-side (rhs).</citsent>
<aftsection>
<nextsent>thelhs and rhs can be words, phrases, or even syntactic trees, depending on smt models.
</nextsent>
<nextsent>translation rules can be learned automatically from parallel corpus.
</nextsent>
<nextsent>usually, an lhs may correspond to multiple rhss in multiple rules.
</nextsent>
<nextsent>therefore, in statistical machine translation, the rule selection taskis to select the correct rhs for an lhs during decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2443">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we successfully integrate the maxent-based rule selection models into the state-of-the-art syntax-based smtmodel.
</prevsent>
<prevsent>experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art smt system.
</prevsent>
</prevsection>
<citsent citstr=" P06-1077 ">
the syntax-based statistical machine translation (smt) models (chiang, 2005; <papid> P05-1033 </papid>liu et al, 2006; <papid> P06-1077 </papid>galley et al, 2006; <papid> P06-1121 </papid>huang et al, 2006) <papid> W06-3601 </papid>use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings.generally, translation rule consists of left-hand side (lhs) 1and right-hand-side (rhs).</citsent>
<aftsection>
<nextsent>thelhs and rhs can be words, phrases, or even syntactic trees, depending on smt models.
</nextsent>
<nextsent>translation rules can be learned automatically from parallel corpus.
</nextsent>
<nextsent>usually, an lhs may correspond to multiple rhss in multiple rules.
</nextsent>
<nextsent>therefore, in statistical machine translation, the rule selection taskis to select the correct rhs for an lhs during decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2445">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we successfully integrate the maxent-based rule selection models into the state-of-the-art syntax-based smtmodel.
</prevsent>
<prevsent>experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art smt system.
</prevsent>
</prevsection>
<citsent citstr=" P06-1121 ">
the syntax-based statistical machine translation (smt) models (chiang, 2005; <papid> P05-1033 </papid>liu et al, 2006; <papid> P06-1077 </papid>galley et al, 2006; <papid> P06-1121 </papid>huang et al, 2006) <papid> W06-3601 </papid>use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings.generally, translation rule consists of left-hand side (lhs) 1and right-hand-side (rhs).</citsent>
<aftsection>
<nextsent>thelhs and rhs can be words, phrases, or even syntactic trees, depending on smt models.
</nextsent>
<nextsent>translation rules can be learned automatically from parallel corpus.
</nextsent>
<nextsent>usually, an lhs may correspond to multiple rhss in multiple rules.
</nextsent>
<nextsent>therefore, in statistical machine translation, the rule selection taskis to select the correct rhs for an lhs during decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2446">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we successfully integrate the maxent-based rule selection models into the state-of-the-art syntax-based smtmodel.
</prevsent>
<prevsent>experiments show that our lexicalized approach for rule selection achieves statistically significant improvements over the state-of-the-art smt system.
</prevsent>
</prevsection>
<citsent citstr=" W06-3601 ">
the syntax-based statistical machine translation (smt) models (chiang, 2005; <papid> P05-1033 </papid>liu et al, 2006; <papid> P06-1077 </papid>galley et al, 2006; <papid> P06-1121 </papid>huang et al, 2006) <papid> W06-3601 </papid>use rules with hierarchical structures as translation knowledge, which can capture long-distance reorderings.generally, translation rule consists of left-hand side (lhs) 1and right-hand-side (rhs).</citsent>
<aftsection>
<nextsent>thelhs and rhs can be words, phrases, or even syntactic trees, depending on smt models.
</nextsent>
<nextsent>translation rules can be learned automatically from parallel corpus.
</nextsent>
<nextsent>usually, an lhs may correspond to multiple rhss in multiple rules.
</nextsent>
<nextsent>therefore, in statistical machine translation, the rule selection taskis to select the correct rhs for an lhs during decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2456">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation systems usually face the selection problem because of the one-tomany correspondence between the source and target language.
</prevsent>
<prevsent>recent researches showed that rich context information can help smt systems perform selection and improves translation quality.
</prevsent>
</prevsection>
<citsent citstr=" P06-1066 ">
the discriminative phrasal reordering models(xiong et al, 2006; <papid> P06-1066 </papid>zens and ney, 2006) <papid> W06-3108 </papid>provided lexicalized method for phrase reordering.in these models, lhs and rhs can be considered as phrases and reordering types, respectively.therefore the selection task is to select reordering type for phrases.</citsent>
<aftsection>
<nextsent>they use maxent model to combine context features and distinguished two kinds of reorderings between two adjacent phrases: monotone or swap.
</nextsent>
<nextsent>however, our method is more generic, we perform lexicalized rule selection for syntax-based smt models.
</nextsent>
<nextsent>in these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases.
</nextsent>
<nextsent>furthermore,the rule selection can be considered as multiclass classification task, while the phrase reordering between two adjacent phrases is two-class classification task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2457">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>statistical machine translation systems usually face the selection problem because of the one-tomany correspondence between the source and target language.
</prevsent>
<prevsent>recent researches showed that rich context information can help smt systems perform selection and improves translation quality.
</prevsent>
</prevsection>
<citsent citstr=" W06-3108 ">
the discriminative phrasal reordering models(xiong et al, 2006; <papid> P06-1066 </papid>zens and ney, 2006) <papid> W06-3108 </papid>provided lexicalized method for phrase reordering.in these models, lhs and rhs can be considered as phrases and reordering types, respectively.therefore the selection task is to select reordering type for phrases.</citsent>
<aftsection>
<nextsent>they use maxent model to combine context features and distinguished two kinds of reorderings between two adjacent phrases: monotone or swap.
</nextsent>
<nextsent>however, our method is more generic, we perform lexicalized rule selection for syntax-based smt models.
</nextsent>
<nextsent>in these models, the rules with hierarchical structures can handle reorderings of non-adjacent phrases.
</nextsent>
<nextsent>furthermore,the rule selection can be considered as multiclass classification task, while the phrase reordering between two adjacent phrases is two-class classification task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2458">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore,the rule selection can be considered as multiclass classification task, while the phrase reordering between two adjacent phrases is two-class classification task.
</prevsent>
<prevsent>recently, word sense disambiguation (wsd)techniques improved the performance of smt systems by helping the decoder perform lexical selection.
</prevsent>
</prevsection>
<citsent citstr=" D07-1007 ">
carpuat and wu (2007<papid> D07-1007 </papid>b) integrated wsd system into phrase-based smt system, pharaoh (koehn, 2004<papid> W04-3250 </papid>a).</citsent>
<aftsection>
<nextsent>furthermore, they extended wsd to phrase sense disambiguation (psd) (carpuat and wu, 2007<papid> D07-1007 </papid>a).</nextsent>
<nextsent>either the wsd or psd system combines rich context information to solve the ambiguity problem for words or phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2459">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore,the rule selection can be considered as multiclass classification task, while the phrase reordering between two adjacent phrases is two-class classification task.
</prevsent>
<prevsent>recently, word sense disambiguation (wsd)techniques improved the performance of smt systems by helping the decoder perform lexical selection.
</prevsent>
</prevsection>
<citsent citstr=" W04-3250 ">
carpuat and wu (2007<papid> D07-1007 </papid>b) integrated wsd system into phrase-based smt system, pharaoh (koehn, 2004<papid> W04-3250 </papid>a).</citsent>
<aftsection>
<nextsent>furthermore, they extended wsd to phrase sense disambiguation (psd) (carpuat and wu, 2007<papid> D07-1007 </papid>a).</nextsent>
<nextsent>either the wsd or psd system combines rich context information to solve the ambiguity problem for words or phrases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2462">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, they incorporated wsd or psd system into phrase-based smt system with weak distortion model for phrase reordering.
</prevsent>
<prevsent>while we incorporate maxent rs models into the state-of-the-art syntax-based smt system,which captures phrase reordering by using hierarchical model.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
322 chan et al (2007) <papid> P07-1005 </papid>incorporated wsd system into the hierarchical smt system, hiero (chi ang, 2005), <papid> P05-1033 </papid>and reported statistically significant improvement.</citsent>
<aftsection>
<nextsent>but they only focused on solving ambiguity for terminals of translation rules, and limited the length of terminals up to 2.
</nextsent>
<nextsent>different from their work, we consider translation rule as awhole, which contains both terminals and nonterminals.
</nextsent>
<nextsent>moreover, they explored features for the wsd system only on the source-side.
</nextsent>
<nextsent>while we define context features for the maxent rs models on both the source-side and target-side.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2466">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>rule (6) contains both terminals and nonterminals, which causes reordering ofphrases.
</prevsent>
<prevsent>the hierarchical model uses the maximum likelihood method to estimate translation probabilities for phrase pair ??, ??, independent of any other context information.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
to perform translation, chiang uses log-linear model (och and ney, 2002) <papid> P02-1038 </papid>to combine various features.</citsent>
<aftsection>
<nextsent>the weight of derivation is computed by: w(d) = ? ? (d) ? i(7) where ? (d) is feature function and ? iis the feature weight of ? i(d).
</nextsent>
<nextsent>during decoding, the decoder searches the best derivation with the lowest cost by applying scfg rules.
</nextsent>
<nextsent>however, the rule selections are independent of context information, except the left neighboring ? 1 target words for computing n-gram language model.
</nextsent>
<nextsent>the rule selection task can be considered as multi-class classification task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2467">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> lexicalized rule selection.  </section>
<citcontext>
<prevsection>
<prevsent>the rule selection task can be considered as multi-class classification task.
</prevsent>
<prevsent>for source-side,each corresponding target-side is label.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the maximum entropy approach (berger et al, 1996) <papid> J96-1002 </papid>is known to be well suited to solve the classification problem.</citsent>
<aftsection>
<nextsent>therefore, we build maximum entropy based rule selection (maxent rs) model for each ambiguous hierarchical lhs.
</nextsent>
<nextsent>in this section, wewill describe how to build the maxent rs models and how to integrate them into the hierarchical smt model.
</nextsent>
<nextsent>3.1 the maxent rs model.
</nextsent>
<nextsent>following (chiang, 2005), <papid> P05-1033 </papid>we use ??, ??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2478">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> lexicalized rule selection.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, the weights of the new features can be trained together with other features of the translation model.
</prevsent>
<prevsent>chiang (2007) uses the cky algorithm with cube pruning method for decoding.
</prevsent>
</prevsection>
<citsent citstr=" W05-1506 ">
this method can significantly reduce the search space by efficiently computing the top-n items rather than all possible items at node, using the k-best algorithms of huang and chiang (2005) <papid> W05-1506 </papid>to speed up the computation.</citsent>
<aftsection>
<nextsent>in cube pruning, the translation model is treated as the monotonic backbone of the search space, while the language model score is non-monotonic cost that distorts the search space (see (huang and chiang, 2005) <papid> W05-1506 </papid>for definition of monotonicity).</nextsent>
<nextsent>similarly, in the maxent rs model, source-side features form monotonic score while target-side features constitute non monotonic cost that can be seen as part of the language model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2484">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>nist-02 test set is used as the development set and nist-03 test set is used as the test set.
</prevsent>
<prevsent>4.2 training.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
to train the translation model, we first run giza++ (och and ney, 2000) <papid> P00-1056 </papid>to obtain word alignment in both translation directions.</citsent>
<aftsection>
<nextsent>then theword alignment is refined by performing grow diag-final?
</nextsent>
<nextsent>method (koehn et al, 2003).<papid> N03-1017 </papid></nextsent>
<nextsent>we use the same method suggested in (chiang, 2005) <papid> P05-1033 </papid>to extract scfg grammar rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2485">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>to train the translation model, we first run giza++ (och and ney, 2000) <papid> P00-1056 </papid>to obtain word alignment in both translation directions.</prevsent>
<prevsent>then theword alignment is refined by performing grow diag-final?</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
method (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>we use the same method suggested in (chiang, 2005) <papid> P05-1033 </papid>to extract scfg grammar rules.</nextsent>
<nextsent>meanwhile, we gather context features for training the maxent rs models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2489">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>meanwhile, we gather context features for training the maxent rs models.
</prevsent>
<prevsent>the maximum initial phrase length is setto 10 and the maximum rule length of the source side is set to 5.we use sri language modeling toolkit (stol cke, 2002) to train language models for both tasks.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
we use minimum error rate training (och, 2003) <papid> P03-1021 </papid>to tune the feature weights for the log-linear model.</citsent>
<aftsection>
<nextsent>the translation quality is evaluated by bleu metric (papineni et al, 2002), <papid> P02-1040 </papid>as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where = 4.</nextsent>
<nextsent>4.3 baseline.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2490">
<title id=" C08-1041.xml">improving statistical machine translation using lexicalized rule selection </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the maximum initial phrase length is setto 10 and the maximum rule length of the source side is set to 5.we use sri language modeling toolkit (stol cke, 2002) to train language models for both tasks.
</prevsent>
<prevsent>we use minimum error rate training (och, 2003) <papid> P03-1021 </papid>to tune the feature weights for the log-linear model.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the translation quality is evaluated by bleu metric (papineni et al, 2002), <papid> P02-1040 </papid>as calculated by mteval-v11b.pl with case-insensitive matching of n-grams, where = 4.</citsent>
<aftsection>
<nextsent>4.3 baseline.
</nextsent>
<nextsent>we re implement the decoder of hiero (chiang, 2007) in c++, which is the state-of-the-art smt 325 system iwslt-05 nist-03 baseline 56.20 28.05 + maxent rs slex 56.51 28.26 pf 56.95 28.78 slex+pf 56.99 28.89 slex+pf+slen 57.10 28.96 slex+pf +slen+tf 57.20 29.02 table 3: bleu-4 scores (case-insensitive) on iwslt-05 task and nist mt-03 task.
</nextsent>
<nextsent>slex = source-side lexical features, pf = pos features, slen = source-side length feature, tf = target-side features.
</nextsent>
<nextsent>system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2493">
<title id=" C02-1165.xml">complexity of event structure in ie scenarios </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4 we present examples of the linguistic cues to disaster date location victim dead damage tornado sunday night georgia one person motel disease date location victim dead victimsickebola since september uganda 156 people table 1: disaster event and disease event recover the complex event structure, followed by discussion in section 5.
</prevsent>
<prevsent>2.1 information extraction.
</prevsent>
</prevsection>
<citsent citstr=" M95-1014 ">
our ie system has been previously customized for several news topics, as part of the muc program, such as terrorist attacks (muc, 1991; muc, 1992) and management succession (muc, 1995; grishman, 1995).<papid> M95-1014 </papid></citsent>
<aftsection>
<nextsent>subsequently to the mucs, we customized proteus to extract, among other scenarios, corporate mergers and acquisitions, natural disasters and infectious disease outbreaks.
</nextsent>
<nextsent>we contrasted the nature scenarios with the earlier muc scenarios (huttunen et al, 2002).
</nextsent>
<nextsent>the \traditional  template structure is such that all the information about the main event can be presented within single template.
</nextsent>
<nextsent>the main events form separate instances, and there are no links between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2494">
<title id=" C04-1091.xml">an algorithmic framework for solving the decoding problem in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use this algorithm as subroutine in the otheralgorithms.
</prevsent>
<prevsent>we believe that decoding algorithms derived from our framework can be of practical significance.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
decoding is one of the three fundamental problems in classical smt (translation model and language model being the other two) as proposed by ibm in the early 1990s (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>in the decoding problem we are given the language and translation models and source language sentence and are asked to find themost probable translation for the sentence.
</nextsent>
<nextsent>decoding is discrete optimization problem whose search space is prohibitively large.
</nextsent>
<nextsent>the challenge is, therefore, in devising scheme to efficiently search the solution space for the solu tion.decoding is known to belong to class of computational problems popularly known as nphard problems (knight, 1999).<papid> J99-4005 </papid></nextsent>
<nextsent>np-hard problems are known to be computationally hard and have eluded polynomial time algorithms (garey and johnson, 1979).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2496">
<title id=" C04-1091.xml">an algorithmic framework for solving the decoding problem in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the decoding problem we are given the language and translation models and source language sentence and are asked to find themost probable translation for the sentence.
</prevsent>
<prevsent>decoding is discrete optimization problem whose search space is prohibitively large.
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
the challenge is, therefore, in devising scheme to efficiently search the solution space for the solu tion.decoding is known to belong to class of computational problems popularly known as nphard problems (knight, 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>np-hard problems are known to be computationally hard and have eluded polynomial time algorithms (garey and johnson, 1979).
</nextsent>
<nextsent>the first algorithms for the decoding problem were based on what isknown among the speech recognition community as stack-based search (jelinek, 1969).
</nextsent>
<nextsent>the original ibm solution to the decoding problem employed restricted stack-based search(berger et al, 1996).
</nextsent>
<nextsent>this idea was further explored by wang and waibel (wang and waibel, 1997) <papid> P97-1047 </papid>who developed faster stack-based search algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2498">
<title id=" C04-1091.xml">an algorithmic framework for solving the decoding problem in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first algorithms for the decoding problem were based on what isknown among the speech recognition community as stack-based search (jelinek, 1969).
</prevsent>
<prevsent>the original ibm solution to the decoding problem employed restricted stack-based search(berger et al, 1996).
</prevsent>
</prevsection>
<citsent citstr=" P97-1047 ">
this idea was further explored by wang and waibel (wang and waibel, 1997) <papid> P97-1047 </papid>who developed faster stack-based search algorithm.</citsent>
<aftsection>
<nextsent>in perhaps the first work on the computational complexity of decoding, kevin knight showed that the problem is closely related to the more famous traveling salesman problem (tsp).
</nextsent>
<nextsent>independently, christoph tillman adapted the held-karp dynamic programming algorithm for tsp (held and karp, 1962)to decoding (tillman, 2001).
</nextsent>
<nextsent>the original held karp algorithm for tsp is an exponential time dynamic programming algorithm and tillmansadaptation to decoding has prohibitive complexity of ( l3m22m ) ? (m52m) (where mand are the lengths of the source and target sentences respectively).
</nextsent>
<nextsent>tillman and ney showed how to improve the complexity of theheld-karp algorithm for restricted word reordering and gave o ( l3m4) ? (m7) algorithm for french-english translation (tillman and ney, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2499">
<title id=" C04-1091.xml">an algorithmic framework for solving the decoding problem in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tillman and ney showed how to improve the complexity of theheld-karp algorithm for restricted word reordering and gave o ( l3m4) ? (m7) algorithm for french-english translation (tillman and ney, 2000).
</prevsent>
<prevsent>an optimal decoder based on the well-known a?
</prevsent>
</prevsection>
<citsent citstr=" W01-1408 ">
heuristic was implemented and benchmarked in (och et al, 2001).<papid> W01-1408 </papid></citsent>
<aftsection>
<nextsent>since optimal solution can not be computed for practical problem instances in reasonable amount of time, much of recent work has focused on good quality sub optimal solutions.
</nextsent>
<nextsent>an ( m6 )greedy search algorithm was developed (germann et al, 2003) whose complexity was reduced further to ( m2 ) (germann, 2003).<papid> N03-1010 </papid></nextsent>
<nextsent>in this paper, we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2500">
<title id=" C04-1091.xml">an algorithmic framework for solving the decoding problem in statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>heuristic was implemented and benchmarked in (och et al, 2001).<papid> W01-1408 </papid></prevsent>
<prevsent>since optimal solution can not be computed for practical problem instances in reasonable amount of time, much of recent work has focused on good quality sub optimal solutions.</prevsent>
</prevsection>
<citsent citstr=" N03-1010 ">
an ( m6 )greedy search algorithm was developed (germann et al, 2003) whose complexity was reduced further to ( m2 ) (germann, 2003).<papid> N03-1010 </papid></citsent>
<aftsection>
<nextsent>in this paper, we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework.
</nextsent>
<nextsent>we model the search problem as an alternating search problem.
</nextsent>
<nextsent>the search, therefore, alternates between two subproblems,both of which are much easier to solve in practice.
</nextsent>
<nextsent>by breaking the decoding problem intotwo simpler search problems, we are able to provide handles for solving the problem efficiently.the solutions of the sub problems can be combined easily to arrive at solution for the original problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2506">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> applying machine learning (ml).  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>previous work..
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
yarowsky (1994) <papid> P94-1013 </papid>defined basic set of features that has been widely used (with some variations) by other wsd systems.</citsent>
<aftsection>
<nextsent>it consisted on words appearing in window of positions around the target and bigrams and trigrams constructed with the target word.
</nextsent>
<nextsent>he used words, lemmas, coarse part-of-speech tags and special classes of words, such as weekday?.
</nextsent>
<nextsent>these features have been used by other approaches, with variations such as the size of the window, the distinction between open class/closed class words, or the pre-selection of significative words to look up in the context of the target word.
</nextsent>
<nextsent>ng (1996) uses basic set of features similar to those defined by yarowsky, but they also use syntactic information: verb-object and subject verb relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2508">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> applying machine learning (ml).  </section>
<citcontext>
<prevsection>
<prevsent>ng (1996) uses basic set of features similar to those defined by yarowsky, but they also use syntactic information: verb-object and subject verb relations.
</prevsent>
<prevsent>the results obtained by the syntactic features are poor, and no analysis of the features or any reason for the low performance is given.
</prevsent>
</prevsection>
<citsent citstr=" W98-0701 ">
stetina et al  (1998) <papid> W98-0701 </papid>achieve good results with syntactic relations as features.</citsent>
<aftsection>
<nextsent>they use measure of semantic distance based on wordnet to find similar features.
</nextsent>
<nextsent>the features are extracted using statistical parser (collins, 1996), <papid> P96-1025 </papid>and consist of the head and modifiers of each phrase.</nextsent>
<nextsent>unfortunately, they do not provide comparison with baseline system that would only use basic features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2509">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> applying machine learning (ml).  </section>
<citcontext>
<prevsection>
<prevsent>stetina et al  (1998) <papid> W98-0701 </papid>achieve good results with syntactic relations as features.</prevsent>
<prevsent>they use measure of semantic distance based on wordnet to find similar features.</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
the features are extracted using statistical parser (collins, 1996), <papid> P96-1025 </papid>and consist of the head and modifiers of each phrase.</citsent>
<aftsection>
<nextsent>unfortunately, they do not provide comparison with baseline system that would only use basic features.
</nextsent>
<nextsent>the senseval-2 workshop was held in toulouse in july 2001 (preiss &amp; yarowsky, 2001).
</nextsent>
<nextsent>most of the supervised systems used only basic set of local and topical features to train their ml systems.
</nextsent>
<nextsent>regarding syntactic information, in the japanese tasks, several groups relied on dependency trees to extract features that were used by different models (svm, bayes, or vector space models).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2510">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> basic feature set.  </section>
<citcontext>
<prevsection>
<prevsent>local features include bigrams and trigrams (coded as big_, trig_ respectively) that contain the target word.
</prevsent>
<prevsent>an index (+1, -1, 0) is used to indicate the position of the target in the bigram or trigram, which can be formed by part of speech, lemmas or word forms (wf, lem, pos).
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
we used tnt (brants, 2000) <papid> A00-1031 </papid>for pos tagging.</citsent>
<aftsection>
<nextsent>for instance, we could extract the following features for the target word known from the sample sentence below: word form whole?
</nextsent>
<nextsent>occurring in 2 sentence window (win_wf_2s), the bigram known widely?
</nextsent>
<nextsent>where target is the last word (big_wf_+1) and the trigram rb rb n?
</nextsent>
<nextsent>formed by the two pos before the target word (trig_pos_+1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2511">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> set of syntactic features..  </section>
<citcontext>
<prevsection>
<prevsent>there is nothing in the whole range of human experience more widely known and universally ??
</prevsent>
<prevsent>in order to extract syntactic features from the tagged examples, we needed parser that would meet the following requirements: free for research, able to provide the whole structure with named syntactic relations (in contrast to shallow parsers), positively evaluated on well established corpora, domain independent, and fast enough.
</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
three parsers fulfilled all the requirements: link grammar (sleator and temperley, 1993), minipar (lin, 1993) <papid> P93-1016 </papid>and (carroll &amp; briscoe, 2001).</citsent>
<aftsection>
<nextsent>we installed the first two parsers, and performed set of small experiments (john carroll helped out running his own parser).
</nextsent>
<nextsent>unfortunately, we did not have comparative evaluation to help choosing the best.
</nextsent>
<nextsent>we performed little comparative test, and all parsers looked similar.
</nextsent>
<nextsent>at this point we chose minipar mainly because it was fast, easy to install and the output could be easily processed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2514">
<title id=" C02-1112.xml">syntactic features for high precision word sense disambiguation </title>
<section> ml algorithms..  </section>
<citcontext>
<prevsection>
<prevsent>a high-precision wsd system can be obtained at the cost of low coverage, preventing the system to return an answer in the lowest confidence cases.
</prevsent>
<prevsent>we have tried two methods on dlists, and one method on boost.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
the first method is based on decision threshold (dagan and itai, 1994): <papid> J94-4003 </papid>the algorithm rejects decisions taken when the difference of the maximum likelihood among the competing senses is not big enough.</citsent>
<aftsection>
<nextsent>for this purpose, one-tailed confidence interval was created so we could state with confidence 1 - ? that the true value of the difference measure was bigger than given threshold (named ?).
</nextsent>
<nextsent>as in (dagan and itai, 1994), <papid> J94-4003 </papid>we adjusted the measure to the amount of evidence.</nextsent>
<nextsent>different values of ? were tested, using 60% confidence interval.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2516">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in this paper we investigate the task of automatically identifying the correct argument structure for set of verbs.
</prevsent>
<prevsent>the argument structure of verb allow sus to predict the relationship between the syntactic arguments of verb and their role in the under lying lexical semantics of the verb.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
following the method described in (merlo and stevenson, 2001),<papid> J01-3003 </papid>we exploit the distributions of some selected features from the local context of verb.</citsent>
<aftsection>
<nextsent>these features were extracted from 23m word wsj corpus based on part-of-speech tags and phrasal chunksalone.
</nextsent>
<nextsent>we constructed several decision tree classifiers trained on this data.
</nextsent>
<nextsent>the best performing classifier achieved an error rate of 33.4%.
</nextsent>
<nextsent>this work shows that subcategorization frame (sf) learning algorithm previously applied to czech (sarkar and zeman, 2000) <papid> C00-2100 </papid>is used to extract sfs in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2517">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we constructed several decision tree classifiers trained on this data.
</prevsent>
<prevsent>the best performing classifier achieved an error rate of 33.4%.
</prevsent>
</prevsection>
<citsent citstr=" C00-2100 ">
this work shows that subcategorization frame (sf) learning algorithm previously applied to czech (sarkar and zeman, 2000) <papid> C00-2100 </papid>is used to extract sfs in english.</citsent>
<aftsection>
<nextsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</nextsent>
<nextsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</nextsent>
<nextsent>this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</nextsent>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2521">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</prevsent>
<prevsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</prevsent>
</prevsection>
<citsent citstr=" P98-2247 ">
this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</citsent>
<aftsection>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
<nextsent>thanks to paola merlo, dan gildea, david chiang, aravind joshi and the anonymous reviewers for their comments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2522">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</prevsent>
<prevsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</prevsent>
</prevsection>
<citsent citstr=" E99-1007 ">
this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</citsent>
<aftsection>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
<nextsent>thanks to paola merlo, dan gildea, david chiang, aravind joshi and the anonymous reviewers for their comments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2526">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</prevsent>
<prevsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</prevsent>
</prevsection>
<citsent citstr=" W99-0503 ">
this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</citsent>
<aftsection>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
<nextsent>thanks to paola merlo, dan gildea, david chiang, aravind joshi and the anonymous reviewers for their comments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2527">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</prevsent>
<prevsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</prevsent>
</prevsection>
<citsent citstr=" P99-1051 ">
this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</citsent>
<aftsection>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
<nextsent>thanks to paola merlo, dan gildea, david chiang, aravind joshi and the anonymous reviewers for their comments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2528">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the extracted sfs are evaluated by classifying verbs into verb alternation classes.
</prevsent>
<prevsent>the classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments.
</prevsent>
</prevsection>
<citsent citstr=" W99-0632 ">
this is often termed as the classification ofverb dia thesis roles or the lexical semantics of predicates in natural language (see (levin, 1993; mccarthy and korhonen, 1998; <papid> P98-2247 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et al, 1999; <papid> W99-0503 </papid>lapata, 1999; <papid> P99-1051 </papid>lapataand brew, 1999; <papid> W99-0632 </papid>schulte im walde, 2000)).</citsent>
<aftsection>
<nextsent>following the method described in (merlo and stevenson, 2001; <papid> J01-3003 </papid>stevenson and merlo, 1999; <papid> E99-1007 </papid>stevenson et? this research was supported in part by nsf grant sbr-8920230.</nextsent>
<nextsent>thanks to paola merlo, dan gildea, david chiang, aravind joshi and the anonymous reviewers for their comments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2555">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> identifying subcategorization frames.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in john saw mary yesterday at the station?, only john?
</prevsent>
<prevsent>and mary?
</prevsent>
</prevsection>
<citsent citstr=" W93-0109 ">
are required arguments while the other constituents are optional (adjuncts).3the problem of sf identification using statistical methods has had rich discussion in the literature (ushioda et al, 1993; <papid> W93-0109 </papid>manning, 1993; <papid> P93-1032 </papid>briscoe and carroll, 1997; <papid> A97-1052 </papid>brent, 1994) (also see the re fences cited in (sarkar and zeman, 2000)).<papid> C00-2100 </papid></citsent>
<aftsection>
<nextsent>in this paper, we use the method of hypothesis testing to discover the sf forgiven verb (brent, 1994).
</nextsent>
<nextsent>along with the techniques given in these papers, (sarkar and zeman, 2000; <papid> C00-2100 </papid>korhonen et al, 2000) <papid> W00-1325 </papid>also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test.</nextsent>
<nextsent>after experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on small set of hand-annotated instances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2556">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> identifying subcategorization frames.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in john saw mary yesterday at the station?, only john?
</prevsent>
<prevsent>and mary?
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
are required arguments while the other constituents are optional (adjuncts).3the problem of sf identification using statistical methods has had rich discussion in the literature (ushioda et al, 1993; <papid> W93-0109 </papid>manning, 1993; <papid> P93-1032 </papid>briscoe and carroll, 1997; <papid> A97-1052 </papid>brent, 1994) (also see the re fences cited in (sarkar and zeman, 2000)).<papid> C00-2100 </papid></citsent>
<aftsection>
<nextsent>in this paper, we use the method of hypothesis testing to discover the sf forgiven verb (brent, 1994).
</nextsent>
<nextsent>along with the techniques given in these papers, (sarkar and zeman, 2000; <papid> C00-2100 </papid>korhonen et al, 2000) <papid> W00-1325 </papid>also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test.</nextsent>
<nextsent>after experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on small set of hand-annotated instances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2557">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> identifying subcategorization frames.  </section>
<citcontext>
<prevsection>
<prevsent>for example, in john saw mary yesterday at the station?, only john?
</prevsent>
<prevsent>and mary?
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
are required arguments while the other constituents are optional (adjuncts).3the problem of sf identification using statistical methods has had rich discussion in the literature (ushioda et al, 1993; <papid> W93-0109 </papid>manning, 1993; <papid> P93-1032 </papid>briscoe and carroll, 1997; <papid> A97-1052 </papid>brent, 1994) (also see the re fences cited in (sarkar and zeman, 2000)).<papid> C00-2100 </papid></citsent>
<aftsection>
<nextsent>in this paper, we use the method of hypothesis testing to discover the sf forgiven verb (brent, 1994).
</nextsent>
<nextsent>along with the techniques given in these papers, (sarkar and zeman, 2000; <papid> C00-2100 </papid>korhonen et al, 2000) <papid> W00-1325 </papid>also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test.</nextsent>
<nextsent>after experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on small set of hand-annotated instances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2560">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> identifying subcategorization frames.  </section>
<citcontext>
<prevsection>
<prevsent>are required arguments while the other constituents are optional (adjuncts).3the problem of sf identification using statistical methods has had rich discussion in the literature (ushioda et al, 1993; <papid> W93-0109 </papid>manning, 1993; <papid> P93-1032 </papid>briscoe and carroll, 1997; <papid> A97-1052 </papid>brent, 1994) (also see the re fences cited in (sarkar and zeman, 2000)).<papid> C00-2100 </papid></prevsent>
<prevsent>in this paper, we use the method of hypothesis testing to discover the sf forgiven verb (brent, 1994).</prevsent>
</prevsection>
<citsent citstr=" W00-1325 ">
along with the techniques given in these papers, (sarkar and zeman, 2000; <papid> C00-2100 </papid>korhonen et al, 2000) <papid> W00-1325 </papid>also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test.</citsent>
<aftsection>
<nextsent>after experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on small set of hand-annotated instances.
</nextsent>
<nextsent>we use the determination of the verbs sf as an input to our argument structure classifier (see section 4).
</nextsent>
<nextsent>the method works as follows: for each verb, weneed to associate score to the hypothesis that particular set of dependents of the verb are arguments of that verb.
</nextsent>
<nextsent>in other words, we need to assign avalue to the hypothesis that the observed frame under consideration is the verbs sf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2561">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> identifying subcategorization frames.  </section>
<citcontext>
<prevsection>
<prevsent>for fur3there is some controversy as to the correct subcategorization of given verb and linguists often disagree as to what is the right set of sfs forgiven verb.
</prevsent>
<prevsent>a machine learning approach such as the one followed in this paper sidesteps this issue altogether, since it is left to the algorithm to learn what is an appropriate sf for verb.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the stance taken in this paper is that the efficacy of sf learning is evaluated on some domain, as is done here on learning verb alternations.ther background on this method of hypothesis testing the reader is referred to (bickel and doksum, 1977; dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>3.1 likelihood ratio test.
</nextsent>
<nextsent>let us take the hypothesis that the distribution of an observed frame in the training data is independent of the distribution of verb v. we can phrase this hypothesis as p( | v) = p( | !v) = p( ), that is distribution of frame given that verb is present is the same as the distribution of given that is not present (written as !v).
</nextsent>
<nextsent>we use the log likelihood test statistic (bickel and doksum, 1977, 209) as measure to discover particular frames and verbs that are highly associated in the training data.
</nextsent>
<nextsent>k1 = c( , v) n1 = c(v) = c( , v) + c(!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2566">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> steps in constructing the classifier.  </section>
<citcontext>
<prevsection>
<prevsent>the features are extracted from 23m word corpus of wsj text (ldc wsj 1988 collection).
</prevsent>
<prevsent>note that the training and test data constructed from this set are produced by the classification of individual verbs into their respective classes taken from (merlo and stevenson, 2001).<papid> J01-3003 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we prepare the corpus bypassing it throughadwait ratnaparkhis part-of-speech tagger (rat naparkhi, 1996) (<papid> W96-0213 </papid>trained on the penn treebank wsj corpus) and then running steve abneys chun ker (abney, 1997) over the entire text.</citsent>
<aftsection>
<nextsent>the output of this stage and the input to our feature extractor is shown below.
</nextsent>
<nextsent>pierre nnp nx 2 vinken nnp , , 61 cd ax 3.
</nextsent>
<nextsent>years nns old jj , , will md vx 2 join vb the dt nx 2 board nn as in dt nx 3 non executive jj director nn nov. nnp 29 cd.
</nextsent>
<nextsent>we use the following features to construct the classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2604">
<title id=" C02-1040.xml">learning verb argument structure from minimally annotated corpora </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>we were able to construct classifier that has an error rate of 33.4%.
</prevsent>
<prevsent>this work shows that subcategorization frame learning algorithm (sarkar and zeman, 2000) <papid> C00-2100 </papid>can be applied to the task of classifying verbs into verb alternation classes.</prevsent>
</prevsection>
<citsent citstr=" C02-1132 ">
in future work, we would like to classify verbs into alternation classes on per-token basis (as is done in the approach taken by gildea (2002)) <papid> C02-1132 </papid>rather than the per-type we currently employ and also incorporate information about word senses in order to feasibly include verb alternation information in statistical parser.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2605">
<title id=" C04-1044.xml">polarization and abstraction of grammatical formalisms as methods for lexical disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, the number of way of associating to each word of sentence corresponding elementary structurea tagging of the sentence?
</prevsent>
<prevsent>is the product of the number of lexical entries foreach word.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
the procedure may have an exponential complexity in the length of the sentence.in order to filter taggings, we can use probabilistic methods (joshi and srinivas, 1994) <papid> C94-1024 </papid>and keep only the most probable ones; but if we want to keep all successful taggings, we must use exact methods.</citsent>
<aftsection>
<nextsent>among these, one consists in abstracting information that is relevant for the filtering process, from the formalism used for representing the concerned grammar g. in this way, we obtain new formalism fabs which is simplification of and the grammar is translated into grammar abs(g) in the abstract framework fabs.
</nextsent>
<nextsent>from this, disambiguat ing with consists in parsing with abs(g).
</nextsent>
<nextsent>the abstraction is relevant if parsing eliminates maximum of bad taggings at minimal cost.(boullier, 2003) uses such method for lexicalized tree adjoining grammars (ltag) by abstracting tree adjoining grammar into context free grammar and further abstracting that one into regular grammar.
</nextsent>
<nextsent>we also propose to apply abstraction but after preprocessing polarization step.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2606">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the meaning of robust processing?
</prevsent>
<prevsent>is not limited to robust processing for ill-formed sentences found in spoken language, but includes robust processing for sentences which are well-formed but beyond the grammar writers expectation.
</prevsent>
</prevsection>
<citsent citstr=" C92-2072 ">
studies of robust parsing within unification-based grammars have been explored by many researchers (douglas and dale, 1992; <papid> C92-2072 </papid>imaichi and matsumoto, 1995).</citsent>
<aftsection>
<nextsent>they classified the errors found in analyzing ill-formed sentences into several categories to make them tractable, e.g., constraint violation, missing orextra elements, etc. in this paper, we focus on recovery from the constraint violation errors, which is violation of feature values.
</nextsent>
<nextsent>all errors in agreement fall into this category.
</nextsent>
<nextsent>since many of the grammatical components in hpsg are written as constraints represented by feature structures, many of the errors are expected to be recovered by the recovery of constraint violation errors.
</nextsent>
<nextsent>this paper proposes two new types of default unification and describes their application to robustprocessing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2607">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 describe our default unification.our robust parsing is explained in section 5.
</prevsent>
<prevsent>section 6 shows series of experiments of robust parsing with default unification.
</prevsent>
</prevsection>
<citsent citstr=" P90-1021 ">
default unification has been investigated by many researchers (bouma, 1990; <papid> P90-1021 </papid>russell et al, 1991; <papid> P91-1028 </papid>copestake, 1993; carpenter, 1993; lascarides and copestake, 1999) <papid> J99-1002 </papid>in the context of developing lexical semantics.</citsent>
<aftsection>
<nextsent>here, we first explain the definition given by carpenter (1993) because his definition is both concise and comprehensive.
</nextsent>
<nextsent>2.1 carpenters default unification.
</nextsent>
<nextsent>carpenter proposed two types of default unification, credulous default unification and skeptical default unification.
</nextsent>
<nextsent>(credulous default unification)  unionsqc = { unionsqg?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2609">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 describe our default unification.our robust parsing is explained in section 5.
</prevsent>
<prevsent>section 6 shows series of experiments of robust parsing with default unification.
</prevsent>
</prevsection>
<citsent citstr=" P91-1028 ">
default unification has been investigated by many researchers (bouma, 1990; <papid> P90-1021 </papid>russell et al, 1991; <papid> P91-1028 </papid>copestake, 1993; carpenter, 1993; lascarides and copestake, 1999) <papid> J99-1002 </papid>in the context of developing lexical semantics.</citsent>
<aftsection>
<nextsent>here, we first explain the definition given by carpenter (1993) because his definition is both concise and comprehensive.
</nextsent>
<nextsent>2.1 carpenters default unification.
</nextsent>
<nextsent>carpenter proposed two types of default unification, credulous default unification and skeptical default unification.
</nextsent>
<nextsent>(credulous default unification)  unionsqc = { unionsqg?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2611">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>sections 3 and 4 describe our default unification.our robust parsing is explained in section 5.
</prevsent>
<prevsent>section 6 shows series of experiments of robust parsing with default unification.
</prevsent>
</prevsection>
<citsent citstr=" J99-1002 ">
default unification has been investigated by many researchers (bouma, 1990; <papid> P90-1021 </papid>russell et al, 1991; <papid> P91-1028 </papid>copestake, 1993; carpenter, 1993; lascarides and copestake, 1999) <papid> J99-1002 </papid>in the context of developing lexical semantics.</citsent>
<aftsection>
<nextsent>here, we first explain the definition given by carpenter (1993) because his definition is both concise and comprehensive.
</nextsent>
<nextsent>2.1 carpenters default unification.
</nextsent>
<nextsent>carpenter proposed two types of default unification, credulous default unification and skeptical default unification.
</nextsent>
<nextsent>(credulous default unification)  unionsqc = { unionsqg?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2616">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> offline robust parsing and grammar.  </section>
<citcontext>
<prevsection>
<prevsent>they can be regarded as exceptions in grammar, which are difficult to be captured only by propagating information from daughters to mother.
</prevsent>
<prevsent>this approach can be regarded as kind of explanation-based learning (samuelsson and rayner, 1991).
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
the explanation-based learning method is recently attracting researchers attention (xia, 1999; chiang, 2000) <papid> P00-1058 </papid>because their parsers are comparative to the state-of-the-art parsers in termsof precision and recall.</citsent>
<aftsection>
<nextsent>in the context of unification based grammars, neumann (1994) has developed parser running with an hpsg grammar learned by explanation-based learning.
</nextsent>
<nextsent>it should be also noted that kiyono and tsujii (1993) <papid> E93-1027 </papid>exemplified the grammar extraction approach using offline parsing in the 3although the size of the grammar becomes very large, the extracted rules can be found by hash algorithm very effi ciently.</nextsent>
<nextsent>this tract ability helps to use this approach in practical applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2617">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> offline robust parsing and grammar.  </section>
<citcontext>
<prevsection>
<prevsent>the explanation-based learning method is recently attracting researchers attention (xia, 1999; chiang, 2000) <papid> P00-1058 </papid>because their parsers are comparative to the state-of-the-art parsers in termsof precision and recall.</prevsent>
<prevsent>in the context of unification based grammars, neumann (1994) has developed parser running with an hpsg grammar learned by explanation-based learning.</prevsent>
</prevsection>
<citsent citstr=" E93-1027 ">
it should be also noted that kiyono and tsujii (1993) <papid> E93-1027 </papid>exemplified the grammar extraction approach using offline parsing in the 3although the size of the grammar becomes very large, the extracted rules can be found by hash algorithm very effi ciently.</citsent>
<aftsection>
<nextsent>this tract ability helps to use this approach in practical applications.
</nextsent>
<nextsent>training test set test set corpus b # of sentences 5,903 1,480 100 avg.
</nextsent>
<nextsent>length 23.59 23.93 6.63 of sentence stable 1: corpus size and average length of sentences
</nextsent>
<nextsent>       figure 4: the average number of edges when tests etb was parsed context of explanation-based learning.finally, we need to remove some values in the extracted rules because they contain too specific information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2618">
<title id=" C02-1100.xml">lenient default unification for robust processing within unification based grammar formalisms </title>
<section> performance evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, value of phonology: represents list of phoneme strings of phrasal structure.
</prevsent>
<prevsent>without removing them, extracted rules cannot be triggered until when completely the same strings appear in text.4
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
we measured the performance of our robust parsing algorithm by measuring coverage and degree of over generation for the wall street journal in the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the training corpus consists of 5,903 sentences selected from the wall street journal (wall street journal 00 ? 02), and we prepared two sets of test corpora, tests eta and testsetb.
</nextsent>
<nextsent>tests eta consists of 1,480 sentences(wall street journal 03) and is used for measuring coverage.5 tests etb consists of 100 sentence sand is used for measuring the degree of over genera tion.
</nextsent>
<nextsent>the sentences of tests etb are the shortest 100 sentences in testseta.
</nextsent>
<nextsent>table 1 shows the average sentence length of each corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2619">
<title id=" C02-1043.xml">computation of modifier scope in np by a language neutral method </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the algorithm is language-neutral, in that it works with minimal errors for wide range of languages without language-specific stipulations.
</prevsent>
<prevsent>noun phrases quite commonly have multiple modifiers, including quantifiers, attributive adjective phrases, relative clauses, possessors, appositives and the like.
</prevsent>
</prevsection>
<citsent citstr=" P99-1018 ">
as frequently noted in the literature (e.g., shaw and hatzivassiloglou, 1999), <papid> P99-1018 </papid>the linear order of modifiers can signify their logical scope (though other factors are involved, too), as in the english examples (1) and (2) (bracketing indicates logical scope): (1) my [favorite [new movie]] (2) my [new [favorite movie]] in (1) favorite modifies the phrase new movie; hence the np refers to my favorite among the new movies (there may be an old movie like better); in (2) new modifies favorite movie; hence the np refers to my favorite movie, which has just become my favorite.</citsent>
<aftsection>
<nextsent>the computation of the scope of modifiers is of inherent linguistic interest: it is necessary for determining the correct interpretation of nps like (1) and (2).
</nextsent>
<nextsent>it follows that it is potentially useful in any application that may depend on such an interpretation.
</nextsent>
<nextsent>in addition, for multilingual applications such as transfer-based machine translation (mt) (as discussed for example by richardson et al  (2001)) modifier scope may itself be used as an abstract, language-neutral representation of their surface configuration, including linear order.
</nextsent>
<nextsent>the generation component of the mt application could then make use of scope information, perhaps in addition to scope-independent ordering conventions (malouf, 2000), <papid> P00-1012 </papid>to generate the modifiers in the correct order.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2620">
<title id=" C02-1043.xml">computation of modifier scope in np by a language neutral method </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it follows that it is potentially useful in any application that may depend on such an interpretation.
</prevsent>
<prevsent>in addition, for multilingual applications such as transfer-based machine translation (mt) (as discussed for example by richardson et al  (2001)) modifier scope may itself be used as an abstract, language-neutral representation of their surface configuration, including linear order.
</prevsent>
</prevsection>
<citsent citstr=" P00-1012 ">
the generation component of the mt application could then make use of scope information, perhaps in addition to scope-independent ordering conventions (malouf, 2000), <papid> P00-1012 </papid>to generate the modifiers in the correct order.</citsent>
<aftsection>
<nextsent>the focus of the current paper is method for computing the relative scope of modifiers based on structural information, which works independently of any particular language; that is, the same algorithm that computes the scope of the modifiers in the english np (3) also correctly computes modifier scope in (4), its french translation, even though the two examples do not have exactly parallel surface structures: (3) the [twenty-ninth [american state]] (4) le [vingt-neuvime [tat amricain]] the twenty-ninth state american the proposed algorithm considers several structural factors in addition to linear order, including the type and internal structure of the modifiers themselves.
</nextsent>
<nextsent>the algorithm described here is currently implemented in the nlpwin system at microsoft research (heidorn, 2000).
</nextsent>
<nextsent>the paper is organized as follows: section 1 examines the various structural factors that determine modifier scope, and preliminary algorithm for modifier scope assignment is proposed; in section 2, we compare the predictions of the algorithm to diverse set of examples from six languages, and propose revised algorithm; section 3 considers some related work; and section 4 is conclusion.
</nextsent>
<nextsent>1 modifier scope.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2621">
<title id=" C02-1043.xml">computation of modifier scope in np by a language neutral method </title>
<section> examination of broader range of.  </section>
<citcontext>
<prevsection>
<prevsent>syntactically complex post nominal modifiers that are not relative clauses have 1 the special status of such adjectives has been noted in.
</prevsent>
<prevsent>other contexts: for example, hawkins (1978) groups same together with superlatives into class of unexplanatory?
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
modifiers; vieira and poesio (2000), <papid> J00-4003 </papid>extending this class to include only and few others, make use of them in identifying discourse-new definite descriptions.</citsent>
<aftsection>
<nextsent>wider scope than other modifiers not covered by (ii.1-2); 4.
</nextsent>
<nextsent>pre nominal modifiers not covered by (ii.1-3) have wider scope than other modifiers not covered by (ii.1-3); 5.
</nextsent>
<nextsent>otherwise, within each group, assign wider scope to post nominal modifiers over pre nominal modifiers; 6.
</nextsent>
<nextsent>among post nominal modifiers in the same group, or among pre nominal modifiers in the same group, assign wider scope to modifiers farther from the head noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2622">
<title id=" C04-1185.xml">constraint based rmrs construction from shallow grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main rationale for this type of underspeci fic ation is to ensure monotonicity, and thus upwards compatibility of the output of shallow parsing with semantic representations obtained from full syntactic parsing.
</prevsent>
<prevsent>thus, cope stakes design of rmrs ? robust minimal recur sion semantics ? provides an important contribution to novel line of research towards integration of shallow and deep nlp.
</prevsent>
</prevsection>
<citsent citstr=" E03-1052 ">
while previous accounts (daum et al, 2003; <papid> E03-1052 </papid>frank et al,2003) <papid> P03-1014 </papid>focus on shallow-deep integration at the syntactic level, copestake aims at integration of shallow and deep nlp at the level of semantics.in this paper we review the rmrs formalism designed by copestake (2003) and present an architecture for principle-based syntax-semantics interface for rmrs construction from shallow gram mars.</citsent>
<aftsection>
<nextsent>we argue for unification-based approach, 1the research reported here was conducted in the project quetal, funded by the german ministry for education and research, bmbf, under grant no. 01 iw c02.
</nextsent>
<nextsent>to account for (underspecified) argument binding in languages with case-marking as opposed to structural argument identification.
</nextsent>
<nextsent>the architecture we propose is especially designed to support flexible adaptation to different types of shallow to intermediate-level syntactic grammars that may serve as basis for rmrs construction.
</nextsent>
<nextsent>a challenge for principle-based semantics construction from shallow grammars is the flat and sometimesnon-compositional nature of the structures they typically produce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2623">
<title id=" C04-1185.xml">constraint based rmrs construction from shallow grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main rationale for this type of underspeci fic ation is to ensure monotonicity, and thus upwards compatibility of the output of shallow parsing with semantic representations obtained from full syntactic parsing.
</prevsent>
<prevsent>thus, cope stakes design of rmrs ? robust minimal recur sion semantics ? provides an important contribution to novel line of research towards integration of shallow and deep nlp.
</prevsent>
</prevsection>
<citsent citstr=" P03-1014 ">
while previous accounts (daum et al, 2003; <papid> E03-1052 </papid>frank et al,2003) <papid> P03-1014 </papid>focus on shallow-deep integration at the syntactic level, copestake aims at integration of shallow and deep nlp at the level of semantics.in this paper we review the rmrs formalism designed by copestake (2003) and present an architecture for principle-based syntax-semantics interface for rmrs construction from shallow gram mars.</citsent>
<aftsection>
<nextsent>we argue for unification-based approach, 1the research reported here was conducted in the project quetal, funded by the german ministry for education and research, bmbf, under grant no. 01 iw c02.
</nextsent>
<nextsent>to account for (underspecified) argument binding in languages with case-marking as opposed to structural argument identification.
</nextsent>
<nextsent>the architecture we propose is especially designed to support flexible adaptation to different types of shallow to intermediate-level syntactic grammars that may serve as basis for rmrs construction.
</nextsent>
<nextsent>a challenge for principle-based semantics construction from shallow grammars is the flat and sometimesnon-compositional nature of the structures they typically produce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2624">
<title id=" C04-1185.xml">constraint based rmrs construction from shallow grammars </title>
<section> comparison to related work.  </section>
<citcontext>
<prevsection>
<prevsent>the architecture we propose can be applied to sentence- or chunk parsing.
</prevsent>
<prevsent>the rule-based sprout system allows the definition of modular projection rules that can be tailored to specific properties of an underlying shallow grammar (e.g. identification of active/passive voice, of syntactic np/pp heads).
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
in future work we will compare our semantics construction principles to the general model of copestake et al (2001).<papid> P01-1019 </papid></citsent>
<aftsection>
<nextsent>acknowledgements am greatly indebted to my colleagues at dfki, especially the sprout team members witold drozdzynski, hans-ulrich krieger,jakub piskorski and ulrich schafer, for their technical support and advice.
</nextsent>
<nextsent>special thanks go to kathrin spreyer for support in grammar development.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2625">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the semi-supervised setting is the most attractive, as it would save developers the need to hand-annotate target corpora every time new domain is to be processed.
</prevsent>
<prevsent>the main goal of this paper is to use unlabeled data in order to get better domain-adaptation results for word sense disambiguation (wsd) inthe semi-supervised setting.
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
singular value decomposition (svd) has been shown to find correlations between terms which are helpful to over come the scarcity of training data in wsd (gliozzo et al, 2005).<papid> P05-1050 </papid></citsent>
<aftsection>
<nextsent>this paper explores how this ability of svd can be applied to the domain-adaptation ofwsd systems, and we show that svd and unlabeled data improve the results of two state-of-the art wsd systems (k-nn and svm).
</nextsent>
<nextsent>for the sake ofthis paper we call this set of experiments the do main adaptation scenario.in addition, we also perform some related experiments on just the target domain.
</nextsent>
<nextsent>we use unlabeled data in order to improve the results of system trained and tested in the target domain.
</nextsent>
<nextsent>these results are complementary to the domain adaptation experiments, and also provide an upper bound for semi-supervised domain adaptation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2627">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we call these experiments the target domain scenario.
</prevsent>
<prevsent>note that both scenarios are semi-supervised, in that our focus is on the use of unlabeled data in addition to the available labeled data.
</prevsent>
</prevsection>
<citsent citstr=" H05-1053 ">
the experiments were performed on publicly available corpus which was designed to study the effect of domain in wsd (koeling et al, 2005).<papid> H05-1053 </papid></citsent>
<aftsection>
<nextsent>it comprises 41 nouns closely related to the sports and finances domains with 300 examples for each.
</nextsent>
<nextsent>the 300 examples were drawn from the british national corpus (leech, 1992) (bnc), the sports section of the reuters corpus (leech, 17 1992), and the finances section of reuters in equal number.the paper is structured as follows.
</nextsent>
<nextsent>section 2 reviews prior work in the area.
</nextsent>
<nextsent>section 3 presents the datasets used, and section 4 the learning methods,including the application of svd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2628">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>section 7 presents the discussion and section 8 the conclusions and future work.
</prevsent>
<prevsent>domain adaptation is subject attracting moreand more attention.
</prevsent>
</prevsection>
<citsent citstr=" W06-1615 ">
in the semi-supervised setting, blitzer et al (2006) <papid> W06-1615 </papid>use structural correspondence learning and unlabeled data to adapta part-of-speech tagger.</citsent>
<aftsection>
<nextsent>they carefully select so called pivot features?
</nextsent>
<nextsent>to learn linear predictors,perform svd on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains.
</nextsent>
<nextsent>our technique also uses svd, but we directly apply it to all features, and thus avoid the need to define pivot features.
</nextsent>
<nextsent>in preliminary work we unsuccessfully tried to carry along the idea of pivot features to wsd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2629">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they use related unlabeled text and include it in the term-by-document matrix to expand it and capture better the interesting properties of the data.
</prevsent>
<prevsent>their approach is similar to our sma method in section 4.2).
</prevsent>
</prevsection>
<citsent citstr=" W04-3237 ">
in the supervised setting, recent paper by daume iii (2007) shows that, using very simple feature augmentation method coupled with support vector machines, he is able to effectively use both labeled target and source data to provide the best results in number of nlp tasks.his method improves or equals over previously explored more sophisticated methods (daume iii and marcu, 2006; chelba and acero, 2004).<papid> W04-3237 </papid>regarding wsd, some initial works made basic analysis of the particular issues.</citsent>
<aftsection>
<nextsent>escudero etal.
</nextsent>
<nextsent>(2000) tested the supervised adaptation setting on the dso corpus, which had examples fromthe brown corpus and wall street journal corpus.
</nextsent>
<nextsent>they found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging large general corpus would not guarantee robust broad-coverage wsd.
</nextsent>
<nextsent>agirre and martnez (2000) also used the dso corpus in the supervised setting to show that training on subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2631">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>their system was based on the use of similarity thesaurus induced from the domain corpus and wordnet.
</prevsent>
<prevsent>they used the same dataset as in this paper for evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P07-1007 ">
chan and ng (2007) <papid> P07-1007 </papid>performed supervised domain adaptation on manually selected subset of 21 nouns from the dso corpus.</citsent>
<aftsection>
<nextsent>they used active learning, count-merging, and predominant sense estimation in order to save target annotation effort.
</nextsent>
<nextsent>they showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved.
</nextsent>
<nextsent>they also showed that using the source corpus allowed to significantly improve results when only 10%-30% of the target corpus was used for training.
</nextsent>
<nextsent>no data was given about the use of both tagged corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2633">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they combined other knowledge sources into complex kernel using svm.
</prevsent>
<prevsent>they report improved performance on number of languages in the senseval-3 lexical sample dataset.
</prevsent>
</prevsection>
<citsent citstr=" W06-2911 ">
our present paper differs from theirs in that we propose an additional method to use svd (the omtmethod, section 4.2), and that we evaluate the contribution of unlabeled data and svd in isolation, leaving combination for future work.ando (2006) <papid> W06-2911 </papid>used alternative structured optimization, which is closely related to structural learning (cited above).</citsent>
<aftsection>
<nextsent>he first trained one linear predictor for each target word, and then performed svd on 7 carefully selected sub matrices 18 of the feature-to-predictor matrix of weights.
</nextsent>
<nextsent>the system attained small but consistent improvements(no significance data was given) on the senseval 3 lexical sample datasets using svd and unlabeled data.
</nextsent>
<nextsent>we have previously shown (agirre et al, 2005;agirre and lopez de lacalle, 2007) that performing svd on the feature-to-documents matrix is asimple technique that allows to improve performance with and without unlabeled data.
</nextsent>
<nextsent>the use of several k-nn classifiers trained on number of reduced and original spaces was shown to rank firstin the senseval-3 dataset and second in the semeval 2007 competition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2635">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> learning features and methods.  </section>
<citcontext>
<prevsection>
<prevsent>local collocations comprise the bigrams and trigrams formed around the target word (using either lem mas, word-forms, and pos tags 1 ), those formed with the previous/posterior lemma/word-form in the sentence, and the content words in 4-wordwindow around the target.
</prevsent>
<prevsent>syntactic dependencies 2use the object, subject, noun-modifier, preposition, and sibling lemmas, when available.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
finally, bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (pedersen, 2001).<papid> N01-1011 </papid></citsent>
<aftsection>
<nextsent>4.2 features from the reduced space.
</nextsent>
<nextsent>apart from the original space of features, we have the so called svd features, obtained from the projection of the feature vectors into the reduced space (deerwester et al, 1990).
</nextsent>
<nextsent>basically, we set term-by-document or feature-by-example matrix from the corpus (see section below for more details).
</nextsent>
<nextsent>svd decomposes it into three matrices, = uv . if the desired number of dimensions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2637">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> learning features and methods.  </section>
<citcontext>
<prevsection>
<prevsent>this technique is very similar to previous work on svd (gliozzo et al, 2005; <papid> P05-1050 </papid>zelikovitz and hirsh, 2001).</prevsent>
<prevsent>the dimensionality reduction is performed once,over the whole unlabeled corpus, and it is then applied to the labeled data of each word.</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
the reduced 1 the pos tagging was performed with the fntbl toolkit (ngai and florian, 2001) <papid> N01-1006 </papid>2 this software was kindly provided by david yarowskys group, from johns hopkins university.</citsent>
<aftsection>
<nextsent>19space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features.
</nextsent>
<nextsent>given that the wsd literature has shown that all features, including local and syntactic features, are necessary for optimal performance (pradhan et al, 2007), <papid> W07-2016 </papid>we propose the following alternative to construct the matrix.</nextsent>
<nextsent>one matrix per target word (svd-omt).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2638">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> learning features and methods.  </section>
<citcontext>
<prevsection>
<prevsent>the reduced 1 the pos tagging was performed with the fntbl toolkit (ngai and florian, 2001) <papid> N01-1006 </papid>2 this software was kindly provided by david yarowskys group, from johns hopkins university.</prevsent>
<prevsent>19space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features.</prevsent>
</prevsection>
<citsent citstr=" W07-2016 ">
given that the wsd literature has shown that all features, including local and syntactic features, are necessary for optimal performance (pradhan et al, 2007), <papid> W07-2016 </papid>we propose the following alternative to construct the matrix.</citsent>
<aftsection>
<nextsent>one matrix per target word (svd-omt).
</nextsent>
<nextsent>foreach word: (i) construct corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the feature by-example matrix, (iv) decompose it with svd, and (v) project all the labeled training and test data for the word.
</nextsent>
<nextsent>note that this variant performs one svd process for each target word separately, hence its name.
</nextsent>
<nextsent>we proposed this technique in (agirre et al., 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2639">
<title id=" C08-1003.xml">on robustness and domain adaptation using svd for word sense disambiguation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>we show that we can obtain it using large, general, unlabeled corpus.
</prevsent>
<prevsent>note that our semi-supervised method to attain robustness for domain shifts is very cost-effective, as it does not require costly hand-tagged material nor even large numbers of unlabeled data from each target domain.
</prevsent>
</prevsection>
<citsent citstr=" W00-1322 ">
these results are more valuable given the lack of substantial positive results on the literature on semi supervised or supervised domain adaptation for wsd (escudero et al, 2000; <papid> W00-1322 </papid>martnez and agirre, 2000; chan and ng, 2007).</citsent>
<aftsection>
<nextsent>compared to other settings, our semi-supervised results improve over the completely unsupervised system in (koeling et al, 2005), <papid> H05-1053 </papid>which had 43.7%and 49.9% precision for the sports and finances domains respectively, but lag well behind the target domain scenario, showing that there is still room for improvement in the semi-supervised setting.while these results are based on lexical sample, and thus not directly generalizable to an all words corpus, we think that they reflect the main trends for nouns, as the 41 nouns where selected among those exhibiting domain dependence (koel ing et al, 2005).<papid> H05-1053 </papid></nextsent>
<nextsent>we can assume, though it would be needed to be explored empirically, that other nouns exhibiting domain independence would degrade less when moving to other domains, and thus corroborate the robustness effect we have discov ered.the fact that we attain robustness rather than do main adaptation proper deserves some analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2646">
<title id=" C02-1158.xml">study of practical effectiveness for machine translation using recursive chainlinktype learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>corpus based approach uses translation examples that keep including linguistic knowledge.
</prevsent>
<prevsent>this means that the system can improve the quality of its translation only by adding new translation examples.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
however, in statistical mt(brown et al, 1990), <papid> J90-2002 </papid>large amounts of translation examples are required in order to obtainhigh-quality translation.</citsent>
<aftsection>
<nextsent>moreover, example based mt(sato and nagao, 1990; <papid> C90-3044 </papid>watanabe and takeda, 1998; <papid> P98-2223 </papid>brown, 2001; carl, 2001) which relies on various knowledge resources results in the same difficulties as rule-based mt. therefore, example-based mt, which automatically acquires the translation rules from only bilingual text corpora, is very effec tive.</nextsent>
<nextsent>however, existing example-based mt systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2647">
<title id=" C02-1158.xml">study of practical effectiveness for machine translation using recursive chainlinktype learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this means that the system can improve the quality of its translation only by adding new translation examples.
</prevsent>
<prevsent>however, in statistical mt(brown et al, 1990), <papid> J90-2002 </papid>large amounts of translation examples are required in order to obtainhigh-quality translation.</prevsent>
</prevsection>
<citsent citstr=" C90-3044 ">
moreover, example based mt(sato and nagao, 1990; <papid> C90-3044 </papid>watanabe and takeda, 1998; <papid> P98-2223 </papid>brown, 2001; carl, 2001) which relies on various knowledge resources results in the same difficulties as rule-based mt. therefore, example-based mt, which automatically acquires the translation rules from only bilingual text corpora, is very effec tive.</citsent>
<aftsection>
<nextsent>however, existing example-based mt systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules.
</nextsent>
<nextsent>in example-based mt based on ana logical reasoning(malavazos, 2000; guvenir, 1998), the different parts are replaced by variables to generalize translation examples as shown in (1) of figure 1.
</nextsent>
<nextsent>however, the number of different parts of the two sl sentences must be same as the number of different parts of the two tlsentences.
</nextsent>
<nextsent>this means that the condition of acquisition of translation rules is very strict be cause this method allows only n:n mappings in the number of the different parts between thesl sentences and the tl sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2648">
<title id=" C02-1158.xml">study of practical effectiveness for machine translation using recursive chainlinktype learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this means that the system can improve the quality of its translation only by adding new translation examples.
</prevsent>
<prevsent>however, in statistical mt(brown et al, 1990), <papid> J90-2002 </papid>large amounts of translation examples are required in order to obtainhigh-quality translation.</prevsent>
</prevsection>
<citsent citstr=" P98-2223 ">
moreover, example based mt(sato and nagao, 1990; <papid> C90-3044 </papid>watanabe and takeda, 1998; <papid> P98-2223 </papid>brown, 2001; carl, 2001) which relies on various knowledge resources results in the same difficulties as rule-based mt. therefore, example-based mt, which automatically acquires the translation rules from only bilingual text corpora, is very effec tive.</citsent>
<aftsection>
<nextsent>however, existing example-based mt systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules.
</nextsent>
<nextsent>in example-based mt based on ana logical reasoning(malavazos, 2000; guvenir, 1998), the different parts are replaced by variables to generalize translation examples as shown in (1) of figure 1.
</nextsent>
<nextsent>however, the number of different parts of the two sl sentences must be same as the number of different parts of the two tlsentences.
</nextsent>
<nextsent>this means that the condition of acquisition of translation rules is very strict be cause this method allows only n:n mappings in the number of the different parts between thesl sentences and the tl sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2649">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, supervised learning methods suffer from the high cost of manually tagging the sense onto each instance of polysemous word in training corpus.
</prevsent>
<prevsent>a number of bootstrapping methods have been proposed to reduce the sense tagging cost (hearst 1991; basili 1997).
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
a variety of unsupervised wsd methods, which use machine readable dictionary or thesaurus in addition to corpus, have also been proposed (yarowsky 1992; <papid> C92-2070 </papid>yarowsky 1995; <papid> P95-1026 </papid>karov and edelman 1998).<papid> J98-1002 </papid></citsent>
<aftsection>
<nextsent>bilingual parallel corpora, in which the senses of words in the text of one language are indicated by their counterparts in the text of another language, have also been used in order to avoid manually sense-tagging training data (brown, et al. 1991).<papid> P91-1034 </papid>unlike the previous methods using bilingual corpora, our method does not require parallel corpora.</nextsent>
<nextsent>the availability of large parallel corpora is extremely limited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2650">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, supervised learning methods suffer from the high cost of manually tagging the sense onto each instance of polysemous word in training corpus.
</prevsent>
<prevsent>a number of bootstrapping methods have been proposed to reduce the sense tagging cost (hearst 1991; basili 1997).
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
a variety of unsupervised wsd methods, which use machine readable dictionary or thesaurus in addition to corpus, have also been proposed (yarowsky 1992; <papid> C92-2070 </papid>yarowsky 1995; <papid> P95-1026 </papid>karov and edelman 1998).<papid> J98-1002 </papid></citsent>
<aftsection>
<nextsent>bilingual parallel corpora, in which the senses of words in the text of one language are indicated by their counterparts in the text of another language, have also been used in order to avoid manually sense-tagging training data (brown, et al. 1991).<papid> P91-1034 </papid>unlike the previous methods using bilingual corpora, our method does not require parallel corpora.</nextsent>
<nextsent>the availability of large parallel corpora is extremely limited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2651">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, supervised learning methods suffer from the high cost of manually tagging the sense onto each instance of polysemous word in training corpus.
</prevsent>
<prevsent>a number of bootstrapping methods have been proposed to reduce the sense tagging cost (hearst 1991; basili 1997).
</prevsent>
</prevsection>
<citsent citstr=" J98-1002 ">
a variety of unsupervised wsd methods, which use machine readable dictionary or thesaurus in addition to corpus, have also been proposed (yarowsky 1992; <papid> C92-2070 </papid>yarowsky 1995; <papid> P95-1026 </papid>karov and edelman 1998).<papid> J98-1002 </papid></citsent>
<aftsection>
<nextsent>bilingual parallel corpora, in which the senses of words in the text of one language are indicated by their counterparts in the text of another language, have also been used in order to avoid manually sense-tagging training data (brown, et al. 1991).<papid> P91-1034 </papid>unlike the previous methods using bilingual corpora, our method does not require parallel corpora.</nextsent>
<nextsent>the availability of large parallel corpora is extremely limited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2652">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a number of bootstrapping methods have been proposed to reduce the sense tagging cost (hearst 1991; basili 1997).
</prevsent>
<prevsent>a variety of unsupervised wsd methods, which use machine readable dictionary or thesaurus in addition to corpus, have also been proposed (yarowsky 1992; <papid> C92-2070 </papid>yarowsky 1995; <papid> P95-1026 </papid>karov and edelman 1998).<papid> J98-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
bilingual parallel corpora, in which the senses of words in the text of one language are indicated by their counterparts in the text of another language, have also been used in order to avoid manually sense-tagging training data (brown, et al. 1991).<papid> P91-1034 </papid>unlike the previous methods using bilingual corpora, our method does not require parallel corpora.</citsent>
<aftsection>
<nextsent>the availability of large parallel corpora is extremely limited.
</nextsent>
<nextsent>in contrast, comparable corpora are available in many domains.
</nextsent>
<nextsent>the comparability required by our method is very weak: any combination of corpora of different languages in the same domain is acceptable as comparable corpus.
</nextsent>
<nextsent>several types of information are useful for wsd (ide and veronis 1998).<papid> J98-1001 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2653">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, comparable corpora are available in many domains.
</prevsent>
<prevsent>the comparability required by our method is very weak: any combination of corpora of different languages in the same domain is acceptable as comparable corpus.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
several types of information are useful for wsd (ide and veronis 1998).<papid> J98-1001 </papid></citsent>
<aftsection>
<nextsent>three major types are the grammatical characteristics of the polysemous word to be disambiguated, words that are syntactically related to the polysemous word, and words that are topically related to the polysemous word.
</nextsent>
<nextsent>among these types,use of grammatical characteristics, which are language dependent, is not compatible with the approach using bilingual corpora.
</nextsent>
<nextsent>on the other hand, since topical relation is language-independent, use of topically related words is most compatible with the approach using bilingual corpora.
</nextsent>
<nextsent>accordingly, we focused on using topically related words as clues for determining the most suitable sense of polysemous word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2654">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>unlike parallel corpus, we cannot align sentences or instances of words translingually.
</prevsent>
<prevsent>therefore, we extract collection of statistically significant pairs of related words from each language corpus independently of the other language, and then align the pairs of related words translingually with the assistance of bilingual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
the underlying assumption isthat translations of words that are related in one language are also related in the other language (rapp 1995).<papid> P95-1050 </papid></citsent>
<aftsection>
<nextsent>trans lingual alignment of pairs of related words enables us to acquire knowledge useful for wsd (i.e., sense-clue pair).
</nextsent>
<nextsent>for example, the alignment of (tank, gasoline) with (??? tanku , ???? gasorin )implies that gasoline?
</nextsent>
<nextsent>is clue for selecting the con tainer?
</nextsent>
<nextsent>sense of tank?, which is translated as ????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2655">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>a japanese word ????? taitoru ?, which represents all these senses, is preferably eliminated from all these synonym sets.
</prevsent>
<prevsent>3.2 extraction of pairs of related words.
</prevsent>
</prevsection>
<citsent citstr=" C00-1059 ">
the corpus of each language is statistically processed in order to extract collection of pairs of related words in the language (kaji et al 2000).<papid> C00-1059 </papid></citsent>
<aftsection>
<nextsent>first, we extract words from the corpus and count the occurrence frequencies of each word.
</nextsent>
<nextsent>we reject words whose frequencies are less than certain threshold.
</nextsent>
<nextsent>we also extract pairs of words co-occurring in window and count the co-occurrence frequency of each pair of words.
</nextsent>
<nextsent>in the present implementation, the words are restricted to nouns and unknown words, which are probably nouns, and the window size is set to 25 words excluding function words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2656">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> experiment.  </section>
<citcontext>
<prevsection>
<prevsent>the instances of test words positioned in the center of each test passage were disambiguated by the method described in section 3.5, and the results were compared with the manually selected senses.
</prevsent>
<prevsent>4.2 results and evaluation.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
we used two measurements, applicability and precision (dagan and itai 1994), <papid> J94-4003 </papid>to evaluate the performance of our method.</citsent>
<aftsection>
<nextsent>the applicability is the proportion of instances of the test word(s) that the method coulddisambiguate.
</nextsent>
<nextsent>the precision is the proportion of dis ambiguated instances of the test word(s) that the method disambiguated correctly.
</nextsent>
<nextsent>the applicability and precision of the proposed method, averaged over the 60 test polysemous words, were 88.5% and 77.7%, respectively.
</nextsent>
<nextsent>the performance of our method on six out of the 60test words is summarized in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2659">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> comparison with other methods.  </section>
<citcontext>
<prevsection>
<prevsent>0 0 0 0 0 1 1 s4={trial, ??, ???}
</prevsent>
<prevsent>0 0 0 0 0 0 0 s5={trial, ??, ??, ??}
</prevsent>
</prevsection>
<citsent citstr=" P98-1110 ">
0 0 0 0 0 0 0 total 66 26 0 0 0 8 100 [note] s1: legal process in which court examines case s2: process of testing to determine quality, value, usefulness, etc. s3: sports competition that tests players ability s4: annoying thing or person s5: difficulties and troubles (dagan and itai 1994; <papid> J94-4003 </papid>kikui 1998), <papid> P98-1110 </papid>where instances of co-occurrence in first-language text are aligned withco-occurrences statistically extracted from the second language corpus.</citsent>
<aftsection>
<nextsent>a comparison of our method with wsd using second-language monolingual corpus is given below.
</nextsent>
<nextsent>first, our method performs alignment during the acquisition phase, and transforms word-word correlation data into sense-clue correlation data, which is farmore informative than the original word-word correlation data.
</nextsent>
<nextsent>in contrast, method using second language monolingual corpus uses original word-word correlation data during the disambiguation phase.this difference results in difference in the performance of wsd, particularly in poor-context situation (e.g., query translation).second, our method can acquire sense-clue correlation even from pair of related words for which alignment results in failure [e.g., c({tank, ??? tanku , ?? suisou , ? sou }, ozone) in figure 3].
</nextsent>
<nextsent>onthe contrary, conventional wsd method using sec ond-language monolingual corpus uses only pairs of related words for which alignment results in success.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2660">
<title id=" C02-1058.xml">unsupervised word sense disambiguation using bilingual comparable corpora </title>
<section> comparison with other methods.  </section>
<citcontext>
<prevsection>
<prevsent>onthe contrary, conventional wsd method using sec ond-language monolingual corpus uses only pairs of related words for which alignment results in success.
</prevsent>
<prevsent>thus, our method can elicit more information than the conventional method.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
tanaka and iwasaki (1996) <papid> C96-2098 </papid>exploited the idea of translingually aligning word co-occurrences to extract pairs consisting of word and its translation form non-aligned (comparable) corpus.</citsent>
<aftsection>
<nextsent>the essence of their method is to obtain translation matrix that maximizes the distance between the co-occurrencematrix of the first language and that of the second language.
</nextsent>
<nextsent>their method is useful for extracting corpus dependent translations; however, it does not extract knowledge for wsd, i.e., which co-occurring word suggests which sense or translation.
</nextsent>
<nextsent>a method for word sense disambiguation using bilingual comparable corpus together with sense definitions by translations into another language was developed.in this method, knowledge for wsd, i.e., sense-vs. clue correlation, is acquired in an unsupervised fashion as follows.
</nextsent>
<nextsent>first, statistically significant pairs of related words are extracted from the corpus of each language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2661">
<title id=" C04-1192.xml">fine grained word sense disambiguation based on parallel corpora word alignment word clustering and aligned wordnets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of the wsd system, implementing the method described herein showed very encouraging results.
</prevsent>
<prevsent>the same system used invalidation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as balkan et and eurowordnet.
</prevsent>
</prevsection>
<citsent citstr=" P92-1032 ">
word sense disambiguation (wsd) is well known as one of the more difficult problems in the field of natural language processing, as noted in (gale et al 1992; <papid> P92-1032 </papid>kilgarriff, 1997; ide and vronis, 1998), <papid> J98-1001 </papid>and others.</citsent>
<aftsection>
<nextsent>the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in given sense, lack of standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms.
</nextsent>
<nextsent>to address the last problem, (gale et al 1992) <papid> P92-1032 </papid>argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.</nextsent>
<nextsent>the lower bound should not drop below the baseline usage of the algorithm (in which every word that is disambiguated is assigned the most frequent sense) whereas the upper bound should not be too restrictive?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2662">
<title id=" C04-1192.xml">fine grained word sense disambiguation based on parallel corpora word alignment word clustering and aligned wordnets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation of the wsd system, implementing the method described herein showed very encouraging results.
</prevsent>
<prevsent>the same system used invalidation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as balkan et and eurowordnet.
</prevsent>
</prevsection>
<citsent citstr=" J98-1001 ">
word sense disambiguation (wsd) is well known as one of the more difficult problems in the field of natural language processing, as noted in (gale et al 1992; <papid> P92-1032 </papid>kilgarriff, 1997; ide and vronis, 1998), <papid> J98-1001 </papid>and others.</citsent>
<aftsection>
<nextsent>the difficulties stem from several sources, including the lack of means to formalize the properties of context that characterize the use of an ambiguous word in given sense, lack of standard (and possibly exhaustive) sense inventory, and the subjectivity of the human evaluation of such algorithms.
</nextsent>
<nextsent>to address the last problem, (gale et al 1992) <papid> P92-1032 </papid>argue for upper and lower bounds of precision when comparing automatically assigned sense labels with those assigned by human judges.</nextsent>
<nextsent>the lower bound should not drop below the baseline usage of the algorithm (in which every word that is disambiguated is assigned the most frequent sense) whereas the upper bound should not be too restrictive?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2664">
<title id=" C04-1192.xml">fine grained word sense disambiguation based on parallel corpora word alignment word clustering and aligned wordnets </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this raises new questions: how many languages, and of which types (e.g., closely related languages, languages from different language families), provide adequate information for this purpose?
</prevsent>
<prevsent>how do we measure the degree to which different lexicalizations provide evidence for distinct sense?
</prevsent>
</prevsection>
<citsent citstr=" W02-0808 ">
we have addressed these questions in experiments involving sense clustering based on translation equivalents extracted from parallel corpora (ide, 199; ide et al, 2002).<papid> W02-0808 </papid></citsent>
<aftsection>
<nextsent>tufi?
</nextsent>
<nextsent>and ion (2003) build on this work and further describe method to accomplish neutral?
</nextsent>
<nextsent>labelling for the sense clusters in romanian and english that is not bound to any particular sense inventory.
</nextsent>
<nextsent>our experiments confirm that the accuracy of word sense clustering based on translation equivalents is heavily dependent on the number and diversity of the languages in the parallel corpus and the language register of the parallel text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2666">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipeline systems make use of simpler models with more efficient inference to reduce the search space of the full model.
</prevsent>
<prevsent>forex ample, the well-known ratnaparkhi (1999) parser used pos-tagger and finite-state np chunker as initial stages of multi-stage maximum entropy parser.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the charniak (2000) <papid> A00-2018 </papid>parser uses simple pcfg to prune the chart for richer model; and charniak and johnson (2005) <papid> P05-1022 </papid>added discrimina tively trained reranker to the end of that pipeline.recent results making use of finite-state chun kers early in syntactic parsing pipeline have shown both an efficiency (glaysher and moldovan, 2006) <papid> P06-2038 </papid>and an accuracy (hollingshead and roark, 2007) <papid> P07-1120 </papid>benefit to the use of such constraints in parsing system.</citsent>
<aftsection>
<nextsent>glaysher and moldovan (2006)<papid> P06-2038 </papid>demonstrated an efficiency gain by explicitly disallowing entries in chart cells that would result in constituents that cross chunk boundaries.</nextsent>
<nextsent>hollingshead and roark (2007) <papid> P07-1120 </papid>demonstrated that high precision constraints on early stages of the charniak and johnson (2005) <papid> P05-1022 </papid>pipeline in the form ofbase phrase constraints derived either from chun ker or from later stages of an earlier iteration of the ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2668">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipeline systems make use of simpler models with more efficient inference to reduce the search space of the full model.
</prevsent>
<prevsent>forex ample, the well-known ratnaparkhi (1999) parser used pos-tagger and finite-state np chunker as initial stages of multi-stage maximum entropy parser.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
the charniak (2000) <papid> A00-2018 </papid>parser uses simple pcfg to prune the chart for richer model; and charniak and johnson (2005) <papid> P05-1022 </papid>added discrimina tively trained reranker to the end of that pipeline.recent results making use of finite-state chun kers early in syntactic parsing pipeline have shown both an efficiency (glaysher and moldovan, 2006) <papid> P06-2038 </papid>and an accuracy (hollingshead and roark, 2007) <papid> P07-1120 </papid>benefit to the use of such constraints in parsing system.</citsent>
<aftsection>
<nextsent>glaysher and moldovan (2006)<papid> P06-2038 </papid>demonstrated an efficiency gain by explicitly disallowing entries in chart cells that would result in constituents that cross chunk boundaries.</nextsent>
<nextsent>hollingshead and roark (2007) <papid> P07-1120 </papid>demonstrated that high precision constraints on early stages of the charniak and johnson (2005) <papid> P05-1022 </papid>pipeline in the form ofbase phrase constraints derived either from chun ker or from later stages of an earlier iteration of the ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2670">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipeline systems make use of simpler models with more efficient inference to reduce the search space of the full model.
</prevsent>
<prevsent>forex ample, the well-known ratnaparkhi (1999) parser used pos-tagger and finite-state np chunker as initial stages of multi-stage maximum entropy parser.
</prevsent>
</prevsection>
<citsent citstr=" P06-2038 ">
the charniak (2000) <papid> A00-2018 </papid>parser uses simple pcfg to prune the chart for richer model; and charniak and johnson (2005) <papid> P05-1022 </papid>added discrimina tively trained reranker to the end of that pipeline.recent results making use of finite-state chun kers early in syntactic parsing pipeline have shown both an efficiency (glaysher and moldovan, 2006) <papid> P06-2038 </papid>and an accuracy (hollingshead and roark, 2007) <papid> P07-1120 </papid>benefit to the use of such constraints in parsing system.</citsent>
<aftsection>
<nextsent>glaysher and moldovan (2006)<papid> P06-2038 </papid>demonstrated an efficiency gain by explicitly disallowing entries in chart cells that would result in constituents that cross chunk boundaries.</nextsent>
<nextsent>hollingshead and roark (2007) <papid> P07-1120 </papid>demonstrated that high precision constraints on early stages of the charniak and johnson (2005) <papid> P05-1022 </papid>pipeline in the form ofbase phrase constraints derived either from chun ker or from later stages of an earlier iteration of the ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2671">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pipeline systems make use of simpler models with more efficient inference to reduce the search space of the full model.
</prevsent>
<prevsent>forex ample, the well-known ratnaparkhi (1999) parser used pos-tagger and finite-state np chunker as initial stages of multi-stage maximum entropy parser.
</prevsent>
</prevsection>
<citsent citstr=" P07-1120 ">
the charniak (2000) <papid> A00-2018 </papid>parser uses simple pcfg to prune the chart for richer model; and charniak and johnson (2005) <papid> P05-1022 </papid>added discrimina tively trained reranker to the end of that pipeline.recent results making use of finite-state chun kers early in syntactic parsing pipeline have shown both an efficiency (glaysher and moldovan, 2006) <papid> P06-2038 </papid>and an accuracy (hollingshead and roark, 2007) <papid> P07-1120 </papid>benefit to the use of such constraints in parsing system.</citsent>
<aftsection>
<nextsent>glaysher and moldovan (2006)<papid> P06-2038 </papid>demonstrated an efficiency gain by explicitly disallowing entries in chart cells that would result in constituents that cross chunk boundaries.</nextsent>
<nextsent>hollingshead and roark (2007) <papid> P07-1120 </papid>demonstrated that high precision constraints on early stages of the charniak and johnson (2005) <papid> P05-1022 </papid>pipeline in the form ofbase phrase constraints derived either from chun ker or from later stages of an earlier iteration of the ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2678">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> starting and ending constituents.  </section>
<citcontext>
<prevsection>
<prevsent>the first word 1 must start constituent spanning the whole string, and the last word n must end that same constituent.
</prevsent>
<prevsent>the first word 1 cannot end constituent of length greater than 1; similarly, the last word n cannot start constituent of length greater than 1.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
hence our classifier evaluation omits those two word positions, leading to n2 classifications for string of length n. table 1 shows statistics from sections 2-21 of the penn wsj treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>from the nearly 1 million words in approximately 40 thousand sentences, just over 870 thousand are neither the first nor the last word in the string, hence possible members of the sets 1 or 1 , i.e., not beginning multi-word constituent (s 1 ) or not ending multi-word constituent (e 1 ).
</nextsent>
<nextsent>of these,over half (50.5%) do not begin multi-word constituents, and nearly three quarters (74.3%) do notend multi-word constituents.
</nextsent>
<nextsent>this high latter percentage reflects english right-branching structure.how well can we perform these binary classification tasks, using simple (linear complexity) classifiers?
</nextsent>
<nextsent>to investigate this question, we used sections 2-21 of the penn wsj treebank as training data, section 00 as heldout, and section 24 as development.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2679">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> starting and ending constituents.  </section>
<citcontext>
<prevsection>
<prevsent>to investigate this question, we used sections 2-21 of the penn wsj treebank as training data, section 00 as heldout, and section 24 as development.
</prevsent>
<prevsent>word classes are straightforwardly extracted from the treebank trees, by measuring the span of constituents starting and ending at each word position.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we trained loglinear models withthe perceptron algorithm (collins, 2002) <papid> W02-1001 </papid>using fea 746 markov order classification task 0 1 2 1 (no multi-word constituent start) 96.7 96.9 96.9 1 (no multi-word constituent end) 97.3 97.3 97.3table 2: classification accuracy on development set for binary classes 1 and 1 , for various markov orders.</citsent>
<aftsection>
<nextsent>tures similar to those used for np chunking in shaand pereira (2003), <papid> N03-1028 </papid>including surrounding pos tags (provided by separately trained log linearpos-tagger) and surrounding words, up to 2 before and 2 after the current word position.</nextsent>
<nextsent>table 2 presents classification accuracy on the development set for both of these classification tasks.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2680">
<title id=" C08-1094.xml">classifying chart cells for quadratic complexity context free inference </title>
<section> starting and ending constituents.  </section>
<citcontext>
<prevsection>
<prevsent>word classes are straightforwardly extracted from the treebank trees, by measuring the span of constituents starting and ending at each word position.
</prevsent>
<prevsent>we trained loglinear models withthe perceptron algorithm (collins, 2002) <papid> W02-1001 </papid>using fea 746 markov order classification task 0 1 2 1 (no multi-word constituent start) 96.7 96.9 96.9 1 (no multi-word constituent end) 97.3 97.3 97.3table 2: classification accuracy on development set for binary classes 1 and 1 , for various markov orders.</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
tures similar to those used for np chunking in shaand pereira (2003), <papid> N03-1028 </papid>including surrounding pos tags (provided by separately trained log linearpos-tagger) and surrounding words, up to 2 before and 2 after the current word position.</citsent>
<aftsection>
<nextsent>table 2 presents classification accuracy on the development set for both of these classification tasks.
</nextsent>
<nextsent>we trained models with markov order 0(each word classified independently), order 1 (fea tures with class pairs) and order 2 (features with class triples).
</nextsent>
<nextsent>this did not change performance for the 1 classification, but markov order 1 was slightly (but significantly) better than order 0 for 1 classification.
</nextsent>
<nextsent>hence, from this point forward, all classification will be markov order 1.we can see from these results that simple classification approaches yield very high classification accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2684">
<title id=" C02-1019.xml">the grammatical function analysis between korean adnoun clause and noun phrase by using support vector machines </title>
<section> korean adnoun clauses and their.  </section>
<citcontext>
<prevsection>
<prevsent>sasil-eul(truth) mal-haess-da(talk).
</prevsent>
<prevsent>(he talked about the truth which he discovered.)
</prevsent>
</prevsection>
<citsent citstr=" P98-2125 ">
li et al (1998)<papid> P98-2125 </papid>described method using conceptual co-occurrence patterns and syntactic role distribution of relative nouns.</citsent>
<aftsection>
<nextsent>linguistic information is extracted from corpus and thesaurus.
</nextsent>
<nextsent>however, he did not take into account appositive adnoun clauses but only considered relative adnoun clauses.
</nextsent>
<nextsent>lee et al (2001) classified adnoun clauses into appositive clauses and one of relative clauses.
</nextsent>
<nextsent>he proposed stochastic method based on maximum likelihood estimation and adopted the backed-off model in estimating the probability p(r|v,e,n) to handle sparse data problem (the symbols r, v, and represent the grammatical relation, the verb of the adnoun clause, the adnominal verb ending, and the head noun modified by an adnoun clause, respectively).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2689">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one important reason seems to be that dependency parsing offers good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.
</prevsent>
<prevsent>thus, whereas complete dependency structure provides fully disambiguated analysis of sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
deterministic methods for dependency parsing have now been applied to variety of languages, including japanese (kudo and matsumoto, 2000), <papid> W00-1303 </papid>english (yamada and matsumoto, 2003), turkish (oflazer, 2003), <papid> J03-4001 </papid>and swedish (nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>for english, the interest in dependency parsing has been weaker than for other languages.
</nextsent>
<nextsent>to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</nextsent>
<nextsent>on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2690">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one important reason seems to be that dependency parsing offers good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.
</prevsent>
<prevsent>thus, whereas complete dependency structure provides fully disambiguated analysis of sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.
</prevsent>
</prevsection>
<citsent citstr=" J03-4001 ">
deterministic methods for dependency parsing have now been applied to variety of languages, including japanese (kudo and matsumoto, 2000), <papid> W00-1303 </papid>english (yamada and matsumoto, 2003), turkish (oflazer, 2003), <papid> J03-4001 </papid>and swedish (nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>for english, the interest in dependency parsing has been weaker than for other languages.
</nextsent>
<nextsent>to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</nextsent>
<nextsent>on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2691">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one important reason seems to be that dependency parsing offers good compromise between the conflicting demands of analysis depth, on the one hand, and robustness and efficiency, on the other.
</prevsent>
<prevsent>thus, whereas complete dependency structure provides fully disambiguated analysis of sentence, this analysis is typically less complex than in frameworks based on constituent analysis and can therefore often be computed deterministically with reasonable accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W04-2407 ">
deterministic methods for dependency parsing have now been applied to variety of languages, including japanese (kudo and matsumoto, 2000), <papid> W00-1303 </papid>english (yamada and matsumoto, 2003), turkish (oflazer, 2003), <papid> J03-4001 </papid>and swedish (nivre et al, 2004).<papid> W04-2407 </papid></citsent>
<aftsection>
<nextsent>for english, the interest in dependency parsing has been weaker than for other languages.
</nextsent>
<nextsent>to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</nextsent>
<nextsent>on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2696">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>deterministic methods for dependency parsing have now been applied to variety of languages, including japanese (kudo and matsumoto, 2000), <papid> W00-1303 </papid>english (yamada and matsumoto, 2003), turkish (oflazer, 2003), <papid> J03-4001 </papid>and swedish (nivre et al, 2004).<papid> W04-2407 </papid></prevsent>
<prevsent>for english, the interest in dependency parsing has been weaker than for other languages.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</citsent>
<aftsection>
<nextsent>on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</nextsent>
<nextsent>moreover, the deterministic dependency parser of yamada and matsumoto (2003), when trained on the penn treebank, gives dependency accuracy that is almost as good as that of collins (1997) <papid> P97-1003 </papid>and charniak (2000).<papid> A00-2018 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2697">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for english, the interest in dependency parsing has been weaker than for other languages.
</prevsent>
<prevsent>to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</citsent>
<aftsection>
<nextsent>moreover, the deterministic dependency parser of yamada and matsumoto (2003), when trained on the penn treebank, gives dependency accuracy that is almost as good as that of collins (1997) <papid> P97-1003 </papid>and charniak (2000).<papid> A00-2018 </papid></nextsent>
<nextsent>the parser described in this paper is similar to that of yamada and matsumoto (2003) in that it uses deterministic parsing algorithm in combination with classifier induced from treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2700">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for english, the interest in dependency parsing has been weaker than for other languages.
</prevsent>
<prevsent>to some extent, this can probably be explained by the strong tradition of constituent analysis in anglo-american linguistics, but this trend has been reinforced by the fact that the major treebank of american english,the penn treebank (marcus et al, 1993), <papid> J93-2004 </papid>is annotated primarily with constituent analysis.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
on the other hand, the best available parsers trained on thepenn treebank, those of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>use statistical models for disambiguation that make crucial use of dependency relations.</citsent>
<aftsection>
<nextsent>moreover, the deterministic dependency parser of yamada and matsumoto (2003), when trained on the penn treebank, gives dependency accuracy that is almost as good as that of collins (1997) <papid> P97-1003 </papid>and charniak (2000).<papid> A00-2018 </papid></nextsent>
<nextsent>the parser described in this paper is similar to that of yamada and matsumoto (2003) in that it uses deterministic parsing algorithm in combination with classifier induced from treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2708">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another difference is that yamada and matsumoto use support vector machines (vapnik, 1995), whilewe instead relyon memory-based learning (daele mans, 1999).
</prevsent>
<prevsent>most importantly, however, the parser presented in this paper constructs labeled dependency graphs, i.e. dependency graphs where arcs are labeled with dependency types.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
as far as we know, this makesit different from all previous systems for dependency parsing applied to the penn treebank (eis ner, 1996; <papid> C96-1058 </papid>yamada and matsumoto, 2003), although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. buchholz (2002).</citsent>
<aftsection>
<nextsent>the fact that we are working with labeled dependency graphs is also one of the motivations for choosing memory-based learning over support vector machines, since we require multi-class classifier.
</nextsent>
<nextsent>even though it is possible to use svmfor multi-class classification, this can get cumbersome when the number of classes is large.
</nextsent>
<nextsent>(for the the   ? dep finger-pointing   ? np-sbj has already   ? advp begun   ? vp . ?   dep figure 1: dependency graph for english sentence unlabeled dependency parser of yamada and matsumoto (2003) the classification problem only involves three classes.)
</nextsent>
<nextsent>the parsing methodology investigated here has previously been applied to swedish, where promising results were obtained with relatively small treebank (approximately 5000 sentences for train ing), resulting in an attachment score of 84.7% and labeled accuracy of 80.6% (nivre et al, 2004).<papid> W04-2407 </papid>1 however, since there are no comparable results available for swedish, it is difficult to assess the significance of these findings, which is one of the reasons why we want to apply the method to benchmark corpus such as the the penn treebank, even though the annotation in this corpus is not ideal for labeled dependency parsing.the paper is structured as follows.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2717">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> shift: in configuration s,n|i,a?, push.  </section>
<citcontext>
<prevsection>
<prevsent>otherwise, it is set of connected components, each of which is well-formed dependency graph for substring of the original input.the transition system defined above is non deterministic in itself, since several transitions can often be applied in given configuration.
</prevsent>
<prevsent>to construct deterministic parsers based on this system,we use classifiers trained on treebank data in order to predict the next transition (and dependency type) given the current configuration of the parser.
</prevsent>
</prevsection>
<citsent citstr=" H92-1026 ">
in this way, our approach can be seen as form ofhistory-based parsing (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>in the experiments reported here, we use memory-based learning to train our classifiers.
</nextsent>
<nextsent>3 memory-based learning.
</nextsent>
<nextsent>memory-based learning and problem solving is based on two fundamental principles: learning is the simple storage of experiences in memory, and solving new problem is achieved by reusing solutions from similar previously solved problems (daele mans, 1999).
</nextsent>
<nextsent>it is inspired by the nearest neighbor approach in statistical pattern recognition and artificial intelligence (fix and hodges, 1952), as well as the ana logical modeling approach in linguistics(skousen, 1989; skousen, 1992).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2718">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> shift: in configuration s,n|i,a?, push.  </section>
<citcontext>
<prevsection>
<prevsent>otherwise, it is set of connected components, each of which is well-formed dependency graph for substring of the original input.the transition system defined above is non deterministic in itself, since several transitions can often be applied in given configuration.
</prevsent>
<prevsent>to construct deterministic parsers based on this system,we use classifiers trained on treebank data in order to predict the next transition (and dependency type) given the current configuration of the parser.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
in this way, our approach can be seen as form ofhistory-based parsing (black et al, 1992; <papid> H92-1026 </papid>magerman, 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>in the experiments reported here, we use memory-based learning to train our classifiers.
</nextsent>
<nextsent>3 memory-based learning.
</nextsent>
<nextsent>memory-based learning and problem solving is based on two fundamental principles: learning is the simple storage of experiences in memory, and solving new problem is achieved by reusing solutions from similar previously solved problems (daele mans, 1999).
</nextsent>
<nextsent>it is inspired by the nearest neighbor approach in statistical pattern recognition and artificial intelligence (fix and hodges, 1952), as well as the ana logical modeling approach in linguistics(skousen, 1989; skousen, 1992).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2732">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> shift: in configuration s,n|i,a?, push.  </section>
<citcontext>
<prevsection>
<prevsent>(1991).
</prevsent>
<prevsent>used for training and section 23 for testing (collins,1999; charniak, 2000).<papid> A00-2018 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
the data has been converted to dependency trees using head rules (magerman, 1995; <papid> P95-1037 </papid>collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>we are grateful to yamada and matsumoto for letting us use their rule set, which is slight modification of the rules used by collins (1999).
</nextsent>
<nextsent>this permits us to make exact comparisons with the parser of yamada and matsumoto (2003), but also the parsers of collins (1997) <papid> P97-1003 </papid>and charniak (2000), <papid> A00-2018 </papid>which are evaluated on the same dataset in yamada and matsumoto (2003).one problem that we had to face is that the standard conversion of phrase structure trees to dependency trees gives unlabeled dependency trees, whereas our parser requires labeled trees.</nextsent>
<nextsent>since the annotation scheme of the penn treebank does not include dependency types, there is no straightforward way to derive such labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2740">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> shift: in configuration s,n|i,a?, push.  </section>
<citcontext>
<prevsection>
<prevsent>we use the following metrics for evaluation: 1.
</prevsent>
<prevsent>unlabeled attachment score (uas): the pro-.
</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
portion of words that are assigned the correct head (or no head if the word is root) (eisner, 1996; <papid> C96-1058 </papid>collins et al, 1999).<papid> P99-1065 </papid></citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>labeled attachment score (las): the pro-.
</nextsent>
<nextsent>portion of words that are assigned the correct head and dependency type (or no head if the word is root) (nivre et al, 2004).<papid> W04-2407 </papid></nextsent>
<nextsent>3.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2757">
<title id=" C04-1010.xml">deterministic dependency parsing of english text </title>
<section> complete match (cm): the proportion of.  </section>
<citcontext>
<prevsection>
<prevsent>this may be taken to suggest that some kind of preprocessing in the form of cla using may help to improve overall accuracy.turning finally to the assessment of labeled dependency accuracy, we are not aware of any strictly comparable results for the given dataset, but buchholz (2002) reports labeled accuracy of 72.6% for the assignment of grammatical relations using cascade of memory-based processors.
</prevsent>
<prevsent>this can be compared with labeled attachment score of 84.4% for model 2 with our set, which is of about the same size as the set used by buchholz, although the labels are not the same.
</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
in another study, blaheta and charniak (2000) <papid> A00-2031 </papid>report an f-measure of 98.9% for the assignment of penn treebank grammatical role labels (our set) to phrases that were correctly parsed by the parser described in charniak (2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>if null labels (corresponding to our dep labels) are excluded, the f-score drops to 95.7%.
</nextsent>
<nextsent>the corresponding f-measures for our best parser (model 2, bg) are 99.0% and 94.7%.
</nextsent>
<nextsent>for the larger set, our best parser achieves an f-measure of 96.9% (dep labels included), which can be compared with 97.0% for similar (but larger) set of labels in collins (1999).6 although none of the previous results on labeling accuracy is strictly comparable to ours, it nevertheless seems fair to conclude that the 6this f-measure is based on the recall and precision figures reported in figure 7.15 in collins (1999).
</nextsent>
<nextsent>model 1 model 2 b bg b bg uas 86.4 86.7 85.8 87.1 las 85.3 84.0 85.5 84.6 84.4 86.0 table 1: parsing accuracy: attachment score (bg = evaluation of restricted to labels) da ra cm charniak 92.1 95.2 45.2 collins 91.5 95.2 43.3 yamada &amp; matsumoto 90.3 91.6 38.4 nivre &amp; scholz 87.3 84.3 30.4 table 2: comparison with related work (yamada and matsumoto, 2003) labeling accuracy of the present parser is close to the state of the art, even if its capacity to derive correct structures is not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2761">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there is big difference between written text corpus and spontaneous speech corpus: in spontaneous speech, especially when it is long, sentence boundaries are often ambiguous.
</prevsent>
<prevsent>in thecsj, therefore, sentence boundaries were defined based on clauses whose boundaries were automatically detected by using surface information (maruyama et al, 2003), and they were detected manually (takanashi et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" P98-1083 ">
our definition of sentence boundaries follows the definition used in the csj.almost all previous research on japanese dependency structure analysis dealt with dependency structures in written text (fujio and matsumoto, 1998; haruno et al, 1998; <papid> P98-1083 </papid>uchimoto etal., 1999; <papid> E99-1026 </papid>uchimoto et al, 2000; kudo and matsumoto, 2000).<papid> W00-1303 </papid></citsent>
<aftsection>
<nextsent>although matsubara and colleagues did investigate dependency structures in spontaneous speech (matsubara et al, 2002),<papid> C02-1136 </papid>the target speech was dialogues where the utterances were short and sentence boundaries could be easily defined based on turn-taking data.</nextsent>
<nextsent>in contrast, we investigated dependency structures in spontaneous and long speeches in the csj.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2762">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there is big difference between written text corpus and spontaneous speech corpus: in spontaneous speech, especially when it is long, sentence boundaries are often ambiguous.
</prevsent>
<prevsent>in thecsj, therefore, sentence boundaries were defined based on clauses whose boundaries were automatically detected by using surface information (maruyama et al, 2003), and they were detected manually (takanashi et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
our definition of sentence boundaries follows the definition used in the csj.almost all previous research on japanese dependency structure analysis dealt with dependency structures in written text (fujio and matsumoto, 1998; haruno et al, 1998; <papid> P98-1083 </papid>uchimoto etal., 1999; <papid> E99-1026 </papid>uchimoto et al, 2000; kudo and matsumoto, 2000).<papid> W00-1303 </papid></citsent>
<aftsection>
<nextsent>although matsubara and colleagues did investigate dependency structures in spontaneous speech (matsubara et al, 2002),<papid> C02-1136 </papid>the target speech was dialogues where the utterances were short and sentence boundaries could be easily defined based on turn-taking data.</nextsent>
<nextsent>in contrast, we investigated dependency structures in spontaneous and long speeches in the csj.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2763">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there is big difference between written text corpus and spontaneous speech corpus: in spontaneous speech, especially when it is long, sentence boundaries are often ambiguous.
</prevsent>
<prevsent>in thecsj, therefore, sentence boundaries were defined based on clauses whose boundaries were automatically detected by using surface information (maruyama et al, 2003), and they were detected manually (takanashi et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
our definition of sentence boundaries follows the definition used in the csj.almost all previous research on japanese dependency structure analysis dealt with dependency structures in written text (fujio and matsumoto, 1998; haruno et al, 1998; <papid> P98-1083 </papid>uchimoto etal., 1999; <papid> E99-1026 </papid>uchimoto et al, 2000; kudo and matsumoto, 2000).<papid> W00-1303 </papid></citsent>
<aftsection>
<nextsent>although matsubara and colleagues did investigate dependency structures in spontaneous speech (matsubara et al, 2002),<papid> C02-1136 </papid>the target speech was dialogues where the utterances were short and sentence boundaries could be easily defined based on turn-taking data.</nextsent>
<nextsent>in contrast, we investigated dependency structures in spontaneous and long speeches in the csj.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2764">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in thecsj, therefore, sentence boundaries were defined based on clauses whose boundaries were automatically detected by using surface information (maruyama et al, 2003), and they were detected manually (takanashi et al, 2003).
</prevsent>
<prevsent>our definition of sentence boundaries follows the definition used in the csj.almost all previous research on japanese dependency structure analysis dealt with dependency structures in written text (fujio and matsumoto, 1998; haruno et al, 1998; <papid> P98-1083 </papid>uchimoto etal., 1999; <papid> E99-1026 </papid>uchimoto et al, 2000; kudo and matsumoto, 2000).<papid> W00-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1136 ">
although matsubara and colleagues did investigate dependency structures in spontaneous speech (matsubara et al, 2002),<papid> C02-1136 </papid>the target speech was dialogues where the utterances were short and sentence boundaries could be easily defined based on turn-taking data.</citsent>
<aftsection>
<nextsent>in contrast, we investigated dependency structures in spontaneous and long speeches in the csj.
</nextsent>
<nextsent>the biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous.
</nextsent>
<nextsent>therefore, sentence boundaries should be detected before or during dependency structure analysis in order to obtain the dependency structure of each sentence.
</nextsent>
<nextsent>in this paper, we first describe the problems with dependency structure analysis of spontaneous speech.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2765">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> approach of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>let us also assume that = {d1, . . .
</prevsent>
<prevsent>,dn1}.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
statistical dependency structure analysis finds dependencies that maximize probability (d|s) given sentence s. the conventional statistical model (collins, 1996; <papid> P96-1025 </papid>fujio and matsumoto, 1998; haruno et al., 1998; <papid> P98-1083 </papid>uchimoto et al, 1999) <papid> E99-1026 </papid>uses onlythe relationship between two bunsetsus to estimate the probability of dependency, whereas the model in this study (uchimoto et al, 2000)takes into account not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all the bunsetsus to its right.</citsent>
<aftsection>
<nextsent>this model uses more information than the conventional model.we implemented this model within maximum entropy modeling framework.
</nextsent>
<nextsent>the features used in the model were basically attributes of bunsetsus, such as character strings, parts of speech, and types of inflections, as well as those that describe the relationships betweenbunsetsus, such as the distance between bun setsus.
</nextsent>
<nextsent>combinations of these features were also used.
</nextsent>
<nextsent>to find best , we analyzed the sentences backwards (from right to left).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2769">
<title id=" C04-1159.xml">dependency structure analysis and sentence boundary detection in spontaneous japanese </title>
<section> approach of dependency.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 sentence boundary detection.
</prevsent>
<prevsent>based on machine learning (method 2) we use support vector machine (svm) as machine learning model and we approached the problem of sentence boundary detection as text chunking task.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
we used yamcha (kudo and matsumoto, 2001) <papid> N01-1025 </papid>as text chunker, which is based on svm and uses polynomial kernel functions.</citsent>
<aftsection>
<nextsent>to determine the appropriate chunk label for target word, yamcha uses two words to the right and two words to the left of the target word as statistical features, and it uses chunk labels that are dynamically assigned to the two preceding or the two following words as dynamic features, depending on the analysis direction.
</nextsent>
<nextsent>to solve the multi-class problem, weused pairwise classification.
</nextsent>
<nextsent>this method generates ?
</nextsent>
<nextsent>(n ? 1)/2 classifiers for all pairs of classes, , and makes final decision by their weighted voting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2770">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>approaches to machine translation (mt) using data-oriented parsing (dop: (bod, 1998; bod et al ., 2003)) require source,target?
</prevsent>
<prevsent>tree fragments aligned at sentential and sub-sentential levels.
</prevsent>
</prevsection>
<citsent citstr=" C00-2092 ">
in previous approaches to data-oriented translation (dot: (poutsma, 2000;<papid> C00-2092 </papid>hearne and way, 2003)), such fragments were produced manually.</citsent>
<aftsection>
<nextsent>this istime-consuming, error-prone, and requires considerable expertise of both source and target language sas well as how they are related.
</nextsent>
<nextsent>the obvious solution, therefore, is to automate the process of sub sentential alignment.
</nextsent>
<nextsent>however, while there are many approaches to sentential alignment e.g.
</nextsent>
<nextsent>(kay and roscheisen, 1993; <papid> J93-1006 </papid>gale &amp; church, 1993), <papid> J93-1004 </papid>no methods exist for aligning non-isomorphic phrase structure (ps) tree fragments at sub-sentential level for use in mt.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2772">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the obvious solution, therefore, is to automate the process of sub sentential alignment.
</prevsent>
<prevsent>however, while there are many approaches to sentential alignment e.g.
</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
(kay and roscheisen, 1993; <papid> J93-1006 </papid>gale &amp; church, 1993), <papid> J93-1004 </papid>no methods exist for aligning non-isomorphic phrase structure (ps) tree fragments at sub-sentential level for use in mt.</citsent>
<aftsection>
<nextsent>(matsumoto et al , 1993) <papid> P93-1004 </papid>alignsource,target?</nextsent>
<nextsent>dependency trees, with view to resolve parsing ambiguities, but their approach can not deal with complex or compound sentences.other researchers (imamura, 2001) also use phrase alignment in parsing but in dot the translation fragments are already in the form of parse-trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2773">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the obvious solution, therefore, is to automate the process of sub sentential alignment.
</prevsent>
<prevsent>however, while there are many approaches to sentential alignment e.g.
</prevsent>
</prevsection>
<citsent citstr=" J93-1004 ">
(kay and roscheisen, 1993; <papid> J93-1006 </papid>gale &amp; church, 1993), <papid> J93-1004 </papid>no methods exist for aligning non-isomorphic phrase structure (ps) tree fragments at sub-sentential level for use in mt.</citsent>
<aftsection>
<nextsent>(matsumoto et al , 1993) <papid> P93-1004 </papid>alignsource,target?</nextsent>
<nextsent>dependency trees, with view to resolve parsing ambiguities, but their approach can not deal with complex or compound sentences.other researchers (imamura, 2001) also use phrase alignment in parsing but in dot the translation fragments are already in the form of parse-trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2774">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, while there are many approaches to sentential alignment e.g.
</prevsent>
<prevsent>(kay and roscheisen, 1993; <papid> J93-1006 </papid>gale &amp; church, 1993), <papid> J93-1004 </papid>no methods exist for aligning non-isomorphic phrase structure (ps) tree fragments at sub-sentential level for use in mt.</prevsent>
</prevsection>
<citsent citstr=" P93-1004 ">
(matsumoto et al , 1993) <papid> P93-1004 </papid>alignsource,target?</citsent>
<aftsection>
<nextsent>dependency trees, with view to resolve parsing ambiguities, but their approach can not deal with complex or compound sentences.other researchers (imamura, 2001) also use phrase alignment in parsing but in dot the translation fragments are already in the form of parse-trees.
</nextsent>
<nextsent>(eisner, 2003) <papid> P03-2041 </papid>outlines computationally expensive structural manipulation tool which he has used forintra-lingual translation but has yet to apply to inter lingual translation.</nextsent>
<nextsent>(gildea, 2003) <papid> P03-1011 </papid>performs tree-to tree alignment, but treats it as part of generative statistical translation model, rather than seperate task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2776">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(matsumoto et al , 1993) <papid> P93-1004 </papid>alignsource,target?</prevsent>
<prevsent>dependency trees, with view to resolve parsing ambiguities, but their approach can not deal with complex or compound sentences.other researchers (imamura, 2001) also use phrase alignment in parsing but in dot the translation fragments are already in the form of parse-trees.</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
(eisner, 2003) <papid> P03-2041 </papid>outlines computationally expensive structural manipulation tool which he has used forintra-lingual translation but has yet to apply to inter lingual translation.</citsent>
<aftsection>
<nextsent>(gildea, 2003) <papid> P03-1011 </papid>performs tree-to tree alignment, but treats it as part of generative statistical translation model, rather than seperate task.</nextsent>
<nextsent>the method of (ding et al , 2003) can cope with limited amount of non-isomorphism, but the algorithm is only suitable for use with dependency trees.we develop novel algorithm which automatically aligns translation ally equivalent tree fragments in fast and consistent fashion, and which requires little or no knowledge of the language pair.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2778">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dependency trees, with view to resolve parsing ambiguities, but their approach can not deal with complex or compound sentences.other researchers (imamura, 2001) also use phrase alignment in parsing but in dot the translation fragments are already in the form of parse-trees.
</prevsent>
<prevsent>(eisner, 2003) <papid> P03-2041 </papid>outlines computationally expensive structural manipulation tool which he has used forintra-lingual translation but has yet to apply to inter lingual translation.</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
(gildea, 2003) <papid> P03-1011 </papid>performs tree-to tree alignment, but treats it as part of generative statistical translation model, rather than seperate task.</citsent>
<aftsection>
<nextsent>the method of (ding et al , 2003) can cope with limited amount of non-isomorphism, but the algorithm is only suitable for use with dependency trees.we develop novel algorithm which automatically aligns translation ally equivalent tree fragments in fast and consistent fashion, and which requires little or no knowledge of the language pair.
</nextsent>
<nextsent>our approach is similar to that of (menezes and richardson, 2003), who use best-first approach to align dependency-type tree structures.
</nextsent>
<nextsent>we conduct number of experiments on the english-french section of the xerox home centre corpus.
</nextsent>
<nextsent>using the manual alignment of (hearne and way, 2003) as gold standard?, we show that our algorithm identifies sub-structural translationalequivalences with 73.7% precision and 67.84% recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2788">
<title id=" C04-1154.xml">robust sub sentential alignment of phrase structure trees </title>
<section> our algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>ps trees and outputs mapping between the nodes of the tree pair.
</prevsent>
<prevsent>as with the majority of previous approaches, the algorithm starts by finding lexical correspondences between the source and target trees.
</prevsent>
</prevsection>
<citsent citstr=" C94-2178 ">
our lexicon is built automatically using previously developed word aligner based on the k-vec aligner as outlined by (fung &amp; church, 1994).<papid> C94-2178 </papid></citsent>
<aftsection>
<nextsent>this lexical aligner uses combination of automatically extracted cognate information, mutual information and probabilistic measures to obtain one-to-one lexical correspondences between the source and target strings.
</nextsent>
<nextsent>during lexical alignment, function words are excluded because, as they are the most common words in language, they tend to co-occur frequently with the content words they precede.
</nextsent>
<nextsent>this can lead to the incorrect alignment of content words with function words.
</nextsent>
<nextsent>the algorithm then proceeds from the aligned lexical terminal nodes in bottom-up fashion, using mixture of node label matching and structural information to perform language-independent linking between all source,target?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2789">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>syntactic simplification is an nlp task, the goal ofwhich is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content.
</prevsent>
<prevsent>text simplification is useful task for varied reasons.
</prevsent>
</prevsection>
<citsent citstr=" C96-2183 ">
chandrasekar et al(1996) <papid> C96-2183 </papid>viewed text simplification as preprocessing tool to improve the performance of their parser.</citsent>
<aftsection>
<nextsent>the pset project (carroll et al, 1999), on the other hand, focused its research on simplifying newspaper text for aph asics, who have trouble with long sentences and complicated grammatical constructs.
</nextsent>
<nextsent>we have previously (siddharthan, 2002; siddharthan,2003) developed shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction.
</nextsent>
<nextsent>in this paper, we explore the use of syntactic simplification in multi-document summarization.
</nextsent>
<nextsent>1.1 sentence shortening for summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2790">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent># richard sullivan faced pointed questioning.
</prevsent>
<prevsent># richard sullivan faced pointed questioning from republicans during day on stand in senate fundraising investigation.grefenstette (1998) provided rule based approach to telegraphic reduction of the kind illustrated above.
</prevsent>
</prevsection>
<citsent citstr=" A00-1043 ">
since then, jing (2000), <papid> A00-1043 </papid>riezler etal.</citsent>
<aftsection>
<nextsent>(2003) and knight and marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences.
</nextsent>
<nextsent>these sentence-shortening approaches have been evaluated by comparison with human-shortenedsentences and have been shown to compare favorably.
</nextsent>
<nextsent>however, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in summary.
</nextsent>
<nextsent>recently, lin (2003) <papid> W03-1101 </papid>showed that statistical sentence-shortening approaches like knight and marcu (2000) do not improve content selection in summaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2791">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these sentence-shortening approaches have been evaluated by comparison with human-shortenedsentences and have been shown to compare favorably.
</prevsent>
<prevsent>however, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in summary.
</prevsent>
</prevsection>
<citsent citstr=" W03-1101 ">
recently, lin (2003) <papid> W03-1101 </papid>showed that statistical sentence-shortening approaches like knight and marcu (2000) do not improve content selection in summaries.</citsent>
<aftsection>
<nextsent>indeed he reported that syntax-based sentence-shortening resulted insignificantly worse content selection by their extractive summarizerneats.
</nextsent>
<nextsent>lin (2003) <papid> W03-1101 </papid>concluded that pure syntax based compression does not improve overall sum marizer performance, even though the compression algorithm performs well at the sentence level.</nextsent>
<nextsent>1.2 simplifying syntax for summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2795">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> the summarizer.  </section>
<citcontext>
<prevsection>
<prevsent>our hypothesis is that simplifying parenthetical units (relative clauses and appositives)will improve the performance of our clustering algorithm, by preventing it from clustering on the basis of background information.
</prevsent>
<prevsent>2.1 simplification and clustering.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
we use sim finder (hatzivassiloglou et al, 1999) <papid> W99-0625 </papid>for sentence clustering and its similarity metric to evaluate cluster quality; sim finder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy andn-gram matches.</citsent>
<aftsection>
<nextsent>we use the average of the simvals for each pair of sentences in cluster to evaluate quality-score for the cluster.
</nextsent>
<nextsent>table 1 below shows the quality-scores averaged over all clusters when the original document set is and is not preprocessed using our syntactic simplification software (described in $ 2.2).
</nextsent>
<nextsent>we use 30 document sets from the 2003 document understanding conference (see $ 3.1 for description).
</nextsent>
<nextsent>for each of the experiments in table 1, sim finder produced around 1500 clusters, with an average cluster size beween 3.6 and 3.8.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2797">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in duc03, the task was to generate 100 word summaries, while in duc04, the limit was changed to 665 bytes.
</prevsent>
<prevsent>3.2 evaluation metric.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
we evaluated our summarizer on the duc test sets using the rouge automatic scoring metric (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>the experiments in lin and hovy(2003) <papid> N03-1020 </papid>show that among n-gram approaches to scoring, rouge-1 (based on unigrams) has the highest correlation with human scores.</nextsent>
<nextsent>in 2004, an additional automatic metric based on longest common sub sequence was included (rouge-l), that aims to overcome some deficiencies of rouge-1, such asits susceptibility to ungrammatical keyword packing by dishonest summarizers2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2799">
<title id=" C04-1129.xml">syntactic simplification for improving content selection in multi document summarization </title>
<section> reference regeneration.  </section>
<citcontext>
<prevsection>
<prevsent>abdullah ocalan apw19981106.1119: [ir] abdullah ocalan; [ap] leader of the outlawed kurdistan worker party; [co] ocalan;apw19981104.0265: [ir] kurdish rebel leader abdullah ocalan; [rc] who is wanted in turkey on charges of heading terrorist organization; [co] ocalan; [rc] who leads the banned kurdish workers party , or pkk , which has been fighting for kurdish autonomy in turkey since 1984; [co] ocalan; [co] ocalan; [co] ocalan; apw19981113.0541: [ir] abdullah ocalan; [ap] leader of kurdish insurgents; [rc ] who has been sought for years by turkey; [co] ocalan; [co] ocalan; [co] ocalan; [pr] he; [co] ocalan; [co] ocalan; [pr] his; [co] ocalan; [co] ocalan; [co] ocalan; [pr] his; [co] ocalan; [co] ocalan; [ap] political science dropout from ankara university in 1978; apw19981021.0554: [ir] rebel leader abdullah ocalan; [pr] he; [co] ocalan; figure 1: example information collected for entities in the input.
</prevsent>
<prevsent>the canonic form of the named entity is shown in bold and the input article id in italic.
</prevsent>
</prevsection>
<citsent citstr=" N03-2024 ">
ir stands for initial reference?, co for subsequent noun co-reference, pr for pronoun reference, ap for apposition and rc for relative clause.we automatically post-edited our summaries using modified version of the module described in nenkova and mckeown (2003).<papid> N03-2024 </papid></citsent>
<aftsection>
<nextsent>this module normalizes references to people in the summary, by introducing them in detail when they are first mentioned and using short reference for subsequent mentions; these operations were shown to improve the readability of the resulting summaries.nenkova and mckeown (2003) <papid> N03-2024 </papid>avoided including parentheticals due to both the unavailability offast and reliable identification and attachment of ap positives and relative clauses, and theoretical issues relating to the selection of the most suitable parenthetical unit in the new summary context.</nextsent>
<nextsent>in orderto ensure balanced inclusion of parenthetical information in our summaries, we modified their initial approach to allow for including relative clauses and appositives in initial references.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2804">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> annotating temporal information.  </section>
<citcontext>
<prevsection>
<prevsent>while the semantics of temporal markers and the temporal structure of discourse are well-developed subjects informal linguistics (steedman, 1997),investigation of quantifiable annotation of unrestricted texts is somewhat recent topic.
</prevsent>
<prevsent>the issue has started to generate some interest in computational linguistics (harper et al, 2001), as it is potentially an important component in information extraction or question-answer systems.
</prevsent>
</prevsection>
<citsent citstr=" W01-1312 ">
a few tasks can be distinguished in that respect: ? detecting dates and temporal markers ? detecting event descriptions ? finding the date of events described ? figuring out the temporal relations between events in text the first task is not too difficult when looking for dates, e.g. using regular expressions (wilson et al., 2001), <papid> W01-1312 </papid>but requires some syntactic analysis in larger framework (vazov, 2001; <papid> W01-1314 </papid>shilder and habel,2001).</citsent>
<aftsection>
<nextsent>the second one raises more difficult, onto logical questions; what counts as an event is not uncontroversial (setzer, 2001): attitude reports, such as beliefs, or reported speech have an unclear status in that respect.
</nextsent>
<nextsent>the third task adds another level of complexity: lot of events described in text do not have an explicit temporal stamp, and it is not always possible to determine one, even when taking context into account (filatova and hovy, 2001).
</nextsent>
<nextsent>this leads to an approach more suited to the level ofunderspecification found in texts: annotating relations between events in symbolic way (e.g. that an event e1 is before another one e2).
</nextsent>
<nextsent>this is the path chosen by (katz and arosio, 2001; <papid> W01-1315 </papid>setzer, 2001) with human annotators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2805">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> annotating temporal information.  </section>
<citcontext>
<prevsection>
<prevsent>while the semantics of temporal markers and the temporal structure of discourse are well-developed subjects informal linguistics (steedman, 1997),investigation of quantifiable annotation of unrestricted texts is somewhat recent topic.
</prevsent>
<prevsent>the issue has started to generate some interest in computational linguistics (harper et al, 2001), as it is potentially an important component in information extraction or question-answer systems.
</prevsent>
</prevsection>
<citsent citstr=" W01-1314 ">
a few tasks can be distinguished in that respect: ? detecting dates and temporal markers ? detecting event descriptions ? finding the date of events described ? figuring out the temporal relations between events in text the first task is not too difficult when looking for dates, e.g. using regular expressions (wilson et al., 2001), <papid> W01-1312 </papid>but requires some syntactic analysis in larger framework (vazov, 2001; <papid> W01-1314 </papid>shilder and habel,2001).</citsent>
<aftsection>
<nextsent>the second one raises more difficult, onto logical questions; what counts as an event is not uncontroversial (setzer, 2001): attitude reports, such as beliefs, or reported speech have an unclear status in that respect.
</nextsent>
<nextsent>the third task adds another level of complexity: lot of events described in text do not have an explicit temporal stamp, and it is not always possible to determine one, even when taking context into account (filatova and hovy, 2001).
</nextsent>
<nextsent>this leads to an approach more suited to the level ofunderspecification found in texts: annotating relations between events in symbolic way (e.g. that an event e1 is before another one e2).
</nextsent>
<nextsent>this is the path chosen by (katz and arosio, 2001; <papid> W01-1315 </papid>setzer, 2001) with human annotators.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2806">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> annotating temporal information.  </section>
<citcontext>
<prevsection>
<prevsent>the third task adds another level of complexity: lot of events described in text do not have an explicit temporal stamp, and it is not always possible to determine one, even when taking context into account (filatova and hovy, 2001).
</prevsent>
<prevsent>this leads to an approach more suited to the level ofunderspecification found in texts: annotating relations between events in symbolic way (e.g. that an event e1 is before another one e2).
</prevsent>
</prevsection>
<citsent citstr=" W01-1315 ">
this is the path chosen by (katz and arosio, 2001; <papid> W01-1315 </papid>setzer, 2001) with human annotators.</citsent>
<aftsection>
<nextsent>this, in turn, raises new problems.
</nextsent>
<nextsent>first, what are the relations best suited to that task, among the many propositions (linguisticor logical) one can find for expressing temporal location ? then, how can an annotation be evaluated, between annotators, or between human annotator and an automated system ? such annotations can not be easy to determine automatically anyway, and must use some level of discourse modeling (cf.
</nextsent>
<nextsent>the work of (grover et al, 1995)).
</nextsent>
<nextsent>we want to show here the feasibility of such an effort, and we propose way of evaluating the success or failure of the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2808">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> a method for annotating temporal </section>
<citcontext>
<prevsection>
<prevsent>for each event associated to temporal adjunct, temporal relation is established (with date when possible).
</prevsent>
<prevsent>a set of discourse rules is used to establish possible relations between two events appearing consecutively in the text, according to the tenses of the verbs introducing the events.
</prevsent>
</prevsection>
<citsent citstr=" P93-1010 ">
these rules for french are similar to rules for english proposed in (grover et al, 1995; song and cohen, 1991; kameyama et al, 1993), <papid> P93-1010 </papid>but 1we have defined 89 rules, divided in 29 levels.</citsent>
<aftsection>
<nextsent>are expressed with allen relations instead of aset of ad hoc relations (see table 1 for subset of the rules).
</nextsent>
<nextsent>these rules are only applied when no temporal marker indicates specific relation between the two events.
</nextsent>
<nextsent>the last step consists in computing fixed pointon the graph of relations between events recognized in the text, and dates.
</nextsent>
<nextsent>we used classical path-consistency algorithm (allen, 1984).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2811">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> measuring success.  </section>
<citcontext>
<prevsection>
<prevsent>they are too numerous and someare too precise to be useful alone, and it is probably dangerous to ask for disjunctive information.
</prevsent>
<prevsent>but we still want to have annotation relations with aclear semantics, that we could link to allens algebra to infer and compare information about temporal situations.
</prevsent>
</prevsection>
<citsent citstr=" W01-1305 ">
so we have chosen relations similar to that of (bruce, 1972) (as in (li et al, 2001)), <papid> W01-1305 </papid>who inspired allen; these relations are equivalent to certain sets of allen relations, as shown table 2.</citsent>
<aftsection>
<nextsent>we thought they were rather intuitive, seem to have an appropriate level of granularity, and since three of them are enough to describe situations (the other 3 being the converse relations), they are not to hard to use by naive annotators.to abstract away from particulars of given annotation for some text, and thus to be able to compare the underlying temporal model described by an annotation, we try to measure similarity between annotations given by system and human annotations, from the saturated graph of detected temporal relations in each case (the human graph is saturated after annotation relations have been translated as equivalent dis junctions of allen relations).
</nextsent>
<nextsent>we do not want to limit the comparison to  simple  (base) relations, as in (setzer, 2001), because it makes the evaluation very dependent on the choice of relations, and we also want to have gradual measure of the imprecision of the system annotation.
</nextsent>
<nextsent>for instance, finding there is  before or during  relation between two events is better than proposing  after  is the human put down  before , and it is less good before ? ? (i before ?
</nextsent>
<nextsent>((i j) ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2813">
<title id=" C04-1008.xml">annotating and measuring temporal relations in texts </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>however,given the method we adopted, one could have expected better coherence results than finesse results.it means we have made decisions that were not cautious enough, for reasons we still have to analyze.one potential reason is that relations offered to humans are maybe too vague in the wrong places: lot of information in text can be asserted to be  strictly before  something else (based on dates for instance), while human annotators can only say that events are  before or meets  some other event; each time this is the case, coherence is only 0.5.
</prevsent>
<prevsent>it is important to note that there are few points of comparison on this problem.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
to the best of our knowledge, only (li et al, 2001) <papid> W01-1305 </papid>and (mani and wilson, 2000) <papid> P00-1010 </papid>mention having tried this kind of annotation, as side job for their temporal expressionsmark-up systems.</citsent>
<aftsection>
<nextsent>the former considers only relations between events within sentence, and the latter did not evaluate their method.finally, it is worth remembering that human annotation itself is difficult task, with potentially lot of disagreement between annotators.
</nextsent>
<nextsent>for now, our texts have been annotated by the two authors, with an posteriori resolution of conflicts.
</nextsent>
<nextsent>we therefore have no measure of inter-annotator agreement which could serve as an upper bound of the performance of the system, although we are planning to do this at later stage.
</nextsent>
<nextsent>the aim of this study was to show the feasibility of annotating temporal relations in text and to propose methodology for the task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2814">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments on chinese-to-english nist and iwslt tasks show that all three methods obtain consistent improvements.
</prevsent>
<prevsent>moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 bleu-score on iwslt06, 0.57 on nist03, 0.61 on nist05 test set respectively.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
state-of-the-art statistical machine translation (smt) systems usually adopt two-pass search strategy (och, 2003; <papid> P03-1021 </papid>koehn, et al, 2003) <papid> N03-1017 </papid>as shown in figure 1.</citsent>
<aftsection>
<nextsent>in the first pass, decoding algorithm is applied to generate an n-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the n-best translations through additional feature functions.
</nextsent>
<nextsent>the fundamental assumption behind using second pass is that the generated n-best list may contain better transla?
</nextsent>
<nextsent>2008.
</nextsent>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2815">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experiments on chinese-to-english nist and iwslt tasks show that all three methods obtain consistent improvements.
</prevsent>
<prevsent>moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.81 bleu-score on iwslt06, 0.57 on nist03, 0.61 on nist05 test set respectively.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
state-of-the-art statistical machine translation (smt) systems usually adopt two-pass search strategy (och, 2003; <papid> P03-1021 </papid>koehn, et al, 2003) <papid> N03-1017 </papid>as shown in figure 1.</citsent>
<aftsection>
<nextsent>in the first pass, decoding algorithm is applied to generate an n-best list of translation hypotheses, while in the second pass, the final translation is selected by rescoring and re-ranking the n-best translations through additional feature functions.
</nextsent>
<nextsent>the fundamental assumption behind using second pass is that the generated n-best list may contain better transla?
</nextsent>
<nextsent>2008.
</nextsent>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2816">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the n-best list (uef fing, 2003; chen et al, 2005; zens and ney, 2006).
</prevsent>
<prevsent>in this two-pass method, translation performance hinges on the n-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other mt systems to these hypotheses could potentially improve the performance.
</prevsent>
</prevsection>
<citsent citstr=" E06-1005 ">
this technique is called system combination (bangalore et al, 2001; matusov et al, 2006; <papid> E06-1005 </papid>sim et al, 2007; rosti et al, 2007<papid> P07-1040 </papid>a; rosti et al, 2007<papid> P07-1040 </papid>b).</citsent>
<aftsection>
<nextsent>we have instead chosen to regenerate new hypotheses from the original n-best list, technique which we call regeneration.
</nextsent>
<nextsent>regeneration is an intermediate pass between decoding and rescoring as depicted in figure 2.
</nextsent>
<nextsent>given the original n-best list (n-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another n-best list (n-best2).
</nextsent>
<nextsent>these two n-best lists are then combined and given to the rescoring pass to derive the best translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2818">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, rescoring allows some feature functions, such as word and n-gram posterior probabilities, to be estimated on the n-best list (uef fing, 2003; chen et al, 2005; zens and ney, 2006).
</prevsent>
<prevsent>in this two-pass method, translation performance hinges on the n-best hypotheses that are generated in the first pass (since rescoring occurs on these), so adding the translation candidates generated by other mt systems to these hypotheses could potentially improve the performance.
</prevsent>
</prevsection>
<citsent citstr=" P07-1040 ">
this technique is called system combination (bangalore et al, 2001; matusov et al, 2006; <papid> E06-1005 </papid>sim et al, 2007; rosti et al, 2007<papid> P07-1040 </papid>a; rosti et al, 2007<papid> P07-1040 </papid>b).</citsent>
<aftsection>
<nextsent>we have instead chosen to regenerate new hypotheses from the original n-best list, technique which we call regeneration.
</nextsent>
<nextsent>regeneration is an intermediate pass between decoding and rescoring as depicted in figure 2.
</nextsent>
<nextsent>given the original n-best list (n-best1) generated by the decoder, this regeneration pass creates new translation hypotheses from this list to form another n-best list (n-best2).
</nextsent>
<nextsent>these two n-best lists are then combined and given to the rescoring pass to derive the best translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2905">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> smt process.  </section>
<citcontext>
<prevsection>
<prevsent>the original n-best translations list (n best1) is expanded to generate new n-best translations list (n-best2) before the rescoring pass.
</prevsent>
<prevsent>phrase-based statistical machine translation systems are usually modeled through log-linear framework (och and ney, 2002).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
by introducing the hidden word alignment variable (brown et al., 1993), <papid> J93-2003 </papid>the optimal translation can be searched for based on the following criterion: * 1, arg max( ( , , )) m mme e h?== f a?</citsent>
<aftsection>
<nextsent>  (1) where is string of phrases in the target language, e f a is the source language string of phrases, e are feature functions, weights ( , , )m m?
</nextsent>
<nextsent>are typically optimized to maximize the scoring function (och, 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>our mt baseline system is based on moses decoder (koehn et al, 2007) <papid> P07-2045 </papid>with word alignment obtained from giza++ (och et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2907">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> smt process.  </section>
<citcontext>
<prevsection>
<prevsent>  (1) where is string of phrases in the target language, e f a is the source language string of phrases, e are feature functions, weights ( , , )m m?
</prevsent>
<prevsent>are typically optimized to maximize the scoring function (och, 2003).<papid> P03-1021 </papid></prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
our mt baseline system is based on moses decoder (koehn et al, 2007) <papid> P07-2045 </papid>with word alignment obtained from giza++ (och et al, 2003).</citsent>
<aftsection>
<nextsent>the translation model (tm), lexicalized word reordering model (rm) are trained using the tools provided in the open source moses package.
</nextsent>
<nextsent>language model (lm) is trained with srilm toolkit (stolcke, 2002) with modified kneser ney smoothing method (chen and goodman, 1998).
</nextsent>
<nextsent>given the original n-best translations, regeneration pass is to generate new target translations which are not seen in the original n-best choices.
</nextsent>
<nextsent>3.1 regeneration with re-decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2955">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> regeneration methods.  </section>
<citcontext>
<prevsection>
<prevsent>its about 5 minutes?
</prevsent>
<prevsent>to walk . 4.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
i walk 5 minutes . ? its 5 minutes on foot . alignments it 5 minutes on foot . is aligning words: as confusion network can be easily built from one-to-one alignment, we develop our algorithm based on the one-to-one assumption and use competitive linking algorithm (melamed, 2000) <papid> J00-2004 </papid>for our word alignment.</citsent>
<aftsection>
<nextsent>firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned.
</nextsent>
<nextsent>then greedy algorithm is applied to select the best word-alignment.
</nextsent>
<nextsent>in this paper, we use linear combination of multiple association scores, as suggested in (kraif and chen, 2004).<papid> C04-1183 </papid></nextsent>
<nextsent>as the two sentences to be aligned are in the same language, the association scores are computed on the following four clues.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2956">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> regeneration methods.  </section>
<citcontext>
<prevsection>
<prevsent>firstly, an association score is computed for every possible word pair from the skeleton and sentence to be aligned.
</prevsent>
<prevsent>then greedy algorithm is applied to select the best word-alignment.
</prevsent>
</prevsection>
<citsent citstr=" C04-1183 ">
in this paper, we use linear combination of multiple association scores, as suggested in (kraif and chen, 2004).<papid> C04-1183 </papid></citsent>
<aftsection>
<nextsent>as the two sentences to be aligned are in the same language, the association scores are computed on the following four clues.
</nextsent>
<nextsent>they are cog nate (s abou tits 5 minutes?
</nextsent>
<nextsent>to walk . 1), word class (s2), synonyms (s3), and position difference (s4).
</nextsent>
<nextsent>the four scores are linearly combined with empirically determined weights as shown is equation 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2957">
<title id=" C08-1014.xml">regenerating hypotheses for statistical machine translation </title>
<section> experiments data chinese english.  </section>
<citcontext>
<prevsection>
<prevsent>reference no tax is needed for this item . thank you . resc2 you don have to do not need to pay duty on this . thank you . 1 comb (rd) not need to pay duty on this . thank you . reference certainly . the fitting room is over there . please come with me . resc2 the fitting room is over there . can you come with me . 2 comb (ne) yes , you can . the fitting room is over there . please come with me . reference ok . will bring it to you in five minutes . resc2 good five minutes , we will give you . 3 comb (cn) ok . after five minutes , will give it to you . table 5: translations output by system resc2 and comb on iwslt task (case-insensitive).
</prevsent>
<prevsent>best from 4,000 (1,600 + 3 800) distinct hypotheses.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
our evaluation metrics are bleu (papineni et al., 2002) <papid> P02-1040 </papid>and nist, which are to perform case insensitive matching of n-grams up to = 4.</citsent>
<aftsection>
<nextsent>the translation performance of iwslt task and nist task is reported in tables 3 and 4 respectively.
</nextsent>
<nextsent>the row 1-best?
</nextsent>
<nextsent>reports the scores of the translations produced by the decoder.
</nextsent>
<nextsent>the column ?#hypo?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2958">
<title id=" C04-1109.xml">discriminative slot detection using kernel methods </title>
<section> a discriminative framework  </section>
<citcontext>
<prevsection>
<prevsent>in the training phase, the target slot fillers are labeled in the text so that svm slot detectors can be trained through the kernels to find fillers for the key slots of events.
</prevsent>
<prevsent>in the testing phase, the svm classifier will predict the slot fillers from unlabeled text and merging procedure will merge slots into events if necessary.
</prevsent>
</prevsection>
<citsent citstr=" W01-1511 ">
the main kernel we propose to use is on glarf (meyers et al, 2001) <papid> W01-1511 </papid>dependency graphs.</citsent>
<aftsection>
<nextsent>fig 1.
</nextsent>
<nextsent>structure of the discriminative model the idea is that an ie model should not commit itself to any syntactic level.
</nextsent>
<nextsent>the low level information, such as word collocations, may also give us important clues.
</nextsent>
<nextsent>our experimentation will show that for the muc-6 management succession domain, even bag-of-words or n-grams can give us helpful information about event occurrence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2961">
<title id=" C04-1109.xml">discriminative slot detection using kernel methods </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>accuracy precision recall f-score per_in 85.7% 51.2% 64.1% per_out 78.4% 58.6% 67.1% post 83.3% 59.7% 69.5% table 5.
</prevsent>
<prevsent>slot performance of the rule-based proteus system for muc-6.
</prevsent>
</prevsection>
<citsent citstr=" P03-1028 ">
(chieu et al, 2003) <papid> P03-1028 </papid>reported feature-based svm system (alice) to extract muc-4 events of 7 terrorist attacks.</citsent>
<aftsection>
<nextsent>the alice-me system demonstrated competitive performance with rule based systems.
</nextsent>
<nextsent>the features used by alice are mainly from parsing.
</nextsent>
<nextsent>comparing with alice, our system uses kernels on dependency graphs to replace explicit features, an approach which is fully automatic and requires no enumeration of features.
</nextsent>
<nextsent>the model we proposed can combine information from different syntactic levels in principled ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2962">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> theory.  </section>
<citcontext>
<prevsection>
<prevsent>this vector defines the parameters of categorical or multinomial probability distribution, giving useful probabilistic interpretation of the distributional model.
</prevsent>
<prevsent>as the vector for each target word must sum to 1, the marginal distributions of target words have little effect on the resulting similarity estimates.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
many 649 similarity measures and weighting functions have been proposed for distributional vectors; comparative studies include lee (1999), <papid> P99-1004 </papid>curran (2003) and weeds and weir (2005).<papid> J05-4002 </papid></citsent>
<aftsection>
<nextsent>2.2 kernel methods for computing.
</nextsent>
<nextsent>similarity and distance in this section we describe two classes of functions, positive semi-definite and negative semi definite kernels, and state some relationships between these classes.
</nextsent>
<nextsent>the mathematical treatment follows berg et al  (1984).
</nextsent>
<nextsent>a good general introduction to kernels and support vector machines is the book by cristianini and shawe-taylor (2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2963">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> theory.  </section>
<citcontext>
<prevsection>
<prevsent>this vector defines the parameters of categorical or multinomial probability distribution, giving useful probabilistic interpretation of the distributional model.
</prevsent>
<prevsent>as the vector for each target word must sum to 1, the marginal distributions of target words have little effect on the resulting similarity estimates.
</prevsent>
</prevsection>
<citsent citstr=" J05-4002 ">
many 649 similarity measures and weighting functions have been proposed for distributional vectors; comparative studies include lee (1999), <papid> P99-1004 </papid>curran (2003) and weeds and weir (2005).<papid> J05-4002 </papid></citsent>
<aftsection>
<nextsent>2.2 kernel methods for computing.
</nextsent>
<nextsent>similarity and distance in this section we describe two classes of functions, positive semi-definite and negative semi definite kernels, and state some relationships between these classes.
</nextsent>
<nextsent>the mathematical treatment follows berg et al  (1984).
</nextsent>
<nextsent>a good general introduction to kernels and support vector machines is the book by cristianini and shawe-taylor (2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2968">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> practice.  </section>
<citcontext>
<prevsection>
<prevsent>the number of optimisation folds differed according to the size of the dataset and the number of training-test splits to be eval uated: we used 10 folds for the compound task, leave-one-out cross-validation for semeval task 4 and 25 folds for the verb classification task.
</prevsent>
<prevsent>3.2 compound noun interpretation.
</prevsent>
</prevsection>
<citsent citstr=" J06-3003 ">
the task of interpreting the semantics of noun compounds is one which has recently received considerable attention (lauer, 1995; girju et al ,2005; turney, 2006).<papid> J06-3003 </papid></citsent>
<aftsection>
<nextsent>forgiven noun-noun compound, the problem is to identify the semantic relation between the compounds constituents ? that kitchen knife is knife used in kitchen but steel knife is knife made of steel.
</nextsent>
<nextsent>2 the difficulty of the task is due to the fact that the knowledge required to interpret compounds is not made explicit in the contexts where they appear, and hence standard context-based methods for classifying semantic relations in text cannot be applied.
</nextsent>
<nextsent>most previous work making use of lexical similarity has been based on wordnet measures (kim and baldwin, 2005; girju et al , 2005).
</nextsent>
<nextsent>o seaghdha and copestake (2007) were to our knowledge the first to apply distributional model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2969">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> practice.  </section>
<citcontext>
<prevsection>
<prevsent>we extracted two feature sets from two very different corpora.
</prevsent>
<prevsent>the first is the 90 million word written component of the british national corpus (burnard, 1995).
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
this corpus was parsed with the rasp parser (briscoeet al , 2006) <papid> P06-4020 </papid>and all instances of the conj grammatical relation were counted.</citsent>
<aftsection>
<nextsent>the co-occurrence vocabulary c was set to the 10,000 words most frequently entering into conj relation across the corpus.
</nextsent>
<nextsent>the second corpus we used was the web 1t 5-gram corpus (brants and franz, 2006), which contains frequency counts for n-grams upto length 5 extracted from googles index of approximately 1 trillion words of web text.
</nextsent>
<nextsent>as the nature of this corpus precludes parsing, we used asimple pattern-based technique to extract conjunctions.
</nextsent>
<nextsent>an n-gram was judged to contain conjunction co-occurrence between i and jif it con 3 this dataset is available from http://www.cl.cam.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2970">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> practice.  </section>
<citcontext>
<prevsection>
<prevsent>the results are presented in table 3.
</prevsent>
<prevsent>again, the jsd kernels outperform the standard 2 kernels by considerable margin.
</prevsent>
</prevsection>
<citsent citstr=" W07-0730 ">
the best performing feature-kernel combination achieves 71.4% accuracy and 68.8% f-score, higher thanthe best performance attained in the semeval competition without using wordnet similarity measures (accuracy = 67.0%, f-score = 65.1%; nakov and hearst (2007)).<papid> W07-0730 </papid></citsent>
<aftsection>
<nextsent>this is also higher than the performance of all but three of the 14 semeval entries which did use wordnet.
</nextsent>
<nextsent>davidov and rappoport (2008) <papid> P08-1027 </papid>have recently described wordnet free method that attains slightly lower accuracy (70.1%) and slightly higher f-score (70.6%) thanour method.</nextsent>
<nextsent>taken together, davidov and rap poports results and ours define the current state of the art on this task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2971">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> practice.  </section>
<citcontext>
<prevsection>
<prevsent>the best performing feature-kernel combination achieves 71.4% accuracy and 68.8% f-score, higher thanthe best performance attained in the semeval competition without using wordnet similarity measures (accuracy = 67.0%, f-score = 65.1%; nakov and hearst (2007)).<papid> W07-0730 </papid></prevsent>
<prevsent>this is also higher than the performance of all but three of the 14 semeval entries which did use wordnet.</prevsent>
</prevsection>
<citsent citstr=" P08-1027 ">
davidov and rappoport (2008) <papid> P08-1027 </papid>have recently described wordnet free method that attains slightly lower accuracy (70.1%) and slightly higher f-score (70.6%) thanour method.</citsent>
<aftsection>
<nextsent>taken together, davidov and rap poports results and ours define the current state of the art on this task.
</nextsent>
<nextsent>3.4 verb classification.
</nextsent>
<nextsent>to investigate the effectiveness of distributional kernels on different kind of semantic classification task, we tested our methods on the verb class data of sun et al  (2008).
</nextsent>
<nextsent>this dataset consists of 204 verbs assigned to 17 of levins (1993) verb classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2973">
<title id=" C08-1082.xml">semantic classification with distributional kernels </title>
<section> discussion and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>we seem to be the first touse distributional kernels for semantic classification and to note their connection with familiar lexical similarity measures.
</prevsent>
<prevsent>indeed, the only research we are aware of on kernels tailored for lexical similarity is the small body of work on wordnet kernels, e.g., basili et al  (2006).
</prevsent>
</prevsection>
<citsent citstr=" P05-1050 ">
in contrast, support vector machines have been widely adopted for computational semantic tasks, from word sense disambiguation (gliozzo et al , 2005) <papid> P05-1050 </papid>to semantic role labelling (pradhan et al , 2004).</citsent>
<aftsection>
<nextsent>the standard feature sets for semantic role labelling and many other tasks are collections of heterogeneous features that do not correspond to probability distributions.
</nextsent>
<nextsent>so long as the features are restricted to positive values, distributional kernels can be ap plied; it will be interesting (and informative) to see whether they retain their superiority in this setting.
</nextsent>
<nextsent>one advantage of kernel methods is that kernels can be defined for non-vectorial data structures such as strings, trees, graphs and sets.
</nextsent>
<nextsent>a promising topic of future research is the design of distributional kernels for comparing structured objects, based on the feature space embedding associated with convolution kernels (haussler, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2974">
<title id=" C04-1059.xml">language model adaptation for statistical machine translation via structured query models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in statistical machine translation we have similar situation, i.e. estimate the model parameter from some data, and use the system to translate sentences which may not be well covered by the training data.
</prevsent>
<prevsent>therefore, the potential of adaptation techniques needs to be explored for machine translation applications.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical machine translation is based on the noisy channel model, where the translation hypothesis is searched over the space defined by translation model and target language (brown et al, 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>statistical machine translation can be formulated as follows: )()|(maxarg)|(maxarg* tptspstpt tt ?== where is the target sentence, and is the source sentence.
</nextsent>
<nextsent>p(t) is the target language model and p(s|t) is the translation model.
</nextsent>
<nextsent>the argmax operation is the search, which is done by the decoder.
</nextsent>
<nextsent>in the current study we modify the target language model p(t), to represent the test data better, and thereby improve the translation quality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2975">
<title id=" C04-1059.xml">language model adaptation for statistical machine translation via structured query models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>experiments are carried out on standard statistical machine translation task defined in the nist evaluation in june 2002.
</prevsent>
<prevsent>there are 878 test sentences in chinese, and each sentence has four human translations as references.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
nist score (nist 2002) and bleu score (papineni et. al. 2002) <papid> P02-1040 </papid>of mteval version 9 are reported to evaluate the translation quality.</citsent>
<aftsection>
<nextsent>4.1 baseline translation system.
</nextsent>
<nextsent>our baseline system (vogel et al, 2003) gives scores of 7.80 nist and 0.1952 bleu for top-1 hypothesis, which is comparable to the best results reported on this task.
</nextsent>
<nextsent>for the baseline system, we built translation model using 284k parallel sentence pairs, and trigram language model from 160 million words general english news text collection.
</nextsent>
<nextsent>this lm is the background model to be adapted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2976">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to build semantic lexicon, one has to identify the relation between words within semantic hierarchy, and to group similar words together into class.
</prevsent>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</citsent>
<aftsection>
<nextsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></nextsent>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2977">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to build semantic lexicon, one has to identify the relation between words within semantic hierarchy, and to group similar words together into class.
</prevsent>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</citsent>
<aftsection>
<nextsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></nextsent>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2978">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to build semantic lexicon, one has to identify the relation between words within semantic hierarchy, and to group similar words together into class.
</prevsent>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</citsent>
<aftsection>
<nextsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></nextsent>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2979">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to build semantic lexicon, one has to identify the relation between words within semantic hierarchy, and to group similar words together into class.
</prevsent>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
</prevsection>
<citsent citstr=" W02-1028 ">
one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</citsent>
<aftsection>
<nextsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></nextsent>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2980">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
<prevsent>one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</prevsent>
</prevsection>
<citsent citstr=" W97-0803 ">
another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></citsent>
<aftsection>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
<nextsent>ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2981">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous work on automatic methods for building semantic lexicons could be divided into two main groups.
</prevsent>
<prevsent>one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</prevsent>
</prevsection>
<citsent citstr=" W04-2103 ">
another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></citsent>
<aftsection>
<nextsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</nextsent>
<nextsent>ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2982">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>one is automatic thesaurus acquisition, that is, to identify synonyms or topically related words from corpora based on various measures of similarity (e.g. riloff and shepherd, 1997; <papid> W97-0313 </papid>lin, 1998; <papid> P98-2127 </papid>caraballo, 1999; <papid> P99-1016 </papid>thelen and riloff, 2002; <papid> W02-1028 </papid>you and chen, 2006).</prevsent>
<prevsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</citsent>
<aftsection>
<nextsent>ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</nextsent>
<nextsent>for chinese, tseng (2003) <papid> P03-2011 </papid>proposed method based on morphological similarity to assign cilin category to unknown words from the sinica corpus which were not in the chinese electronic dictionary and cilin; but somehow the test data were taken from cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the sinica corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2983">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another line of research, which is more closely related to the current study, is to extend existing thesauri by classifying new words with respect to their given structures (e.g. tokunaga et al., 1997; <papid> W97-0803 </papid>pekar, 2004).<papid> W04-2103 </papid></prevsent>
<prevsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</prevsent>
</prevsection>
<citsent citstr=" W02-0903 ">
ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</citsent>
<aftsection>
<nextsent>for chinese, tseng (2003) <papid> P03-2011 </papid>proposed method based on morphological similarity to assign cilin category to unknown words from the sinica corpus which were not in the chinese electronic dictionary and cilin; but somehow the test data were taken from cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the sinica corpus.</nextsent>
<nextsent>kwong and tsou (2007) <papid> D07-1034 </papid>attempted to classify words distinctly used in beijing, hongkong, singapore, and taiwan, with respect to the cilin classificatory structure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2984">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>an early effort along this line is hearst (1992), <papid> C92-2082 </papid>who attempted to identify hyponyms from large text corpora, based on set of lexico-syntactic patterns, to augment and critique the content of wordnet.</prevsent>
<prevsent>ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</prevsent>
</prevsection>
<citsent citstr=" P03-2011 ">
for chinese, tseng (2003) <papid> P03-2011 </papid>proposed method based on morphological similarity to assign cilin category to unknown words from the sinica corpus which were not in the chinese electronic dictionary and cilin; but somehow the test data were taken from cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the sinica corpus.</citsent>
<aftsection>
<nextsent>kwong and tsou (2007) <papid> D07-1034 </papid>attempted to classify words distinctly used in beijing, hongkong, singapore, and taiwan, with respect to the cilin classificatory structure.</nextsent>
<nextsent>they brought up the issue of data heterogeneity in the task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2985">
<title id=" C08-1058.xml">extending a thesaurus with words from pan chinese sources </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ciaramita (2002) <papid> W02-0903 </papid>compared several models in classifying nouns with respect to simplified version of wordnet and signified the gain in performance with morphological features.</prevsent>
<prevsent>for chinese, tseng (2003) <papid> P03-2011 </papid>proposed method based on morphological similarity to assign cilin category to unknown words from the sinica corpus which were not in the chinese electronic dictionary and cilin; but somehow the test data were taken from cilin, and therefore could not really demonstrate the effectiveness with unknown words found in the sinica corpus.</prevsent>
</prevsection>
<citsent citstr=" D07-1034 ">
kwong and tsou (2007) <papid> D07-1034 </papid>attempted to classify words distinctly used in beijing, hongkong, singapore, and taiwan, with respect to the cilin classificatory structure.</citsent>
<aftsection>
<nextsent>they brought up the issue of data heterogeneity in the task.
</nextsent>
<nextsent>in general, automatic classification of words via similarity measurement between two words, or between word and class of words, was often done on words from similar data source, with the assumption that the feature vectors under comparison are directly comparable.
</nextsent>
<nextsent>in the pan-chinese context, however, the words to be classified come from corpora collected from various chinese speech communities, but the words in the thesaurus are often based on usages found in particular community, such as mainland china in the case of cilin.
</nextsent>
<nextsent>it is thus questionable whether the words in cilin would appear incomparable contexts in texts from other places, thus affecting the similarity measurement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2986">
<title id=" C02-1087.xml">self organizing classification on the reuters news corpus </title>
<section> dimensionality reduction.  </section>
<citcontext>
<prevsection>
<prevsent>words in the same synset have the same or similar concept and vice versa.
</prevsent>
<prevsent>in addition to synonymy, there are several different types of semantic relations such as antonymy, hyponymy, meronymy, troponomy, and entailment in each different syntactic category, i.e. nouns, verbs, adjectives and adverbs.
</prevsent>
</prevsection>
<citsent citstr=" W98-0706 ">
this semantic dictionary is useful in extracting the real concept of word, query or document in the field of text mining (richardson 1994; richardson and smeaton 1995; voorhees 1993; voorhees 1998; scott and matwin 1998; <papid> W98-0706 </papid>gonzalo et al 1998; <papid> W98-0705 </papid>moldovan and mihalcea 1998; moldovan and mihalcea 2000).</citsent>
<aftsection>
<nextsent>using these semantic relations in wordnet, one index word may present its many synonyms, siblings or other relevant words.
</nextsent>
<nextsent>therefore, by mapping words to more general concepts, wordnet can be used to reduce the dimensionality.
</nextsent>
<nextsent>instead of using these approaches to reduce multi-dimensional vectors, we apply significance vectors to present the importance of words in each semantic category and use pre-assigned topics as axes of multi-dimensional space.
</nextsent>
<nextsent>thus news article can be represented by n-dimension vector, where is the number of pre-assigned topics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2987">
<title id=" C02-1087.xml">self organizing classification on the reuters news corpus </title>
<section> dimensionality reduction.  </section>
<citcontext>
<prevsection>
<prevsent>words in the same synset have the same or similar concept and vice versa.
</prevsent>
<prevsent>in addition to synonymy, there are several different types of semantic relations such as antonymy, hyponymy, meronymy, troponomy, and entailment in each different syntactic category, i.e. nouns, verbs, adjectives and adverbs.
</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
this semantic dictionary is useful in extracting the real concept of word, query or document in the field of text mining (richardson 1994; richardson and smeaton 1995; voorhees 1993; voorhees 1998; scott and matwin 1998; <papid> W98-0706 </papid>gonzalo et al 1998; <papid> W98-0705 </papid>moldovan and mihalcea 1998; moldovan and mihalcea 2000).</citsent>
<aftsection>
<nextsent>using these semantic relations in wordnet, one index word may present its many synonyms, siblings or other relevant words.
</nextsent>
<nextsent>therefore, by mapping words to more general concepts, wordnet can be used to reduce the dimensionality.
</nextsent>
<nextsent>instead of using these approaches to reduce multi-dimensional vectors, we apply significance vectors to present the importance of words in each semantic category and use pre-assigned topics as axes of multi-dimensional space.
</nextsent>
<nextsent>thus news article can be represented by n-dimension vector, where is the number of pre-assigned topics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2988">
<title id=" C02-1087.xml">self organizing classification on the reuters news corpus </title>
<section> e21 government finance 42,573  </section>
<citcontext>
<prevsection>
<prevsent>table 9.
</prevsent>
<prevsent>accuracy without and with the help of wordnet 2-level hypernym on 100,000 full-text for training set method som som with wordnet 1 85.70% 94.21% 2 92.77% 98.95% table 10.
</prevsent>
</prevsection>
<citsent citstr=" W00-1104 ">
accuracy without and with the help of wordnet 2-level hypernym on 100,000 news titles for training set method som som with wordnet 1 88.85% 89.94% 2 91.07% 90.65% discussion and conclusion in the past there had been no consistent conclusions about the value of wordnet for information retrieval tasks (mihalcea and moldovan 2000).<papid> W00-1104 </papid></citsent>
<aftsection>
<nextsent>experiments performed using different methodologies led to various, sometime contradicting results (voorhees 1998).
</nextsent>
<nextsent>this is probably because extracting the concept of word is seriously dependent on other unambiguous words.
</nextsent>
<nextsent>text classification is mapping documents with similar concepts to cluster with more general concept.
</nextsent>
<nextsent>if vector label matches one of the original labels assigned by reuters, it is considered correct mapping.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2989">
<title id=" C08-1089.xml">a method for automatic pos guessing of chinese unknown words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>considering the features used during pos guessing, we have classified previous studies on pos guessing of unknown words into three types.
</prevsent>
<prevsent>the first type use only contextual features, including local context and global context.
</prevsent>
</prevsection>
<citsent citstr=" P06-1089 ">
for example, nakagawa and matsumoto (2006) <papid> P06-1089 </papid>proposed probabilistic model to guess the pos tags of unknown words by considering all the occurrences of unknown words with the same lexical form in document.</citsent>
<aftsection>
<nextsent>the parameters were estimated using gibbs sampling.
</nextsent>
<nextsent>they also attempted to apply the model to semi-supervised learning, and conducted experiments on multiple corpora.
</nextsent>
<nextsent>the highest precision in the chinese corpus of their experiments was 67.85%.
</nextsent>
<nextsent>the second type use only internal component features, such as that of chen and bai (1997) and wu and jiang (2000).<papid> W00-1207 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2990">
<title id=" C08-1089.xml">a method for automatic pos guessing of chinese unknown words </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they also attempted to apply the model to semi-supervised learning, and conducted experiments on multiple corpora.
</prevsent>
<prevsent>the highest precision in the chinese corpus of their experiments was 67.85%.
</prevsent>
</prevsection>
<citsent citstr=" W00-1207 ">
the second type use only internal component features, such as that of chen and bai (1997) and wu and jiang (2000).<papid> W00-1207 </papid></citsent>
<aftsection>
<nextsent>chen and bai (1997) examined all unknown nouns, verbs, and adjectives and reported 69.13% precision using dice metrics to measure the affix-category association strength and an affix-dependent entropy weighting scheme for determining the weightings between prefix-category and suffix-category associations.
</nextsent>
<nextsent>this approach is effective in processing derived words such as ???
</nextsent>
<nextsent>xiandai-hua (mod ernize), but performs poorly when encountering compounds such as ??
</nextsent>
<nextsent>baozhi (inflation-proof).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2997">
<title id=" C04-1150.xml">quantitative and qualitative evaluation of the ontolearn ontology learning system </title>
<section> evaluating ontologies.  </section>
<citcontext>
<prevsection>
<prevsent>both evaluation strategies are experimented in two application domains: tourism and economy.
</prevsent>
<prevsent>the subsequent section provides sketchy description of the ontolearn algorithms.
</prevsent>
</prevsection>
<citsent citstr=" J04-2002 ">
details are found in (navigli and velardi, 2004) <papid> J04-2002 </papid>and (navigli, velardi and gangemi, 2003).</citsent>
<aftsection>
<nextsent>sections 3 and 4 are dedicated to the quantitative and qualitative analyses of ontolearn.
</nextsent>
<nextsent>ontolearn is an ontology population method based on text mining and machine learning techniques.
</nextsent>
<nextsent>ontolearn starts with an existing generic ontology (we use wordnet, though other choices are possible) and set of documents in given domain, and produces domain extended and trimmed version of the initial ontology.
</nextsent>
<nextsent>the ontology generated by ontolearn is anchored to texts, it can be therefore classified as linguistic ontology (gmez-prez et al 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B2999">
<title id=" C04-1181.xml">interpreting vague utterances in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evolving state of real-world activity proceeds predictably according to background plans and principles of coordination (rich et al, 2001).
</prevsent>
<prevsent>the status of the dialogue itself is defined by circumscribed obligations to ground prior utterances, follow up open issues, and advance real world negotiation (larsson and traum, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P01-1031 ">
finally, the evolving state of the linguistic context is direct outgrowth of the linguistic forms interlocutors use and the linguistic relationships among successive utterances (ginzburg and cooper, 2001; <papid> P01-1031 </papid>asher and lascarides, 2003).</citsent>
<aftsection>
<nextsent>these compatible models combine directly to characterize an aggregate information state that provides general back ground for interpretation (bunt, 2000).we argue in this paper that such integrated models enable systems to calculate useful, fine-grainedutterance interpretations from radically underspecified semantic forms.
</nextsent>
<nextsent>we focus in particular on vague scalar predicates like small or long.
</nextsent>
<nextsent>these predicates typify qualitative linguistic expression of quantitative information, and are thus both challenging and commonplace.
</nextsent>
<nextsent>building on multidimensional treatment of dialogue context, we develop and implement theoretically-motivated model of vagueness which is unique in treating vague predicates as genuinely vague and genuinely context-sensitive, yet amenable to general processes of contextual and interpretive inference.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3000">
<title id=" C04-1181.xml">interpreting vague utterances in context </title>
<section> interpreting vague utterances in context.  </section>
<citcontext>
<prevsection>
<prevsent>in figlets drawing domain, possibilities include the actual measurements of objects that have already been drawn.
</prevsent>
<prevsent>they also include the default domain measurements for new objects that task context says could be added.
</prevsent>
</prevsection>
<citsent citstr=" J03-2002 ">
setting standards by measurement is our shorthand for adopting an implicit range of compatible standards; these standards remain vague, especially since many options are normally available (graff, 2000).we treat the use of new candidate standards in interpretation as case of pre supposition accommodation (bos, 2003).<papid> J03-2002 </papid></citsent>
<aftsection>
<nextsent>in pre supposition accommodation, the interpretation of an utterance must be constructed using context that differs from the actual context.
</nextsent>
<nextsent>when speakers use an utterance which requires accommodation, they typically expect that interlocutors will update the dialogue context to include the additional presumptions the utterance requires.
</nextsent>
<nextsent>we assume that all accommodation is subject to two gricean constraints.
</nextsent>
<nextsent>first, we assume whenever possible that an utterance should have uniquely identifiable intended interpretation in the context in which it is to be interpreted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3001">
<title id=" C04-1181.xml">interpreting vague utterances in context </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>as usual,chart edges indicate the presence of recognized partial constituents within the input sequence.
</prevsent>
<prevsent>in addition, edges now carry constraint networks that specify the contextual reasoning required for understanding.
</prevsent>
</prevsection>
<citsent citstr=" P01-1061 ">
in addition to finite instances (schuler, 2001), <papid> P01-1061 </papid>these networks include real constraints that formalize metric and spatial relationships.</citsent>
<aftsection>
<nextsent>interpretation of these networks is carried out incrementally, during parsing; each edge thus records setof associated candidate interpretations.
</nextsent>
<nextsent>since do main reasoning can be somewhat time-intensive in our current implementation, we adopt strategy of delaying the solution of certain constraints until enough lexical material has accrued that the associated problem-solving is judged tractable (devault and stone, 2003).
</nextsent>
<nextsent>in our approach, we specify genuinely vague se mantics: vague words evoke domain-specific scale that can differentiate alternative domain individuals.to find unique interpretation for vague utterance, we leverage ordinary inference about the do main, task, and linguistic context to recover implicit thresholds on this scale.
</nextsent>
<nextsent>we believe that further methodological advances will be required to evaluate treatments of vagueness in indefinite reference, such as that considered here.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3002">
<title id=" C02-1136.xml">stochastic dependency parsing of spontaneous japanese spoken language </title>
<section> dependencies dont cross each other..  </section>
<citcontext>
<prevsection>
<prevsent>several techniques for dependency parsing based on stochastic approaches have been proposed so far.
</prevsent>
<prevsent>fujio and matsumoto have used the probability based on the frequency of cooccurrence between two bunsetsus for dependency parsing (fujio and matsumoto,1998).
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
uchimoto et al have proposed technique for learning the dependency probability model based on maximum entropy method (uchimoto et al, 1999).<papid> E99-1026 </papid></citsent>
<aftsection>
<nextsent>however, since these 1a bunsetsu is one of the linguistic units in japanese, and roughly corresponds to basic phrase in english.
</nextsent>
<nextsent>a bunsetsu consists of one independent word and more than zero ancillary words.
</nextsent>
<nextsent>a dependency is modification relation between two bunsetsus.
</nextsent>
<nextsent>techniques are for written language, whether they are available for spoken language or notis not clear.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3006">
<title id=" C02-2026.xml">creating a finite state parser with application semantics </title>
<section> parsing and application-specific.  </section>
<citcontext>
<prevsection>
<prevsent>wewill call such nlu applications application semantic nlu?.
</prevsent>
<prevsent>other examples of application semantic nlu include interfaces to command based applications (such as airline reservation systems), often in the guise of dialog systems.
</prevsent>
</prevsection>
<citsent citstr=" C94-1079 ">
several general-purpose off-the-shelf (ots) parsers have become widely available (lin, 1994; <papid> C94-1079 </papid>collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>for application-semantic nlu, it is possible to use such an ots parser in conjunction with post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics.
</nextsent>
<nextsent>in addition to mapping the parser output to application semantics, the post-processor often must also correct?
</nextsent>
<nextsent>the output of the parser: the parser may be tailored for particular domain (such aswall street journal (wsj) text), but the new do main presents linguistic constructions not found in the original domain (such as questions).
</nextsent>
<nextsent>itmay also be the case that the ots parser consistently mis analyzes certain lexemes because they do not occur in the ots corpus, or occur there with different syntactic properties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3007">
<title id=" C02-2026.xml">creating a finite state parser with application semantics </title>
<section> parsing and application-specific.  </section>
<citcontext>
<prevsection>
<prevsent>wewill call such nlu applications application semantic nlu?.
</prevsent>
<prevsent>other examples of application semantic nlu include interfaces to command based applications (such as airline reservation systems), often in the guise of dialog systems.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
several general-purpose off-the-shelf (ots) parsers have become widely available (lin, 1994; <papid> C94-1079 </papid>collins, 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>for application-semantic nlu, it is possible to use such an ots parser in conjunction with post-processor which transfers the output of the parser (be it phrase structure or dependency) to the domain semantics.
</nextsent>
<nextsent>in addition to mapping the parser output to application semantics, the post-processor often must also correct?
</nextsent>
<nextsent>the output of the parser: the parser may be tailored for particular domain (such aswall street journal (wsj) text), but the new do main presents linguistic constructions not found in the original domain (such as questions).
</nextsent>
<nextsent>itmay also be the case that the ots parser consistently mis analyzes certain lexemes because they do not occur in the ots corpus, or occur there with different syntactic properties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3008">
<title id=" C02-2026.xml">creating a finite state parser with application semantics </title>
<section> constructing the parser.  </section>
<citcontext>
<prevsection>
<prevsent>we use 5 rounds of iter ation; obviously, the number of iterations restrict the syntactic complexity (but not the length) of recognized input.
</prevsent>
<prevsent>however, because we output brackets in the fsts, we obtain parse with full syntactic/lexical semantic (i.e., dependency) structure, not shallow parse?.
</prevsent>
</prevsection>
<citsent citstr=" J00-1003 ">
this construction is in many ways similar to similar constructions proposed for cfgs, in particular that of (nederhof, 2000).<papid> J00-1003 </papid></citsent>
<aftsection>
<nextsent>one difference is that, since we start from tag, recur sion is already factored, and we need not find cycles in the rules of the grammar.
</nextsent>
<nextsent>we present results in which our classes are defined entirely with respect to syntactic behavior.
</nextsent>
<nextsent>this is because we do not have available an important corpus annotated with semantics.we train on the wall street journal (wsj) corpus.
</nextsent>
<nextsent>we evaluate by taking list of 205 sentences which are chosen at random from entries to words eye made by the developers (whowere testing the graphical component using different parser).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3009">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> open text word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>first introduced by (lesk, 1986), these algorithms attempt to identify the most likely meanings for the words in given context based on measure of contextual overlap between the dictionary definitions of the ambiguous words,or between the current context and dictionary definitions provided forgiven target word.semantic similarity.
</prevsent>
<prevsent>measures of semantic similarity computed on semantic networks (rada et al, 1989).
</prevsent>
</prevsection>
<citsent citstr=" W98-0701 ">
depending on the size of the context they span, these measures are in turn divided into two main categories: (1) local context ? where the semantic measures are used to disambiguate words additionally connected by syntactic relations (stetina et al, 1998).<papid> W98-0701 </papid></citsent>
<aftsection>
<nextsent>(2) global context ? where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (morris and hirst, 1991).<papid> J91-1002 </papid>selectional preferences.</nextsent>
<nextsent>automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that word might have, based on the relation it has with other words in context (resnik, 1997).<papid> W97-0209 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3010">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> open text word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>measures of semantic similarity computed on semantic networks (rada et al, 1989).
</prevsent>
<prevsent>depending on the size of the context they span, these measures are in turn divided into two main categories: (1) local context ? where the semantic measures are used to disambiguate words additionally connected by syntactic relations (stetina et al, 1998).<papid> W98-0701 </papid></prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
(2) global context ? where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (morris and hirst, 1991).<papid> J91-1002 </papid>selectional preferences.</citsent>
<aftsection>
<nextsent>automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that word might have, based on the relation it has with other words in context (resnik, 1997).<papid> W97-0209 </papid></nextsent>
<nextsent>heuristic-based methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3011">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> open text word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>depending on the size of the context they span, these measures are in turn divided into two main categories: (1) local context ? where the semantic measures are used to disambiguate words additionally connected by syntactic relations (stetina et al, 1998).<papid> W98-0701 </papid></prevsent>
<prevsent>(2) global context ? where the semantic measures are employed to derive lexical chains, which are threads of meaning often drawn throughout an entire text (morris and hirst, 1991).<papid> J91-1002 </papid>selectional preferences.</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that word might have, based on the relation it has with other words in context (resnik, 1997).<papid> W97-0209 </papid></citsent>
<aftsection>
<nextsent>heuristic-based methods.
</nextsent>
<nextsent>these methods consist of simple rules that can reliably assign sense to certain word categories: one sense per collocation (yarowsky, 1993), <papid> H93-1052 </papid>and one sense per discourse (gale et al, 1992).<papid> H92-1045 </papid>in this paper, we propose new open-text disambiguation algorithm that combines information drawn from semantic network (wordnet) with graph-based ranking algorithms (pagerank).</nextsent>
<nextsent>we compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new pagerank-based method exceeds the performance obtained by other knowledge-based methods.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3012">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> open text word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that word might have, based on the relation it has with other words in context (resnik, 1997).<papid> W97-0209 </papid></prevsent>
<prevsent>heuristic-based methods.</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
these methods consist of simple rules that can reliably assign sense to certain word categories: one sense per collocation (yarowsky, 1993), <papid> H93-1052 </papid>and one sense per discourse (gale et al, 1992).<papid> H92-1045 </papid>in this paper, we propose new open-text disambiguation algorithm that combines information drawn from semantic network (wordnet) with graph-based ranking algorithms (pagerank).</citsent>
<aftsection>
<nextsent>we compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new pagerank-based method exceeds the performance obtained by other knowledge-based methods.
</nextsent>
<nextsent>in this section, we briefly describe page rank (brin and page, 1998), and describe the view of wordnet as graph, which facilitates the application of thegraph-based ranking algorithm on this semantic network.
</nextsent>
<nextsent>3.1 the page rank algorithm.
</nextsent>
<nextsent>iterative graph-based ranking algorithms are essentially way of deciding the importance of vertex within graph; in the context of search engines, it is way of deciding how important page is on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3013">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> open text word sense disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>automatically or semi automatically acquired selectional preferences, as means for constraining the number of possible senses that word might have, based on the relation it has with other words in context (resnik, 1997).<papid> W97-0209 </papid></prevsent>
<prevsent>heuristic-based methods.</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
these methods consist of simple rules that can reliably assign sense to certain word categories: one sense per collocation (yarowsky, 1993), <papid> H93-1052 </papid>and one sense per discourse (gale et al, 1992).<papid> H92-1045 </papid>in this paper, we propose new open-text disambiguation algorithm that combines information drawn from semantic network (wordnet) with graph-based ranking algorithms (pagerank).</citsent>
<aftsection>
<nextsent>we compare our method with other open-text word sense disambiguation algorithms, and show that the accuracy achieved through our new pagerank-based method exceeds the performance obtained by other knowledge-based methods.
</nextsent>
<nextsent>in this section, we briefly describe page rank (brin and page, 1998), and describe the view of wordnet as graph, which facilitates the application of thegraph-based ranking algorithm on this semantic network.
</nextsent>
<nextsent>3.1 the page rank algorithm.
</nextsent>
<nextsent>iterative graph-based ranking algorithms are essentially way of deciding the importance of vertex within graph; in the context of search engines, it is way of deciding how important page is on the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3014">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> related algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>namely, given two words, w1 and w2, each with nw1 andnw2 senses defined in dictionary, for each possible sense pair i1 and j 2 , i=1..nw1, j=1..nw2, first determine their definitions overlap, by counting the number of words they have in common.
</prevsent>
<prevsent>next, the sense pair with the highest overlap is selected, and consequently sense is assigned to each of the two words involved in the initial pair.when applied to open text, the original definition of the algorithm faces an explosion of word sense combinations4 , and alternative solutions arerequired.
</prevsent>
</prevsection>
<citsent citstr=" C92-1056 ">
one solution is to use simulated annealing, as proposed in (cowie et al, 1992).<papid> C92-1056 </papid></citsent>
<aftsection>
<nextsent>another solution ? which we adopt in our experiments ? is to use variation of the lesk algorithm (kilgarriff and rosenzweig, 2000), where meanings of words in the text are determined individually, by finding the highest overlap between the sense definitions ofeach word and the current context.
</nextsent>
<nextsent>rather than seeking to simultaneously determine the meanings of all words in given text, this approach determines word senses individually, and therefore it avoids the combinatorial explosion of senses.
</nextsent>
<nextsent>5.2 most frequent sense.
</nextsent>
<nextsent>wordnet keeps track of the frequency of each word meaning within sense-annotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3015">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>fr ? pr if = 1 fr ? pr if   1 where fr represents the wordnet sense frequency, pr represents the rank computed by page rank, is the position in the frequency ordered synset list, and rank represents the combined rank.
</prevsent>
<prevsent>we evaluate the accuracy of the word sense disambiguation algorithms on benchmark of sense annotated texts, in which each open-class word is mapped to the meaning selected by lexicographer as being the most appropriate one in the context of sentence.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
we are using subset of the semcor texts (miller et al, 1993) ? <papid> H93-1061 </papid>five randomly selected files covering different topics: news, sports, entertainment, law, and debates ? as well as the dataset provided for the english all words task during senseval-2.</citsent>
<aftsection>
<nextsent>the average size of file is 600-800 open class words.
</nextsent>
<nextsent>on each file, we run two sets of evaluations.
</nextsent>
<nextsent>(1) one set consisting of the basic uninformed?
</nextsent>
<nextsent>version of the knowledge-based algorithms, where the sense ordering provided by the dictionary is not taken into account at any point.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3017">
<title id=" C04-1162.xml">page rank on semantic networks with application to word sense disambiguation </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the most closely related method is perhaps the lexical chains algorithm (morris and hirst, 1991) ? <papid> J91-1002 </papid>where threads of meaning are identified throughout text.</prevsent>
<prevsent>lexical chains however only take into account possible relations between concepts in static way, without considering the importance of the concepts that participate in relation, which is recursively determined by pagerank.</prevsent>
</prevsection>
<citsent citstr=" C90-2067 ">
another related line of work is the word sense disambiguation algorithm proposed in (veronis and ide, 1990), <papid> C90-2067 </papid>where large neural network is built by relating words through their dictionary definitions.</citsent>
<aftsection>
<nextsent>the analogy.
</nextsent>
<nextsent>in the context of web surfing, page rank implements the random surfer model?, where user surfs the web by following links from any given web page.
</nextsent>
<nextsent>in the context of text meaning, page rank implements the concept of text cohesion (halliday and hasan, 1976), where from certain concept in text, we are likely to follow?
</nextsent>
<nextsent>linksto related concepts ? that is, concepts that have semantic relation with the current concept . intuitively, pagerank-style algorithms work well for finding the meaning of all words in open text because they combine together information drawn from the entire text (graph), and try to identify those synsets (vertices) that are of highest importance for the text unity and understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3018">
<title id=" C02-1049.xml">unknown word extraction for chinese documents </title>
<section> unknown word detection.  </section>
<citcontext>
<prevsection>
<prevsent>the final verification process comes to rescue.
</prevsent>
<prevsent>it resolves ambiguous and false extractions based on the morphological validity, syntactic validity, and statistical validity.
</prevsent>
</prevsection>
<citsent citstr=" C92-1019 ">
conventionally word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (chen &amp; liu, 1992, <papid> C92-1019 </papid>sproat et al 1996).<papid> J96-3004 </papid></citsent>
<aftsection>
<nextsent>hence after segmentation process the unknown words in the text would be incorrectly segmented into pieces of single character word or shorter words.
</nextsent>
<nextsent>if all occurrences of monosyllabic words are considered as morphemes of unknown words, the recall rate of the detection will be about 99%, but the precision is as low as 13.4% (chen &amp; bai, 1998).
</nextsent>
<nextsent>hence the complementary problem of unknown word detection is the problem of monosyllabic known-word detection, i.e. to remove the monosyllabic known-words as the candidates of unknown morphemes.
</nextsent>
<nextsent>a corpus-based learning method is proposed to derive set of syntactic discriminators for monosyllabic words and monosyllabic morphemes (chen &amp; bai, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3019">
<title id=" C02-1049.xml">unknown word extraction for chinese documents </title>
<section> unknown word detection.  </section>
<citcontext>
<prevsection>
<prevsent>the final verification process comes to rescue.
</prevsent>
<prevsent>it resolves ambiguous and false extractions based on the morphological validity, syntactic validity, and statistical validity.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
conventionally word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching (chen &amp; liu, 1992, <papid> C92-1019 </papid>sproat et al 1996).<papid> J96-3004 </papid></citsent>
<aftsection>
<nextsent>hence after segmentation process the unknown words in the text would be incorrectly segmented into pieces of single character word or shorter words.
</nextsent>
<nextsent>if all occurrences of monosyllabic words are considered as morphemes of unknown words, the recall rate of the detection will be about 99%, but the precision is as low as 13.4% (chen &amp; bai, 1998).
</nextsent>
<nextsent>hence the complementary problem of unknown word detection is the problem of monosyllabic known-word detection, i.e. to remove the monosyllabic known-words as the candidates of unknown morphemes.
</nextsent>
<nextsent>a corpus-based learning method is proposed to derive set of syntactic discriminators for monosyllabic words and monosyllabic morphemes (chen &amp; bai, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3021">
<title id=" C02-1049.xml">unknown word extraction for chinese documents </title>
<section> unknown word extraction.  </section>
<citcontext>
<prevsection>
<prevsent>is the correct extraction, since the statistical constraint rejects???.
</prevsent>
<prevsent>( 1|  document prob 4.2 statistical rules.
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
it is well known that keywords often reoccur in document (church, 2000) <papid> C00-1027 </papid>and very possible the keywords are also unknown words.</citsent>
<aftsection>
<nextsent>therefore statistical extraction methods utilize the locality of unknown words.
</nextsent>
<nextsent>the idea is that if two consecutive morphemes are highly associated then combine them to form new word.
</nextsent>
<nextsent>mutual information-like statistics are very often adopted in measuring association strength between two morphemes (church &amp; merser, 1993, sproat et al 1996).<papid> J96-3004 </papid></nextsent>
<nextsent>however such kind of statistic does not work well when the sample size is very limited.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3024">
<title id=" C02-1088.xml">unsupervised named entity classification models and their ensembles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental result shows 73.16% in precision and 72.98% in recall for korean news articles.
</prevsent>
<prevsent>named entity extraction is an important step for various applications in natural language processing.
</prevsent>
</prevsection>
<citsent citstr=" C00-2167 ">
named entity extraction involves identifying named entities in the text and classifying their types such as person, organization, location, time expressions, numeric expressions, and so on (sekine and eriguchi, 2000).<papid> C00-2167 </papid></citsent>
<aftsection>
<nextsent>one might think the named entities can be classified easily using dictionaries because most of named entities are proper nouns, but this is wrong opinion.
</nextsent>
<nextsent>as time passes, new proper nouns are created continuously.
</nextsent>
<nextsent>therefore it is impossible to add all those proper nouns to dictionary.
</nextsent>
<nextsent>even though named entities are registered in the dictionary it is not easy to decide their senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3025">
<title id=" C02-1088.xml">unsupervised named entity classification models and their ensembles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore it is impossible to add all those proper nouns to dictionary.
</prevsent>
<prevsent>even though named entities are registered in the dictionary it is not easy to decide their senses.
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
they have semantic (sense) ambiguity that proper noun has different senses according to the context (nina wacholder, et al, 1997).<papid> A97-1030 </papid></citsent>
<aftsection>
<nextsent>for example, united states?
</nextsent>
<nextsent>refers either to geographical area or to the political body which governs this area.
</nextsent>
<nextsent>the semantic ambiguity is occured frequently in korean (seon, et al 2001).
</nextsent>
<nextsent>let us illustrate this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3026">
<title id=" C02-1088.xml">unsupervised named entity classification models and their ensembles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it costs too much to maintain rules because rules and dictionaries have to be changed according to the application.
</prevsent>
<prevsent>the second belongs to supervised learning approach, which employs statistical method.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
as it is more robust and requires less human intervention, several statistical methods based on hidden markov model (bikel et al, 1997), <papid> A97-1029 </papid>maximum entropy model (borthwich et al, 1998) and decision tree model (bchet et al 2000) have been studied.</citsent>
<aftsection>
<nextsent>the supervised learning approach requires hand-tagged training corpus, but it can not achieve good performance without large amount of data because of data sparseness problem.
</nextsent>
<nextsent>for example, borthwich (1999) showed the performance of 83.45% in precision and 77.42% in f-measure for identifying and classifying the 8 irex (irex committee, 1999) categories,.
</nextsent>
<nextsent>with 294,000 tokens irex training corpus.
</nextsent>
<nextsent>it takes lot of time and labor to build large corpus like this.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3027">
<title id=" C02-1088.xml">unsupervised named entity classification models and their ensembles </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it takes lot of time and labor to build large corpus like this.
</prevsent>
<prevsent>this paper proposes an unsupervised learning model that uses small-scale named entity dictionary and an unlabeled corpus for classifiying named entities.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
collins and singer (1999) <papid> W99-0613 </papid>opened the possibility of using an unlabeled corpus to classify named entities.</citsent>
<aftsection>
<nextsent>they showed that the use of unlabeled data can reduce the requirements for supervision to just 7 simple seed rules.
</nextsent>
<nextsent>they used natural redundancy in the data : for many named-entity instances, both the spelling of the name and the context in which it appears are sufficient to determine its type.
</nextsent>
<nextsent>our model considers syntactic relations in sentence to resolve the semantic ambiguity and uses the ensemble of three different learning methods to improve the performance.
</nextsent>
<nextsent>they are maximum entropy model, memory-based learning and sparse network of winnows (roth, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3028">
<title id=" C02-1088.xml">unsupervised named entity classification models and their ensembles </title>
<section> the system for ne classification.  </section>
<citcontext>
<prevsection>
<prevsent>after the learning, the system modifies test examples by using rule, one sense per discourse.
</prevsent>
<prevsent>one sense per discourse means that the sense of target word is highly consistent within any given document.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
david yarowsky (1995) <papid> P95-1026 </papid>showed it was accurate in the word sense disambiguation.</citsent>
<aftsection>
<nextsent>we label the examples that are not labeled yet as the category of the labeled word in the discourse as following example and we output named entity tagged corpus.
</nextsent>
<nextsent>example after the ensemble learning ...
</nextsent>
<nextsent>kia type=organization  reul ji-won-han-da.
</nextsent>
<nextsent>kia neon ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3029">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>he introduced an information-content-based similarity measure that uses wordnet (fellbaum, 1998) as lexico-semantic resource and came up with the claim that semantic similarity is helpful to achieve higher coverage in coordination resolution for coordinated noun phrases of the form noun1 and noun2 noun3?
</prevsent>
<prevsent>than similarity measures based on morphological information only.
</prevsent>
</prevsection>
<citsent citstr=" P07-2038 ">
in similar vein, hogan (2007<papid> P07-2038 </papid>b) inspected wordnet similarity and relatedness measure sand investigated their role in conjunct identifi cation.</citsent>
<aftsection>
<nextsent>her data reveals that several measures of semantic word similarity can indeed detect conjunct similarity.
</nextsent>
<nextsent>for the majority of these similarity measures, the differences between themean similarity of coordinated elements and non coordinated ones were statistically significant.
</nextsent>
<nextsent>however, it also became evident that these were only slight differences, and not all coordinated heads were semantically related as evidenced, e.g., by work?/harmony?
</nextsent>
<nextsent>in hard work and harmony?.the significance tests did also not reveal particularly useful measures for conjunct identification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3035">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the authors demonstrated that wordnet was really helpful in coordination resolution, the evaluation was only conducted on compound nouns extracted from wordnets noun hierachy and, furthermore, the senses of nouns were manually tagged in advance for the experiments.
</prevsent>
<prevsent>despite this preference for semantic criteria, one might still raise the question how far non-semantic criteria might guide the resolution of noun phrase coordination ambiguities, e.g., by means of the distribution of resolution alternatives in large corpus or plain lexical or morpho-syntactic criteria.
</prevsent>
</prevsection>
<citsent citstr=" H05-1105 ">
this idea has already been explored before by various researchers from different methodological angles including distribution-based statistical approaches (e.g., chantree et al (2005), nakov and hearst(2005)), <papid> H05-1105 </papid>similarity-based approaches incorporating ortho graphical, morpho-syntactic, and syntactic similarity criteria (e.g., agarwal and boggess (1992), <papid> P92-1003 </papid>okumura and muraki (1994)), <papid> A94-1007 </papid>as well asa combination of distribution information and syntactic criteria (hogan, 2007<papid> P07-2038 </papid>a).</citsent>
<aftsection>
<nextsent>statistical approaches enumerate all candidate conjuncts and calculate the respective likelihood according to distribution estimated on corpus.
</nextsent>
<nextsent>for the coordination movie and television industry?
</nextsent>
<nextsent>the distributional similarity of movie?
</nextsent>
<nextsent>and industry?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3036">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the authors demonstrated that wordnet was really helpful in coordination resolution, the evaluation was only conducted on compound nouns extracted from wordnets noun hierachy and, furthermore, the senses of nouns were manually tagged in advance for the experiments.
</prevsent>
<prevsent>despite this preference for semantic criteria, one might still raise the question how far non-semantic criteria might guide the resolution of noun phrase coordination ambiguities, e.g., by means of the distribution of resolution alternatives in large corpus or plain lexical or morpho-syntactic criteria.
</prevsent>
</prevsection>
<citsent citstr=" P92-1003 ">
this idea has already been explored before by various researchers from different methodological angles including distribution-based statistical approaches (e.g., chantree et al (2005), nakov and hearst(2005)), <papid> H05-1105 </papid>similarity-based approaches incorporating ortho graphical, morpho-syntactic, and syntactic similarity criteria (e.g., agarwal and boggess (1992), <papid> P92-1003 </papid>okumura and muraki (1994)), <papid> A94-1007 </papid>as well asa combination of distribution information and syntactic criteria (hogan, 2007<papid> P07-2038 </papid>a).</citsent>
<aftsection>
<nextsent>statistical approaches enumerate all candidate conjuncts and calculate the respective likelihood according to distribution estimated on corpus.
</nextsent>
<nextsent>for the coordination movie and television industry?
</nextsent>
<nextsent>the distributional similarity of movie?
</nextsent>
<nextsent>and industry?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3037">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although the authors demonstrated that wordnet was really helpful in coordination resolution, the evaluation was only conducted on compound nouns extracted from wordnets noun hierachy and, furthermore, the senses of nouns were manually tagged in advance for the experiments.
</prevsent>
<prevsent>despite this preference for semantic criteria, one might still raise the question how far non-semantic criteria might guide the resolution of noun phrase coordination ambiguities, e.g., by means of the distribution of resolution alternatives in large corpus or plain lexical or morpho-syntactic criteria.
</prevsent>
</prevsection>
<citsent citstr=" A94-1007 ">
this idea has already been explored before by various researchers from different methodological angles including distribution-based statistical approaches (e.g., chantree et al (2005), nakov and hearst(2005)), <papid> H05-1105 </papid>similarity-based approaches incorporating ortho graphical, morpho-syntactic, and syntactic similarity criteria (e.g., agarwal and boggess (1992), <papid> P92-1003 </papid>okumura and muraki (1994)), <papid> A94-1007 </papid>as well asa combination of distribution information and syntactic criteria (hogan, 2007<papid> P07-2038 </papid>a).</citsent>
<aftsection>
<nextsent>statistical approaches enumerate all candidate conjuncts and calculate the respective likelihood according to distribution estimated on corpus.
</nextsent>
<nextsent>for the coordination movie and television industry?
</nextsent>
<nextsent>the distributional similarity of movie?
</nextsent>
<nextsent>and industry?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3047">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hogan (2007<papid> P07-2038 </papid>a) presented method for the disambiguation of noun phrase coordination by modelling two sources of information, viz.</prevsent>
<prevsent>distribution-based similarity between conjuncts and the dependency between con junct heads.</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
this method was incorporated in bikels parsing model (bikel, 2004) <papid> J04-4004 </papid>and achieve dan increase in np coordination dependency score from 69.9% to 73.8%.similarity-based approaches consider those elements of coordination as conjuncts which are most similar?</citsent>
<aftsection>
<nextsent>under syntactic, morphological, or even semantic aspects.
</nextsent>
<nextsent>agarwal and boggess (1992) <papid> P92-1003 </papid>include in their np coordination analysis syntactic and some semantic information about candidate conjuncts and achieve an accuracy boost up to 82%.</nextsent>
<nextsent>okumura and muraki (1994) <papid> A94-1007 </papid>estimate the similarity of candidate conjuncts by means of similarity function which incorporates syntactic, ortho graphical, and semantic information about the conjuncts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3052">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the model provides about 75% accuracy.
</prevsent>
<prevsent>the resolution of coordination ambiguity canalso be tried at parsing time.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
charniak and johnson (2005), <papid> P05-1022 </papid>e.g., supply discriminative reranker that uses e.g., features to capture syntactic parallelism across conjuncts.</citsent>
<aftsection>
<nextsent>the reranker achieves an f-score of 91%.recently, discriminative learning-based approaches were proposed, which exploit only lexical, morpho-syntactic features and the symmetry of conjuncts.
</nextsent>
<nextsent>shimbo and hara (2007) <papid> D07-1064 </papid>incorporate morpho-syntactic and symmetry features in discriminative learning model and end up with 57% f-measure on the genia corpus (ohta et al, 2002).</nextsent>
<nextsent>buyko et al (2007) employ conditional random fields (lafferty et al, 2001) and successfully tested this technique in the biomedical domain for the identification and resolution of ellipti fied conjuncts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3053">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>charniak and johnson (2005), <papid> P05-1022 </papid>e.g., supply discriminative reranker that uses e.g., features to capture syntactic parallelism across conjuncts.</prevsent>
<prevsent>the reranker achieves an f-score of 91%.recently, discriminative learning-based approaches were proposed, which exploit only lexical, morpho-syntactic features and the symmetry of conjuncts.</prevsent>
</prevsection>
<citsent citstr=" D07-1064 ">
shimbo and hara (2007) <papid> D07-1064 </papid>incorporate morpho-syntactic and symmetry features in discriminative learning model and end up with 57% f-measure on the genia corpus (ohta et al, 2002).</citsent>
<aftsection>
<nextsent>buyko et al (2007) employ conditional random fields (lafferty et al, 2001) and successfully tested this technique in the biomedical domain for the identification and resolution of ellipti fied conjuncts.
</nextsent>
<nextsent>they evaluate on the genia corpus and report an f-score of 93% for the reconstruction of the elliptical conjuncts employing lexical and morpho-syntactic criteria only.
</nextsent>
<nextsent>at least two questions remain ? whether the latter approach can achieve similar results in the newswire language domain (and is thus portable), and whether the incorporation of additional semantic criteria in this approach might boost the resolution rate, or not (and is thus possibly more parsimonious).
</nextsent>
<nextsent>the latter question is the main problem we deal with in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3054">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> datasets for the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>at least two questions remain ? whether the latter approach can achieve similar results in the newswire language domain (and is thus portable), and whether the incorporation of additional semantic criteria in this approach might boost the resolution rate, or not (and is thus possibly more parsimonious).
</prevsent>
<prevsent>the latter question is the main problem we deal with in this paper.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
2.1 coordination annotation in the penn. treebank for our experiments, we used the wsj part of thepenn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>some researchers (e.g., hogan (2007<papid> P07-2038 </papid>a)) had recently found several inconsistencies in its annotation of the bracketing of coordinations in nps.</nextsent>
<nextsent>these bugs were shown to pose problems for training and testing of coordination resolution and parsing tools.fortunately, re-annotated version has been provided by vadas and curran (2007), <papid> P07-1031 </papid>with focus 90on the internal structure of nps.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3061">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> datasets for the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 coordination annotation in the penn. treebank for our experiments, we used the wsj part of thepenn treebank (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>some researchers (e.g., hogan (2007<papid> P07-2038 </papid>a)) had recently found several inconsistencies in its annotation of the bracketing of coordinations in nps.</prevsent>
</prevsection>
<citsent citstr=" P07-1031 ">
these bugs were shown to pose problems for training and testing of coordination resolution and parsing tools.fortunately, re-annotated version has been provided by vadas and curran (2007), <papid> P07-1031 </papid>with focus 90on the internal structure of nps.</citsent>
<aftsection>
<nextsent>they added additional bracketing annotation for each noun phra sein the wsj section of the penn treebank assuming right-bracketing structure in nps.
</nextsent>
<nextsent>in addition, they introduced tags, e.g., nml?
</nextsent>
<nextsent>forex plicitly marking any left-branching constituents as in (np (nml (jj industrial) (cc and) (nn food)) (nns goods)) where industrial?
</nextsent>
<nextsent>and food?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3062">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>we used three baselines for resolving noun phrase coordination ambiguities ? one incorporating only lexico-semantic information, the wordnet similarity baseline, and two alternative ones incorporating only morpho-syntactic and syntactic parse information, the number agreement and the bikel parser baseline, respectively.
</prevsent>
<prevsent>3.1.1 wordnet similarity (wn) baseline our lexico-semantic baseline comes with wordnet semantic similarity scores of puta tively coordinated nouns.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
for our experiments,we used the implementation of wordnet similarity and relatedness measures provided by ted pedersen.2 the following similarity measures were considered: two measures based on path lenghts between concepts (path and lch (leacocket al, 1998)), <papid> J98-1006 </papid>three measures based on information content, i.e., corpus-based measures of the specificity of concept (res (resnik, 1999), lin (lin, 1998), and jcn (jiang and conrath, 1997)).</citsent>
<aftsection>
<nextsent>furthermore, we used two relatedness measures, namely, lesk (banerjee and pedersen, 2003) and vector (patwardhan et al, 2003), which score the similarity of the glosses of both concepts.
</nextsent>
<nextsent>we applied these similarity measures to any pair of putatively coordinated nouns in the noun phrases from our datasets, and b. to determine potential conjuncts we calculate two similarity scores relative to the structures discussed in section 2.2: s1 = sim(n1,n2) and s2 = sim(n1,n3) our final score is the maximum over both scores which is then the semantic indicator for the most plausible resolution of the coordination.
</nextsent>
<nextsent>3.1.2 number agreement (na) baseline we compared here the number agreement between selected nouns (see resnik (1999)).
</nextsent>
<nextsent>accordingly, n1 and n2 are coordinated, if number(n1) = number(n2) and number(n1) 6= number(n3), while n1 and n3 are coordinated, if number(n1) = number(n3) and number(n1) 6= number(n2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3066">
<title id=" C08-1012.xml">are morphosyntactic features more predictive for the resolution of noun phrase coordination ambiguity than lexico semantic similarity scores </title>
<section> methods.  </section>
<citcontext>
<prevsection>
<prevsent>the final resolution looks like s cc s?.
</prevsent>
<prevsent>if n1 and n3 were hypothesized to be coordinated, then all other elements except conjunctions were tagged as parts of conjuncts, as well.
</prevsent>
</prevsection>
<citsent citstr=" J03-4003 ">
3.1.4 bikel parser (bp) baseline we used the well-known bikel parser (bikel, 2004) <papid> J04-4004 </papid>in its original version and the one used by collins (2003).<papid> J03-4003 </papid></citsent>
<aftsection>
<nextsent>we trained both of them only with nps extracted from the re-annotated version of wsj (see section 2) and converted the bracketing output of the parsers to the io representation for np coordinations for further evaluations.
</nextsent>
<nextsent>3.2 chunking of conjuncts with crfs.
</nextsent>
<nextsent>the approach to conjunct identification presented by buyko et al (2007) employs conditional random fields (crf) (lafferty et al, 2001),3 which assign label to each token of coordinated nps according to its function in the coordination: c? for conjuncts, cc?
</nextsent>
<nextsent>for conjunctions, and s? for shared elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3070">
<title id=" C02-1117.xml">extension of zipfs law to words and phrases </title>
<section> zipf curves for large corpora.  </section>
<citcontext>
<prevsection>
<prevsent>this paper is principally concerned with exploring the above invalidity of zipfs law for large corpora in two languages, english and mandarin.
</prevsent>
<prevsent>we begin with english.
</prevsent>
</prevsection>
<citsent citstr=" H92-1073 ">
english corpora the english corpora used in our experiments are taken from the wall street journal (paul &amp; baker, 1992) <papid> H92-1073 </papid>for 1987, 1988, 1989, with sizes approximately 19 million, 16 million and 6 million tokens respectively.</citsent>
<aftsection>
<nextsent>the zipf curves for the 3 corpora are shown in figure 3.
</nextsent>
<nextsent>1 10 100 1000 10000 100000 1000000 1 10 100 1000 10000 100000 1000000 log rank wsj87 1-gram wsj88 1-gram wsj89 1-gram figure 3 zipf curves for the unigrams extracted from the 3 training corpora of the wsj the curves are parallel, showing similar structures and all 3 deviating from zipfs law for larger r. their separation is due to their different sizes.
</nextsent>
<nextsent>language is not made of individual words but also consists of phrases of 2, 3 and more words, usually called n-grams for n=2, 3, etc. for each value of between 2 and 5, we computed the frequencies of all n-gram in each corpus and put them in rank order as we had done for the words.
</nextsent>
<nextsent>this enabled us to draw the zipf curves for 2-grams to 5-grams which are shown along with the single word curves in figures 4, 5 and 6 for the three corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3071">
<title id=" C04-1152.xml">efficient unsupervised recursive word segmentation using minimum description length </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this reason, there is increasing interest in unsupervised learning of morphology, in which unannotated text is analysed to find morphological structures.
</prevsent>
<prevsent>even approximate unsupervised morphological analysis can be useful, as an aid to human annotators.this paper addresses key task for unsupervised morphological analysis: word segmentation,segmenting words into their most basic meaningful constituents (substrings), called morphs (ortho graphic realizations of morphemes).
</prevsent>
</prevsection>
<citsent citstr=" J01-2001 ">
we adopt the minimum description length (mdl) approach toword segmentation, which has been shown to be effective in recent work (notably (goldsmith, 2001)<papid> J01-2001 </papid>and (brent et al, 1995)).</citsent>
<aftsection>
<nextsent>the minimum description length principle (barron et al, 1998) is an information-theoretic criterion to prefer that model for observed data which gives minimal length coding of the observed dataset (given the model) together with the model itself.
</nextsent>
<nextsent>1.1 our approach.
</nextsent>
<nextsent>our approach in this paper is to better clarify theuse of mdl for morphological segmentation by enabling direct use of variety of mdl coding criteria in general and efficient search algorithm.
</nextsent>
<nextsent>issues of computational efficiency have been bottleneck inwork on unsupervised morphological analysis, leading to various approximations and heuristics beingused.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3072">
<title id=" C04-1152.xml">efficient unsupervised recursive word segmentation using minimum description length </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several systems for unsupervised learning of morphology have been developed over the last decade or so.
</prevsent>
<prevsent>dejean (1998), extending ideas in harris (1955),describes system for finding the most frequent affixes in language and identifying possible morpheme boundaries by frequency bounds on the number of possible characters following given character sequence.
</prevsent>
</prevsection>
<citsent citstr=" W02-0602 ">
brent et al (1995) give an information theoretic method for discovering meaningful affixes, which was later extended to enable novel search algorithm based on probabilisticword-generation model (snover et al, 2002).<papid> W02-0602 </papid></citsent>
<aftsection>
<nextsent>goldsmith (2001) <papid> J01-2001 </papid>gives comprehensive heuristic algorithm for unsupervised morphological analysis, which uses an mdl criterion to segment word sand find morphological paradigms (called signa tures).</nextsent>
<nextsent>similarly, creutz and lagus (2002) <papid> W02-0603 </papid>use an mdl formulation for word segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3074">
<title id=" C04-1152.xml">efficient unsupervised recursive word segmentation using minimum description length </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>brent et al (1995) give an information theoretic method for discovering meaningful affixes, which was later extended to enable novel search algorithm based on probabilisticword-generation model (snover et al, 2002).<papid> W02-0602 </papid></prevsent>
<prevsent>goldsmith (2001) <papid> J01-2001 </papid>gives comprehensive heuristic algorithm for unsupervised morphological analysis, which uses an mdl criterion to segment word sand find morphological paradigms (called signa tures).</prevsent>
</prevsection>
<citsent citstr=" W02-0603 ">
similarly, creutz and lagus (2002) <papid> W02-0603 </papid>use an mdl formulation for word segmentation.</citsent>
<aftsection>
<nextsent>all ofthese approaches assume stem+affix morphological paradigm.further, the above approaches only consider information in words?
</nextsent>
<nextsent>character sequences for improve morphological segmentation, and do not consider syntactic or semantic context.
</nextsent>
<nextsent>schone and jurafsky (2000) <papid> W00-0712 </papid>extend this by using latent semantic analysis (dumais et al, 1988) to require that proposed stem+affix split is sufficiently semantically similar to the stem before the split is accepted.</nextsent>
<nextsent>a conceptually similar approach is taken by baroni etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3075">
<title id=" C04-1152.xml">efficient unsupervised recursive word segmentation using minimum description length </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>all ofthese approaches assume stem+affix morphological paradigm.further, the above approaches only consider information in words?
</prevsent>
<prevsent>character sequences for improve morphological segmentation, and do not consider syntactic or semantic context.
</prevsent>
</prevsection>
<citsent citstr=" W00-0712 ">
schone and jurafsky (2000) <papid> W00-0712 </papid>extend this by using latent semantic analysis (dumais et al, 1988) to require that proposed stem+affix split is sufficiently semantically similar to the stem before the split is accepted.</citsent>
<aftsection>
<nextsent>a conceptually similar approach is taken by baroni etal.
</nextsent>
<nextsent>(2002) who combine use of edit distance to measure orthographic similarity and mutual information to measure semantic similarity, to determine morphologically related word pairs.
</nextsent>
<nextsent>in this section we provide an overview of our approach to greedy construction of set of morphs (a dictionary), using minimal description length (mdl) criterion (barron et al, 1998) (we present three alternative mdl-type criteria below, of varying levels of sophistication).
</nextsent>
<nextsent>the idea is to initialize dictionary of morphs to the set of all word types inthe corpus, and incrementally refine it by re segmenting affixes (either prefixes or suffixes) from the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3077">
<title id=" C04-1103.xml">direct ortho graphical mapping for machine transliteration </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>to achieve phonetic equivalent transliteration, phoneme-based approach has become the most popular approach.
</prevsent>
<prevsent>however, the success of phoneme-based approach is limited by the following constraints: 1) grapheme-to-phoneme conversion, originated from text-to-speech (tts) research, is far from perfect (the onomastica consortium, 1995), especially for the name of different language origins.
</prevsent>
</prevsection>
<citsent citstr=" P98-2220 ">
2) cross-lingual phonemic mapping presents great challenge due to phonemic divergence between some language pairs, such as chinese/english, japanese/english (wan and verspoor, 1998; <papid> P98-2220 </papid>meng et al, 2001).</citsent>
<aftsection>
<nextsent>3) the conversion of phoneme-to-grapheme introduces yet another level of imprecision, esp. for the ideographic language, such as chinese.
</nextsent>
<nextsent>virga and khudanpur (2003) <papid> W03-1508 </papid>reported 8.3% absolute accuracy drops when converting from pinyin to chinese character.</nextsent>
<nextsent>the three error-prone steps as stated above lead to an inferior overall system performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3078">
<title id=" C04-1103.xml">direct ortho graphical mapping for machine transliteration </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2) cross-lingual phonemic mapping presents great challenge due to phonemic divergence between some language pairs, such as chinese/english, japanese/english (wan and verspoor, 1998; <papid> P98-2220 </papid>meng et al, 2001).</prevsent>
<prevsent>3) the conversion of phoneme-to-grapheme introduces yet another level of imprecision, esp. for the ideographic language, such as chinese.</prevsent>
</prevsection>
<citsent citstr=" W03-1508 ">
virga and khudanpur (2003) <papid> W03-1508 </papid>reported 8.3% absolute accuracy drops when converting from pinyin to chinese character.</citsent>
<aftsection>
<nextsent>the three error-prone steps as stated above lead to an inferior overall system performance.
</nextsent>
<nextsent>the complication of multiple steps and introduction of intermediate phonemes also incur high cost in system development when moving from one language pair to another, because we have to work on language specific ad-hoc phonic rules.
</nextsent>
<nextsent>2.2 transliteration model.
</nextsent>
<nextsent>transliteration model is knowledge base to support the execution of transliteration strategy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3079">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>considering results jointly is not new idea and is very similar to the concept of diversity based ranking introduced in their community by carbonell and goldstein (1998).
</prevsent>
<prevsent>in short, selecting an n-best list is balancing act between maximizing the relevance of the list and the information novelty of its results.
</prevsent>
</prevsection>
<citsent citstr=" N07-1013 ">
one commonly used approach is to define measure of novel ty/semantic similarity between documents and to apply heuristics to reduce the relevance score of result item (a hit) by function of the similarity of this item to other results in the list (carbonell and goldstein 1998; zhu et al 2007).<papid> N07-1013 </papid></citsent>
<aftsection>
<nextsent>another common approach is to cluster result documents according to their semantic similarity and present clusters to users instead of individual documents (hearst and pedersen 1996; leuski 2001; liu and croft 2004).
</nextsent>
<nextsent>in this paper, we argue that the balance between relevance and novelty can be captured by formal model that maximizes the joint information content of result set.
</nextsent>
<nextsent>instead of ranking documents in an ir setting, we focus in this paper on new task of selecting the best semantic properties that describe the similarity of set of query terms.
</nextsent>
<nextsent>by no means an exhaustive list, the most commonly cited ranking and scoring algorithms are hits (kleinberg 1998) and page rank (page et al 1998), which rank hyper linked documents using the concepts of hubs and authorities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3080">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>instead of ranking documents in an ir setting, we focus in this paper on new task of selecting the best semantic properties that describe the similarity of set of query terms.
</prevsent>
<prevsent>by no means an exhaustive list, the most commonly cited ranking and scoring algorithms are hits (kleinberg 1998) and page rank (page et al 1998), which rank hyper linked documents using the concepts of hubs and authorities.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
the most well-known keyword scoring methods within their community are the tf-idf (salton and mcgill 1983) and pointwise mutual information (church and hanks 1989) <papid> P89-1010 </papid>measures, which put more importance on matching keywords that occur frequently in document relative to the total number of documents that contain the keyword (by normalizing term frequencies with inverse document frequencies).</citsent>
<aftsection>
<nextsent>various methods including tf-idf have been comparatively evaluated by salton and buckley (1987).
</nextsent>
<nextsent>creating best lists using the above algorithms produce result sets where each result is considered independently.
</nextsent>
<nextsent>in this paper, we investigate the utility of considering the result sets jointly and compare our joint method to pointwise mutual information model.
</nextsent>
<nextsent>within the nlp community, n-best list ranking has been looked at carefully in parsing, extractive summarization (barzilay et al 1999; <papid> P99-1071 </papid>hovy and lin 1998), <papid> P98-2127 </papid>and machine translation (zhang et al 2006), to name few.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3081">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>creating best lists using the above algorithms produce result sets where each result is considered independently.
</prevsent>
<prevsent>in this paper, we investigate the utility of considering the result sets jointly and compare our joint method to pointwise mutual information model.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
within the nlp community, n-best list ranking has been looked at carefully in parsing, extractive summarization (barzilay et al 1999; <papid> P99-1071 </papid>hovy and lin 1998), <papid> P98-2127 </papid>and machine translation (zhang et al 2006), to name few.</citsent>
<aftsection>
<nextsent>the problem of learning to rank set of objects by combining given collection of ranking functions using boosting techniques is investigated in (freund et al. 2003).
</nextsent>
<nextsent>this rank boosting technique has been used in re-ranking parsers (collins and koo 2000; charniak and johnson 2005).<papid> P05-1022 </papid></nextsent>
<nextsent>such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3082">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>creating best lists using the above algorithms produce result sets where each result is considered independently.
</prevsent>
<prevsent>in this paper, we investigate the utility of considering the result sets jointly and compare our joint method to pointwise mutual information model.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
within the nlp community, n-best list ranking has been looked at carefully in parsing, extractive summarization (barzilay et al 1999; <papid> P99-1071 </papid>hovy and lin 1998), <papid> P98-2127 </papid>and machine translation (zhang et al 2006), to name few.</citsent>
<aftsection>
<nextsent>the problem of learning to rank set of objects by combining given collection of ranking functions using boosting techniques is investigated in (freund et al. 2003).
</nextsent>
<nextsent>this rank boosting technique has been used in re-ranking parsers (collins and koo 2000; charniak and johnson 2005).<papid> P05-1022 </papid></nextsent>
<nextsent>such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3083">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>within the nlp community, n-best list ranking has been looked at carefully in parsing, extractive summarization (barzilay et al 1999; <papid> P99-1071 </papid>hovy and lin 1998), <papid> P98-2127 </papid>and machine translation (zhang et al 2006), to name few.</prevsent>
<prevsent>the problem of learning to rank set of objects by combining given collection of ranking functions using boosting techniques is investigated in (freund et al. 2003).</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
this rank boosting technique has been used in re-ranking parsers (collins and koo 2000; charniak and johnson 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>such reranking approaches usually improve the likelihood of candidate results using extraneous features and, for example in parsing, the properties of the trees.
</nextsent>
<nextsent>in this paper, we focus on difference task: the lexical semantics task of selecting the best semantic properties that help explain why set of query terms are similar.
</nextsent>
<nextsent>unlike in parsing and machine translation, we are not ulti 682mately looking for the best single result, but instead the n-best.
</nextsent>
<nextsent>looking at commercial applications, there are many examples showcasing the importance of ranking, for example internet search engines like google and yahoo (brin and page 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3085">
<title id=" C08-1086.xml">a joint information model for nbest ranking </title>
<section> explaining similarity.  </section>
<citcontext>
<prevsection>
<prevsent>given set of similar terms, we look at the overlapping syntactic dependencies between the words in the set to form candidate semantic properties.
</prevsent>
<prevsent>example properties extracted by our system (described below) for random sample of two instances from cluster of food, {apple, beef}, include4: shredded, sliced, lean, sour, delicious, cooked, import, export, eat, cook, dice, taste, market, consume, slice, ...
</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
we obtain candidate properties by parsing large textual corpus with the minipar parser (lin 1993)<papid> P93-1016 </papid>5.</citsent>
<aftsection>
<nextsent>for each word in the corpus, we extract all of its dependency links, forming feature vector of syntactic dependencies.
</nextsent>
<nextsent>for example, below is sample of the feature vector for the word apple: adj-mod:gala, adj-mod:shredded, object-of:caramelize, object-of:eat, object-of:import, ...
</nextsent>
<nextsent>intersecting apples feature vector with beefs, we are left with the following candidate properties: adj-mod:shredded, object-of:eat, object-of:import, ...
</nextsent>
<nextsent>in this paper, we omit the relation name of the syntactic dependencies, and instead write: shredded, eat, import, ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3088">
<title id=" C04-1171.xml">a system for generating descriptions of sets of objects in a rich variety </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>we follow by illustrating improvements of expressiveness.
</prevsent>
<prevsent>finally, we evaluate several efficiency-related techniques.
</prevsent>
</prevsection>
<citsent citstr=" P99-1017 ">
identifying sets of objects originally followed the incremental algorithm (dale and reiter 1995), as in (bateman 1999), (<papid> P99-1017 </papid>stone 2000) <papid> W00-1416 </papid>and (krahmer et al 2003), <papid> J03-1003 </papid>with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors.</citsent>
<aftsection>
<nextsent>therefore, van deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations.
</nextsent>
<nextsent>unfortunately, when applying the incremental strategy, this may lead to the inclusion of too many redundant descriptors in the final specification.
</nextsent>
<nextsent>this deficit disappeared using an exhaustive search (gardent 2002), <papid> P02-1013 </papid>but run-time then increases considerably.</nextsent>
<nextsent>mediating between these two extreme search paradigms, we have developed best-first searching algorithm that avoids the major deficit of the incremental approach (horacek 2003).<papid> E03-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3089">
<title id=" C04-1171.xml">a system for generating descriptions of sets of objects in a rich variety </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>we follow by illustrating improvements of expressiveness.
</prevsent>
<prevsent>finally, we evaluate several efficiency-related techniques.
</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
identifying sets of objects originally followed the incremental algorithm (dale and reiter 1995), as in (bateman 1999), (<papid> P99-1017 </papid>stone 2000) <papid> W00-1416 </papid>and (krahmer et al 2003), <papid> J03-1003 </papid>with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors.</citsent>
<aftsection>
<nextsent>therefore, van deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations.
</nextsent>
<nextsent>unfortunately, when applying the incremental strategy, this may lead to the inclusion of too many redundant descriptors in the final specification.
</nextsent>
<nextsent>this deficit disappeared using an exhaustive search (gardent 2002), <papid> P02-1013 </papid>but run-time then increases considerably.</nextsent>
<nextsent>mediating between these two extreme search paradigms, we have developed best-first searching algorithm that avoids the major deficit of the incremental approach (horacek 2003).<papid> E03-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3090">
<title id=" C04-1171.xml">a system for generating descriptions of sets of objects in a rich variety </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>we follow by illustrating improvements of expressiveness.
</prevsent>
<prevsent>finally, we evaluate several efficiency-related techniques.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
identifying sets of objects originally followed the incremental algorithm (dale and reiter 1995), as in (bateman 1999), (<papid> P99-1017 </papid>stone 2000) <papid> W00-1416 </papid>and (krahmer et al 2003), <papid> J03-1003 </papid>with limited coverage, since only few attributes typically apply to all intended referents and to none of the potential distractors.</citsent>
<aftsection>
<nextsent>therefore, van deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations.
</nextsent>
<nextsent>unfortunately, when applying the incremental strategy, this may lead to the inclusion of too many redundant descriptors in the final specification.
</nextsent>
<nextsent>this deficit disappeared using an exhaustive search (gardent 2002), <papid> P02-1013 </papid>but run-time then increases considerably.</nextsent>
<nextsent>mediating between these two extreme search paradigms, we have developed best-first searching algorithm that avoids the major deficit of the incremental approach (horacek 2003).<papid> E03-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3091">
<title id=" C04-1171.xml">a system for generating descriptions of sets of objects in a rich variety </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, van deemter (2002) has extended the set of descriptors to boolean combinations of attributes, including negations.
</prevsent>
<prevsent>unfortunately, when applying the incremental strategy, this may lead to the inclusion of too many redundant descriptors in the final specification.
</prevsent>
</prevsection>
<citsent citstr=" P02-1013 ">
this deficit disappeared using an exhaustive search (gardent 2002), <papid> P02-1013 </papid>but run-time then increases considerably.</citsent>
<aftsection>
<nextsent>mediating between these two extreme search paradigms, we have developed best-first searching algorithm that avoids the major deficit of the incremental approach (horacek 2003).<papid> E03-1017 </papid></nextsent>
<nextsent>since its intermediate results can also be used as partial descriptions, we build on the flexibility of this new algorithm to extend its expressive capabilities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3092">
<title id=" C04-1171.xml">a system for generating descriptions of sets of objects in a rich variety </title>
<section> motivation.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, when applying the incremental strategy, this may lead to the inclusion of too many redundant descriptors in the final specification.
</prevsent>
<prevsent>this deficit disappeared using an exhaustive search (gardent 2002), <papid> P02-1013 </papid>but run-time then increases considerably.</prevsent>
</prevsection>
<citsent citstr=" E03-1017 ">
mediating between these two extreme search paradigms, we have developed best-first searching algorithm that avoids the major deficit of the incremental approach (horacek 2003).<papid> E03-1017 </papid></citsent>
<aftsection>
<nextsent>since its intermediate results can also be used as partial descriptions, we build on the flexibility of this new algorithm to extend its expressive capabilities.
</nextsent>
<nextsent>in addition, we further enhance its efficiency-seeking measures.
</nextsent>
<nextsent>these extensions attack the deficits previous algorithms share, according to (horacek 2004): ? expressions produced may become lengthy: for identifying sets of vehicles in the scenario in figure 1, we have obtained non-redundant specifications with up to 8 descriptors.
</nextsent>
<nextsent>specifications may contain some dis junctions, frequently causing the production of structurally ambiguous expressions (gardent 2002) ? <papid> P02-1013 </papid>trucks and sport scars which are white or in the center?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3095">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word dependency is important in parsing technology.
</prevsent>
<prevsent>figure 1 shows word dependency tree.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
eisner (1996) <papid> C96-1058 </papid>proposed probabilistic models of dependency parsing.</citsent>
<aftsection>
<nextsent>collins (1999) used dependency analysis for phrase structure parsing.
</nextsent>
<nextsent>it is also studied by other researchers (sleator and temperley,1991; hockenmaier and steedman, 2002).<papid> P02-1043 </papid></nextsent>
<nextsent>however, statistical dependency analysis of english sentences without phrase labels is not studied very much while phrase structure parsing is intensivelystudied.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3096">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>eisner (1996) <papid> C96-1058 </papid>proposed probabilistic models of dependency parsing.</prevsent>
<prevsent>collins (1999) used dependency analysis for phrase structure parsing.</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
it is also studied by other researchers (sleator and temperley,1991; hockenmaier and steedman, 2002).<papid> P02-1043 </papid></citsent>
<aftsection>
<nextsent>however, statistical dependency analysis of english sentences without phrase labels is not studied very much while phrase structure parsing is intensivelystudied.
</nextsent>
<nextsent>recent studies show that information extraction (ie) and question answering (qa) benefit from word dependency analysis without phrase labels.
</nextsent>
<nextsent>(suzuki et al, 2003; <papid> P03-1005 </papid>sudo et al, 2003)<papid> P03-1029 </papid>recently, yamada and matsumoto (2003) proposed trainable english word dependency analyzer based on support vector machines (svm).they did not use phrase labels by considering annotation of documents inexpert domains.</nextsent>
<nextsent>svm(vapnik, 1995) has shown good performance in dif he girl telescope with saw he saw girl with telescope.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3097">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, statistical dependency analysis of english sentences without phrase labels is not studied very much while phrase structure parsing is intensivelystudied.
</prevsent>
<prevsent>recent studies show that information extraction (ie) and question answering (qa) benefit from word dependency analysis without phrase labels.
</prevsent>
</prevsection>
<citsent citstr=" P03-1005 ">
(suzuki et al, 2003; <papid> P03-1005 </papid>sudo et al, 2003)<papid> P03-1029 </papid>recently, yamada and matsumoto (2003) proposed trainable english word dependency analyzer based on support vector machines (svm).they did not use phrase labels by considering annotation of documents inexpert domains.</citsent>
<aftsection>
<nextsent>svm(vapnik, 1995) has shown good performance in dif he girl telescope with saw he saw girl with telescope.
</nextsent>
<nextsent>figure 1: word dependency tree ferent tasks of natural language processing (kudo and matsumoto, 2001; <papid> N01-1025 </papid>isozaki and kazawa, 2002).<papid> C02-1054 </papid></nextsent>
<nextsent>most machine learning methods do not work well when the number of given features (dimensionality) is large, but svm is relatively robust.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3098">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, statistical dependency analysis of english sentences without phrase labels is not studied very much while phrase structure parsing is intensivelystudied.
</prevsent>
<prevsent>recent studies show that information extraction (ie) and question answering (qa) benefit from word dependency analysis without phrase labels.
</prevsent>
</prevsection>
<citsent citstr=" P03-1029 ">
(suzuki et al, 2003; <papid> P03-1005 </papid>sudo et al, 2003)<papid> P03-1029 </papid>recently, yamada and matsumoto (2003) proposed trainable english word dependency analyzer based on support vector machines (svm).they did not use phrase labels by considering annotation of documents inexpert domains.</citsent>
<aftsection>
<nextsent>svm(vapnik, 1995) has shown good performance in dif he girl telescope with saw he saw girl with telescope.
</nextsent>
<nextsent>figure 1: word dependency tree ferent tasks of natural language processing (kudo and matsumoto, 2001; <papid> N01-1025 </papid>isozaki and kazawa, 2002).<papid> C02-1054 </papid></nextsent>
<nextsent>most machine learning methods do not work well when the number of given features (dimensionality) is large, but svm is relatively robust.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3099">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(suzuki et al, 2003; <papid> P03-1005 </papid>sudo et al, 2003)<papid> P03-1029 </papid>recently, yamada and matsumoto (2003) proposed trainable english word dependency analyzer based on support vector machines (svm).they did not use phrase labels by considering annotation of documents inexpert domains.</prevsent>
<prevsent>svm(vapnik, 1995) has shown good performance in dif he girl telescope with saw he saw girl with telescope.</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
figure 1: word dependency tree ferent tasks of natural language processing (kudo and matsumoto, 2001; <papid> N01-1025 </papid>isozaki and kazawa, 2002).<papid> C02-1054 </papid></citsent>
<aftsection>
<nextsent>most machine learning methods do not work well when the number of given features (dimensionality) is large, but svm is relatively robust.
</nextsent>
<nextsent>in natural language processing, we use tens of thousands of words as features.
</nextsent>
<nextsent>therefore, svm often gives good performance.
</nextsent>
<nextsent>however, the accuracy of yamadas analyzer is lower than state-of-the-art phrase structure parsers such as charniaks maximum-entropy-inspired parser (meip) (charniak, 2000) <papid> A00-2018 </papid>and collins?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3100">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(suzuki et al, 2003; <papid> P03-1005 </papid>sudo et al, 2003)<papid> P03-1029 </papid>recently, yamada and matsumoto (2003) proposed trainable english word dependency analyzer based on support vector machines (svm).they did not use phrase labels by considering annotation of documents inexpert domains.</prevsent>
<prevsent>svm(vapnik, 1995) has shown good performance in dif he girl telescope with saw he saw girl with telescope.</prevsent>
</prevsection>
<citsent citstr=" C02-1054 ">
figure 1: word dependency tree ferent tasks of natural language processing (kudo and matsumoto, 2001; <papid> N01-1025 </papid>isozaki and kazawa, 2002).<papid> C02-1054 </papid></citsent>
<aftsection>
<nextsent>most machine learning methods do not work well when the number of given features (dimensionality) is large, but svm is relatively robust.
</nextsent>
<nextsent>in natural language processing, we use tens of thousands of words as features.
</nextsent>
<nextsent>therefore, svm often gives good performance.
</nextsent>
<nextsent>however, the accuracy of yamadas analyzer is lower than state-of-the-art phrase structure parsers such as charniaks maximum-entropy-inspired parser (meip) (charniak, 2000) <papid> A00-2018 </papid>and collins?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3101">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in natural language processing, we use tens of thousands of words as features.
</prevsent>
<prevsent>therefore, svm often gives good performance.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
however, the accuracy of yamadas analyzer is lower than state-of-the-art phrase structure parsers such as charniaks maximum-entropy-inspired parser (meip) (charniak, 2000) <papid> A00-2018 </papid>and collins?</citsent>
<aftsection>
<nextsent>model3 parser.
</nextsent>
<nextsent>one reason is the lack of top-down information that is available in phrase structure parsers.
</nextsent>
<nextsent>in this paper, we show that the accuracy of the word dependency parser can be improved by adding base-np chunker, root-node finder, and prepositional phrase (pp) attachment resolver.
</nextsent>
<nextsent>we introduce the base-np chunker because base nps are important components of sentence and can be easily annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3102">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>root-node finder?
</prevsent>
<prevsent>base np chunker?
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
(pos tagger) ? = svm, ? = preference learning figure 2: module layers in the system that is, we use penn treebanks wall street journal data (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>sections 02 through 21 are used as training data (about 40,000 sentences) and section 23 is used as test data (2,416 sentences).we converted them to word dependency data by using collins?
</nextsent>
<nextsent>head rules (collins, 1999).the proposed method uses the following procedures.
</nextsent>
<nextsent>a base np chunker: we implemented ansvm-based base np chunker, which is simplified version of kudos method (kudo and matsumoto, 2001).<papid> N01-1025 </papid></nextsent>
<nextsent>we use the one vs. all others?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3104">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>a pp-attatchment re solver (ppar): this re solver improves the dependency accuracy of prepositions whose part-of-speech tags are in or to.
</prevsent>
<prevsent>the above procedures require part-of-speech tagger.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
here, we extract part-of-speech tags fromthe collins parsers output (collins, 1997) <papid> P97-1003 </papid>for section 23 instead of reinventing tagger.</citsent>
<aftsection>
<nextsent>according to the document, it is the output of ratnaparkhi stagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></nextsent>
<nextsent>figure 2 shows the architecture of the system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3105">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the above procedures require part-of-speech tagger.
</prevsent>
<prevsent>here, we extract part-of-speech tags fromthe collins parsers output (collins, 1997) <papid> P97-1003 </papid>for section 23 instead of reinventing tagger.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
according to the document, it is the output of ratnaparkhi stagger (ratnaparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>figure 2 shows the architecture of the system.
</nextsent>
<nextsent>ppars output is used to rewrite the output of the dependency analyzer.
</nextsent>
<nextsent>2.1 finding root nodes.
</nextsent>
<nextsent>when we use svm, we regard root-node finding asa classification task: root nodes are positive examples and other words are negative examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3107">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(empirically, it takes o(`2) ormore.)
</prevsent>
<prevsent>further study is needed to reduce the computational complexity.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
(since we used isozakis methods (isozaki and kazawa, 2002), <papid> C02-1054 </papid>the run-time complexity is not problem.)kudo and matsumoto (2002) <papid> W02-2016 </papid>proposed an svm based dependency analyzer for japanese sentences.</citsent>
<aftsection>
<nextsent>japanese word dependency is simpler be cause no word modifies left word.
</nextsent>
<nextsent>collins and duffy (2002) <papid> P02-1034 </papid>improved collins?</nextsent>
<nextsent>model 2 parser by reranking possible parse trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3108">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>(since we used isozakis methods (isozaki and kazawa, 2002), <papid> C02-1054 </papid>the run-time complexity is not problem.)kudo and matsumoto (2002) <papid> W02-2016 </papid>proposed an svm based dependency analyzer for japanese sentences.</prevsent>
<prevsent>japanese word dependency is simpler be cause no word modifies left word.</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
collins and duffy (2002) <papid> P02-1034 </papid>improved collins?</citsent>
<aftsection>
<nextsent>model 2 parser by reranking possible parse trees.
</nextsent>
<nextsent>shen and joshi (2003) <papid> W03-0402 </papid>also used the preference kernel k(xi.?, xj.?)</nextsent>
<nextsent>for reranking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3109">
<title id=" C04-1040.xml">a deterministic word dependency analyzer enhanced with preference learning </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>collins and duffy (2002) <papid> P02-1034 </papid>improved collins?</prevsent>
<prevsent>model 2 parser by reranking possible parse trees.</prevsent>
</prevsection>
<citsent citstr=" W03-0402 ">
shen and joshi (2003) <papid> W03-0402 </papid>also used the preference kernel k(xi.?, xj.?)</citsent>
<aftsection>
<nextsent>for reranking.
</nextsent>
<nextsent>they compare parse trees, but our system compares words.
</nextsent>
<nextsent>dependency analysis is useful and annotation of word dependency seems easier than annotation of phrase labels.
</nextsent>
<nextsent>however, lack of phrase labels makes dependency analysis more difficult than phrase structure parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3110">
<title id=" C02-1104.xml">from shallow to deep parsing using constraint satisfaction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conclude in presenting some perspectives for such technique.
</prevsent>
<prevsent>1 property grammars.
</prevsent>
</prevsection>
<citsent citstr=" P90-1005 ">
the notion of constraints is of deep importance in linguistics, see for example maruyama (1990), <papid> P90-1005 </papid>pollard (1994), sag (1999).</citsent>
<aftsection>
<nextsent>recent theories (from the constraint-based paradigm to the principle and parameters one) relyon this notion.
</nextsent>
<nextsent>one of the main interests in using constraints comes from the fact that it becomes possible to represent any kind of information (very general as well as local or contextual one) by means of unique device.
</nextsent>
<nextsent>we present in this section formalism, called property grammars, described in bs (1999) or blache (2001), that makes it possible to conceive and represent all linguistic information in terms of constraints over linguistic objects.
</nextsent>
<nextsent>in this approach, constraints are seen as relations between two (or more) objects: it is then possible to represent information in flat manner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3111">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic roles describe an important aspect of phrasal meaning by characterising the relationship between predicates and their arguments on semantic level (e.g., agent, patient).
</prevsent>
<prevsent>they generalise over surface categories (such as subject, object) and variations (such as dia thesis alternations).
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
two frameworks for semantic roles have found wide use in the community, propbank (palmer et al, 2005) <papid> J05-1004 </papid>and framenet (fillmore et al, 2003).</citsent>
<aftsection>
<nextsent>their corpora are used to train supervised models for semantic role labelling (srl) of new text (gildea and jurafsky, 2002; <papid> J02-3001 </papid>carreras and m`arquez, 2005).</nextsent>
<nextsent>the resulting analysis can benefit number of applications, such ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3112">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they generalise over surface categories (such as subject, object) and variations (such as dia thesis alternations).
</prevsent>
<prevsent>two frameworks for semantic roles have found wide use in the community, propbank (palmer et al, 2005) <papid> J05-1004 </papid>and framenet (fillmore et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
their corpora are used to train supervised models for semantic role labelling (srl) of new text (gildea and jurafsky, 2002; <papid> J02-3001 </papid>carreras and m`arquez, 2005).</citsent>
<aftsection>
<nextsent>the resulting analysis can benefit number of applications, such ? 2008.
</nextsent>
<nextsent>licensed under the creative commons attribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>as information extraction (moschitti et al, 2003) or question answering (frank et al, 2007).a commonly encountered criticism of semantic roles, and arguably major obstacle to their adoption in nlp, is their limited coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3113">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since manual semantic role tagging is costly, it is hardly conceivable that gold standard annotation will ultimately be available for every predicate of english.in addition, the lexically specific nature of the mapping between surface syntax and semantic roles makes it difficult to generalise from seen predicates to unseen predicates for which no training data is available.
</prevsent>
<prevsent>techniques for extending the coverage of srl therefore address an important need.
</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
unfortunately, pioneering work in unsupervised srl (swier and stevenson, 2004; <papid> W04-3213 </papid>grenager and manning, 2006) <papid> W06-1601 </papid>currently either relies on small number of semantic roles, or cannot identify equivalent roles across predicates.</citsent>
<aftsection>
<nextsent>a promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates.
</nextsent>
<nextsent>the feasibility of this approach was demonstrated by gordon and swanson (2007)<papid> P07-1025 </papid>for syntactically similar verbs.</nextsent>
<nextsent>however, their approach requires at least one annotated instance of each new predicate, limiting its practicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3114">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since manual semantic role tagging is costly, it is hardly conceivable that gold standard annotation will ultimately be available for every predicate of english.in addition, the lexically specific nature of the mapping between surface syntax and semantic roles makes it difficult to generalise from seen predicates to unseen predicates for which no training data is available.
</prevsent>
<prevsent>techniques for extending the coverage of srl therefore address an important need.
</prevsent>
</prevsection>
<citsent citstr=" W06-1601 ">
unfortunately, pioneering work in unsupervised srl (swier and stevenson, 2004; <papid> W04-3213 </papid>grenager and manning, 2006) <papid> W06-1601 </papid>currently either relies on small number of semantic roles, or cannot identify equivalent roles across predicates.</citsent>
<aftsection>
<nextsent>a promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates.
</nextsent>
<nextsent>the feasibility of this approach was demonstrated by gordon and swanson (2007)<papid> P07-1025 </papid>for syntactically similar verbs.</nextsent>
<nextsent>however, their approach requires at least one annotated instance of each new predicate, limiting its practicability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3115">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, pioneering work in unsupervised srl (swier and stevenson, 2004; <papid> W04-3213 </papid>grenager and manning, 2006) <papid> W06-1601 </papid>currently either relies on small number of semantic roles, or cannot identify equivalent roles across predicates.</prevsent>
<prevsent>a promising alternative direction is automatic data expansion, i.e., leveraging existing annotations to classify unseen, but similar, predicates.</prevsent>
</prevsection>
<citsent citstr=" P07-1025 ">
the feasibility of this approach was demonstrated by gordon and swanson (2007)<papid> P07-1025 </papid>for syntactically similar verbs.</citsent>
<aftsection>
<nextsent>however, their approach requires at least one annotated instance of each new predicate, limiting its practicability.
</nextsent>
<nextsent>in this paper, we present pilot study on the application of automatic data expansion to event nominalisations of verbs, such as agreement for agree or destruction for destroy.
</nextsent>
<nextsent>while event nom inalisations often afford the same semantic rolesas verbs, and often replace them in written language (gurevich et al, 2006), they have played largely marginal role in annotation.
</nextsent>
<nextsent>propbank has only annotated verbs.1 framenet annotates nouns, but covers far fewer nouns than verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3116">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>propbank has only annotated verbs.1 framenet annotates nouns, but covers far fewer nouns than verbs.
</prevsent>
<prevsent>the same 1a follow-up project, nombank (meyers et al, 2004), has since provided annotations for nominal instances, too.
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
665 situation holds in other languages (erk et al, 2003).<papid> P03-1068 </papid></citsent>
<aftsection>
<nextsent>our fundamental intuition is that it is possible to increase the annotation coverage of event nominal is ations by data expansion from verbal instances, since the verbal and nominal predicates share large part of the underlying argument structure.
</nextsent>
<nextsent>we assume that annotation is available for verbal instances.
</nextsent>
<nextsent>then, forgiven instance of nominal isation and its arguments, the aim is to assign semantic role labels to these arguments.
</nextsent>
<nextsent>we solve this task by constructing mappings between the arguments of the noun and the semantic roles realised by the verbs arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3117">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>then, forgiven instance of nominal isation and its arguments, the aim is to assign semantic role labels to these arguments.
</prevsent>
<prevsent>we solve this task by constructing mappings between the arguments of the noun and the semantic roles realised by the verbs arguments.
</prevsent>
</prevsection>
<citsent citstr=" P07-1027 ">
crucially, unlike previous work (liu and ng, 2007), <papid> P07-1027 </papid>we do not employ classical supervised approach, and thus do not require any nominal annotations.structure of the paper.</citsent>
<aftsection>
<nextsent>sec.
</nextsent>
<nextsent>2 provides background on nominalisations and srl.
</nextsent>
<nextsent>sec.
</nextsent>
<nextsent>3 provides concrete details on our expansion-based approach to srl for nominalisations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3119">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> nominalisations.  </section>
<citcontext>
<prevsection>
<prevsent>agent nominals are usually identified by suffixes such as -er, -or, -ant (e.g. speaker, applicant), while patient nominalisations end with -ee, -ed (e.g. employee).
</prevsent>
<prevsent>while these nominalisations can be analysed as events (the bakers bread implies that baking has taken place), they more naturally refer toparticipants.
</prevsent>
</prevsection>
<citsent citstr=" N04-4036 ">
in consequence, agent/patient nomi nals tend to realise fewer arguments ? the average in framenet is 1.46 arguments, compared to 1.74 propbank verbs (carreras and m`arquez, 2005) 80% nouns (liu and ng, 2007) <papid> P07-1027 </papid>73% framenet verbs (mihalcea and edmonds, 2005) 72% nouns (pradhan et al, 2004) <papid> N04-4036 </papid>64% table 1: f-scores for supervised srl (end-to-end) for events/results.</citsent>
<aftsection>
<nextsent>as our goal is nominal srl, we concentrate on the event/results class.
</nextsent>
<nextsent>srl for nominalisations.
</nextsent>
<nextsent>compared to the wealth of studies on verbal srl (e.g., gildea and jurafsky (2002); <papid> J02-3001 </papid>fleischman and hovy (2003)), there is relatively little work that specifically addresses nominal srl.</nextsent>
<nextsent>nouns are generally treated like verbs: the task is split into two classification steps,argument recognition (telling arguments from nonarguments) and argument labelling (labelling recognised arguments with role).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3123">
<title id=" C08-1084.xml">semantic role assignment for event nominalisations by leveraging verbal data </title>
<section> argument labelling.  </section>
<citcontext>
<prevsection>
<prevsent>6.3 distributional models.
</prevsent>
<prevsent>the distributional models construct mappings between verbal and nominal semantic heads.
</prevsent>
</prevsection>
<citsent citstr=" J02-3004 ">
in con4lapata (2002) <papid> J02-3004 </papid>has shown that the mapping can be dis ambiguated for individual nominalisations.</citsent>
<aftsection>
<nextsent>her model, usinglexical-semantic, contextual and pragmatic information, is out side the scope of the present paper.
</nextsent>
<nextsent>trast to the naive semantic model, they make use ofsome measure of semantic similarity to find mappings, and optionally use syntactic constraints to guide generalisation.
</nextsent>
<nextsent>in this manner, distributional models can deal with unseen feature values more effectively.
</nextsent>
<nextsent>in sentences (1) and (2), for example, an ideal distributional model would find the head word event in (2) to be more semantically similar to the head joke in (1a) than to head him in (1b).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3124">
<title id=" C04-1045.xml">improving word alignment quality using morphosyntactic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the improvements of the alignment quality compared to the, to our knowledge, best system are reported on the german-english verb mobil corpus.
</prevsent>
<prevsent>in statistical machine translation, translation model pr(fj1 |ei1) describes the correspondences between the words in the source language sentence fj1 and the words in the target language sentence ei1.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
statistical alignment models are created by introducing hidden variable aj1 representing mapping from the source word fj into the target word eaj . so far, most of the statistical machine translation systems are based on the single-word alignment models as described in (brown et al, 1993) <papid> J93-2003 </papid>as well as the hidden markov alignment model (vogel et al, 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>the lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs.in this work, we propose an approach to im prove the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words.
</nextsent>
<nextsent>we are getting use of the hierarchical representation of the statistical lexicon model as proposed in (nieen and ney, 2001) for the conventional em training procedure.
</nextsent>
<nextsent>experimental results are reported for the german-english verb mobil corpus andthe evaluation is done by comparing the obtained viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with manually annotated reference alignment.
</nextsent>
<nextsent>the popular ibm models for statistical machine translation are described in (brown et al., 1993) <papid> J93-2003 </papid>and the hmm-based alignment model was introduced in (vogel et al, 1996).<papid> C96-2141 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3125">
<title id=" C04-1045.xml">improving word alignment quality using morphosyntactic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the improvements of the alignment quality compared to the, to our knowledge, best system are reported on the german-english verb mobil corpus.
</prevsent>
<prevsent>in statistical machine translation, translation model pr(fj1 |ei1) describes the correspondences between the words in the source language sentence fj1 and the words in the target language sentence ei1.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
statistical alignment models are created by introducing hidden variable aj1 representing mapping from the source word fj into the target word eaj . so far, most of the statistical machine translation systems are based on the single-word alignment models as described in (brown et al, 1993) <papid> J93-2003 </papid>as well as the hidden markov alignment model (vogel et al, 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>the lexicon models used in these systems typically do not include any linguistic or contextual information which often results in inadequate alignments between the sentence pairs.in this work, we propose an approach to im prove the quality of the statistical alignments by taking into account the interdependencies of different derivations of the words.
</nextsent>
<nextsent>we are getting use of the hierarchical representation of the statistical lexicon model as proposed in (nieen and ney, 2001) for the conventional em training procedure.
</nextsent>
<nextsent>experimental results are reported for the german-english verb mobil corpus andthe evaluation is done by comparing the obtained viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with manually annotated reference alignment.
</nextsent>
<nextsent>the popular ibm models for statistical machine translation are described in (brown et al., 1993) <papid> J93-2003 </papid>and the hmm-based alignment model was introduced in (vogel et al, 1996).<papid> C96-2141 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3130">
<title id=" C04-1045.xml">improving word alignment quality using morphosyntactic information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>experimental results are reported for the german-english verb mobil corpus andthe evaluation is done by comparing the obtained viterbi alignments after the training of conventional models and models which are using morpho-syntactic information with manually annotated reference alignment.
</prevsent>
<prevsent>the popular ibm models for statistical machine translation are described in (brown et al., 1993) <papid> J93-2003 </papid>and the hmm-based alignment model was introduced in (vogel et al, 1996).<papid> C96-2141 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
a good overview of all these models is given in (och and ney, 2003) <papid> J03-1002 </papid>where the model ibm-6 is also introduced as the log-linear interpolation of the other models.</citsent>
<aftsection>
<nextsent>context dependencies have been introduced into the training of alignments in (varea et al,2002), but they do not take any linguistic information into account.
</nextsent>
<nextsent>some recent publications have proposed theuse of morpho-syntactic knowledge for statistical machine translation, but mostly only for the preprocessing step whereas training procedure of the statistical models remains the same (e.g.
</nextsent>
<nextsent>(nieen and ney, 2001a)).incorporation of the morpho-syntactic knowl egde into statistical models has been dealtin (nieen and ney, 2001): hierarchical lexicon models containing base forms and set of morpho-syntactic tags are proposed for the translation from german into english.
</nextsent>
<nextsent>how ever, these lexicon models are not used for the training but have been created from the viterbialignment obtained after the usual training procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3132">
<title id=" C04-1045.xml">improving word alignment quality using morphosyntactic information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(nieen and ney, 2001a)).incorporation of the morpho-syntactic knowl egde into statistical models has been dealtin (nieen and ney, 2001): hierarchical lexicon models containing base forms and set of morpho-syntactic tags are proposed for the translation from german into english.
</prevsent>
<prevsent>how ever, these lexicon models are not used for the training but have been created from the viterbialignment obtained after the usual training procedure.
</prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
the use of pos information for improving statistical alignment quality of the hmm-based model is described in (toutanova et al, 2002).<papid> W02-1012 </papid></citsent>
<aftsection>
<nextsent>they introduce additional lexicon probability for pos tags in both languages, but actually are not going beyond full forms.
</nextsent>
<nextsent>the goal of statistical machine translation is to translate an input word sequence f1, . . .
</nextsent>
<nextsent>, fj in the source language into target language word sequence e1, . . .
</nextsent>
<nextsent>, ei . given the source language sequence, we have to choose the target language sequence that maximises the product of the language model probability pr(ei1) and the translation model probability pr(fj1 |ei1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3138">
<title id=" C04-1045.xml">improving word alignment quality using morphosyntactic information </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we trained the ibm-1 and hmm model using hierarchical lexicon counts, and the parameters of the other models were also indirectly improved thanks to the refined parameters of the initial models.
</prevsent>
<prevsent>german english train sentences 34446 words 329625 343076 vocabulary 5936 3505 singletons 2600 1305 test sentences 354 words 3233 3109 relations 2559 relations 4596 table 1: corpus statistics for verb mobil task 6.1 evaluation method.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
we use the evaluation criterion described in (och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>the obtained word alignment is compared to reference alignment produced by human experts.
</nextsent>
<nextsent>the annotation scheme explicitly takes into account the ambiguity of the word alignment.
</nextsent>
<nextsent>the unambiguous alignments are annotated as sure alignments (s) and the ambiguous ones as possible alignments (p ).
</nextsent>
<nextsent>the set of possible alignments is used especially for idiomatic expressions, free translations and missing function words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3139">
<title id=" C02-1048.xml">answering it with charts dialogue in natural language and charts </title>
<section> handling dialogue in natural language.  </section>
<citcontext>
<prevsection>
<prevsent>or hpl? xpw\ if possible, otherwise it takes ? pl? xpw\ . when two such vari2chart realization has dimensions other than those discussed here.
</prevsent>
<prevsent>for example, while the independent variable is always assigned to the vertical axis in our discussion, it can be assigned to the horizontal axis.
</prevsent>
</prevsection>
<citsent citstr=" W96-0406 ">
the rank of instances on an axis, the scales of axes, and visual prompts such as labels and arrows are also dimensions which should be considered (mit tal, 1998; fasciano and lapalme, 1996).<papid> W96-0406 </papid></citsent>
<aftsection>
<nextsent>although discussion of those dime sions exceeds the scope of this paper, we believe that natural extension of perspective would cover them.
</nextsent>
<nextsent>3in implementation, perspective completion based on these features is more sophisticated, referring to heuristics from textbook for drawing charts (zelazny, 1996) and knowledge acquired from chart corpus using machine learning technique (yonezawa et al, 2000).
</nextsent>
<nextsent>figure 3: relationship between perspectives and chart forms ables are left, one takes hpl? xpw\ , the other takes ? pl? xpw\ . 3.3 utterance fragments and chart.
</nextsent>
<nextsent>appropriateness utterance fragments in specific context should be interpreted not as logical form, but as request to revise the logical form given as the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3141">
<title id=" C02-1048.xml">answering it with charts dialogue in natural language and charts </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>those, however, are concerned with tools for producing agraph interactively that achieves the users intention.
</prevsent>
<prevsent>their standpoint differs from ours, and themode used for their interactions is direct manipulation not natural language.our proposal partially overlaps with recent studies on automatic chart design.
</prevsent>
</prevsection>
<citsent citstr=" W98-0210 ">
our logical form has lot in common with the content language in (green et al, 1998).<papid> W98-0210 </papid></citsent>
<aftsection>
<nextsent>the objective of their research, however, is to describe communicative goals to be achieved through generating graphics and text, and differs from ours, which is to describe the users requests in order to respond to them using charts.our perspective plays similar role to that of intention in postgraphe (fasciano and lapalme, 1996).<papid> W96-0406 </papid></nextsent>
<nextsent>however, there is crucial difference in that, while their intention is given as input, our perspective is acquired from the users utterances, data characteristics and dialogue context.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3144">
<title id=" C08-1021.xml">knownet building a large net of knowledge from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fortunately, during the last years the research community has devised large set of innovative methods and tools for large-scale automatic acquisition of lexical knowledge from structured and unstructured corpora.
</prevsent>
<prevsent>among others we can men 1 symmetric relations are counted only once.
</prevsent>
</prevsection>
<citsent citstr=" W01-0703 ">
161 tion extended wordnet (mihalcea and moldovan, 2001), large collections of semantic preferences acquired from semcor (agirre and martinez, 2001; <papid> W01-0703 </papid>agirre and martinez, 2002) or acquired from british national corpus (bnc) (mccarthy, 2001),large-scale topic signatures for each synset acquired from the web (agirre and de lacalle, 2004) or knowledge about individuals from wikipedia(suchanek et al, 2007).</citsent>
<aftsection>
<nextsent>obviously, all these semantic resources have been acquired using very different methods, tools and corpora.
</nextsent>
<nextsent>as expected, each semantic resource has different volume and accuracy figures when evaluated in common and controlled framework (cuadros and rigau, 2006).<papid> W06-1663 </papid>however, not all these large-scale resources encode semantic relations between synsets.</nextsent>
<nextsent>in some cases, only relations between synsets and words have been acquired.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3145">
<title id=" C08-1021.xml">knownet building a large net of knowledge from the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>161 tion extended wordnet (mihalcea and moldovan, 2001), large collections of semantic preferences acquired from semcor (agirre and martinez, 2001; <papid> W01-0703 </papid>agirre and martinez, 2002) or acquired from british national corpus (bnc) (mccarthy, 2001),large-scale topic signatures for each synset acquired from the web (agirre and de lacalle, 2004) or knowledge about individuals from wikipedia(suchanek et al, 2007).</prevsent>
<prevsent>obviously, all these semantic resources have been acquired using very different methods, tools and corpora.</prevsent>
</prevsection>
<citsent citstr=" W06-1663 ">
as expected, each semantic resource has different volume and accuracy figures when evaluated in common and controlled framework (cuadros and rigau, 2006).<papid> W06-1663 </papid>however, not all these large-scale resources encode semantic relations between synsets.</citsent>
<aftsection>
<nextsent>in some cases, only relations between synsets and words have been acquired.
</nextsent>
<nextsent>this is the case of the topic signatures acquired from the web (agirre and dela calle, 2004).
</nextsent>
<nextsent>this is one of the largest semantic resources ever built with around one hundred million relations between synsets and semantically related words 2 .a knowledge net or knownet (kn), is an exten sible, large and accurate knowledge base, which has been derived by semantically disambiguating small portions of the topic signatures acquired from the web.
</nextsent>
<nextsent>basically, the method uses robust and accurate knowledge-based word sense disambiguation algorithm to assign the most appropriate senses to the topic words associated to particular synset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3146">
<title id=" C08-1021.xml">knownet building a large net of knowledge from the web </title>
<section> topic signatures.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4, we present the evaluation framework used in this study.
</prevsent>
<prevsent>section 5describes the results when evaluating different versions of knownet and finally, section 6 presents some concluding remarks and future work.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
topic signatures (ts) are word vectors related to aparticular topic (lin and hovy, 2000).<papid> C00-1072 </papid></citsent>
<aftsection>
<nextsent>topic signatures are built by retrieving context words of atarget topic from large corpora.
</nextsent>
<nextsent>this study considers word senses as topics.
</nextsent>
<nextsent>basically, the acquisition of ts consists of: ? acquiring the best possible corpus examples for particular word sense (usually characterizing each word sense as query and performing search on the corpus for those examples that best match the queries) ? building the ts by selecting the context words that best represent the word sense from the selected corpora.
</nextsent>
<nextsent>the topic signatures acquired from the web (hereinafter tsweb) constitutes one of the largest semantic resource available with around 100 million relations (between synsets and words) (agirre and de lacalle, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3147">
<title id=" C08-1021.xml">knownet building a large net of knowledge from the web </title>
<section> topic signatures.  </section>
<citcontext>
<prevsection>
<prevsent>basically, the acquisition of ts consists of: ? acquiring the best possible corpus examples for particular word sense (usually characterizing each word sense as query and performing search on the corpus for those examples that best match the queries) ? building the ts by selecting the context words that best represent the word sense from the selected corpora.
</prevsent>
<prevsent>the topic signatures acquired from the web (hereinafter tsweb) constitutes one of the largest semantic resource available with around 100 million relations (between synsets and words) (agirre and de lacalle, 2004).
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
inspired by the work of (leacock et al, 1998), <papid> J98-1006 </papid>tsweb was constructed using monosemous relatives from wn (synonyms,hypernyms, direct and indirect hyponyms, and sib lings), querying google and retrieving up to one thousand snippets per query (that is, word sense),extracting the salient words with distinctive frequency using tfidf.</citsent>
<aftsection>
<nextsent>thus, tsweb consist oflarge ordered lists of words with weights associated to the polysemous nouns of wn1.6.
</nextsent>
<nextsent>the number of constructed topic signatures is 35,250 with an average size per signature of 6,877 words.
</nextsent>
<nextsent>162 tammany#n 0.0319 federalist#n 0.0315 whig#n 0.0300 missionary#j 0.0229 democratic#n 0.0218 nazi#j 0.0202 republican#n 0.0189 constitutional#n 0.0186 conservative#j 0.0148 socialist#n 0.0140 table 2: ts of party#n#1 (first 10 out of 12,890 total words) when evaluating tsweb, we used at maximum the first 700 words while for building knownet we used at maximum the first 20 words.
</nextsent>
<nextsent>for example, table 2 presents the first words (lemmas and part-of-speech) and weights of the topic signature acquired for party#n#1 4 .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3148">
<title id=" C08-1021.xml">knownet building a large net of knowledge from the web </title>
<section> evaluation framework.  </section>
<citcontext>
<prevsection>
<prevsent>this means that the restof relations from knownet-20 are new.
</prevsent>
<prevsent>as expected, each knownet is very large, ranging from hundreds of thousands to millions of new semantic relations between synsets among increasing sets of synsets.
</prevsent>
</prevsection>
<citsent citstr=" W07-2015 ">
in order to empirically establish the relative quality of these new semantic resources, we used the evaluation framework of task 16 of semeval-2007: evaluation of wide coverage knowledge resources (cuadros and rigau, 2007).<papid> W07-2015 </papid></citsent>
<aftsection>
<nextsent>in this framework all knowledge resources are evaluated on common wsd task.
</nextsent>
<nextsent>in particular, we used the noun-sets of the english lexical sample task of senseval-3 and semeval-2007exercises which consists of 20 and 35 nouns respectively.
</nextsent>
<nextsent>all performances are evaluated on thetest data using the fine-grained scoring system provided by the organizers.
</nextsent>
<nextsent>furthermore, trying to be as neutral as possible with respect to the resources studied, we applied systematically the same disambiguation method toall of them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3149">
<title id=" C02-1133.xml">analysis of titles and readers for title generation centered on the readers </title>
<section> comparison between paper titles.  </section>
<citcontext>
<prevsection>
<prevsent>(each alphabet means syntactic function tag.
</prevsent>
<prevsent>means either zero times or one time?.)using the chasen?
</prevsent>
</prevsection>
<citsent citstr=" C00-1004 ">
japanese morphological analyzer (asahara and matsumoto, 2000) <papid> C00-1004 </papid>and syntactic clues (such as word order, syntactic category and specific prepositions).</citsent>
<aftsection>
<nextsent>the results are reviewed and errors are manually corrected.
</nextsent>
<nextsent>as for headlines, automatic tagging is difficult because of the frequent use of verbal omissions and inversion.
</nextsent>
<nextsent>therefore, we manually divided the headlines.
</nextsent>
<nextsent>in order to improve the precision of human tagging as highly as possible, the human tagger divided them according to the following procedure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3150">
<title id=" C02-1081.xml">data driven classification of linguistic styles in spoken dialogues </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>studies have indicated that variations between conversations is high but low within conversations(brennan, 1996) because people mark their shared conceptualizations by using the same term, lexical entrainment.
</prevsent>
<prevsent>a spoken dialogue system can use stylistic information to adapt its output behavior.
</prevsent>
</prevsection>
<citsent citstr=" C94-2174 ">
further determiners of style are the domain or genre (karlgren and cutting, 1994; <papid> C94-2174 </papid>wolters and kirsten, 1999),<papid> E99-1019 </papid>the modalities (speech only vs. speech and visual interfaces) (fais et al, 1996; oviatt et al, 1994), and the degree of interactivity (oviatt and cohen, 1991) which are more or less determined by the application scenario of spoken dialogue system.one specific aspect of spoken input is its larger irregularity.</citsent>
<aftsection>
<nextsent>for written texts, robust parsers can be employed to obtain style relevant information (karlgren,1994; paiva, 2000), while for spontaneous speech simpler measurements have to be used like partofspeech tags (ries, 1999).
</nextsent>
<nextsent>klarner (1997) investigated stylistic differences of speakers in the verb mobil dialogue corpus in order to improve speech recognition by using speaker type dependent language models.
</nextsent>
<nextsent>the achieved reduction in perplexity, however, is relatively low.for the research project smartkom funded by the german ministry of research (bmbf) (wahlster et al, 2001)a module is being developed that constructs and maintains model of human computer interaction.
</nextsent>
<nextsent>one part models the interaction style of the user (experience withthe system, experience with the task, preferred modalities for input and output), the other part the linguistic style.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3151">
<title id=" C02-1081.xml">data driven classification of linguistic styles in spoken dialogues </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>studies have indicated that variations between conversations is high but low within conversations(brennan, 1996) because people mark their shared conceptualizations by using the same term, lexical entrainment.
</prevsent>
<prevsent>a spoken dialogue system can use stylistic information to adapt its output behavior.
</prevsent>
</prevsection>
<citsent citstr=" E99-1019 ">
further determiners of style are the domain or genre (karlgren and cutting, 1994; <papid> C94-2174 </papid>wolters and kirsten, 1999),<papid> E99-1019 </papid>the modalities (speech only vs. speech and visual interfaces) (fais et al, 1996; oviatt et al, 1994), and the degree of interactivity (oviatt and cohen, 1991) which are more or less determined by the application scenario of spoken dialogue system.one specific aspect of spoken input is its larger irregularity.</citsent>
<aftsection>
<nextsent>for written texts, robust parsers can be employed to obtain style relevant information (karlgren,1994; paiva, 2000), while for spontaneous speech simpler measurements have to be used like partofspeech tags (ries, 1999).
</nextsent>
<nextsent>klarner (1997) investigated stylistic differences of speakers in the verb mobil dialogue corpus in order to improve speech recognition by using speaker type dependent language models.
</nextsent>
<nextsent>the achieved reduction in perplexity, however, is relatively low.for the research project smartkom funded by the german ministry of research (bmbf) (wahlster et al, 2001)a module is being developed that constructs and maintains model of human computer interaction.
</nextsent>
<nextsent>one part models the interaction style of the user (experience withthe system, experience with the task, preferred modalities for input and output), the other part the linguistic style.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3152">
<title id=" C02-1081.xml">data driven classification of linguistic styles in spoken dialogues </title>
<section> corpora.  </section>
<citcontext>
<prevsection>
<prevsent>both parts are supposed to make use of stereotypes (rich, 1979).the experiments described below explore the possibility to consistently extract linguistic parameters from spoken dialogues, to use these parameters in order to group speakers into several classes, and to train learning algorithms that classify users by their parameter values.
</prevsent>
<prevsent>the task of spoken dialogue system is to engage in spoken human computer interaction.
</prevsent>
</prevsection>
<citsent citstr=" W01-1607 ">
it is well known that spoken human computer interaction differs from its human human counterpart in various dimensions (doranet al, 2001) <papid> W01-1607 </papid>including linguistic complexity.</citsent>
<aftsection>
<nextsent>for the purpose of this investigation three sources were exploited: corpus of task dependent human human interactions (negotiation dialogues), corpus of free humanhumanconversations, and corpus of human computer interactions.
</nextsent>
<nextsent>for all corpora the partofspeech information for each word was automatically annotated by the ims tree tagger (schmid, 1994) using the stts tagset (schiller et al., 1995).
</nextsent>
<nextsent>verb mobil the verb mobil (vm) corpus (wahlster,1993) is one of the largest spoken dialogue corpora available for german.
</nextsent>
<nextsent>it contains spontaneous speech human human dialogues in the appointment negotiation and travel planning domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3153">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it was also observed that the bidirectional method was better for english-to-japanese translation.
</prevsent>
<prevsent>the statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of translation target text given translation source text.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
according to the bayes rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum posteriori solution of distribution for channel target text given channel source text and prior distribution for the channel source text (brown et al, 1993).<papid> J93-2003 </papid>although there exists efficient algorithms to estimate the parameters for the statistical machine translation (smt), one of the problems of smt isthe search algorithms for the translation given sequence of words.</citsent>
<aftsection>
<nextsent>there exists stack decoding algorithm (berger et al, 1996), a* search algorithm (och et al, 2001; <papid> W01-1408 </papid>wang and waibel, 1997) <papid> P97-1047 </papid>and dynamic-programming algorithms (tillmann and ney, 2000; <papid> C00-2123 </papid>garcia-varea and casacuberta, 2001), and all translate given input string word-by-wordand render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</nextsent>
<nextsent>the algorithms proposed above cannot deal with drastically different word correspondence, such as japanese and english translation, where japanese is sov while svo in english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3154">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of translation target text given translation source text.
</prevsent>
<prevsent>according to the bayes rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum posteriori solution of distribution for channel target text given channel source text and prior distribution for the channel source text (brown et al, 1993).<papid> J93-2003 </papid>although there exists efficient algorithms to estimate the parameters for the statistical machine translation (smt), one of the problems of smt isthe search algorithms for the translation given sequence of words.</prevsent>
</prevsection>
<citsent citstr=" W01-1408 ">
there exists stack decoding algorithm (berger et al, 1996), a* search algorithm (och et al, 2001; <papid> W01-1408 </papid>wang and waibel, 1997) <papid> P97-1047 </papid>and dynamic-programming algorithms (tillmann and ney, 2000; <papid> C00-2123 </papid>garcia-varea and casacuberta, 2001), and all translate given input string word-by-wordand render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</citsent>
<aftsection>
<nextsent>the algorithms proposed above cannot deal with drastically different word correspondence, such as japanese and english translation, where japanese is sov while svo in english.
</nextsent>
<nextsent>germann et al (2001) <papid> P01-1030 </papid>suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</nextsent>
<nextsent>this paper presents two decoding methods, oneis the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3156">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of translation target text given translation source text.
</prevsent>
<prevsent>according to the bayes rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum posteriori solution of distribution for channel target text given channel source text and prior distribution for the channel source text (brown et al, 1993).<papid> J93-2003 </papid>although there exists efficient algorithms to estimate the parameters for the statistical machine translation (smt), one of the problems of smt isthe search algorithms for the translation given sequence of words.</prevsent>
</prevsection>
<citsent citstr=" P97-1047 ">
there exists stack decoding algorithm (berger et al, 1996), a* search algorithm (och et al, 2001; <papid> W01-1408 </papid>wang and waibel, 1997) <papid> P97-1047 </papid>and dynamic-programming algorithms (tillmann and ney, 2000; <papid> C00-2123 </papid>garcia-varea and casacuberta, 2001), and all translate given input string word-by-wordand render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</citsent>
<aftsection>
<nextsent>the algorithms proposed above cannot deal with drastically different word correspondence, such as japanese and english translation, where japanese is sov while svo in english.
</nextsent>
<nextsent>germann et al (2001) <papid> P01-1030 </papid>suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</nextsent>
<nextsent>this paper presents two decoding methods, oneis the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3157">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of translation target text given translation source text.
</prevsent>
<prevsent>according to the bayes rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum posteriori solution of distribution for channel target text given channel source text and prior distribution for the channel source text (brown et al, 1993).<papid> J93-2003 </papid>although there exists efficient algorithms to estimate the parameters for the statistical machine translation (smt), one of the problems of smt isthe search algorithms for the translation given sequence of words.</prevsent>
</prevsection>
<citsent citstr=" C00-2123 ">
there exists stack decoding algorithm (berger et al, 1996), a* search algorithm (och et al, 2001; <papid> W01-1408 </papid>wang and waibel, 1997) <papid> P97-1047 </papid>and dynamic-programming algorithms (tillmann and ney, 2000; <papid> C00-2123 </papid>garcia-varea and casacuberta, 2001), and all translate given input string word-by-wordand render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</citsent>
<aftsection>
<nextsent>the algorithms proposed above cannot deal with drastically different word correspondence, such as japanese and english translation, where japanese is sov while svo in english.
</nextsent>
<nextsent>germann et al (2001) <papid> P01-1030 </papid>suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</nextsent>
<nextsent>this paper presents two decoding methods, oneis the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3158">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there exists stack decoding algorithm (berger et al, 1996), a* search algorithm (och et al, 2001; <papid> W01-1408 </papid>wang and waibel, 1997) <papid> P97-1047 </papid>and dynamic-programming algorithms (tillmann and ney, 2000; <papid> C00-2123 </papid>garcia-varea and casacuberta, 2001), and all translate given input string word-by-wordand render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</prevsent>
<prevsent>the algorithms proposed above cannot deal with drastically different word correspondence, such as japanese and english translation, where japanese is sov while svo in english.</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
germann et al (2001) <papid> P01-1030 </papid>suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</citsent>
<aftsection>
<nextsent>this paper presents two decoding methods, oneis the right-to-left decoding based on the left-toright beam search algorithm, which generates outputs from the end of sentence.
</nextsent>
<nextsent>the second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one.
</nextsent>
<nextsent>the experimental results of japanese and english translation indicated that the right-to-leftdecoding was better for english-to-japanese translation, while the left-to-right decoding was better for japanese-to-english decoding.
</nextsent>
<nextsent>the above results could be justified by the structural difference of japanese and english, where english takes the prefix structure that places emphasis at the beginning of sentence, hence prefers left-to-right decoding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3160">
<title id=" C02-1050.xml">bidirectional decoding for statistical machine translation </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given and the model, p(f|e) = p(f, a|e) and p(e).
</prevsent>
<prevsent>for the space of is extremely large, |a|l+1, where the is the output length, an approximation of p(f|e)   p(f, a|e) is used when exploring the possible candidates of translation.
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
this problem is known to be np-complete (knight, 1999), <papid> J99-4005 </papid>for the re-ordering property in the model further complicates the search.</citsent>
<aftsection>
<nextsent>one of the solution is the left-to-right generation of output by consuming input words in any-order.
</nextsent>
<nextsent>under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as berger et al (1996), och et al (2001), <papid> W01-1408 </papid>wang andwaibel (1997), <papid> P97-1047 </papid>tillmann and ney (2000) <papid> C00-2123 </papid>garcia varea and casacuberta (2001) and germann et al (2001), <papid> P01-1030 </papid>though they all based on almost linearly translation model lexical model ? t( j|ei) fertility model ? n(i |ei) distortion model head ? d1( ? ci|a(ei )b( j)) non-head ? d1 ( ? j?|b( j)) null translation model (m0 0 ) pm200 0 1 figure 2: translation model (ibm model 4)aligned language pairs, and not suitable for language pairs with totally different alignment correspondence, such as japanese and english.</nextsent>
<nextsent>the decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in tillmann and ney (2000) <papid> C00-2123 </papid>and och et al (2001), <papid> W01-1408 </papid>and operation applied to each hypothesis is similar to those explained in berger et al (1996), och et al (2001) <papid> W01-1408 </papid>and germann et al.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3182">
<title id=" C08-1022.xml">a classifier based approach to preposition and determiner error correction in l2 english </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they report results for different error types (e.g. omission - precision 75.7%, recall 45.67%; replacement - 31.17%, 8%), but there is no break-down of results by individual pos.
</prevsent>
<prevsent>han et al (2006) use maximum entropy classifier to detect determiner errors, achieving 83% accuracy.
</prevsent>
</prevsection>
<citsent citstr=" W07-1604 ">
chodorow et al (2007) <papid> W07-1604 </papid>present an approach to preposition error detection which also uses model based on maximum entropy classifier trained on set of contextual features, together with rule-based filter.</citsent>
<aftsection>
<nextsent>they report 80% precision and 30% recall.
</nextsent>
<nextsent>finally, gamon etal.
</nextsent>
<nextsent>(2008) use complex system including decision tree and language model for both preposition and determiner errors, while yi et al (2008)propose web count-based system to correct determiner errors (p 62%, 41%).the work presented here displays some similarities to the papers mentioned above in its use of maximum entropy classifier and set of features.however, our feature set is more linguistically sophisticated in that it relies on full syntactic analysis of the data.
</nextsent>
<nextsent>it includes some semantic components which we believe play role in correct class assignment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3183">
<title id=" C08-1022.xml">a classifier based approach to preposition and determiner error correction in l2 english </title>
<section> acquiring the models.  </section>
<citcontext>
<prevsection>
<prevsent>we achieve accuracy of 92.15% on thel1 data (test set size: 305,264), as shown in table 7.
</prevsent>
<prevsent>again, the baseline refers to the most frequent class, null.
</prevsent>
</prevsection>
<citsent citstr=" N07-2045 ">
the best reported results to date on determiner selection are those in turner and charniak (2007).<papid> N07-2045 </papid></citsent>
<aftsection>
<nextsent>our model outperforms their n-gram language model approach by over 5%.
</nextsent>
<nextsent>since the two approaches are not tested on the same data this comparison is not conclusive, but we are optimistic that there is real difference inaccuracy since the type of texts used are not dissimilar.
</nextsent>
<nextsent>as in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across the three classes; this information is reported in table 8.
</nextsent>
<nextsent>here we can see that there is very strong correlation between amount of data seen in training and precision and recall.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3184">
<title id=" C08-1049.xml">word lattice reranking for chinese word segmentation and partofspeech tagging </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>in this paper, we describe new reranking strategy named word lattice reranking,for the task of joint chinese word segmentation and part-of-speech (pos) tagging.
</prevsent>
</prevsection>
<citsent citstr=" P08-1067 ">
as derivation of the forest reranking for parsing (huang, 2008), <papid> P08-1067 </papid>this strategy reranks on the pruned word lattice, which potentially contains much more candidates while using less storage, compared with the traditional n-best list reranking.</citsent>
<aftsection>
<nextsent>with aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local features that cant be easily incorporated intothe perceptron baseline.
</nextsent>
<nextsent>experimental results show that, this strategy achieves improvement on both segmentation and pos tagging, above the perceptron baseline and the n-best list reranking.
</nextsent>
<nextsent>recent work for chinese word segmentation and pos tagging pays much attention to discriminative methods, such as maximum entropy model (me)(ratnaparkhi and adwait, 1996), conditional random fields (crfs) (lafferty et al, 2001), perceptron training algorithm (collins, 2002), <papid> W02-1001 </papid>etc. compared to generative ones such as hidden markov model (hmm) (rabiner, 1989; fine et al, 1998),discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks.originated by xue and shen (2003), the typical approach of discriminative models conducts c?</nextsent>
<nextsent>2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3186">
<title id=" C08-1049.xml">word lattice reranking for chinese word segmentation and partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with aperceptron classifier trained with local features as the baseline, word lattice reranking performs reranking with non-local features that cant be easily incorporated intothe perceptron baseline.
</prevsent>
<prevsent>experimental results show that, this strategy achieves improvement on both segmentation and pos tagging, above the perceptron baseline and the n-best list reranking.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
recent work for chinese word segmentation and pos tagging pays much attention to discriminative methods, such as maximum entropy model (me)(ratnaparkhi and adwait, 1996), conditional random fields (crfs) (lafferty et al, 2001), perceptron training algorithm (collins, 2002), <papid> W02-1001 </papid>etc. compared to generative ones such as hidden markov model (hmm) (rabiner, 1989; fine et al, 1998),discriminative models have the advantage of flexibility in representing features, and usually obtains almost perfect accuracy in two tasks.originated by xue and shen (2003), the typical approach of discriminative models conducts c?</citsent>
<aftsection>
<nextsent>2008.
</nextsent>
<nextsent>licensed to the coling 2008 organizing committee for publication in coling 2008 and for re-publishing in any form or medium.segmentation in classification style, by assigning each character positional tag indicating its relative position in the word.
</nextsent>
<nextsent>if we extend these positional tags to include pos information, segmentation and pos tagging can be performed by single pass under unify classification framework (ng and low, 2004).<papid> W04-3236 </papid></nextsent>
<nextsent>in the rest of the paper, we call this operation mode joint s&amp;t.; experiments of ng and low (2004) <papid> W04-3236 </papid>shown that, compared with performing segmentation and pos tagging one at time, joint s&t; can achieve higher accuracy not only on segmentation but also on pos tagging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3187">
<title id=" C08-1049.xml">word lattice reranking for chinese word segmentation and partofspeech tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2008.
</prevsent>
<prevsent>licensed to the coling 2008 organizing committee for publication in coling 2008 and for re-publishing in any form or medium.segmentation in classification style, by assigning each character positional tag indicating its relative position in the word.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
if we extend these positional tags to include pos information, segmentation and pos tagging can be performed by single pass under unify classification framework (ng and low, 2004).<papid> W04-3236 </papid></citsent>
<aftsection>
<nextsent>in the rest of the paper, we call this operation mode joint s&amp;t.; experiments of ng and low (2004) <papid> W04-3236 </papid>shown that, compared with performing segmentation and pos tagging one at time, joint s&t; can achieve higher accuracy not only on segmentation but also on pos tagging.</nextsent>
<nextsent>besides the usual local features such as the character-based ones (xue and shen, 2003; ng and low, 2004), <papid> W04-3236 </papid>many non-local features related to poss or words can also be employed to improveperformance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3204">
<title id=" C08-1049.xml">word lattice reranking for chinese word segmentation and partofspeech tagging </title>
<section> baseline perceptron classifier.  </section>
<citcontext>
<prevsection>
<prevsent>so we use buffer buf to keep extracted derivations (line 16), then sort buf and put its first items to v (line 18).
</prevsent>
<prevsent>4.1 joint s&t; as classification.
</prevsent>
</prevsection>
<citsent citstr=" P08-1102 ">
following jiang et al (2008), <papid> P08-1102 </papid>we describe segmentation and joint s&t; as below: forgiven chinese sentence appearing as character sequence: 1:n = 1 2 ..</citsent>
<aftsection>
<nextsent>c the goal of segmentation is splitting the sequence into several subsequences: 1:e 1 e 1 +1:e 2 ..
</nextsent>
<nextsent>c m1 +1:e while in joint s&t;, each of these sub sequences is labelled pos tag: 1:e 1 /t 1 e 1 +1:e 2 /t 2 ..
</nextsent>
<nextsent>c m1 +1:e /t where i (i = 1..n) denotes character, l:r (l ? r) denotes the sub sequence ranging from l to r , and i (i = 1..m,m ? n) denotes the pos tag of e i1 +1:e .if we label each character positional tag indicating its relative position in an expected sub sequence, we can obtain the segmentation result accordingly.
</nextsent>
<nextsent>as described in ng and low (2004) <papid> W04-3236 </papid>and jiang et al (2008), <papid> P08-1102 </papid>we use indicating single character word, while b, and indicating the begin, middle and end of word respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3222">
<title id=" C02-1130.xml">fine grained classification of named entities </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>inspection of the data made clear the need for semantic information during classification.
</prevsent>
<prevsent>we therefore created features that use topic signatures for each of the person subcategories.
</prevsent>
</prevsection>
<citsent citstr=" C00-1072 ">
a topic signature, as described in (lin and hovy, 2000), <papid> C00-1072 </papid>is list of terms that can be used to signal the membership of text in the relevant topic or category.</citsent>
<aftsection>
<nextsent>each term in text is given topic signature score that indicates its ability to signal that the text is in relevant category (the higher the score, the more that term is indicative of that category).
</nextsent>
<nextsent>the topic signatures are automatically generated for each specific term by computing the likelihood ratio (?-score) between two hypotheses (dunning, 1993).<papid> J93-1003 </papid></nextsent>
<nextsent>the first hypothesis (h1) is that the probability (p1) that the text is in the relevant category, given specific term, is equivalent to the probability (p2) that the text is in the relevant category, given any other term (h1: p1=p2).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3223">
<title id=" C02-1130.xml">fine grained classification of named entities </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>a topic signature, as described in (lin and hovy, 2000), <papid> C00-1072 </papid>is list of terms that can be used to signal the membership of text in the relevant topic or category.</prevsent>
<prevsent>each term in text is given topic signature score that indicates its ability to signal that the text is in relevant category (the higher the score, the more that term is indicative of that category).</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the topic signatures are automatically generated for each specific term by computing the likelihood ratio (?-score) between two hypotheses (dunning, 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>the first hypothesis (h1) is that the probability (p1) that the text is in the relevant category, given specific term, is equivalent to the probability (p2) that the text is in the relevant category, given any other term (h1: p1=p2).
</nextsent>
<nextsent>the second hypothesis (h2) is that these two probabilities are not equivalent, and that p1 is much greater than p2 (h2: p1  p2).
</nextsent>
<nextsent>the calculation of this likelihood ratio [-2logl(h1)/l(h2)] for each feature and for each category gives list of all the terms in document set with scores indicating how much the presence of that term in specific document indicates that the document is in specific category.
</nextsent>
<nextsent>politician entertainer word ?-score word ?-score campaign 3457.049 star 3283.872 republican 1969.707 actor 2478.675 budget 140.292 budget 17.312 bigot 2.577 sexist 3.874 figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3225">
<title id=" C02-1130.xml">fine grained classification of named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in future work we will examine how ensemble learning (hastie, 2001) might be used to capitalize further on these qualitatively different feature sets.
</prevsent>
<prevsent>while much research has gone into the coarse categorization of named entities, we are not aware of much previous work using learning algorithms to perform more fine-grained classification.
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
wacholder et al  (1997) <papid> A97-1030 </papid>use hand-written rules and knowledge bases to classify proper names into broad categories.</citsent>
<aftsection>
<nextsent>they employ an aggregation method similar to memrun, but do not use multiple thresholds to increase accuracy.
</nextsent>
<nextsent>macdonald (1993) also uses hand-written rules for coarse named entity categorization.
</nextsent>
<nextsent>however, where wacholder et al  use evidence internal to the entity name, macdonald employs local context to aid in classification.
</nextsent>
<nextsent>such hand-written heuristic rules resemble those we automatically generate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3226">
<title id=" C02-1130.xml">fine grained classification of named entities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, where wacholder et al  use evidence internal to the entity name, macdonald employs local context to aid in classification.
</prevsent>
<prevsent>such hand-written heuristic rules resemble those we automatically generate.
</prevsent>
</prevsection>
<citsent citstr=" P00-1011 ">
bechet et al  (2000) <papid> P00-1011 </papid>use decision tree algorithm to classify unknown proper names into the categories: first name, last name, country, town, and organization.</citsent>
<aftsection>
<nextsent>this is still much coarser distinction than that focused on in this research.
</nextsent>
<nextsent>further, bechet et al  focused only on those proper names embedded in complex noun phrases (nps), using only elements in the np as its feature set.
</nextsent>
<nextsent>the results of these experiments, though preliminary, are very promising.
</nextsent>
<nextsent>our research makes clear that positive results are possible with relatively simple statistical techniques.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3227">
<title id=" C02-1130.xml">fine grained classification of named entities </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>this failure to include such lower level business persons means that large space of the classification domain is not covered by the training set, which in turn leads to poor results on the held out test set.
</prevsent>
<prevsent>the results of these experiments suggest that better fine-grained classification of named entities will require not only more sophisticated feature selection, but also better data generation procedure.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
in future work, we will investigate more sophisticated bootstrapping methods, as (collins &amp; singer, 1999) <papid> W99-0613 </papid>as well as co-training and co-testing (muslea et al , 2000).</citsent>
<aftsection>
<nextsent>in future work we will also examine adapting the hierarchical decision list algorithm from (yarowsky, 2000) to our task.
</nextsent>
<nextsent>treating fine-grained classification of named entities as word sense disambiguation problem (where categories are treated as different senses of generic person name?)
</nextsent>
<nextsent>allows these methods to be directly applicable.
</nextsent>
<nextsent>the algorithm is particularly relevant in that it provides an intuitive way to take advantage of the similarities of certain categories (e.g., athlete and entertainer).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3228">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, many corpora have become available, and have been used to generate natural surface sentences.
</prevsent>
<prevsent>for example, corpora have been used to generate sentences for language model estimation in statistical machine translation.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
in such translation, given source language text, s, the translated text, , in the target language that maximizes the probability (t |s) is selected as the most appropriate translation, best , which is represented as (brown et al, 1990) <papid> J90-2002 </papid>tbest = argmaxtp (t |s) = argmaxt (p (s|t ) ? (t )) .</citsent>
<aftsection>
<nextsent>(1) in this equation, (s|t ) represents the model used to replace words or phrases in source language with those in the target language.
</nextsent>
<nextsent>it is called translation model.
</nextsent>
<nextsent>p (t ) represents alan guage model that is used to reorder translated words or phrases into natural order in the target language.
</nextsent>
<nextsent>the input of the language model is bag of words,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3229">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> the candidate-text sentence that maxi-.  </section>
<citcontext>
<prevsection>
<prevsent>(4); that is, keyword-production model, morpheme model that estimates how likely string is to be morpheme, and dependency model.
</prevsent>
<prevsent>the goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we implemented these models within an maximum entropy framework (berger et al, 1996; <papid> J96-1002 </papid>ristad, 1997; ristad, 1998).</citsent>
<aftsection>
<nextsent>4.1 keyword-production models.
</nextsent>
<nextsent>this section describes five keyword-production models which are represented by (k|m,d,t ) in eq.
</nextsent>
<nextsent>(4).
</nextsent>
<nextsent>in these models, we define the set of headwords whose frequency in the corpus is over certain threshold as set of keywords, ks, and we restrict the bunsetsus to those generated by the generation rules represented in form (5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3230">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for headwords that were not in ks, we added their major part-of-speech categories to the set.
</prevsent>
<prevsent>we trained our keyword-production models byusing 1,129 sentences (containing 10,201 head words) from newspaper articles appearing on january 1st.
</prevsent>
</prevsection>
<citsent citstr=" W01-0512 ">
we used morpheme model and dependency model identical to those proposed by uchimoto et al (uchimoto et al, 2001; <papid> W01-0512 </papid>uchimoto et al, 1999; <papid> E99-1026 </papid>uchimoto et al, 2000<papid> C00-2126 </papid>b).</citsent>
<aftsection>
<nextsent>to train the models, we used 8,835 sentences from newspaper articles appearing from january 1st to 9th in 1995.
</nextsent>
<nextsent>generation rules were acquired from newspaper articles appearing from january 1st to 16th.
</nextsent>
<nextsent>the total number of sentences was 18,435.
</nextsent>
<nextsent>first, we evaluated the outputs generated when the rightmost two keywords, such as ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3231">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for headwords that were not in ks, we added their major part-of-speech categories to the set.
</prevsent>
<prevsent>we trained our keyword-production models byusing 1,129 sentences (containing 10,201 head words) from newspaper articles appearing on january 1st.
</prevsent>
</prevsection>
<citsent citstr=" E99-1026 ">
we used morpheme model and dependency model identical to those proposed by uchimoto et al (uchimoto et al, 2001; <papid> W01-0512 </papid>uchimoto et al, 1999; <papid> E99-1026 </papid>uchimoto et al, 2000<papid> C00-2126 </papid>b).</citsent>
<aftsection>
<nextsent>to train the models, we used 8,835 sentences from newspaper articles appearing from january 1st to 9th in 1995.
</nextsent>
<nextsent>generation rules were acquired from newspaper articles appearing from january 1st to 16th.
</nextsent>
<nextsent>the total number of sentences was 18,435.
</nextsent>
<nextsent>first, we evaluated the outputs generated when the rightmost two keywords, such as ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3232">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>for headwords that were not in ks, we added their major part-of-speech categories to the set.
</prevsent>
<prevsent>we trained our keyword-production models byusing 1,129 sentences (containing 10,201 head words) from newspaper articles appearing on january 1st.
</prevsent>
</prevsection>
<citsent citstr=" C00-2126 ">
we used morpheme model and dependency model identical to those proposed by uchimoto et al (uchimoto et al, 2001; <papid> W01-0512 </papid>uchimoto et al, 1999; <papid> E99-1026 </papid>uchimoto et al, 2000<papid> C00-2126 </papid>b).</citsent>
<aftsection>
<nextsent>to train the models, we used 8,835 sentences from newspaper articles appearing from january 1st to 9th in 1995.
</nextsent>
<nextsent>generation rules were acquired from newspaper articles appearing from january 1st to 16th.
</nextsent>
<nextsent>the total number of sentences was 18,435.
</nextsent>
<nextsent>first, we evaluated the outputs generated when the rightmost two keywords, such as ??
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3234">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to distinguish definite and indefinite nouns, and so on, and in this case there are no corresponding words in japanese.
</prevsent>
<prevsent>knight et al proposed way to compensate for missing information caused by lack of language-dependent knowledge, or knowledge gap?
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
(knight and hatzivassiloglou, 1995; <papid> P95-1034 </papid>langkilde and knight, 1998<papid> W98-1426 </papid>a; langkildeand knight, 1998<papid> W98-1426 </papid>b).</citsent>
<aftsection>
<nextsent>they use semantic expressions as input, whereas we use keywords.
</nextsent>
<nextsent>also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select themost appropriate surface text.
</nextsent>
<nextsent>while we can not use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generatesurface-text sentences from candidate-text sentences in the form of dependency trees.
</nextsent>
<nextsent>we can also apply the formalism proposed by langkilde (langkilde, 2000) <papid> A00-2023 </papid>to express the candidate-text sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3235">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to distinguish definite and indefinite nouns, and so on, and in this case there are no corresponding words in japanese.
</prevsent>
<prevsent>knight et al proposed way to compensate for missing information caused by lack of language-dependent knowledge, or knowledge gap?
</prevsent>
</prevsection>
<citsent citstr=" W98-1426 ">
(knight and hatzivassiloglou, 1995; <papid> P95-1034 </papid>langkilde and knight, 1998<papid> W98-1426 </papid>a; langkildeand knight, 1998<papid> W98-1426 </papid>b).</citsent>
<aftsection>
<nextsent>they use semantic expressions as input, whereas we use keywords.
</nextsent>
<nextsent>also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select themost appropriate surface text.
</nextsent>
<nextsent>while we can not use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generatesurface-text sentences from candidate-text sentences in the form of dependency trees.
</nextsent>
<nextsent>we can also apply the formalism proposed by langkilde (langkilde, 2000) <papid> A00-2023 </papid>to express the candidate-text sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3239">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select themost appropriate surface text.
</prevsent>
<prevsent>while we can not use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generatesurface-text sentences from candidate-text sentences in the form of dependency trees.
</prevsent>
</prevsection>
<citsent citstr=" A00-2023 ">
we can also apply the formalism proposed by langkilde (langkilde, 2000) <papid> A00-2023 </papid>to express the candidate-text sentences.</citsent>
<aftsection>
<nextsent>bangalore and rambow proposed method to generate candidate-text sentences in the form of trees (bangalore and rambow, 2000).<papid> C00-1007 </papid></nextsent>
<nextsent>they consider dependency information when deriving trees by using xtag grammar, but they assume that the input contains dependency infor mation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3240">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>while we can not use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generatesurface-text sentences from candidate-text sentences in the form of dependency trees.
</prevsent>
<prevsent>we can also apply the formalism proposed by langkilde (langkilde, 2000) <papid> A00-2023 </papid>to express the candidate-text sentences.</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
bangalore and rambow proposed method to generate candidate-text sentences in the form of trees (bangalore and rambow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>they consider dependency information when deriving trees by using xtag grammar, but they assume that the input contains dependency information.
</nextsent>
<nextsent>our system generates candidate-textsentences without relying on dependency information in the input, and our model estimates the dependencies between keywords.
</nextsent>
<nextsent>ratnaparkhi proposed models to generate text from semantic attributes (ratnaparkhi, 2000).<papid> A00-2026 </papid></nextsent>
<nextsent>the input of these models is semantic attributes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3241">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they consider dependency information when deriving trees by using xtag grammar, but they assume that the input contains dependency information.
</prevsent>
<prevsent>our system generates candidate-textsentences without relying on dependency information in the input, and our model estimates the dependencies between keywords.
</prevsent>
</prevsection>
<citsent citstr=" A00-2026 ">
ratnaparkhi proposed models to generate text from semantic attributes (ratnaparkhi, 2000).<papid> A00-2026 </papid></citsent>
<aftsection>
<nextsent>the input of these models is semantic attributes.
</nextsent>
<nextsent>his models are similar to ours if the semantic attributes are replaced with keywords.
</nextsent>
<nextsent>however, his models need training corpus inwhich certain words are replaced with semantic attributes.
</nextsent>
<nextsent>although our model also needsa training corpus, the corpus can be automatically created by using morphological analyzer and dependency analyzer, both of which are readily available.humphreys et al proposed using models developed for sentence-structure analysis to rank candidate-text sentences (humphreys et al., 2001).<papid> W01-0812 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3242">
<title id=" C02-1064.xml">text generation from keywords </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>his models are similar to ours if the semantic attributes are replaced with keywords.
</prevsent>
<prevsent>however, his models need training corpus inwhich certain words are replaced with semantic attributes.
</prevsent>
</prevsection>
<citsent citstr=" W01-0812 ">
although our model also needsa training corpus, the corpus can be automatically created by using morphological analyzer and dependency analyzer, both of which are readily available.humphreys et al proposed using models developed for sentence-structure analysis to rank candidate-text sentences (humphreys et al., 2001).<papid> W01-0812 </papid></citsent>
<aftsection>
<nextsent>as well as models developed for sentence-structure analysis, we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text.
</nextsent>
<nextsent>berger and lafferty proposed language model for information retrieval (berger and lafferty, 1999).
</nextsent>
<nextsent>their concept is similar to that of our model, which can be regarded as model that translates keywords into text, while their model can be regarded as one that translates query words into documents.
</nextsent>
<nextsent>however, the purpose of their model is different: their goal is to retrieve text that already exists while ours is to generate new text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3245">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W04-0713 ">
this paper describes an extension of thedar-algorithm (navarretta, 2004) <papid> W04-0713 </papid>for resolving inter sentential pronominal anaphors referring to individual and abstract entities in texts and dialogues.</citsent>
<aftsection>
<nextsent>in dar individual entities are resolved combining models which identify high degree of salience with high degree of givenness (topicality) of entities in the hearers cognitive model, e.g.
</nextsent>
<nextsent>(grosz et al , 1995), <papid> J95-2003 </papid>with hajicova?</nextsent>
<nextsent>et al .s (1990) salience account which assigns the highest degree of salience to entities in the focal part of an utterance in information structure terms, which often introduce new information in discourse.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3246">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this paper describes an extension of thedar-algorithm (navarretta, 2004) <papid> W04-0713 </papid>for resolving inter sentential pronominal anaphors referring to individual and abstract entities in texts and dialogues.</prevsent>
<prevsent>in dar individual entities are resolved combining models which identify high degree of salience with high degree of givenness (topicality) of entities in the hearers cognitive model, e.g.</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
(grosz et al , 1995), <papid> J95-2003 </papid>with hajicova?</citsent>
<aftsection>
<nextsent>et al .s (1990) salience account which assigns the highest degree of salience to entities in the focal part of an utterance in information structure terms, which often introduce new information in discourse.
</nextsent>
<nextsent>anaphors referring to abstract entities are resolved with an extension of the algorithm presented by eckert and strube (2000).
</nextsent>
<nextsent>the extended dar-algorithm accounts for differences in the resolution mechanisms of different types of danish pronouns.
</nextsent>
<nextsent>manual tests of the algorithm show that dar performs better than other resolution algorithms on the same data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3257">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> background for dar.  </section>
<citcontext>
<prevsection>
<prevsent>nearly all salience-based algorithms identify high degree of salience with high degree of givenness of des.
</prevsent>
<prevsent>in fact, although different criteria are used for ranking des, such as linear order, hierarchy of grammatical roles, information structure, princes familiarity scale (prince, 1981), all algorithms assign the highest prominence to the des which are most topical, known, bound, familiar and thus given, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
(grosz et al , 1995; <papid> J95-2003 </papid>brennan et al , 1987; <papid> P87-1022 </papid>strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>analysing the danish data we found restricted number of cases where high degree of salience did not correspond to high degree of topicality, as it is the case in example (1).
</nextsent>
<nextsent>(1) a: hvem...hvem arbejdede [din mor] med?
</nextsent>
<nextsent>(with whom... whom did [your mother] work) b: [hun] arbejdede med [vores nabo] ([she] worked with [our neighbour] ) [hun] var enke ... havde tre snner [bysoc] ([she] was widow... had three sons) in (1) the antecedent of the second occurrence of the pronoun hun (she) is the object vores nabo (ourneighbour) which provides the information requested in the preceding question.
</nextsent>
<nextsent>this nominal is assigned lower prominence than the subject pronoun hun (she) in most salience models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3258">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> background for dar.  </section>
<citcontext>
<prevsection>
<prevsent>nearly all salience-based algorithms identify high degree of salience with high degree of givenness of des.
</prevsent>
<prevsent>in fact, although different criteria are used for ranking des, such as linear order, hierarchy of grammatical roles, information structure, princes familiarity scale (prince, 1981), all algorithms assign the highest prominence to the des which are most topical, known, bound, familiar and thus given, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
(grosz et al , 1995; <papid> J95-2003 </papid>brennan et al , 1987; <papid> P87-1022 </papid>strube, 1998).<papid> P98-2204 </papid></citsent>
<aftsection>
<nextsent>analysing the danish data we found restricted number of cases where high degree of salience did not correspond to high degree of topicality, as it is the case in example (1).
</nextsent>
<nextsent>(1) a: hvem...hvem arbejdede [din mor] med?
</nextsent>
<nextsent>(with whom... whom did [your mother] work) b: [hun] arbejdede med [vores nabo] ([she] worked with [our neighbour] ) [hun] var enke ... havde tre snner [bysoc] ([she] was widow... had three sons) in (1) the antecedent of the second occurrence of the pronoun hun (she) is the object vores nabo (ourneighbour) which provides the information requested in the preceding question.
</nextsent>
<nextsent>this nominal is assigned lower prominence than the subject pronoun hun (she) in most salience models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3260">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> the dar algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>x will one) (one will).
</prevsent>
<prevsent>3.1 search space and de lists.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
dar presupposes the discourse structure described by grosz and sidner (1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>the minimal discourse unit is the utterance . paragraphs correspond to discourse segments in texts.
</nextsent>
<nextsent>in dialogues discourse segments were manually marked (se section 4).
</nextsent>
<nextsent>the dialogues were structured with synchronising units (su) according to the definitions in es00.
</nextsent>
<nextsent>the immediate antecedent search space of pronoun in utterance nis the previous utterance, n1 . if u. is the first component in su in dialogues the immediate search space for is su m1 . dar assumes two antecedent domains depending on whether the pronoun has or has not been recognised as an ipa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3264">
<title id=" C04-1034.xml">resolving individual and abstract anaphora in texts and dialogues </title>
<section> tests and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in all tests the intra sentential anaphors have been manually resolved and expletive and cat aphoric uses of pronouns have been marked and excluded from the test.
</prevsent>
<prevsent>dialogue act units we remarked and classified by three annotators following (eckert and strube, 2000).
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the reliability for the two annotation tasks (?-statistics(carletta, 1996)) <papid> J96-2004 </papid>was of 0.94 and 0.90 respectively.</citsent>
<aftsection>
<nextsent>pronominal anaphors were marked, classified and resolved by two annotators.
</nextsent>
<nextsent>the ? statistics for the pronoun classification was 0.86.in few cases (one in the texts and two in the dia logues) where the annotators did not agree upon resolution, the pronouns were marked as ambiguous and were excluded from the test.
</nextsent>
<nextsent>the results obtained for bfp and str98 are given in table 1, while the results of dars resolve ipa are given in table 2.
</nextsent>
<nextsent>in the tables cr stands for correctly resolved?, hr stands for resolved by humans?, ra stands for resolved over all?, stands for precision and stands for recall.because dar both classifies and resolves anaphors, both precision and recall (respect to human resolution) are given in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3265">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in to spill gut?
</prevsent>
<prevsent>cannot be substituted by intestine?
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
(see also lin (1999)).  <papid> P99-1041 </papid>non-(or limited) modifiability.</citsent>
<aftsection>
<nextsent>many collocations cannot be supplemented by additional lexical material.
</nextsent>
<nextsent>for example, the noun in to kick the bucket?
</nextsent>
<nextsent>cannot be modified as to kick the
</nextsent>
<nextsent>holey/plastic/water  bucket?.considering these observations, from natural language processing perspective, collocations should not enter, e.g., the standard syntax-semantics pipeline so as to prevent compositional semantic readings of expressions for which this is absolutely not desired.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3266">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, collocations need to be identified as such and subsequently be blocked, e.g., from compositional semantic interpretation.
</prevsent>
<prevsent>in computational linguistics, wide variety of lexical association measures have been employed for the task of (semi-)automatic collocation identification and extraction.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
almost all of these measure scan be grouped into one of the following three cate gories:  frequency-based measures (e.g., based on absolute and relative co-occurrence frequencies)   information-theoretic measures (e.g., mutual information, entropy)   statistical measures (e.g., chi-square, t-test, log-likelihood, dices coefficient) the corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (dunning, 1993; <papid> J93-1003 </papid>manning and schutze, 1999) and their suitability for the task of collocation extraction (see evert and krenn(2001) <papid> P01-1025 </papid>and krenn and evert (2001) for recent evaluations).</citsent>
<aftsection>
<nextsent>typically, they are applied to set of candidate lexeme pairs which were obtained from pre processors varying in linguistic sophistication.1 the selected measure then assigns an association score 1on the low end, this may just be preset numeric windowspan.
</nextsent>
<nextsent>in order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as pos tagging, chunking, or even parsing, is increasingly being applied.
</nextsent>
<nextsent>to each candidate pair, which is computed from its joint and marginal frequencies, thus expressing the strength of the hypothesis stating whether it constitutes collocation or not.while these association measures have their statistical merits in collocation identification, it is interesting to note that they have relatively little to dowith the linguistic properties (such as those mentioned at the beginning) which are typically associated with the notion of collocativity.
</nextsent>
<nextsent>therefore, it may be interesting to investigate whether there is away to implement measure which directly incorporates linguistic criteria in the collocation identification task, and even more important, whether such linguistically rooted approach would fare better in comparison to some of the standard lexical association measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3267">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, collocations need to be identified as such and subsequently be blocked, e.g., from compositional semantic interpretation.
</prevsent>
<prevsent>in computational linguistics, wide variety of lexical association measures have been employed for the task of (semi-)automatic collocation identification and extraction.
</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
almost all of these measure scan be grouped into one of the following three cate gories:  frequency-based measures (e.g., based on absolute and relative co-occurrence frequencies)   information-theoretic measures (e.g., mutual information, entropy)   statistical measures (e.g., chi-square, t-test, log-likelihood, dices coefficient) the corresponding metrics have been extensively discussed in the literature both in terms of their mathematical properties (dunning, 1993; <papid> J93-1003 </papid>manning and schutze, 1999) and their suitability for the task of collocation extraction (see evert and krenn(2001) <papid> P01-1025 </papid>and krenn and evert (2001) for recent evaluations).</citsent>
<aftsection>
<nextsent>typically, they are applied to set of candidate lexeme pairs which were obtained from pre processors varying in linguistic sophistication.1 the selected measure then assigns an association score 1on the low end, this may just be preset numeric windowspan.
</nextsent>
<nextsent>in order to reduce the noise among the candidates, however, more elaborate linguistic processing, such as pos tagging, chunking, or even parsing, is increasingly being applied.
</nextsent>
<nextsent>to each candidate pair, which is computed from its joint and marginal frequencies, thus expressing the strength of the hypothesis stating whether it constitutes collocation or not.while these association measures have their statistical merits in collocation identification, it is interesting to note that they have relatively little to dowith the linguistic properties (such as those mentioned at the beginning) which are typically associated with the notion of collocativity.
</nextsent>
<nextsent>therefore, it may be interesting to investigate whether there is away to implement measure which directly incorporates linguistic criteria in the collocation identification task, and even more important, whether such linguistically rooted approach would fare better in comparison to some of the standard lexical association measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3268">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> fixed phrases. here, all basic lexical mean-.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 construction and statistics of the testset.
</prevsent>
<prevsent>we used 114-million-word german-languagenewspaper corpus extracted from the web to acquire candidate pp-verb collocations.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
the corpus was first processed by means of the tnt part of-speech tagger (brants, 2000).<papid> A00-1031 </papid></citsent>
<aftsection>
<nextsent>then we ran sentence/clause recognizer and an np/pp chunker,both developed at the text knowledge engineering labat freiburg university, on the pos-taggedcorpus.
</nextsent>
<nextsent>from the xml-marked-up tree output, pp verb complexes were automatically selected in the following way: taking particular pp node asa fixed point, either the preceding or the following sibling node was taken.2 from such pp verb combination, we extracted and counted both its various heads, in terms of preposition-noun-verb (pnv) triples, and all its associated supplements, i.e., here in this case any additional lexical material which also occurs in the nominal group of the pp, such as articles, adjectives, adverbs, cardinals, etc.3the extraction of the associated supplements is essential to the linguistic measure described in subsection 3.3 below.
</nextsent>
<nextsent>in order to reduce the amount of candidates for evaluation and to eliminate low-frequency data, we only considered pnv-triples with frequency
</nextsent>
<nextsent> . this was also motivated by the well-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3271">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> experimental results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>thus, we are able to add some empirical grounding to the widespread textbook assumption about the limited modifiablity of collocations.
</prevsent>
<prevsent>another observation (which is also inherent to our linguistic measure) based on this experiment is that some collocations do possess at least limited modifiability.
</prevsent>
</prevsection>
<citsent citstr=" P90-1032 ">
collocation acquisition is, of course,not goal by itself, but rather aims at creating collocation lexicons for both language processing and generation (smadja and mckeown, 1990).<papid> P90-1032 </papid></citsent>
<aftsection>
<nextsent>from this perspective, our linguistic modifiabilty measure actually yields quite valuable by-product for the development of lexicons or collocational knowledgebases: list of possible structural and lexical modifications associated with particular collocational entry candidate.
</nextsent>
<nextsent>in our case, these modifications refer to the nominal group of the pp.
</nextsent>
<nextsent>we illustrate this point in table 4 with two collocational pnv triples and some of their associated np supplements plus their frequencies.as can be seen, both structural and lexical attributes of collocations can thus be obtained.
</nextsent>
<nextsent>the structural information comes in the form of part-of speech (pos) tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3272">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(heavy?), erheblich?
</prevsent>
<prevsent>(consid erable?, grave?).
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
although there have been many studies on collocation extraction and mining using only statistical approaches (church and hanks, 1990; <papid> J90-1003 </papid>ikehara et al, 1996), <papid> C96-1097 </papid>there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations.smadja (1993), <papid> J93-1007 </papid>which is the classic work on collocation extraction, uses two-stage filtering modelin which, in the first step, n-gram statistics determine possible collocations and, in the second step,these candidates are submitted to syntactic valida7of course, lexical material is always at least partially dependent on the domain in question.</citsent>
<aftsection>
<nextsent>in our case, this is the news domain with all its associated sub domains (politics, economics, finance, culture, etc.).tion procedure (e.g., determining verb-object collo cations) in order to filter out invalid collocations.
</nextsent>
<nextsent>ina single-judge evaluation of 4,000 collocation candidates, the incorporation of linguistic criteria (viatagging and predicate-argument parsing) boosts precision up to level of 80% and recall to 94%.
</nextsent>
<nextsent>these results are, of course, not comparable to ours.
</nextsent>
<nextsent>first of all, precision and recall are measured at fixed point for fixed unranked candidate list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3273">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(heavy?), erheblich?
</prevsent>
<prevsent>(consid erable?, grave?).
</prevsent>
</prevsection>
<citsent citstr=" C96-1097 ">
although there have been many studies on collocation extraction and mining using only statistical approaches (church and hanks, 1990; <papid> J90-1003 </papid>ikehara et al, 1996), <papid> C96-1097 </papid>there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations.smadja (1993), <papid> J93-1007 </papid>which is the classic work on collocation extraction, uses two-stage filtering modelin which, in the first step, n-gram statistics determine possible collocations and, in the second step,these candidates are submitted to syntactic valida7of course, lexical material is always at least partially dependent on the domain in question.</citsent>
<aftsection>
<nextsent>in our case, this is the news domain with all its associated sub domains (politics, economics, finance, culture, etc.).tion procedure (e.g., determining verb-object collo cations) in order to filter out invalid collocations.
</nextsent>
<nextsent>ina single-judge evaluation of 4,000 collocation candidates, the incorporation of linguistic criteria (viatagging and predicate-argument parsing) boosts precision up to level of 80% and recall to 94%.
</nextsent>
<nextsent>these results are, of course, not comparable to ours.
</nextsent>
<nextsent>first of all, precision and recall are measured at fixed point for fixed unranked candidate list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3274">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(heavy?), erheblich?
</prevsent>
<prevsent>(consid erable?, grave?).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
although there have been many studies on collocation extraction and mining using only statistical approaches (church and hanks, 1990; <papid> J90-1003 </papid>ikehara et al, 1996), <papid> C96-1097 </papid>there has been much less work on collocation acquisition which takes into account the linguistic properties typically associated with collocations.smadja (1993), <papid> J93-1007 </papid>which is the classic work on collocation extraction, uses two-stage filtering modelin which, in the first step, n-gram statistics determine possible collocations and, in the second step,these candidates are submitted to syntactic valida7of course, lexical material is always at least partially dependent on the domain in question.</citsent>
<aftsection>
<nextsent>in our case, this is the news domain with all its associated sub domains (politics, economics, finance, culture, etc.).tion procedure (e.g., determining verb-object collo cations) in order to filter out invalid collocations.
</nextsent>
<nextsent>ina single-judge evaluation of 4,000 collocation candidates, the incorporation of linguistic criteria (viatagging and predicate-argument parsing) boosts precision up to level of 80% and recall to 94%.
</nextsent>
<nextsent>these results are, of course, not comparable to ours.
</nextsent>
<nextsent>first of all, precision and recall are measured at fixed point for fixed unranked candidate list.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3275">
<title id=" C04-1141.xml">collocation extraction based on modifiability statistics </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in order to obtain more reliable evaluation results, weplot these values continuously on ranked candidate list.
</prevsent>
<prevsent>secondly, our kind of syntactic preprocessing (which is standard nowadays) allows collocation extraction algorithms to better control the structural types of collocations.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
lin (1998) <papid> P98-2127 </papid>acquires lexical dependency database by assembling dependency relationships from parsed corpus.</citsent>
<aftsection>
<nextsent>an entry in this database is classified as collocation if its log-likelihood value is greater than some threshold.
</nextsent>
<nextsent>using an automatically constructed similarity thesaurus, lin (1999) <papid> P99-1041 </papid>then separates compositional from non-compositionalcollocations by taking into account the second linguistic property described in section 1, viz.</nextsent>
<nextsent>their non- or limited substitutability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3277">
<title id=" C04-1094.xml">using syntactic information to extract relevant terms for multi document summarization </title>
<section> evaluation of automatically extracted.  </section>
<citcontext>
<prevsection>
<prevsent>in section 4 we study the correlation between key concepts and syntactic function in texts, and in section 5 we discuss the experimental results of syntactic function as predictor to extract key concepts.
</prevsent>
<prevsent>finally, in section 6 we draw some conclusions.
</prevsent>
</prevsection>
<citsent citstr=" A97-1042 ">
key concept sit is necessary, in the context of an interactive summarization system, to measure the quality of the terms suggested by the system, i.e., to what extent they are related to the key topics of the document set.(lin and hovy, 1997) <papid> A97-1042 </papid>compared different strategies to generate lists of relevant terms for summarization using topic signatures.</citsent>
<aftsection>
<nextsent>the evaluation was extrinsic, comparing the quality of the summaries generated by system using different term lists asinput.
</nextsent>
<nextsent>the results, however, cannot be directly extrapolated to interactive summarization systems, because the evaluation does not consider how informative terms are for user.
</nextsent>
<nextsent>from an interactive point of view, the evaluation of term extraction approaches can be done, at least, in two ways:?
</nextsent>
<nextsent>evaluating the summaries produced in the interactive summarization process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3281">
<title id=" C04-1094.xml">using syntactic information to extract relevant terms for multi document summarization </title>
<section> test bed: the is corpus.  </section>
<citcontext>
<prevsection>
<prevsent>previous work includes using noun phrases (boguraev et al, 1998; jones et al, 2002), words (buyukkokten et al, 1999), n-grams (leuski et al, 2003; lin and hovy, 1997) <papid> A97-1042 </papid>or proper nouns, multi-word terms and abbreviations (neff and cooper, 1999).here we will focus, however, in finding appropriate weighting schemes on the set of candidate terms.</prevsent>
<prevsent>the most common approach in interactivesingle-document summarization is using tf.idf measures (jones et al, 2002; buyukkokten et al, 1999; neff and cooper, 1999), which favour terms which are frequent in document and infrequent across the collection.</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
in the ineast system (leuski et al,2003), the identification of relevant terms is oriented towards multi-document summarization, and they use likelihood ratio (dunning, 1993) <papid> J93-1003 </papid>which favours terms which are representative of the set of documents as opposed to the full collection.</citsent>
<aftsection>
<nextsent>other sources of information that have been used as complementary measures consider, for instance, the number of references of concept (boguraev et al, 1998), its localization (jones et al, 2002) or the distribution of the term along the document (buyukkokten et al, 1999; boguraev et al, 1998).
</nextsent>
<nextsent>5.1 experimental setup.
</nextsent>
<nextsent>a technical difficulty is that the key concepts introduced by the users are intellectual elaborations, which result in complex expressions which might even not be present (literally) in the documents.
</nextsent>
<nextsent>hence, we will concentrate on extracting lists of terms, checking whether these terms are part of some key concept.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3282">
<title id=" C04-1094.xml">using syntactic information to extract relevant terms for multi document summarization </title>
<section> test bed: the is corpus.  </section>
<citcontext>
<prevsection>
<prevsent>formally: = |cl| |c| noise = |ln|where is the set of key concepts manually selected by users; is (ranked) list of terms generated by some weighting schema; ln is the subset of terms in which do not belong to any key con cept; and cl is the subset of key concepts which are represented by at least one term in the ranked list l. here is (fictitious) example of how and noise are computed: = {haiti, reinstatement of democracy, un and usa troops} = {haiti, soldiers, un, usa, attempt} ? cl = {haiti, un and usa troops} = 2/3 ln = {soldiers,attempt} noise = 2we will compare the following weighting strate gies: tf the frequency of word in the set of documents is taken as baseline measure.
</prevsent>
<prevsent>likelihood ratio this is taken from (leuski et al, 2003) and used as reference measure.
</prevsent>
</prevsection>
<citsent citstr=" W00-0901 ">
we have implemented the procedure described in (rayson and garside, 2000) <papid> W00-0901 </papid>using unigrams only.</citsent>
<aftsection>
<nextsent>okapimod we have also considered measure derived from okapi and used in (robertson et al., 1992).
</nextsent>
<nextsent>we have adapted the measure to consider the set of 100 documents as one single document.
</nextsent>
<nextsent>tfsyntax using our first experimental result, tfsyntax computes the weight of term as the number of times it appears preceding verb.figure 4: comparison of weighting schemes to extract relevant terms 5.2 results.
</nextsent>
<nextsent>figure 4 draws recall/noise curves for all weighting criteria.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3283">
<title id=" C04-1078.xml">cascading use of soft and hard matching pattern rules for weakly supervised information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the learning of soft pattern rules in this paper augments the soft matching method advocated by nahm and mooney (2001) by combining lexical tokens alongside syntactic and semantic features and adopting probabilistic framework that combines slot content and sequential fidelity in computing the degree of pattern match.
</prevsent>
<prevsent>the bootstrapping scheme using the co-training (blum and mitchell, 1998) technique has been widely explored for ie tasks in recent years.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
collins and singer (1999) <papid> W99-0613 </papid>presented several techniques using co-training schemes for named entity (ne) extraction seeded by small set of manually crafted ne rules.</citsent>
<aftsection>
<nextsent>riloff and jones (1999) presented multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously.
</nextsent>
<nextsent>yangarber (2003) <papid> P03-1044 </papid>proposed counter-training approach to provide natural stopping criteria for unsupervised learning.</nextsent>
<nextsent>our framework of combining two pattern learners is close to niu, et al (2003) <papid> P03-1043 </papid>in which two successive learners are used to learn named entities classifiers starting from small set of concept based seed words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3284">
<title id=" C04-1078.xml">cascading use of soft and hard matching pattern rules for weakly supervised information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>collins and singer (1999) <papid> W99-0613 </papid>presented several techniques using co-training schemes for named entity (ne) extraction seeded by small set of manually crafted ne rules.</prevsent>
<prevsent>riloff and jones (1999) presented multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously.</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
yangarber (2003) <papid> P03-1044 </papid>proposed counter-training approach to provide natural stopping criteria for unsupervised learning.</citsent>
<aftsection>
<nextsent>our framework of combining two pattern learners is close to niu, et al (2003) <papid> P03-1043 </papid>in which two successive learners are used to learn named entities classifiers starting from small set of concept based seed words.</nextsent>
<nextsent>the bootstrapping procedure is implemented as training decision list and an hmm classifier sequentially.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3285">
<title id=" C04-1078.xml">cascading use of soft and hard matching pattern rules for weakly supervised information extraction </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>riloff and jones (1999) presented multi-level bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously.
</prevsent>
<prevsent>yangarber (2003) <papid> P03-1044 </papid>proposed counter-training approach to provide natural stopping criteria for unsupervised learning.</prevsent>
</prevsection>
<citsent citstr=" P03-1043 ">
our framework of combining two pattern learners is close to niu, et al (2003) <papid> P03-1043 </papid>in which two successive learners are used to learn named entities classifiers starting from small set of concept based seed words.</citsent>
<aftsection>
<nextsent>the bootstrapping procedure is implemented as training decision list and an hmm classifier sequentially.
</nextsent>
<nextsent>the hmm classifier uses the training corpus automatically, tagged by the first learner, i.e., the decision list learner.
</nextsent>
<nextsent>our work differs from niu, et al (2003) <papid> P03-1043 </papid>in two ways.</nextsent>
<nextsent>first, we repeat the automatic annotation process until it satisfies the stopping criteria.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3289">
<title id=" C02-2005.xml">scaled log likelihood ratios for the detection of abbreviations in text corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in case of an abbreviation, we expect the occurrence of ? following the previous word?
</prevsent>
<prevsent>to be more likely than in case of an end-of-sentence punctation.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the starting point is the log likelihood ratio (log ?, dunning 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>if the null hypothesis (h0) ? as given in (1) ? expresses that the occurrence of period is independent of the preceeding word, the alternative hypothesis (ha) in (2) assumes that the occurrence of period is not independent of the occurrence of the word preceeding it.
</nextsent>
<nextsent>(1) h0: p(?|w) = = p(?|w) (2) ha: p(?|w) = p1 ? p2 = p(?|w) the log ? of the two hypotheses is given in (3).
</nextsent>
<nextsent>its distribution is asymptotic to 2 distribution and can hence be used as test statistic (dun ning 1993).<papid> J93-1003 </papid></nextsent>
<nextsent>(3) ( ) ( )0log - 2 log l l ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3292">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a crucial issue is how to obtain and maintain potentially huge collection of such event relation instances.
</prevsent>
<prevsent>this paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from large-scale text collection.
</prevsent>
</prevsection>
<citsent citstr=" N06-1008 ">
motivated by this issue, several research group shave reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (lin and pantel, 2001; inui et al ., 2003; chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid>pekar, 2006; <papid> N06-1007 </papid>zanzotto et al , 2006; <papid> P06-1107 </papid>abe et al ., 2008, etc.).</citsent>
<aftsection>
<nextsent>as we explain below, however, none of these studies fully achieves the goal we pursue in this paper.
</nextsent>
<nextsent>an important aspect to consider in event relation acquisition is that each event has arguments.
</nextsent>
<nextsent>for example, the causal relation between wash some thing and something is clean can be represented naturally as: (1) wash(obj:x) ? cause is clean(subj:x), where is logical variable denoting that the filler of the object slot of the wash event should be shared (i.e. identical) with the filler of the subject slot of the is clean event.to be more general, an instance of given relation can be represented as: (2) pred 1 (arg 1 :x) ? pred 2 (arg 2 :x), where pred iis natural language predicate, typically verb or adjective, and is logical variable denoting which argument of one predicate and which argument of the other are shared.
</nextsent>
<nextsent>the goal we pursue in this paper is therefore not only (a) to find predicate pairs that are of given relation 1 type, but also (b) to identify the arguments shared between the predicates if any.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3294">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a crucial issue is how to obtain and maintain potentially huge collection of such event relation instances.
</prevsent>
<prevsent>this paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from large-scale text collection.
</prevsent>
</prevsection>
<citsent citstr=" N06-1007 ">
motivated by this issue, several research group shave reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (lin and pantel, 2001; inui et al ., 2003; chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid>pekar, 2006; <papid> N06-1007 </papid>zanzotto et al , 2006; <papid> P06-1107 </papid>abe et al ., 2008, etc.).</citsent>
<aftsection>
<nextsent>as we explain below, however, none of these studies fully achieves the goal we pursue in this paper.
</nextsent>
<nextsent>an important aspect to consider in event relation acquisition is that each event has arguments.
</nextsent>
<nextsent>for example, the causal relation between wash some thing and something is clean can be represented naturally as: (1) wash(obj:x) ? cause is clean(subj:x), where is logical variable denoting that the filler of the object slot of the wash event should be shared (i.e. identical) with the filler of the subject slot of the is clean event.to be more general, an instance of given relation can be represented as: (2) pred 1 (arg 1 :x) ? pred 2 (arg 2 :x), where pred iis natural language predicate, typically verb or adjective, and is logical variable denoting which argument of one predicate and which argument of the other are shared.
</nextsent>
<nextsent>the goal we pursue in this paper is therefore not only (a) to find predicate pairs that are of given relation 1 type, but also (b) to identify the arguments shared between the predicates if any.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3295">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a crucial issue is how to obtain and maintain potentially huge collection of such event relation instances.
</prevsent>
<prevsent>this paper addresses the issue of how to automatically acquire such instances of relations between events (henceforth, event relation instances) from large-scale text collection.
</prevsent>
</prevsection>
<citsent citstr=" P06-1107 ">
motivated by this issue, several research group shave reported their experiments on automatic acquisition of causal, temporal and entailment relations between event expressions (typically verbs or verb phrases) (lin and pantel, 2001; inui et al ., 2003; chklovski and pantel, 2005; torisawa, 2006;<papid> N06-1008 </papid>pekar, 2006; <papid> N06-1007 </papid>zanzotto et al , 2006; <papid> P06-1107 </papid>abe et al ., 2008, etc.).</citsent>
<aftsection>
<nextsent>as we explain below, however, none of these studies fully achieves the goal we pursue in this paper.
</nextsent>
<nextsent>an important aspect to consider in event relation acquisition is that each event has arguments.
</nextsent>
<nextsent>for example, the causal relation between wash some thing and something is clean can be represented naturally as: (1) wash(obj:x) ? cause is clean(subj:x), where is logical variable denoting that the filler of the object slot of the wash event should be shared (i.e. identical) with the filler of the subject slot of the is clean event.to be more general, an instance of given relation can be represented as: (2) pred 1 (arg 1 :x) ? pred 2 (arg 2 :x), where pred iis natural language predicate, typically verb or adjective, and is logical variable denoting which argument of one predicate and which argument of the other are shared.
</nextsent>
<nextsent>the goal we pursue in this paper is therefore not only (a) to find predicate pairs that are of given relation 1 type, but also (b) to identify the arguments shared between the predicates if any.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3298">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach uses information of argument fillers (i.e. anchors) of each event expression as useful clue for identifying event relations.
</prevsent>
<prevsent>a popular way of using such argument information relies on the distributional hypothesis (harris, 1968) and identifies synonymous event expressions by seeking set of event expressions whose argument fillers have similar distribution.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
such algorithms as dirt (lin and pantel,2001) and te/ase (szpektor et al , 2004) <papid> W04-3206 </papid>represent this line of research.</citsent>
<aftsection>
<nextsent>another way of using argument information is proposed by pekar (2006)<papid> N06-1007 </papid>which identifies candidate verb pairs for the entailment relation by imposing criteria: (a) the two verbs must appear in the same local discourse-related context and (b)their arguments need to refer to the same participant, i.e. anchor.</nextsent>
<nextsent>for example, if pair of clauses mary bought house.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3302">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> two-phased event relation acquisition.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 predicate pair acquisition.
</prevsent>
<prevsent>for predicate pair acquisition, we can choose one from range of state-of-the-art pattern-based methods.
</prevsent>
</prevsection>
<citsent citstr=" P06-1015 ">
among others, in our experiments, we adopted abe et al  (2008)s method because it hadan advantage in that it was capable of learning patterns as well as relation instances.abe et al  (2008)s method is based on pan tel and pennacchiotti (2006)<papid> P06-1015 </papid>s espresso algorithm, which is originally designed to acquire relations between entities.</citsent>
<aftsection>
<nextsent>espresso takes as input small number of seed instances of given target relation and iteratively learns co-occurrence patterns and relation instances in bootstrapping manner.
</nextsent>
<nextsent>abe et al  have made several extensions to it so that it can be applied to event relations.
</nextsent>
<nextsent>since the details of this phase are not the focus of this paper, we refer the reader to (abe et al , 2008) for further information.
</nextsent>
<nextsent>3.3 shared argument identification.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3304">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for each chosen argument pair (l = 3).
</prevsent>
<prevsent>4.1 settings.
</prevsent>
</prevsection>
<citsent citstr=" N06-1023 ">
for an empirical evaluation, we used sample of approximately 500m sentences taken from theweb corpus collected by kawahara and kurohashi (2006).<papid> N06-1023 </papid></citsent>
<aftsection>
<nextsent>the sentences were dependency parsed with cabocha (kudo and matsumoto,2002), <papid> W02-2016 </papid>and co-occurrence samples of event mentions were extracted.</nextsent>
<nextsent>event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3305">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 settings.
</prevsent>
<prevsent>for an empirical evaluation, we used sample of approximately 500m sentences taken from theweb corpus collected by kawahara and kurohashi (2006).<papid> N06-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
the sentences were dependency parsed with cabocha (kudo and matsumoto,2002), <papid> W02-2016 </papid>and co-occurrence samples of event mentions were extracted.</citsent>
<aftsection>
<nextsent>event mentions with patterns whose frequency was less than 20 were discarded in order to reduce computational costs.
</nextsent>
<nextsent>in our experiments, we considered two of inui etal.
</nextsent>
<nextsent>(2003)s four types of causal relations: action effect relations (effect in inui et al terminology)and action-means relations (means).
</nextsent>
<nextsent>an action effect relation holds between events and if and only if non-volitional event is likely to happen as either direct or indirect effect of volitional actionx.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3306">
<title id=" C08-1001.xml">two phased event relation acquisition coupling the relation oriented and argument oriented approaches </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>clearly, more comprehensive evaluation needs to be done.
</prevsent>
<prevsent>for example, the acquired relation instances should be evaluated in some task-oriented manner.
</prevsent>
</prevsection>
<citsent citstr=" P06-1079 ">
the other intriguing issue is how our anchor-based method for shared argument identification can benefit from recent advances in coreference and zero-anaphora resolution (iida et al , 2006; <papid> P06-1079 </papid>komachi et al , 2007, etc.).</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3307">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present our conclusions and future work in this area.
</prevsent>
<prevsent>our work is informed by efforts to automate the acquisition of ontology concepts from text.
</prevsent>
</prevsection>
<citsent citstr=" J04-2002 ">
ontolearn (navigli and velardi, 2004) <papid> J04-2002 </papid>extracts do main terminology from collection of texts using syntactic parse to identify candidate terms that are filtered based on domain relevance and connected using semantic interpretation based on word sense disambiguation.</citsent>
<aftsection>
<nextsent>the newly identified concepts and relationships are used to update an existing ontology.
</nextsent>
<nextsent>knowledge puzzle focuses on n-grams to produce candidate terms filtered based on term frequency in the input documents and on the number of relationships associated with given term (zouaq et al, 2007).
</nextsent>
<nextsent>this approach leverages pattern extraction techniques to identify concepts and relationships.
</nextsent>
<nextsent>while these approaches produce ontologies useful for computational purposes, the identified concepts are of very fine granularity and therefore may yield graphs not suitable for identifying student misconceptions or for presentation back to the student.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3308">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this approach leverages pattern extraction techniques to identify concepts and relationships.
</prevsent>
<prevsent>while these approaches produce ontologies useful for computational purposes, the identified concepts are of very fine granularity and therefore may yield graphs not suitable for identifying student misconceptions or for presentation back to the student.
</prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
clustering by committee has also been used to discover concepts from text by grouping terms into conceptually related clusters (lin and pantel, 2002).<papid> C02-1144 </papid></citsent>
<aftsection>
<nextsent>the resulting clusters appear to be tightly related, but operate at very fine level of granularity.
</nextsent>
<nextsent>our approach focuses on sentences as units of knowledge to produce concise representations that may be useful both as computational objects and as learning resources to present back to the student.
</nextsent>
<nextsent>therefore, extractive summarization research also informs our work.
</nextsent>
<nextsent>topic representation and topic themes have been used to explore promising mds techniques (harabagiu and lacatusu, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3309">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, extractive summarization research also informs our work.
</prevsent>
<prevsent>topic representation and topic themes have been used to explore promising mds techniques (harabagiu and lacatusu, 2005).
</prevsent>
</prevsection>
<citsent citstr=" N06-2046 ">
recent efforts in graph-based mds have integrated sentence affinity, information richness and diversity penalties to produce very promising results (wan and yang, 2006).<papid> N06-2046 </papid></citsent>
<aftsection>
<nextsent>finally, mead is widely used multi-document summarization and evaluation platform (radev et al, 2000).<papid> W00-0403 </papid></nextsent>
<nextsent>mead research efforts have resulted insignificant contributions to support the development of summarization applications (radev et al, 2000).<papid> W00-0403 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3310">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>topic representation and topic themes have been used to explore promising mds techniques (harabagiu and lacatusu, 2005).
</prevsent>
<prevsent>recent efforts in graph-based mds have integrated sentence affinity, information richness and diversity penalties to produce very promising results (wan and yang, 2006).<papid> N06-2046 </papid></prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
finally, mead is widely used multi-document summarization and evaluation platform (radev et al, 2000).<papid> W00-0403 </papid></citsent>
<aftsection>
<nextsent>mead research efforts have resulted insignificant contributions to support the development of summarization applications (radev et al, 2000).<papid> W00-0403 </papid></nextsent>
<nextsent>while all these systems have produced promising results in automated evaluations, none have directly targeted educational content as input or the generation of pedagogic ally useful summaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3319">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the default and cogent configurations use the mead cosine function with threshold of 0.7 to eliminate redundant sen 180tences.
</prevsent>
<prevsent>all three configurations use word compression factor of 5% resulting in summaries of very similar length.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
for this evaluation, we leverage rouge (lin and hovy, 2003) <papid> N03-1020 </papid>to address the relative quality of the generated summaries based on common gram counts and longest common sub sequence (lcs).</citsent>
<aftsection>
<nextsent>we report on rouge-1 (unigrams), rouge-2 (bigrams), rouge w-1.2 (weighted lcs), and rouge-s* (skip bigrams) as they appear to correlate well with human judgments for longer multi-document summaries, particularly rouge-1 (lin, 2004).<papid> W04-1013 </papid></nextsent>
<nextsent>table 2 shows the results of this rouge-based evaluation including recall (r), precision (p), and balanced measure (f).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3320">
<title id=" C08-1023.xml">pedagogic ally useful extractive summaries for science education </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>all three configurations use word compression factor of 5% resulting in summaries of very similar length.
</prevsent>
<prevsent>for this evaluation, we leverage rouge (lin and hovy, 2003) <papid> N03-1020 </papid>to address the relative quality of the generated summaries based on common gram counts and longest common sub sequence (lcs).</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
we report on rouge-1 (unigrams), rouge-2 (bigrams), rouge w-1.2 (weighted lcs), and rouge-s* (skip bigrams) as they appear to correlate well with human judgments for longer multi-document summaries, particularly rouge-1 (lin, 2004).<papid> W04-1013 </papid></citsent>
<aftsection>
<nextsent>table 2 shows the results of this rouge-based evaluation including recall (r), precision (p), and balanced measure (f).
</nextsent>
<nextsent>random default cogent 0.4855 0.4976 0.6073 0.5026 0.5688 0.6034 r-1 0.4939 0.5308 0.6054 0.0972 0.1321 0.1907 0.1006 0.1510 0.1895 r-2 0.0989 0.1409 0.1901 0.0929 0.0951 0.1185 0.1533 0.1733 0.1877 r-w-1.2 0.1157 0.1228 0.1453 0.2481 0.2620 0.3820 0.2657 0.3424 0.3772 r-s* 0.2566 0.2969 0.3796 table 2.
</nextsent>
<nextsent>quality evaluation results (5% word compression) cogent consistently outperforms the random and default baselines based on all four reported rouge measures.
</nextsent>
<nextsent>given that much of the original research efforts on mead have centered on news articles, this result is not surprising.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3321">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 selectional preference acquisition:.
</prevsent>
<prevsent>current state of the art predicate subcategorization information constitutes an essential part of the computational lexicon entry.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
in recent years, number of approaches have been proposed for dealing computationally with selectional preference acquisition (resnik (1996); briscoe and carroll (1997); <papid> A97-1052 </papid>mccarthy (1997); <papid> W97-0808 </papid>rooth et al (1999); <papid> P99-1014 </papid>abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>korhonen (2002)).</citsent>
<aftsection>
<nextsent>the currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as distribution over words (cf.
</nextsent>
<nextsent>abney and light (1999)).
</nextsent>
<nextsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</nextsent>
<nextsent>overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3322">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 selectional preference acquisition:.
</prevsent>
<prevsent>current state of the art predicate subcategorization information constitutes an essential part of the computational lexicon entry.
</prevsent>
</prevsection>
<citsent citstr=" W97-0808 ">
in recent years, number of approaches have been proposed for dealing computationally with selectional preference acquisition (resnik (1996); briscoe and carroll (1997); <papid> A97-1052 </papid>mccarthy (1997); <papid> W97-0808 </papid>rooth et al (1999); <papid> P99-1014 </papid>abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>korhonen (2002)).</citsent>
<aftsection>
<nextsent>the currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as distribution over words (cf.
</nextsent>
<nextsent>abney and light (1999)).
</nextsent>
<nextsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</nextsent>
<nextsent>overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3323">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 selectional preference acquisition:.
</prevsent>
<prevsent>current state of the art predicate subcategorization information constitutes an essential part of the computational lexicon entry.
</prevsent>
</prevsection>
<citsent citstr=" P99-1014 ">
in recent years, number of approaches have been proposed for dealing computationally with selectional preference acquisition (resnik (1996); briscoe and carroll (1997); <papid> A97-1052 </papid>mccarthy (1997); <papid> W97-0808 </papid>rooth et al (1999); <papid> P99-1014 </papid>abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>korhonen (2002)).</citsent>
<aftsection>
<nextsent>the currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as distribution over words (cf.
</nextsent>
<nextsent>abney and light (1999)).
</nextsent>
<nextsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</nextsent>
<nextsent>overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3324">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 selectional preference acquisition:.
</prevsent>
<prevsent>current state of the art predicate subcategorization information constitutes an essential part of the computational lexicon entry.
</prevsent>
</prevsection>
<citsent citstr=" C00-1028 ">
in recent years, number of approaches have been proposed for dealing computationally with selectional preference acquisition (resnik (1996); briscoe and carroll (1997); <papid> A97-1052 </papid>mccarthy (1997); <papid> W97-0808 </papid>rooth et al (1999); <papid> P99-1014 </papid>abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>korhonen (2002)).</citsent>
<aftsection>
<nextsent>the currently available best algorithms developed for the acquisition of selectional preferences for predicates are induction algorithms modeling selectional behavior as distribution over words (cf.
</nextsent>
<nextsent>abney and light (1999)).
</nextsent>
<nextsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</nextsent>
<nextsent>overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3331">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>abney and light (1999)).
</prevsent>
<prevsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</prevsent>
</prevsection>
<citsent citstr=" W01-0703 ">
overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></citsent>
<aftsection>
<nextsent>those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (korhonen(2002); korhonen and preiss (2003)) <papid> P03-1007 </papid>do not use corpus analysis for verb sense classification.</nextsent>
<nextsent>1.2 word sense disambiguation: current.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3332">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>abney and light (1999)).
</prevsent>
<prevsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</prevsent>
</prevsection>
<citsent citstr=" N01-1013 ">
overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></citsent>
<aftsection>
<nextsent>those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (korhonen(2002); korhonen and preiss (2003)) <papid> P03-1007 </papid>do not use corpus analysis for verb sense classification.</nextsent>
<nextsent>1.2 word sense disambiguation: current.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3333">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>abney and light (1999)).
</prevsent>
<prevsent>semantic classes assigned to predicate arguments in subcategorization frames are either derived automatically through statistical clustering techniques (rooth et al (1999), <papid> P99-1014 </papid>light and greiff (2002)) or assigned using hand-constructed lexical taxonomies such as the wordnet hierarchy orldoce semantic classes.</prevsent>
</prevsection>
<citsent citstr=" P03-1007 ">
overwhelmingly, wordnet is chosen as the default resource for dealing with the sparse data problem (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>agirre and martinez (2001); <papid> W01-0703 </papid>clark and weir (2001); <papid> N01-1013 </papid>carroll and mccarthy (2000); korhonen and preiss (2003)).<papid> P03-1007 </papid>much of the work on inducing selectional preferences for verbs from corpora deals with predicates indiscriminately, assuming no differentiation between predicate senses (resnik (1996); abney and light (1999); ciaramita and johnson (2000); <papid> C00-1028 </papid>rooth et al(1999)).<papid> P99-1014 </papid></citsent>
<aftsection>
<nextsent>those approaches that do distinguish between predicate senses or complementation patterns in acquisition of selectional constraints (korhonen(2002); korhonen and preiss (2003)) <papid> P03-1007 </papid>do not use corpus analysis for verb sense classification.</nextsent>
<nextsent>1.2 word sense disambiguation: current.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3339">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 word sense disambiguation: current.
</prevsent>
<prevsent>state of the art previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
however, following the work of yarowsky (1992), <papid> C92-2070 </papid>yarowsky (1995), <papid> P95-1026 </papid>many supervised wsd systems use minimal information about syntactic structures,for the most part restricting the notion of context to topical and local features.</citsent>
<aftsection>
<nextsent>topical features track open-class words that appear within certain window around target word, and local features track small n-grams associated with the target word.
</nextsent>
<nextsent>disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities.
</nextsent>
<nextsent>that remains the case for most systems that participated in senseval-2 (preiss and yarowsky (2001)).
</nextsent>
<nextsent>some recent work (stetina et al (1998);<papid> W98-0701 </papid>agirre et al (2002); yamashita et al (2003)) <papid> P03-2029 </papid>attempts to change this situation and presents directed effort to investigate the impact of using syntactic features for wsd learning algorithms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3340">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 word sense disambiguation: current.
</prevsent>
<prevsent>state of the art previous computational concerns for economy of grammatical representation have given way to models of language that not only exploit generative grammatical resources but also have access to large lists of contexts of linguistic items (words), to which new structures can be compared in new usages.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
however, following the work of yarowsky (1992), <papid> C92-2070 </papid>yarowsky (1995), <papid> P95-1026 </papid>many supervised wsd systems use minimal information about syntactic structures,for the most part restricting the notion of context to topical and local features.</citsent>
<aftsection>
<nextsent>topical features track open-class words that appear within certain window around target word, and local features track small n-grams associated with the target word.
</nextsent>
<nextsent>disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities.
</nextsent>
<nextsent>that remains the case for most systems that participated in senseval-2 (preiss and yarowsky (2001)).
</nextsent>
<nextsent>some recent work (stetina et al (1998);<papid> W98-0701 </papid>agirre et al (2002); yamashita et al (2003)) <papid> P03-2029 </papid>attempts to change this situation and presents directed effort to investigate the impact of using syntactic features for wsd learning algorithms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3341">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities.
</prevsent>
<prevsent>that remains the case for most systems that participated in senseval-2 (preiss and yarowsky (2001)).
</prevsent>
</prevsection>
<citsent citstr=" W98-0701 ">
some recent work (stetina et al (1998);<papid> W98-0701 </papid>agirre et al (2002); yamashita et al (2003)) <papid> P03-2029 </papid>attempts to change this situation and presents directed effort to investigate the impact of using syntactic features for wsd learning algorithms.</citsent>
<aftsection>
<nextsent>agirreet al(2002) and yamashita et al (2003) <papid> P03-2029 </papid>report resulting improvement in precision.</nextsent>
<nextsent>stevenson and wilks (2001) <papid> J01-3001 </papid>propose somewhat related technique to handle wsd, based on integrating ldoce classes with simulated annealing.although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns with senses, however.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3342">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>disambiguation therefore relies on word co-occurrence statistics, rather than on structural similarities.
</prevsent>
<prevsent>that remains the case for most systems that participated in senseval-2 (preiss and yarowsky (2001)).
</prevsent>
</prevsection>
<citsent citstr=" P03-2029 ">
some recent work (stetina et al (1998);<papid> W98-0701 </papid>agirre et al (2002); yamashita et al (2003)) <papid> P03-2029 </papid>attempts to change this situation and presents directed effort to investigate the impact of using syntactic features for wsd learning algorithms.</citsent>
<aftsection>
<nextsent>agirreet al(2002) and yamashita et al (2003) <papid> P03-2029 </papid>report resulting improvement in precision.</nextsent>
<nextsent>stevenson and wilks (2001) <papid> J01-3001 </papid>propose somewhat related technique to handle wsd, based on integrating ldoce classes with simulated annealing.although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns with senses, however.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3344">
<title id=" C04-1133.xml">automated induction of sense in context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some recent work (stetina et al (1998);<papid> W98-0701 </papid>agirre et al (2002); yamashita et al (2003)) <papid> P03-2029 </papid>attempts to change this situation and presents directed effort to investigate the impact of using syntactic features for wsd learning algorithms.</prevsent>
<prevsent>agirreet al(2002) and yamashita et al (2003) <papid> P03-2029 </papid>report resulting improvement in precision.</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
stevenson and wilks (2001) <papid> J01-3001 </papid>propose somewhat related technique to handle wsd, based on integrating ldoce classes with simulated annealing.although space does not permit discussion here, initial comparisons suggest that our selection contexts could incorporate similar knowledge resources; it is not clear what role model bias plays in associating patterns with senses, however.</citsent>
<aftsection>
<nextsent>in this paper we modify the notion of word sense, and at the same time revise the manner in which senses are encoded.
</nextsent>
<nextsent>the notion of word sense that has been generally adopted in the literature is an artifact of several factors in the status quo, notably the availability of lexical resources such as machine readable dictionaries, in which fine sense distinctions are not supported by criteria for selecting one sense rather than another, and wordnet, where synsetgroupings are taken as defining word sense distinctions.
</nextsent>
<nextsent>thus, for instance, senseval-2 wsd tasks required disambiguation using wordnet senses (see, e.g., discussion in palmer et al (2004)).
</nextsent>
<nextsent>the feature sets used in the supervised wsd algorithms at bestuse only minimal information about the typing of arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3345">
<title id=" C04-1066.xml">japanese unknown word identification by character based chunking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the modular approach, separate off-line module is used to extract unknown words from text (mori 1996; ikeya 2000).
</prevsent>
<prevsent>they are checked and added tothe lexicon of morphological analyzers.
</prevsent>
</prevsection>
<citsent citstr=" P99-1036 ">
in the embedded approach, an on-line module which statistically induces the likelihood of particular string being word is embedded in morphological analyzer (nagata, 1999; <papid> P99-1036 </papid>uchimoto et al, 2001).<papid> W01-0512 </papid></citsent>
<aftsection>
<nextsent>a modular approach is generally preferable in practice, sinceit allows developers to maintain high quality lexicon which is crucial for good performance.
</nextsent>
<nextsent>previous work of the modular approach was either unable to detect low frequency unknown words (mori 1996) or limited to predefined character patterns for low frequency unknown words (ikeya 2000).
</nextsent>
<nextsent>we propose general-purpose unknown word identification based on character-based chunking in order to address these shortcomings.
</nextsent>
<nextsent>a cascade model of morphological analyzer (trained with markov model) and chunker (trained with support vector machines) is applied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3346">
<title id=" C04-1066.xml">japanese unknown word identification by character based chunking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the modular approach, separate off-line module is used to extract unknown words from text (mori 1996; ikeya 2000).
</prevsent>
<prevsent>they are checked and added tothe lexicon of morphological analyzers.
</prevsent>
</prevsection>
<citsent citstr=" W01-0512 ">
in the embedded approach, an on-line module which statistically induces the likelihood of particular string being word is embedded in morphological analyzer (nagata, 1999; <papid> P99-1036 </papid>uchimoto et al, 2001).<papid> W01-0512 </papid></citsent>
<aftsection>
<nextsent>a modular approach is generally preferable in practice, sinceit allows developers to maintain high quality lexicon which is crucial for good performance.
</nextsent>
<nextsent>previous work of the modular approach was either unable to detect low frequency unknown words (mori 1996) or limited to predefined character patterns for low frequency unknown words (ikeya 2000).
</nextsent>
<nextsent>we propose general-purpose unknown word identification based on character-based chunking in order to address these shortcomings.
</nextsent>
<nextsent>a cascade model of morphological analyzer (trained with markov model) and chunker (trained with support vector machines) is applied.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3347">
<title id=" C04-1066.xml">japanese unknown word identification by character based chunking </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>the character type is directly or indirectly used in most of previous work and appears an important feature to characterize unknown words in japanese texts.
</prevsent>
<prevsent>table 1: tags for positions in word tag description one-character word first character in multi-character word last character in multi-character word intermediate character in multi-character word (only for words longer than 2 chars) 2.3 support vector machine-based chunking.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
we use the chunker yamcha (kudo and matsumoto, 2001), <papid> N01-1025 </papid>which is based on svms (vapnik, 1998).suppose we have set of training data for binary class problem: (x 1 , 1 ), . . .</citsent>
<aftsection>
<nextsent>, (xn , yn ), where xi ? rn is feature vector of the th sample in the training data and yi ? {+1,1} is the label of the sample.
</nextsent>
<nextsent>the goal is to find decision function which accurately predicts for an unseen x. an support vector machine classifier gives decision function f(x) = sign(g(x)) for an input vector where g(x) = ? i sv iyik(x, zi) + b.k(x, z) is kernel function which maps vectors into higher dimensional space.
</nextsent>
<nextsent>we use polynomial kernel of degree 2 given by k(x, z) = (1 + ? z)2.
</nextsent>
<nextsent>svms are binary classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3348">
<title id=" C04-1066.xml">japanese unknown word identification by character based chunking </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>we use pairwise method?
</prevsent>
<prevsent>since it is efficient to train than the one vs. rest method?.chunking is performed by deterministically annotating tag on each character.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
table 2 shows the unknown word tags for chunking, which are known as the iob2 model (ramshaw and marcus, 1995).<papid> W95-0107 </papid></citsent>
<aftsection>
<nextsent>table 2: tags for unknown word chunking tag description first character in an unknown word character in an unknown word (except b) character in known word we perform chunking either from the beginning or from the end of the sentence.
</nextsent>
<nextsent>figure 1 illustrates snapshot of chunking procedure.
</nextsent>
<nextsent>two character contexts on both sides are referred to.
</nextsent>
<nextsent>information of two preceding unknown word tags is also used since the chunker has already determined them and they are available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3350">
<title id=" C04-1066.xml">japanese unknown word identification by character based chunking </title>
<section> summary and future direction.  </section>
<citcontext>
<prevsection>
<prevsent>the method can identify unknown words regardless of their occurence frequencies.
</prevsent>
<prevsent>our research need to include pos guessing for the identified words.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
one would argue that, oncethe word boundaries are identified, the pos guessing method in european language can be applied (brants 2000; <papid> A00-1031 </papid>nakagawa 2001).</citsent>
<aftsection>
<nextsent>in our preliminary experiments of pos guessing, both svm and maximum entropy with contextual information achieves 93% with coarse-grained pos set evaluation, but reaches only around 65% with fine-grained pos set evaluation.the poor result may be due to the possibility based pos tagset?.
</nextsent>
<nextsent>the tagset is not necessarily friendly for statistical morphological analyzer development, but is widely used in japanese corpus annotation.
</nextsent>
<nextsent>in the scheme, the fine-grained pos verbal noun in japanese means that the word can be used both as verbal noun with verb and general noun without verb.
</nextsent>
<nextsent>it is difficult to estimate the pos verbal noun, if the word appear in the context with out verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3351">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>235 2.3 acquisition of bilingual collocations.
</prevsent>
<prevsent>the final step is to estimate bilingual correspondences from relevant documents.
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
all japanese documents were parsed using the syntactic analyzer cabocha (kudo and matsumoto, 2003).<papid> P03-1004 </papid></citsent>
<aftsection>
<nextsent>english documents were parsed with the syntactic analyzer (lin, 1993).<papid> P93-1016 </papid></nextsent>
<nextsent>in both english and japanese, we extracted all the dependency triplets(obj, n, v).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3352">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>the final step is to estimate bilingual correspondences from relevant documents.
</prevsent>
<prevsent>all japanese documents were parsed using the syntactic analyzer cabocha (kudo and matsumoto, 2003).<papid> P03-1004 </papid></prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
english documents were parsed with the syntactic analyzer (lin, 1993).<papid> P93-1016 </papid></citsent>
<aftsection>
<nextsent>in both english and japanese, we extracted all the dependency triplets(obj, n, v).
</nextsent>
<nextsent>here, refers to noun which is an object(obj)of verb in sentence.3 hereafter, we describe the reuters english dependency triplet as vn , and that of mainichi as vn . the method to.
</nextsent>
<nextsent>retrieve bilingual correspondences consists of twosub-steps: document-based retrieval and sentence based retrieval.
</nextsent>
<nextsent>2.3.1 document-based retrieval we extract vn and vn pairs from the results of relevant documents: {vn , vn } s.t. ? r  vn , ? m  vn bm25(dr , m mt ) ? ? .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3353">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>no hierarchy: categories with each hierar-.
</prevsent>
<prevsent>chy are not used in the approach.
</prevsent>
</prevsection>
<citsent citstr=" P98-1041 ">
the approach is the same as the method reported by collier et al (1998) <papid> P98-1041 </papid>except for term weight sand similarities.</citsent>
<aftsection>
<nextsent>we calculate similarities between reuters and translated mainichi documents, where the difference in dates is less than ? 3 days.
</nextsent>
<nextsent>(no hi &amp; eng).
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>hierarchy: the approach uses only reuters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3354">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>much of the previous work on finding bilingual lexicons used comparable corpora.
</prevsent>
<prevsent>one attempt involved directly retrieving bilingual lexicons from corpora.
</prevsent>
</prevsection>
<citsent citstr=" P04-1067 ">
one approach focused on extracting word translations (gaussier et al, 2004).<papid> P04-1067 </papid></citsent>
<aftsection>
<nextsent>the techniques were based on the idea that semantically similar words appear in similar contexts.
</nextsent>
<nextsent>unlike parallel corpora, the position of word in document is useless for translation into the other language.
</nextsent>
<nextsent>in these techniques, the frequency of words in the monolingual document is calculated and their contextual similarity is measured across languages.
</nextsent>
<nextsent>another approach focused on sentence extraction (fung and cheung, 2004).<papid> W04-3208 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3355">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>unlike parallel corpora, the position of word in document is useless for translation into the other language.
</prevsent>
<prevsent>in these techniques, the frequency of words in the monolingual document is calculated and their contextual similarity is measured across languages.
</prevsent>
</prevsection>
<citsent citstr=" W04-3208 ">
another approach focused on sentence extraction (fung and cheung, 2004).<papid> W04-3208 </papid></citsent>
<aftsection>
<nextsent>one limitation of all these methods is that they need to control the experimental evaluation to avoid estimation of every bilingual lexicon appearing incomparable corpora.the alternative consists of two steps: first, cross lingual relevant documents are retrieved from comparable corpora, then bilingual term correspondences within these relevant documents areestimated.
</nextsent>
<nextsent>thus, the accuracy depends on the performance of relevant documents retrieval.
</nextsent>
<nextsent>muchof the previous work in finding relevant documents used mt systems or existing bilingual lexicons to translate one language into another.
</nextsent>
<nextsent>document pairs are then retrieved using some measure of document similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3356">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>muchof the previous work in finding relevant documents used mt systems or existing bilingual lexicons to translate one language into another.
</prevsent>
<prevsent>document pairs are then retrieved using some measure of document similarity.
</prevsent>
</prevsection>
<citsent citstr=" J03-3002 ">
another approach to retrieving relevant documents involves the collection of relevant document urls from the www (resnik and smith, 2003).<papid> J03-3002 </papid></citsent>
<aftsection>
<nextsent>utsuro et al (2003)<papid> E03-1023 </papid>proposed method for acquiring bilingual lexicons that involved retrieval of relevant english and japanese documents from news sites on the www.</nextsent>
<nextsent>our work is also applicable to retrieval of relevant documents on the web because it estimates every bilingual lexicon only appearing in 239 table 6: numbers of monolingual and bilingual verb noun collocations approach &amp; (l ? ) # of candidate collocations # of correct collocations inverse (top 1,000) rank score monolingual patterns # of collocations rate # of collocations rate(d &amp; s/ (d &amp; s/ (top 1,000) jap eng &amp; doc doc) &amp; doc doc) &amp; doc no hi &amp; eng (40) 25,163 44,762 25,163 6,976,214 .361 177 62 2.9 1.35 0.71 reu hierarchy (20) 10,576 37,022 10,576 1,272,102 .831 268 64 4.2 2.24 1.41 int hi &amp; eng (20) 8,347 21,524 8,347 560,472 1.489 328 72 4.6 2.33 1.46 table 7: examples of bilingual verb noun collocations approach &amp; (l ? ) category or # of collocations # of correct examples (english) category pair &amp; doc collocations(%) reu hierarchy (20) sport 262 19,391 36(13.7) create chance, earn medal, feel pressure block shot, establish record, take chance (sport, baseball) 110 8,838 24(21.8) get strike out, leave base, throw pitch (sport, relay) 177 3,418 18(10.2) lead ranking, run km, win athletic (sport, tennis) 115 2,656 32(27.8) lose prize money, play exhibition game int hi &amp; eng (20) (sport, golf) 131 2,654 28(21.4) make birdie, have birdie, hole putt, miss putt (sport, soccer) 86 1,317 34(39.5) block shot, score defender, give free kick (sport, sumo) 75 773 2(2.7) lead sumo, set championship (sport, ski jump) 68 661 10(14.7) postpone downhill, earn medal (sport, football) 37 461 6(16.2) play football, lease football stadium set of smaller documents belonging to pairs of similar categories.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3357">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>document pairs are then retrieved using some measure of document similarity.
</prevsent>
<prevsent>another approach to retrieving relevant documents involves the collection of relevant document urls from the www (resnik and smith, 2003).<papid> J03-3002 </papid></prevsent>
</prevsection>
<citsent citstr=" E03-1023 ">
utsuro et al (2003)<papid> E03-1023 </papid>proposed method for acquiring bilingual lexicons that involved retrieval of relevant english and japanese documents from news sites on the www.</citsent>
<aftsection>
<nextsent>our work is also applicable to retrieval of relevant documents on the web because it estimates every bilingual lexicon only appearing in 239 table 6: numbers of monolingual and bilingual verb noun collocations approach &amp; (l ? ) # of candidate collocations # of correct collocations inverse (top 1,000) rank score monolingual patterns # of collocations rate # of collocations rate(d &amp; s/ (d &amp; s/ (top 1,000) jap eng &amp; doc doc) &amp; doc doc) &amp; doc no hi &amp; eng (40) 25,163 44,762 25,163 6,976,214 .361 177 62 2.9 1.35 0.71 reu hierarchy (20) 10,576 37,022 10,576 1,272,102 .831 268 64 4.2 2.24 1.41 int hi &amp; eng (20) 8,347 21,524 8,347 560,472 1.489 328 72 4.6 2.33 1.46 table 7: examples of bilingual verb noun collocations approach &amp; (l ? ) category or # of collocations # of correct examples (english) category pair &amp; doc collocations(%) reu hierarchy (20) sport 262 19,391 36(13.7) create chance, earn medal, feel pressure block shot, establish record, take chance (sport, baseball) 110 8,838 24(21.8) get strike out, leave base, throw pitch (sport, relay) 177 3,418 18(10.2) lead ranking, run km, win athletic (sport, tennis) 115 2,656 32(27.8) lose prize money, play exhibition game int hi &amp; eng (20) (sport, golf) 131 2,654 28(21.4) make birdie, have birdie, hole putt, miss putt (sport, soccer) 86 1,317 34(39.5) block shot, score defender, give free kick (sport, sumo) 75 773 2(2.7) lead sumo, set championship (sport, ski jump) 68 661 10(14.7) postpone downhill, earn medal (sport, football) 37 461 6(16.2) play football, lease football stadium set of smaller documents belonging to pairs of similar categories.
</nextsent>
<nextsent>munteanu and marcu (2006)<papid> P06-1011 </papid>proposed method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora.</nextsent>
<nextsent>the method is based on the fact that very non-parallel corpora has none or few good sentence pairs, while existing methods for exploiting comparable corpora look for parallel data at the sentence level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3358">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>utsuro et al (2003)<papid> E03-1023 </papid>proposed method for acquiring bilingual lexicons that involved retrieval of relevant english and japanese documents from news sites on the www.</prevsent>
<prevsent>our work is also applicable to retrieval of relevant documents on the web because it estimates every bilingual lexicon only appearing in 239 table 6: numbers of monolingual and bilingual verb noun collocations approach &amp; (l ? ) # of candidate collocations # of correct collocations inverse (top 1,000) rank score monolingual patterns # of collocations rate # of collocations rate(d &amp; s/ (d &amp; s/ (top 1,000) jap eng &amp; doc doc) &amp; doc doc) &amp; doc no hi &amp; eng (40) 25,163 44,762 25,163 6,976,214 .361 177 62 2.9 1.35 0.71 reu hierarchy (20) 10,576 37,022 10,576 1,272,102 .831 268 64 4.2 2.24 1.41 int hi &amp; eng (20) 8,347 21,524 8,347 560,472 1.489 328 72 4.6 2.33 1.46 table 7: examples of bilingual verb noun collocations approach &amp; (l ? ) category or # of collocations # of correct examples (english) category pair &amp; doc collocations(%) reu hierarchy (20) sport 262 19,391 36(13.7) create chance, earn medal, feel pressure block shot, establish record, take chance (sport, baseball) 110 8,838 24(21.8) get strike out, leave base, throw pitch (sport, relay) 177 3,418 18(10.2) lead ranking, run km, win athletic (sport, tennis) 115 2,656 32(27.8) lose prize money, play exhibition game int hi &amp; eng (20) (sport, golf) 131 2,654 28(21.4) make birdie, have birdie, hole putt, miss putt (sport, soccer) 86 1,317 34(39.5) block shot, score defender, give free kick (sport, sumo) 75 773 2(2.7) lead sumo, set championship (sport, ski jump) 68 661 10(14.7) postpone downhill, earn medal (sport, football) 37 461 6(16.2) play football, lease football stadium set of smaller documents belonging to pairs of similar categories.</prevsent>
</prevsection>
<citsent citstr=" P06-1011 ">
munteanu and marcu (2006)<papid> P06-1011 </papid>proposed method for extracting parallel sub sentential fragments from very non-parallel bilingual corpora.</citsent>
<aftsection>
<nextsent>the method is based on the fact that very non-parallel corpora has none or few good sentence pairs, while existing methods for exploiting comparable corpora look for parallel data at the sentence level.
</nextsent>
<nextsent>their methodology is the first aimed at detecting sub-sentential correspondences, while they have not reported that the method is also applicable for large amount of data with good performance, especially in the case of large-scale evaluation such as that presented in this paper.
</nextsent>
<nextsent>we have developed an approach to bilingual verb?
</nextsent>
<nextsent>noun collocations from non-parallel corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3359">
<title id=" C08-1030.xml">retrieving bilingual verb noun collocations by integrating cross language category hierarchies </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>noun collocations from non-parallel corpora.
</prevsent>
<prevsent>the results showed the effectiveness of the method.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
future work will include: (i) applying the method to retrieve other types of collocations (smadja,1993), <papid> J93-1007 </papid>and (ii) evaluating the method using internet directories.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3360">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the improved alignments allow the word(s) in the source language to be aligned to one or more words in the target language.
</prevsent>
<prevsent>experimental results show significant improvement in precision and recall of word alignment.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
bilingual word alignment is first introduced as an intermediate result in statistical machine translation (smt) (brown et al 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></nextsent>
<nextsent>some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3361">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show significant improvement in precision and recall of word alignment.
</prevsent>
<prevsent>bilingual word alignment is first introduced as an intermediate result in statistical machine translation (smt) (brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-1406 ">
besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</nextsent>
<nextsent>in addition, wu (1997) <papid> J97-3002 </papid>used stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3363">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show significant improvement in precision and recall of word alignment.
</prevsent>
<prevsent>bilingual word alignment is first introduced as an intermediate result in statistical machine translation (smt) (brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</nextsent>
<nextsent>in addition, wu (1997) <papid> J97-3002 </papid>used stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3364">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results show significant improvement in precision and recall of word alignment.
</prevsent>
<prevsent>bilingual word alignment is first introduced as an intermediate result in statistical machine translation (smt) (brown et al 1993).<papid> J93-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</nextsent>
<nextsent>in addition, wu (1997) <papid> J97-3002 </papid>used stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3365">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bilingual word alignment is first introduced as an intermediate result in statistical machine translation (smt) (brown et al 1993).<papid> J93-2003 </papid></prevsent>
<prevsent>besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1004 ">
some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</citsent>
<aftsection>
<nextsent>in addition, wu (1997) <papid> J97-3002 </papid>used stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.</nextsent>
<nextsent>generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word align ment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3366">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides being used in smt, it is also used in translation lexicon building (melamed 1996), transfer rule learning (menezes and richardson 2001), <papid> W01-1406 </papid>example-based machine translation (somers 1999), etc. in previous alignment methods, some researches modeled the alignments as hidden parameters in statistical translation model (brown et al 1993; <papid> J93-2003 </papid>och and ney 2000) <papid> P00-1056 </papid>or directly modeled them given the sentence pairs (cherry and lin 2003).<papid> P03-1012 </papid></prevsent>
<prevsent>some researchers used similarity and association measures to build alignment links (ahrenberg et al. 1998; <papid> P98-1004 </papid>tufis and barbu 2002).</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
in addition, wu (1997) <papid> J97-3002 </papid>used stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments.</citsent>
<aftsection>
<nextsent>generally speaking, there are four cases in word alignment: word to word alignment, word to multi-word alignment, multi-word to word alignment, and multi-word to multi-word alignment.
</nextsent>
<nextsent>one of the most difficult tasks in word alignment is to find out the alignments that include multi-word units.
</nextsent>
<nextsent>for example, the statistical word alignment in ibm translation models (brown et al 1993) <papid> J93-2003 </papid>can only handle word to word and multi-word to word alignments.</nextsent>
<nextsent>some studies have been made to tackle this problem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3370">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their results showed that this method improved precision without loss of recall in english to german alignments.
</prevsent>
<prevsent>however, if the same unit is aligned to two different target units, this method is unlikely to make selection.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
some researchers used preprocessing steps to identity multi-word units for word alignment (ahrenberg et al 1998; <papid> P98-1004 </papid>tiedemann 1999; melamed 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>the methods obtained multi-word candidates based on continuous n-gram statistics.
</nextsent>
<nextsent>the main limitation of these methods is that they cannot handle separated phrases and multi-word units in low frequencies.
</nextsent>
<nextsent>in order to handle all of the four cases in word alignment, our approach uses both the alignment information in statistical translation models and translation information in rule-based machine translation system.
</nextsent>
<nextsent>it includes three steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3375">
<title id=" C04-1005.xml">improving statistical word alignment with a rule based machine translation system </title>
<section> rule-based translation system.  </section>
<citcontext>
<prevsection>
<prevsent>then we will describe the algorithm used to improve word alignment results.
</prevsent>
<prevsent>f word similarity calculation this section describes the method for monolingual word similarity calculation.
</prevsent>
</prevsection>
<citsent citstr=" W03-1610 ">
this method calculates word similarity by using bilingual dictionary, which is first introduced by wu and zhou (2003).<papid> W03-1610 </papid></citsent>
<aftsection>
<nextsent>the basic assumptions of this method are that the translations of word can express its meanings and that two words are similar in meanings if they have mutual translations.
</nextsent>
<nextsent>given chinese word, we get its translations with chinese-english bilingual dictionary.
</nextsent>
<nextsent>the translations of word are used to construct its feature vector.
</nextsent>
<nextsent>the similarity of two words is estimated through their feature vectors with the cosine measure as shown in (wu and zhou 2003).<papid> W03-1610 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3384">
<title id=" C08-1019.xml">mind the gap dangers of divorcing evaluations of summary content from linguistic quality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition to teasing out gaps in the current automatic evaluation, we propose method to maximize the strength of current automatic evaluations by using the method of canonical correlation.
</prevsent>
<prevsent>we apply this new evaluation method, which we call rose(rouge optimal summarization evaluation), to find the optimal linear combination of rouge scores to maximize correlation with human responsiveness.
</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
rouge (lin, 2004) <papid> W04-1013 </papid>and its linguistically motivated descend ent, basic elements (be) (hovy et al, 2005), evaluate summary by computing its overlap with set of model (human) summaries; rouge considers lexical n-grams as the unit for comparing the overlap between summaries,while basic elements uses larger units of comparison based on the output of syntactic parsers.</citsent>
<aftsection>
<nextsent>the rouge/be toolkit has become the standard automatic method for evaluating the content of ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>machine-generated summaries, but the correlation of these automatic scores with human evaluation metrics has not always been consistent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3385">
<title id=" C08-1019.xml">mind the gap dangers of divorcing evaluations of summary content from linguistic quality </title>
<section> duc 2005-2007 task and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 evaluation of content.
</prevsent>
<prevsent>nist performed manual pseudo-extrinsic evaluation of peer summaries in the form of assessment of responsiveness.
</prevsent>
</prevsection>
<citsent citstr=" W02-0406 ">
responsiveness differs from other measures of summary content such as see coverage (lin and hovy, 2002) <papid> W02-0406 </papid>and pyramid scores (nenkova and passonneau, 2004) <papid> N04-1019 </papid>in that it does not compare peer summary against set of known human summaries.</citsent>
<aftsection>
<nextsent>rather, the assessor isgiven list of randomly ordered, unlabeled summaries (both human and system-generated) for topic, and must assign responsiveness score to each summary (after having read all the summaries first).
</nextsent>
<nextsent>in duc 2005-2007, nist assessors assigned content responsiveness score to each summary;content responsiveness indicated the amount of information in the summary that helped to satisfy the information need expressed in the topic statement.
</nextsent>
<nextsent>for content responsiveness, the linguistic quality of the summary was to play role in the assessment only insofar as it interfered with the expression of information and reduced the amount of information that was conveyed.
</nextsent>
<nextsent>in duc 2006, assessors assigned an additional overall responsiveness score, which was based onboth information content and readability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3386">
<title id=" C08-1019.xml">mind the gap dangers of divorcing evaluations of summary content from linguistic quality </title>
<section> duc 2005-2007 task and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 evaluation of content.
</prevsent>
<prevsent>nist performed manual pseudo-extrinsic evaluation of peer summaries in the form of assessment of responsiveness.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
responsiveness differs from other measures of summary content such as see coverage (lin and hovy, 2002) <papid> W02-0406 </papid>and pyramid scores (nenkova and passonneau, 2004) <papid> N04-1019 </papid>in that it does not compare peer summary against set of known human summaries.</citsent>
<aftsection>
<nextsent>rather, the assessor isgiven list of randomly ordered, unlabeled summaries (both human and system-generated) for topic, and must assign responsiveness score to each summary (after having read all the summaries first).
</nextsent>
<nextsent>in duc 2005-2007, nist assessors assigned content responsiveness score to each summary;content responsiveness indicated the amount of information in the summary that helped to satisfy the information need expressed in the topic statement.
</nextsent>
<nextsent>for content responsiveness, the linguistic quality of the summary was to play role in the assessment only insofar as it interfered with the expression of information and reduced the amount of information that was conveyed.
</nextsent>
<nextsent>in duc 2006, assessors assigned an additional overall responsiveness score, which was based onboth information content and readability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3387">
<title id=" C08-1019.xml">mind the gap dangers of divorcing evaluations of summary content from linguistic quality </title>
<section> rose: un melange de rouges.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 blending rouge scoring with a. canonical correlation model suppose we are given set of rouge scores and the corresponding content responsiveness scores.
</prevsent>
<prevsent>149 we let ij , for = 1, ...,m and = 1, .., n, be the rouge score of type for the summarizer i, and i the human content evaluation metric.
</prevsent>
</prevsection>
<citsent citstr=" N07-1006 ">
canonical correlation finds an nlong vector such that = argmax ?( ? j=1 ij j , i ), (1) where ?(x, y) is the pearson correlation between and y. similar approach has been used by liu and gildea (2007) <papid> N07-1006 </papid>in the application of machine translation metrics, where they use gradient optimization method to solve the maximization prob lem.</citsent>
<aftsection>
<nextsent>canonical correlation actually solves more general correlation optimization problem, where the goal is to find two linear combinations of variables to maximize the correlation between twosub-spaces.
</nextsent>
<nextsent>in the application of document summarization, we may wish to consider matrix of human evaluation metrics where ijis the jth human evaluation for the ith summarizer.
</nextsent>
<nextsent>we could include, for example, content and overall responsiveness or linguistic questions.
</nextsent>
<nextsent>here we solve for (x, y) in the equation below: (x, y) = argmax ?( ? j=1 ij j , ? j=1 ij j ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3388">
<title id=" C08-1019.xml">mind the gap dangers of divorcing evaluations of summary content from linguistic quality </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>when the linguistic questions q1 and q4 were added to the rose model,correlations of up to 0.96 were observed.
</prevsent>
<prevsent>this result leads to natural question: what automatic methods could be used to approximate the linguistic questions?
</prevsent>
</prevsection>
<citsent citstr=" P05-1018 ">
the work of barzilay and lapata(2005) <papid> P05-1018 </papid>on local coherence might be possible candidate for estimating focus (q4), while an automatic parser could be run on the summaries and the induced score could be used as surrogate for grammaticality (q1).</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3389">
<title id=" C08-1057.xml">the choice of features for classification of verbs in biomedical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their members have similar subcategorization frames scfs (e.g. activate / up-regulate / induce / stimulate np) and selectional preferences (e.g. activate / up-regulate / induce / stimulategenes:waf1), and they can be used to make similar statements describing similar events (e.g. pro teins:p53 activate genes:waf1).
</prevsent>
<prevsent>lexical classes can be used to abstract away from individual words, or to build lexical organization which predicts much of the behaviour of new word by associating it with an appropriate class.
</prevsent>
</prevsection>
<citsent citstr=" C00-2094 ">
they have proved useful for various nlp application tasks, e.g. parsing, word sense dis 1 http://www.nlm.nih.gov/research/umls 449 ambiguation, semantic role labeling, information extraction, question-answering, machine translation (dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swierand stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</citsent>
<aftsection>
<nextsent>a large-scale classification specific to the biomedical data could support key bionlp tasks such as anaphora resolution, predicate argument identification, event extraction and the identification of biomedical (e.g. interaction) relations.
</nextsent>
<nextsent>however, no such classification is available.recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (schulte im walde, 2006; joa nis et al, 2007; sun et al, 2008).
</nextsent>
<nextsent>a number of machine learning (ml) methods have been applied to classify mainly syntactic features (e.g. subcategorization frames (scfs)) extracted from cross domain corpora using e.g. part-of-speech tagging or robust statistical parsing techniques.
</nextsent>
<nextsent>korhonen et al (2006) <papid> P06-1044 </papid>have recently applied such an approach to biomedical texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3390">
<title id=" C08-1057.xml">the choice of features for classification of verbs in biomedical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>their members have similar subcategorization frames scfs (e.g. activate / up-regulate / induce / stimulate np) and selectional preferences (e.g. activate / up-regulate / induce / stimulategenes:waf1), and they can be used to make similar statements describing similar events (e.g. pro teins:p53 activate genes:waf1).
</prevsent>
<prevsent>lexical classes can be used to abstract away from individual words, or to build lexical organization which predicts much of the behaviour of new word by associating it with an appropriate class.
</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
they have proved useful for various nlp application tasks, e.g. parsing, word sense dis 1 http://www.nlm.nih.gov/research/umls 449 ambiguation, semantic role labeling, information extraction, question-answering, machine translation (dorr, 1997; prescher et al, 2000; <papid> C00-2094 </papid>swierand stevenson, 2004; <papid> W04-3213 </papid>dang, 2004; shi and mihalcea, 2005).</citsent>
<aftsection>
<nextsent>a large-scale classification specific to the biomedical data could support key bionlp tasks such as anaphora resolution, predicate argument identification, event extraction and the identification of biomedical (e.g. interaction) relations.
</nextsent>
<nextsent>however, no such classification is available.recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (schulte im walde, 2006; joa nis et al, 2007; sun et al, 2008).
</nextsent>
<nextsent>a number of machine learning (ml) methods have been applied to classify mainly syntactic features (e.g. subcategorization frames (scfs)) extracted from cross domain corpora using e.g. part-of-speech tagging or robust statistical parsing techniques.
</nextsent>
<nextsent>korhonen et al (2006) <papid> P06-1044 </papid>have recently applied such an approach to biomedical texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3391">
<title id=" C08-1057.xml">the choice of features for classification of verbs in biomedical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, no such classification is available.recent research shows that it is possible to automatically induce lexical classes from corpora with promising accuracy (schulte im walde, 2006; joa nis et al, 2007; sun et al, 2008).
</prevsent>
<prevsent>a number of machine learning (ml) methods have been applied to classify mainly syntactic features (e.g. subcategorization frames (scfs)) extracted from cross domain corpora using e.g. part-of-speech tagging or robust statistical parsing techniques.
</prevsent>
</prevsection>
<citsent citstr=" P06-1044 ">
korhonen et al (2006) <papid> P06-1044 </papid>have recently applied such an approach to biomedical texts.</citsent>
<aftsection>
<nextsent>their preliminary experiment shows encouraging results but further work is required before such an approach can be used to benefit practical bio-nlp.
</nextsent>
<nextsent>we conduct large-scale investigation to find optimal features for biomedical verb classification.
</nextsent>
<nextsent>we introduce range of theoretically-motivated feature sets and evaluate them thoroughly using robust method new to the task: cost-basedframework for pairwise clustering.
</nextsent>
<nextsent>our best results compare favourably with earlier ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3393">
<title id=" C08-1057.xml">the choice of features for classification of verbs in biomedical texts </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>most verb classification approaches have therefore employed shallow syntactic slots or scfs as basic features.
</prevsent>
<prevsent>some have supplemented them with further information about verb tense, voice, and/or semantic selectional preferences on argument heads.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
2 the preliminary experiment on biomedical verb classification (korhonen et al, 2006) <papid> P06-1044 </papid>employed basic syntactic features only: scfs extracted from corpus data using the system of briscoe and carroll (1997) <papid> A97-1052 </papid>which operates on the output of domain-independent robust statistical parser (rasp) (briscoe and carroll, 2002).</citsent>
<aftsection>
<nextsent>because such deep syntactic features seem ideally suited for challenging biomedical data, we adopted the same basic approach, but we designed and extracted arange of novel feature sets which include additional syntactic and semantic information.the scf extraction system assigns each occurrence of verb in the parsed data as member of one of the 163 verbal scfs, builds lexical entry for each verb (type) and scf combination, and filters noisy entries out of the lexicon.
</nextsent>
<nextsent>we do not employ the filter in our work because its primary aim is to filter out scfs containing adjuncts (as opposed to arguments).
</nextsent>
<nextsent>adjuncts have been shownto be beneficial for general language verb classification (sun et al, 2008; joanis et al, 2007) and particularly meaningful in biomedical texts (co hen and hunter, 2006).
</nextsent>
<nextsent>the lexical entries provide various information useful for verb classification, including e.g. the frequency of the entry in the data, the part-of-speech (pos) tags of verb tokens, the argument heads in argument positions, the prepositions in pp frames, and the number of verbal occurrences inactive andpassive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3396">
<title id=" C02-1161.xml">lexical query paraphrasing for document retrieval </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>this research was supported in part by australian research council grant dp0209565.
</prevsent>
<prevsent>the vocabulary mis-match between user queries and indexed documents is often addressed through query expansion.
</prevsent>
</prevsection>
<citsent citstr=" P99-1020 ">
two common techniques for query expansion are blind relevance feedback (buckley et al, 1995; mitra et al, 1998) and word sense disambiguation (wsd) (mihalcea and moldovan, 1999; <papid> P99-1020 </papid>lytinen et al, 2000; schutze and pedersen, 1995; lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>blind relevance feedback consists of retrieving small number of documents using query given by user, and then constructing an expanded query that includes content words that appear frequently in these documents.
</nextsent>
<nextsent>this expanded query is used to retrieve new set of documents.
</nextsent>
<nextsent>wsd often precedes query expansion to avoid retrieving irrelevant information.
</nextsent>
<nextsent>mihalcea and moldovan (1999) <papid> P99-1020 </papid>and lytinen et al (2000) used machine readable thesaurus, specifically wordnet (miller et al, 1990), to obtain the sense of word, while schutze and pedersen (1995) and lin (1998) <papid> P98-2127 </papid>used automatically constructed thesauri.the improvements in retrieval performance reported in (mitra et al, 1998) are comparable tothose reported here (note that these researchers consider precision, while we consider recall).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3397">
<title id=" C02-1161.xml">lexical query paraphrasing for document retrieval </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>this research was supported in part by australian research council grant dp0209565.
</prevsent>
<prevsent>the vocabulary mis-match between user queries and indexed documents is often addressed through query expansion.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
two common techniques for query expansion are blind relevance feedback (buckley et al, 1995; mitra et al, 1998) and word sense disambiguation (wsd) (mihalcea and moldovan, 1999; <papid> P99-1020 </papid>lytinen et al, 2000; schutze and pedersen, 1995; lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>blind relevance feedback consists of retrieving small number of documents using query given by user, and then constructing an expanded query that includes content words that appear frequently in these documents.
</nextsent>
<nextsent>this expanded query is used to retrieve new set of documents.
</nextsent>
<nextsent>wsd often precedes query expansion to avoid retrieving irrelevant information.
</nextsent>
<nextsent>mihalcea and moldovan (1999) <papid> P99-1020 </papid>and lytinen et al (2000) used machine readable thesaurus, specifically wordnet (miller et al, 1990), to obtain the sense of word, while schutze and pedersen (1995) and lin (1998) <papid> P98-2127 </papid>used automatically constructed thesauri.the improvements in retrieval performance reported in (mitra et al, 1998) are comparable tothose reported here (note that these researchers consider precision, while we consider recall).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3400">
<title id=" C02-1161.xml">lexical query paraphrasing for document retrieval </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>mihalcea and moldovan (1999) <papid> P99-1020 </papid>and lytinen et al (2000) used machine readable thesaurus, specifically wordnet (miller et al, 1990), to obtain the sense of word, while schutze and pedersen (1995) and lin (1998) <papid> P98-2127 </papid>used automatically constructed thesauri.the improvements in retrieval performance reported in (mitra et al, 1998) are comparable tothose reported here (note that these researchers consider precision, while we consider recall).</prevsent>
<prevsent>there sults obtained by schutze and pedersen (1995) and by lytinen et al (2000) are encouraging.</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
however, experimental results reported in (sanderson, 1994; gonzalo et al, 1998) <papid> W98-0705 </papid>indicate that the improvement in ir performance due to wsd is restricted to short queries, and that ir performance is very sensitive to disambiguation errors.</citsent>
<aftsection>
<nextsent>our approach to document retrieval differs from the above approaches in that the expansion of aquery takes the form of alternative lexical paraphrases.
</nextsent>
<nextsent>like harabagiu et al (2001), <papid> P01-1037 </papid>we use wordnet to propose synonyms for the words in query.</nextsent>
<nextsent>however, they apply heuristics to select which words to paraphrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3401">
<title id=" C02-1161.xml">lexical query paraphrasing for document retrieval </title>
<section> related research.  </section>
<citcontext>
<prevsection>
<prevsent>however, experimental results reported in (sanderson, 1994; gonzalo et al, 1998) <papid> W98-0705 </papid>indicate that the improvement in ir performance due to wsd is restricted to short queries, and that ir performance is very sensitive to disambiguation errors.</prevsent>
<prevsent>our approach to document retrieval differs from the above approaches in that the expansion of aquery takes the form of alternative lexical para phrases.</prevsent>
</prevsection>
<citsent citstr=" P01-1037 ">
like harabagiu et al (2001), <papid> P01-1037 </papid>we use wordnet to propose synonyms for the words in query.</citsent>
<aftsection>
<nextsent>however, they apply heuristics to select which words to paraphrase.
</nextsent>
<nextsent>in contrast, we usecorpus-based information in the context of the entire query to calculate the score of paraphrase and select which paraphrases to retain, and then usethe paraphrase scores to influence the document retrieval process.
</nextsent>
<nextsent>our system uses syntactic, semantic and statistical information for paraphrase generation.
</nextsent>
<nextsent>syntactic information for each query was obtained from brillspart-of-speech (pos) tagger (brill, 1992).<papid> A92-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3402">
<title id=" C02-1161.xml">lexical query paraphrasing for document retrieval </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we usecorpus-based information in the context of the entire query to calculate the score of paraphrase and select which paraphrases to retain, and then usethe paraphrase scores to influence the document retrieval process.
</prevsent>
<prevsent>our system uses syntactic, semantic and statistical information for paraphrase generation.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
syntactic information for each query was obtained from brillspart-of-speech (pos) tagger (brill, 1992).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>semantic information consisting of different types of synonyms for the words in each query was obtained from wordnet (miller et al, 1990).
</nextsent>
<nextsent>the corpus used for information retrieval and for the collection of statistical information was the latimes portion of the nist text research collection (//trec.nist.gov).
</nextsent>
<nextsent>this corpus was small enough to satisfy our disk space limitations, and sufficiently large to yield statistically significant results(131,896 documents).
</nextsent>
<nextsent>full-text indexing was performed for the documents in the la times collection, using lemmas (rather than words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3405">
<title id=" C04-1050.xml">improving japanese zero pronoun resolution by global word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for long time, parsing has been central issue for the area of natural language analyses.
</prevsent>
<prevsent>in recent years, its accuracy has improved toover 90%, and it became the fundamental technology that is applied to lot of nlp applications, such as question answering, text summarization, and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
accordingly, anaphora resolution, which is positioned as the next step of parsing, has been studied actively (ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003; <papid> P03-1023 </papid>iidaet al, 2003; isozaki and hirao, 2003; <papid> W03-1024 </papid>kawahara and kurohashi, 2004).</citsent>
<aftsection>
<nextsent>its performance, however, is not satisfactory enough to benefit the nlp applications.
</nextsent>
<nextsent>we investigated errors of our japanese zero pronoun resolution system (kawahara and kurohashi, 2004), and found that word sense ambiguity causes major part of errors.our zero pronoun resolution system utilizes the general-purpose thesaurusnihongo goi taikei (ikehara et al, 1997) (hereafter, ntt thesaurus) to do matching of example words.
</nextsent>
<nextsent>in this thesaurus, one or more semantic features are given to each word, and similarity between words can be calculated by comparing closeness of their semantic features in the thesaurus tree (appendix a).
</nextsent>
<nextsent>multiple semantic features fora word, i.e. word sense ambiguity, cause in correct matching, and furthermore deteriorate accuracy of the zero pronoun resolution system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3406">
<title id=" C04-1050.xml">improving japanese zero pronoun resolution by global word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for long time, parsing has been central issue for the area of natural language analyses.
</prevsent>
<prevsent>in recent years, its accuracy has improved toover 90%, and it became the fundamental technology that is applied to lot of nlp applications, such as question answering, text summarization, and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
accordingly, anaphora resolution, which is positioned as the next step of parsing, has been studied actively (ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003; <papid> P03-1023 </papid>iidaet al, 2003; isozaki and hirao, 2003; <papid> W03-1024 </papid>kawahara and kurohashi, 2004).</citsent>
<aftsection>
<nextsent>its performance, however, is not satisfactory enough to benefit the nlp applications.
</nextsent>
<nextsent>we investigated errors of our japanese zero pronoun resolution system (kawahara and kurohashi, 2004), and found that word sense ambiguity causes major part of errors.our zero pronoun resolution system utilizes the general-purpose thesaurusnihongo goi taikei (ikehara et al, 1997) (hereafter, ntt thesaurus) to do matching of example words.
</nextsent>
<nextsent>in this thesaurus, one or more semantic features are given to each word, and similarity between words can be calculated by comparing closeness of their semantic features in the thesaurus tree (appendix a).
</nextsent>
<nextsent>multiple semantic features fora word, i.e. word sense ambiguity, cause in correct matching, and furthermore deteriorate accuracy of the zero pronoun resolution system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3407">
<title id=" C04-1050.xml">improving japanese zero pronoun resolution by global word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for long time, parsing has been central issue for the area of natural language analyses.
</prevsent>
<prevsent>in recent years, its accuracy has improved toover 90%, and it became the fundamental technology that is applied to lot of nlp applications, such as question answering, text summarization, and machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W03-1024 ">
accordingly, anaphora resolution, which is positioned as the next step of parsing, has been studied actively (ng and cardie, 2002; <papid> P02-1014 </papid>yang et al, 2003; <papid> P03-1023 </papid>iidaet al, 2003; isozaki and hirao, 2003; <papid> W03-1024 </papid>kawahara and kurohashi, 2004).</citsent>
<aftsection>
<nextsent>its performance, however, is not satisfactory enough to benefit the nlp applications.
</nextsent>
<nextsent>we investigated errors of our japanese zero pronoun resolution system (kawahara and kurohashi, 2004), and found that word sense ambiguity causes major part of errors.our zero pronoun resolution system utilizes the general-purpose thesaurusnihongo goi taikei (ikehara et al, 1997) (hereafter, ntt thesaurus) to do matching of example words.
</nextsent>
<nextsent>in this thesaurus, one or more semantic features are given to each word, and similarity between words can be calculated by comparing closeness of their semantic features in the thesaurus tree (appendix a).
</nextsent>
<nextsent>multiple semantic features fora word, i.e. word sense ambiguity, cause in correct matching, and furthermore deteriorate accuracy of the zero pronoun resolution system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3408">
<title id=" C04-1050.xml">improving japanese zero pronoun resolution by global word sense disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows the upper levels of the semantic feature tree.
</prevsent>
<prevsent>the similarity between two words is defined by formula (1) in appendix a. 2.2 automatically constructed case.
</prevsent>
</prevsection>
<citsent citstr=" C02-1122 ">
frames we employ the automatically constructed case frames (kawahara and kurohashi, 2002) <papid> C02-1122 </papid>as the basic resource for zero pronoun resolution and word sense disambiguation.</citsent>
<aftsection>
<nextsent>this section outlines the method of constructing the case frames.
</nextsent>
<nextsent>the biggest problem in automatic case frame construction is verb sense ambiguity.
</nextsent>
<nextsent>verbs which have different meanings should have different case frames, but it is hard to disambiguate verb senses precisely.
</nextsent>
<nextsent>to deal with this problem, predicate-argument examples which are collected from large corpus are distinguished by coupling verb andits closest case component.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3409">
<title id=" C04-1050.xml">improving japanese zero pronoun resolution by global word sense disambiguation </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>to sum up, the procedure for the automatic case frame construction is as follows.
</prevsent>
<prevsent>1.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
a large raw corpus is parsed by the japanese parser, knp (kurohashi andnagao, 1994<papid> J94-4001 </papid>b), and reliable predicate argument examples are extracted from the parse results.</citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>the extracted examples are bundled ac-.
</nextsent>
<nextsent>cording to the verb and its closest case component, making initial case frames.similarity measure function.
</nextsent>
<nextsent>this similarity is calculated by formula (5) in appendix b. 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3412">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>combining outputs of multiple systems performing the same task has been widely explored in various fields such as speech recognition, word sense disambiguation, and word alignments, and it had been shown that the combination approaches yielded significantly better outputs than the individual systems.
</prevsent>
<prevsent>system combination has also been explored in the mt field, especially with the emergence of various structurally different mt systems.
</prevsent>
</prevsection>
<citsent citstr=" N07-1029 ">
various techniques include hypothesis selection from different systems using sentence level scores, re-decoding source sentences using phrases that are used by individual systems (rosti et al, 2007<papid> N07-1029 </papid>a; huang and papineni, 2007) <papid> D07-1029 </papid>and ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.word-based combination techniques using confusion networks (matusov et al, 2006; <papid> E06-1005 </papid>sim et al,2007; rosti et al, 2007<papid> N07-1029 </papid>b).</nextsent>
<nextsent>among these, confusion network decoding of the system outputs has been shown to be more effective than the others in terms of the overall translation quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3416">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>combining outputs of multiple systems performing the same task has been widely explored in various fields such as speech recognition, word sense disambiguation, and word alignments, and it had been shown that the combination approaches yielded significantly better outputs than the individual systems.
</prevsent>
<prevsent>system combination has also been explored in the mt field, especially with the emergence of various structurally different mt systems.
</prevsent>
</prevsection>
<citsent citstr=" D07-1029 ">
various techniques include hypothesis selection from different systems using sentence level scores, re-decoding source sentences using phrases that are used by individual systems (rosti et al, 2007<papid> N07-1029 </papid>a; huang and papineni, 2007) <papid> D07-1029 </papid>and ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.word-based combination techniques using confusion networks (matusov et al, 2006; <papid> E06-1005 </papid>sim et al,2007; rosti et al, 2007<papid> N07-1029 </papid>b).</nextsent>
<nextsent>among these, confusion network decoding of the system outputs has been shown to be more effective than the others in terms of the overall translation quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3417">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>various techniques include hypothesis selection from different systems using sentence level scores, re-decoding source sentences using phrases that are used by individual systems (rosti et al, 2007<papid> N07-1029 </papid>a; huang and papineni, 2007) <papid> D07-1029 </papid>and ? 2008.</prevsent>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</prevsent>
</prevsection>
<citsent citstr=" E06-1005 ">
some rights reserved.word-based combination techniques using confusion networks (matusov et al, 2006; <papid> E06-1005 </papid>sim et al,2007; rosti et al, 2007<papid> N07-1029 </papid>b).</citsent>
<aftsection>
<nextsent>among these, confusion network decoding of the system outputs has been shown to be more effective than the others in terms of the overall translation quality.
</nextsent>
<nextsent>one of the crucial steps in confusion network decoding is the alignment of hypotheses to each other because the same meaning can be expressed with synonymous words and/or with different word ordering in different hypotheses.
</nextsent>
<nextsent>unfortunately, all the alignment algorithms used in confusion network decoding are insensitive to synonyms of words when aligning two hypotheses to each other.
</nextsent>
<nextsent>this paper extends the previous alignment approaches to handle word synonyms more effectively to improve alignment of different hypotheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3422">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>system combination for machine translation canbe done at three levels: sentence-level, phrase level or word-level.
</prevsent>
<prevsent>sentence-level combination is done by choosing one hypothesis amongmultiplemt system outputs (and possibly among n-best lists).
</prevsent>
</prevsection>
<citsent citstr=" C02-1076 ">
the selection criterion can be combination of translation mod eland language model scores with multiple comparison tests (akiba et al, 2002), <papid> C02-1076 </papid>or statistical confidence models (nomoto, 2004).<papid> P04-1063 </papid></citsent>
<aftsection>
<nextsent>phrase-level combination systems assume thatthe input systems provide some internal information about the system, such as phrases used by the system, and the task is to re-decode the source sentence using this additional information.
</nextsent>
<nextsent>the first example of this approach was the multi-engine mt system (frederking and nirenburg, 1994), <papid> A94-1016 </papid>which builds chart using the translation units inside each input system and then uses chart walk algorithm to find the best cover of the source sentence.rosti et al (2007<papid> N07-1029 </papid>a) collect source-to-target correspondences from the input systems, create new translation option table using only these phrases,and re-decode the source sentence to generate better translations.</nextsent>
<nextsent>in similar work, it has been demonstrated that pruning the original phrase table according to reliable mt hypotheses and enforcing the decoder to obey the word orderings inthe original system outputs improves the performance of the phrase-based combination systems (huang and papineni, 2007).<papid> D07-1029 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3423">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>system combination for machine translation canbe done at three levels: sentence-level, phrase level or word-level.
</prevsent>
<prevsent>sentence-level combination is done by choosing one hypothesis amongmultiplemt system outputs (and possibly among n-best lists).
</prevsent>
</prevsection>
<citsent citstr=" P04-1063 ">
the selection criterion can be combination of translation mod eland language model scores with multiple comparison tests (akiba et al, 2002), <papid> C02-1076 </papid>or statistical confidence models (nomoto, 2004).<papid> P04-1063 </papid></citsent>
<aftsection>
<nextsent>phrase-level combination systems assume thatthe input systems provide some internal information about the system, such as phrases used by the system, and the task is to re-decode the source sentence using this additional information.
</nextsent>
<nextsent>the first example of this approach was the multi-engine mt system (frederking and nirenburg, 1994), <papid> A94-1016 </papid>which builds chart using the translation units inside each input system and then uses chart walk algorithm to find the best cover of the source sentence.rosti et al (2007<papid> N07-1029 </papid>a) collect source-to-target correspondences from the input systems, create new translation option table using only these phrases,and re-decode the source sentence to generate better translations.</nextsent>
<nextsent>in similar work, it has been demonstrated that pruning the original phrase table according to reliable mt hypotheses and enforcing the decoder to obey the word orderings inthe original system outputs improves the performance of the phrase-based combination systems (huang and papineni, 2007).<papid> D07-1029 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3424">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the selection criterion can be combination of translation mod eland language model scores with multiple comparison tests (akiba et al, 2002), <papid> C02-1076 </papid>or statistical confidence models (nomoto, 2004).<papid> P04-1063 </papid></prevsent>
<prevsent>phrase-level combination systems assume thatthe input systems provide some internal information about the system, such as phrases used by the system, and the task is to re-decode the source sentence using this additional information.</prevsent>
</prevsection>
<citsent citstr=" A94-1016 ">
the first example of this approach was the multi-engine mt system (frederking and nirenburg, 1994), <papid> A94-1016 </papid>which builds chart using the translation units inside each input system and then uses chart walk algorithm to find the best cover of the source sentence.rosti et al (2007<papid> N07-1029 </papid>a) collect source-to-target correspondences from the input systems, create new translation option table using only these phrases,and re-decode the source sentence to generate better translations.</citsent>
<aftsection>
<nextsent>in similar work, it has been demonstrated that pruning the original phrase table according to reliable mt hypotheses and enforcing the decoder to obey the word orderings inthe original system outputs improves the performance of the phrase-based combination systems (huang and papineni, 2007).<papid> D07-1029 </papid></nextsent>
<nextsent>in the absence of source-to-target phrase alignments, the sentences can be split into simple chunks using recursive decomposition as input to mt systems (mellebeek et al, 2006).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3431">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> system combination with confusion.  </section>
<citcontext>
<prevsection>
<prevsent>multiple string-matching algorithm based.
</prevsent>
<prevsent>on levenshtein edit distance (bangalore et al., 2001)2.
</prevsent>
</prevsection>
<citsent citstr=" P05-3026 ">
a heuristic-based matching algorithm (ja yaraman and lavie, 2005) <papid> P05-3026 </papid>3.</citsent>
<aftsection>
<nextsent>using giza++ (och and ney, 2000) <papid> P00-1056 </papid>with.</nextsent>
<nextsent>possibly additional training data (matusov et al, 2006)<papid> E06-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3432">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> system combination with confusion.  </section>
<citcontext>
<prevsection>
<prevsent>on levenshtein edit distance (bangalore et al., 2001)2.
</prevsent>
<prevsent>a heuristic-based matching algorithm (ja yaraman and lavie, 2005) <papid> P05-3026 </papid>3.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
using giza++ (och and ney, 2000) <papid> P00-1056 </papid>with.</citsent>
<aftsection>
<nextsent>possibly additional training data (matusov et al, 2006)<papid> E06-1005 </papid></nextsent>
<nextsent>the skeleton and given hypothesis (sim et al., 2007; rosti et al, 2007<papid> N07-1029 </papid>b) none of these methods takes word synonyms into account during alignment of hypotheses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3439">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> evaluation and results.  </section>
<citcontext>
<prevsection>
<prevsent>for chinese-english, the first system used allthe training data without any sentence segmentation.
</prevsent>
<prevsent>the second system used all training data after ibm-1 based sentence segmentation, with different weightings on different corpora.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the third system is the same as the second system except that it used different word alignment symmetrization heuristics (grow-diag-final-and vs. grow-diag final (koehn et al, 2003)).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>5.2 empirical results.
</nextsent>
<nextsent>all input systems were optimized on randomly selected subset of the nist mteval02,mteval03, and mteval04 test sets using min system mt05 mt06 mt08 system 1 53.4 43.8 43.2 system 2 53.9 46.0 42.8 system 3 56.1 45.3 43.3 no syns, 1-pass 56.7 47.5 44.9 w/syns, 2-pass 57.9 48.4 46.2 table 2: lowercase bleu scores (in percentages) on arabic nist mteval test sets.
</nextsent>
<nextsent>imum error rate training (mert) (och, 2003) <papid> P03-1021 </papid>to maximize bleu score (papineni et al, 2002).</nextsent>
<nextsent>system combination was optimized on the rest of this data using mert to maximize bleu score.as inputs to the system combination, we used 10 best hypotheses from each of the re-ranked n-bestlists.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3440">
<title id=" C08-1005.xml">improving alignments for better confusion networks for combining machine translation systems </title>
<section> evaluation and results.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 empirical results.
</prevsent>
<prevsent>all input systems were optimized on randomly selected subset of the nist mteval02,mteval03, and mteval04 test sets using min system mt05 mt06 mt08 system 1 53.4 43.8 43.2 system 2 53.9 46.0 42.8 system 3 56.1 45.3 43.3 no syns, 1-pass 56.7 47.5 44.9 w/syns, 2-pass 57.9 48.4 46.2 table 2: lowercase bleu scores (in percentages) on arabic nist mteval test sets.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
imum error rate training (mert) (och, 2003) <papid> P03-1021 </papid>to maximize bleu score (papineni et al, 2002).</citsent>
<aftsection>
<nextsent>system combination was optimized on the rest of this data using mert to maximize bleu score.as inputs to the system combination, we used 10 best hypotheses from each of the re-ranked n-bestlists.
</nextsent>
<nextsent>to optimize system combination, we generated unique 1000-best lists from lattice we created from the input hypotheses, and used mert in similar way to mt system optimization.we evaluated system combination with improved alignments on three different nist mteval test sets (mteval05, mteval06 nist portion, and mteval08).
</nextsent>
<nextsent>the final mt outputs were evaluated using lower cased bleu scores.
</nextsent>
<nextsent>6tables 2 and 3 present the bleu scores (in per centages) for the input systems and for different combination strategies on three test sets in arabic english and chinese-english, respectively.on arabic-english, the combination with synonym matching and two-pass alignment yields absolute improvements of 1.8 to 2.9 bleu point on three test sets over the best input system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3441">
<title id=" C04-1119.xml">back transliteration from japanese to english using target english context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, back transliteration framework for creating new words would be very useful.
</prevsent>
<prevsent>a number of back transliteration methods for selecting english words from an english pronunciation dictionary have been proposed.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
they include japanese-to-english (knight and graehl, 1998) <papid> J98-4003 </papid>1 , arabic-to-english (stalls and knight, 1 their english letter-to-sound wfst does not convert eng-.</citsent>
<aftsection>
<nextsent>lish words that are not registered in pronunciation dictionary.
</nextsent>
<nextsent>1998), and korean-to-english (lin and chen, 2002).<papid> W02-2017 </papid></nextsent>
<nextsent>there are also methods that select english words from an english word list, e.g., japaneseto-english (fujii and ishikawa, 2001) and chi nese-to-english (chen et al, 1998).<papid> P98-1036 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3442">
<title id=" C04-1119.xml">back transliteration from japanese to english using target english context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they include japanese-to-english (knight and graehl, 1998) <papid> J98-4003 </papid>1 , arabic-to-english (stalls and knight, 1 their english letter-to-sound wfst does not convert eng-.</prevsent>
<prevsent>lish words that are not registered in pronunciation diction ary.</prevsent>
</prevsection>
<citsent citstr=" W02-2017 ">
1998), and korean-to-english (lin and chen, 2002).<papid> W02-2017 </papid></citsent>
<aftsection>
<nextsent>there are also methods that select english words from an english word list, e.g., japaneseto-english (fujii and ishikawa, 2001) and chi nese-to-english (chen et al, 1998).<papid> P98-1036 </papid></nextsent>
<nextsent>moreover, there are back transliteration methods capable of generating new words, there are some methods for back transliteration from korean to english (jeong et al, 1999; kang and choi, 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3443">
<title id=" C04-1119.xml">back transliteration from japanese to english using target english context </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lish words that are not registered in pronunciation dictionary.
</prevsent>
<prevsent>1998), and korean-to-english (lin and chen, 2002).<papid> W02-2017 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1036 ">
there are also methods that select english words from an english word list, e.g., japaneseto-english (fujii and ishikawa, 2001) and chi nese-to-english (chen et al, 1998).<papid> P98-1036 </papid></citsent>
<aftsection>
<nextsent>moreover, there are back transliteration methods capable of generating new words, there are some methods for back transliteration from korean to english (jeong et al, 1999; kang and choi, 2000).
</nextsent>
<nextsent>these previous works did not take the target english context into account for calculating the plausibility of matching target characters with the source characters.
</nextsent>
<nextsent>this paper presents method of taking the target english context into account to generate an english word from japanese katakana word.
</nextsent>
<nextsent>our character-based method can produce new english words that are not listed in the learning corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3444">
<title id=" C02-1098.xml">annotation based multimedia summarization and translation </title>
<section> multimedia summarization and.  </section>
<citcontext>
<prevsection>
<prevsent>the proposed video summarization is performed as by-product of text summarization.
</prevsent>
<prevsent>the text summarization is an application of linguistic annotation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2151 ">
the method is cohesion-based and employs spreading activation to calculate the importance values of words and phrases in the document (nagao and hasida, 1998).<papid> P98-2151 </papid></citsent>
<aftsection>
<nextsent>thus, the video summarization works in terms of summarization of transcript from multimedia annotation data and extraction of the video scene related to the summary.
</nextsent>
<nextsent>since summarized transcript contains important words and phrases, corresponding video sequences will produce collection of significant scenes in the video.
</nextsent>
<nextsent>the summarization results in revised version of multimodal documemtthat contains key frame images and summarized transcripts of selected important scenes.
</nextsent>
<nextsent>key frames of less important scenes are shownin smaller size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3445">
<title id=" C02-1098.xml">annotation based multimedia summarization and translation </title>
<section> multimedia summarization and.  </section>
<citcontext>
<prevsection>
<prevsent>first, transcripts in the annotation data are translated into different languages for the user choice, andthen, the results are shown as subtitles synchronized with the video.
</prevsent>
<prevsent>the video translation module invokes an annotation-based text translation mechanism.
</prevsent>
</prevsection>
<citsent citstr=" C02-2006 ">
text translation is also greatly improved by using linguistic annotation (watanabe et al, 2002).<papid> C02-2006 </papid></citsent>
<aftsection>
<nextsent>the other type of translation is performed in terms of synchronization of video playing and speech synthesis of the translation results.
</nextsent>
<nextsent>this translation makes another-language version of the original video clip.
</nextsent>
<nextsent>if comments, notes, or keywords are included in the annotation data on visual/auditory objects, then they are also translated and shown on popup window.
</nextsent>
<nextsent>in the case of bilingual broadcasting, since our annotation system generates transcripts in every audio channel, multimodal documents can be coming from both channels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3446">
<title id=" C02-1054.xml">efficient support vector classifiers for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we are building trainable open-domain question answering system called saiqa-ii.
</prevsent>
<prevsent>in this paper, we show that an ne recognizer based on support vector machines (svms) gives better scores than conventional systems.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
svms have given high performance in various classification tasks (joachims, 1998; kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>however, it turned out that off-the-shelf svm classifiers are too inefficient for ne recognition.
</nextsent>
<nextsent>the recognizer runs at rate of only 85 bytes/sec on an athlon 1.3 ghz linux pc, while rule-based systems (e.g., isozaki, (2001)) <papid> P01-1041 </papid>can process several kilobytes in second.</nextsent>
<nextsent>the major reason is the inefficiency of svm classifiers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3447">
<title id=" C02-1054.xml">efficient support vector classifiers for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>svms have given high performance in various classification tasks (joachims, 1998; kudo and matsumoto, 2001).<papid> N01-1025 </papid></prevsent>
<prevsent>however, it turned out that off-the-shelf svm classifiers are too inefficient for ne recognition.</prevsent>
</prevsection>
<citsent citstr=" P01-1041 ">
the recognizer runs at rate of only 85 bytes/sec on an athlon 1.3 ghz linux pc, while rule-based systems (e.g., isozaki, (2001)) <papid> P01-1041 </papid>can process several kilobytes in second.</citsent>
<aftsection>
<nextsent>the major reason is the inefficiency of svm classifiers.
</nextsent>
<nextsent>there are other reports on the slowness of svm classifiers.
</nextsent>
<nextsent>another svm-based ne recognizer (yamada and matsumoto, 2001) is 0.8 sentences/sec on pentium iii 933 mhz pc.
</nextsent>
<nextsent>an svm-based part-of-speech (pos).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3449">
<title id=" C02-1054.xml">efficient support vector classifiers for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, since yamada has not compared it with other methods under the same conditions, it is not clear whether his ne system is better or not.
</prevsent>
<prevsent>here, we show that our svm-based ne system ismore accurate than conventional systems.
</prevsent>
</prevsection>
<citsent citstr=" C00-2167 ">
our system uses the viterbi search (allen, 1995) instead of sequential determination.for training, we use crl data?, which was prepared for irex (information retrieval and extraction exercise1, sekine and eriguchi (2000)).<papid> C00-2167 </papid></citsent>
<aftsection>
<nextsent>it has about 19,000 nes in 1,174 articles.
</nextsent>
<nextsent>we also use additional data by isozaki (2001).<papid> P01-1041 </papid></nextsent>
<nextsent>both datasets are based on mainichi newspapers 1994 and 1995 cd-roms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3455">
<title id=" C02-1054.xml">efficient support vector classifiers for named entity recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it uses 17character types such as all-kanji and small integer.
</prevsent>
<prevsent>see isozaki (2001) <papid> P01-1041 </papid>for details.</prevsent>
</prevsection>
<citsent citstr=" W98-1120 ">
now, japanese ne recognition is solved by the classification of words (sekine et al, 1998; <papid> W98-1120 </papid>borthwick, 1999; uchimoto et al, 2000).</citsent>
<aftsection>
<nextsent>for instance, the words in president george herbert bush said clinton is . . .
</nextsent>
<nextsent>are classified as follows: president?
</nextsent>
<nextsent>= other, george?
</nextsent>
<nextsent>= person-begin, her bert?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3463">
<title id=" C04-1004.xml">discriminative hidden markov modeling with long state dependence using a knn ensemble </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a hidden markov model (hmm) is model where sequence of observations is generated in addition to the markov state sequence.
</prevsent>
<prevsent>it is latent variable model in the sense that only the observation sequence is known while the state sequence remains hidden?.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
in recent years, hmms have enjoyed great success in many tagging applications, most notably part-of-speech (pos) tagging (church 1988; weischedel et al  1993; <papid> J93-2006 </papid>merialdo 1994) <papid> J94-2001 </papid>and named entity recognition (bikel et al 1999; zhou et al 2002).</citsent>
<aftsection>
<nextsent>moreover, there have been also efforts to extend the use of hmms to word sense disambiguation (segond et al 1997) <papid> W97-0811 </papid>and shallow/full parsing (brants et al 1997; <papid> W97-0307 </papid>skut et al 1998; zhou et al  2000).</nextsent>
<nextsent>traditionally, hmm segments and labels sequential data in generative way, assigning joint probability to paired observation and state sequences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3464">
<title id=" C04-1004.xml">discriminative hidden markov modeling with long state dependence using a knn ensemble </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a hidden markov model (hmm) is model where sequence of observations is generated in addition to the markov state sequence.
</prevsent>
<prevsent>it is latent variable model in the sense that only the observation sequence is known while the state sequence remains hidden?.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
in recent years, hmms have enjoyed great success in many tagging applications, most notably part-of-speech (pos) tagging (church 1988; weischedel et al  1993; <papid> J93-2006 </papid>merialdo 1994) <papid> J94-2001 </papid>and named entity recognition (bikel et al 1999; zhou et al 2002).</citsent>
<aftsection>
<nextsent>moreover, there have been also efforts to extend the use of hmms to word sense disambiguation (segond et al 1997) <papid> W97-0811 </papid>and shallow/full parsing (brants et al 1997; <papid> W97-0307 </papid>skut et al 1998; zhou et al  2000).</nextsent>
<nextsent>traditionally, hmm segments and labels sequential data in generative way, assigning joint probability to paired observation and state sequences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3465">
<title id=" C04-1004.xml">discriminative hidden markov modeling with long state dependence using a knn ensemble </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is latent variable model in the sense that only the observation sequence is known while the state sequence remains hidden?.
</prevsent>
<prevsent>in recent years, hmms have enjoyed great success in many tagging applications, most notably part-of-speech (pos) tagging (church 1988; weischedel et al  1993; <papid> J93-2006 </papid>merialdo 1994) <papid> J94-2001 </papid>and named entity recognition (bikel et al 1999; zhou et al 2002).</prevsent>
</prevsection>
<citsent citstr=" W97-0811 ">
moreover, there have been also efforts to extend the use of hmms to word sense disambiguation (segond et al 1997) <papid> W97-0811 </papid>and shallow/full parsing (brants et al 1997; <papid> W97-0307 </papid>skut et al 1998; zhou et al  2000).</citsent>
<aftsection>
<nextsent>traditionally, hmm segments and labels sequential data in generative way, assigning joint probability to paired observation and state sequences.
</nextsent>
<nextsent>more formally, generative (first-order) hmm (ghmm) is given by finite set of states including an designated initial state and an designated final state, set of possible observation , two conditional probability distributions: state transition model from to , for and an output model, for o ,    )|(  ssp )|( sopss?
</nextsent>
<nextsent>so s??
</nextsent>
<nextsent>, )|  ss . sequence of observations is generated by starting from the designated initial state, transmiting to new state according to , emitting an observation selected by that new state according to , transmiting to another new state and so on until the designated final state is generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3466">
<title id=" C04-1004.xml">discriminative hidden markov modeling with long state dependence using a knn ensemble </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is latent variable model in the sense that only the observation sequence is known while the state sequence remains hidden?.
</prevsent>
<prevsent>in recent years, hmms have enjoyed great success in many tagging applications, most notably part-of-speech (pos) tagging (church 1988; weischedel et al  1993; <papid> J93-2006 </papid>merialdo 1994) <papid> J94-2001 </papid>and named entity recognition (bikel et al 1999; zhou et al 2002).</prevsent>
</prevsection>
<citsent citstr=" W97-0307 ">
moreover, there have been also efforts to extend the use of hmms to word sense disambiguation (segond et al 1997) <papid> W97-0811 </papid>and shallow/full parsing (brants et al 1997; <papid> W97-0307 </papid>skut et al 1998; zhou et al  2000).</citsent>
<aftsection>
<nextsent>traditionally, hmm segments and labels sequential data in generative way, assigning joint probability to paired observation and state sequences.
</nextsent>
<nextsent>more formally, generative (first-order) hmm (ghmm) is given by finite set of states including an designated initial state and an designated final state, set of possible observation , two conditional probability distributions: state transition model from to , for and an output model, for o ,    )|(  ssp )|( sopss?
</nextsent>
<nextsent>so s??
</nextsent>
<nextsent>, )|  ss . sequence of observations is generated by starting from the designated initial state, transmiting to new state according to , emitting an observation selected by that new state according to , transmiting to another new state and so on until the designated final state is generated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3467">
<title id=" C04-1004.xml">discriminative hidden markov modeling with long state dependence using a knn ensemble </title>
<section> lsd-dhmm: discriminative hmm with.  </section>
<citcontext>
<prevsection>
<prevsent>6.
</prevsent>
<prevsent>experimentation.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the corpus used in shallow parsing is extracted from the penn treebank (marcus et al  1993) <papid> J93-2004 </papid>of 1 million words (25 sections) by program provided by sabine buchholz from tilburg university.</citsent>
<aftsection>
<nextsent>all the evaluations are 5-fold crossvalidated.
</nextsent>
<nextsent>for shallow parsing, we use the fmeasure to measure the performance.
</nextsent>
<nextsent>here, the measure is the weighted harmonic mean of the precision (p) and the recall (r): pr rp + += 2 2 )1( ? with =1 (rijsbergen 1979), where the precision (p) is the percentage of predicted phrase chunks that are actually correct and the recall (r) is the percentage of correct phrase chunks that are actually found.
</nextsent>
<nextsent>2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3468">
<title id=" C02-1128.xml">text authoring knowledge acquisition and description logics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show how datalog kb is sufficient for the closed-world situation, while description logic kbis better-adapted to the more complex open-world situation.
</prevsent>
<prevsent>all along, we pay special attention to logically sound solutions and to decidability issues in the different processes.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
recently there has been surge of interest in interactive natural language generation systems (paris et al, 1995; power and scott, 1998; <papid> P98-2173 </papid>coch and chevreau, 2001); such systems relyon capability of generating natural language text from an abstract content representation, but?</citsent>
<aftsection>
<nextsent>contrary to traditional nlg (natural language gen eration) systems ? this representation is only partially available at the beginning of the text production process;it is then gradually completed by human author, typically using content-selection menus correlated with regions of the evolving generated text..one such system, mda (multilingual document authoring) (citation omitted) is based on formal specification ? using variant of definite clause grammars (dcgs) (pereira and warren, 1980) ? of what counts as valid abstract content representation.
</nextsent>
<nextsent>the different derivation trees in the grammar correspond to texts with different contents, and at each step of the authoring process the user is asked to make interactive choices on howto expand the current partial derivation tree one step further.
</nextsent>
<nextsent>there are important analogies between this process and the process of authoring an xml document under the control of dtd or schema, but dcgs are more expressive in terms of the contextual constraints that can be expressed and also are more adapted to the production of grammatical text.1 in published mda work, all the knowledge about what constitutes valid document is provided in the grammars, with no clear separation between (1) world knowledge (the fact that certain pharmaceutical drug contains some molecule makes it dangerous for certain patient condition) and (2) constraints about document organization (if certain drug is dangerous for certain condition, then warning should be generated at certain place in the document).
</nextsent>
<nextsent>a more principled and modular solution is to leavein the grammar all constraints pertaining to docu ment/textual organization, and to use an external logical theory to express knowledge about the world described by the documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3469">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the expression resolution is high?.
</prevsent>
<prevsent>we regard this task as translation from text to sentiment units, because we noticed that the deep language analysis techniques which are required for the extraction of sentiment units are analogous to those which have been studied for the purpose of language translation.
</prevsent>
</prevsection>
<citsent citstr=" C92-2115 ">
we implemented an accurate sentiment analyzer by making use of an existingtransfer-based machine translation engine (watan abe, 1992), <papid> C92-2115 </papid>replacing the translation patterns and bilingual lexicons with sentiment patterns and sentiment polarity lexicon.</citsent>
<aftsection>
<nextsent>although we used many techniques for deep language analysis, the system was implemented at surprisingly low development cost because the techniques for machine translation could be reused in the architecture described in this paper.we aimed at the high precision extraction of sentiment units.
</nextsent>
<nextsent>in other words, our sa system attaches importance to each individual sentiment expression,rather than to the quantitative tendencies of reputation.
</nextsent>
<nextsent>this is in order to meet the requirement ofthe sa users who want to know not only the over all goodness of an object, but also the breakdown ofopinions.
</nextsent>
<nextsent>for example, when there are many positive opinions and only one negative opinion, the negative one should not be ignored because of its low percentage, but should be investigated thoroughly since valuable knowledge is often found in such minority opinion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3470">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some components are shared between them.
</prevsent>
<prevsent>also other components are similar between mt and sa.
</prevsent>
</prevsection>
<citsent citstr=" P99-1001 ">
this means that the approach forsa should be switched from the rather shallow analysis techniques used for text mining (hearst, 1999; <papid> P99-1001 </papid>nasukawa and nagano, 2001), where some errors can be treated as noise, into deep analysis techniques such as those used for machine translation (mt) where all of the syntactic and semantic phenomena must be handled.</citsent>
<aftsection>
<nextsent>we implemented japanese sa system using ajapanese to english translation engine.
</nextsent>
<nextsent>figure 2 illustrates our sa system, which utilizes mt engine, where techniques for parsing and pattern matching on the tree structures are shared between mt and sa.
</nextsent>
<nextsent>section 2 reviews previous studies of sentiment analysis.
</nextsent>
<nextsent>in section 3 we define the sentiment unit to be extracted for sentiment analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3471">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> previous work on sentiment.  </section>
<citcontext>
<prevsection>
<prevsent>our system is evaluated in section 5.
</prevsent>
<prevsent>in the rest of paper we mainly use japanese examples because some of the operations depend on the japanese language, but we also use english examples to express the sentiment units andsome language-independent issues, for understand ability.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
analysis some prior studies on sentiment analysis focused onthe document-level classification of sentiment (tur ney, 2002; <papid> P02-1053 </papid>pang et al, 2002) <papid> W02-1011 </papid>where document is assumed to have only single sentiment, thus these studies are not applicable to our goal.</citsent>
<aftsection>
<nextsent>other work (subasic and huettner, 2001; morinaga et al, 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of fa vorability.
</nextsent>
<nextsent>automatic acquisition of sentiment expression shave also been studied (hatzivassiloglou and mckeown, 1997), <papid> P97-1023 </papid>but limited to adjectives, and only one sentiment could be assigned to each word.</nextsent>
<nextsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3472">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> previous work on sentiment.  </section>
<citcontext>
<prevsection>
<prevsent>our system is evaluated in section 5.
</prevsent>
<prevsent>in the rest of paper we mainly use japanese examples because some of the operations depend on the japanese language, but we also use english examples to express the sentiment units andsome language-independent issues, for understand ability.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
analysis some prior studies on sentiment analysis focused onthe document-level classification of sentiment (tur ney, 2002; <papid> P02-1053 </papid>pang et al, 2002) <papid> W02-1011 </papid>where document is assumed to have only single sentiment, thus these studies are not applicable to our goal.</citsent>
<aftsection>
<nextsent>other work (subasic and huettner, 2001; morinaga et al, 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of fa vorability.
</nextsent>
<nextsent>automatic acquisition of sentiment expression shave also been studied (hatzivassiloglou and mckeown, 1997), <papid> P97-1023 </papid>but limited to adjectives, and only one sentiment could be assigned to each word.</nextsent>
<nextsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3473">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> previous work on sentiment.  </section>
<citcontext>
<prevsection>
<prevsent>analysis some prior studies on sentiment analysis focused onthe document-level classification of sentiment (tur ney, 2002; <papid> P02-1053 </papid>pang et al, 2002) <papid> W02-1011 </papid>where document is assumed to have only single sentiment, thus these studies are not applicable to our goal.</prevsent>
<prevsent>other work (subasic and huettner, 2001; morinaga et al, 2002) assigned sentiment to words, but they relied on quantitative information such as the frequencies of word associations or statistical predictions of fa vorability.</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
automatic acquisition of sentiment expression shave also been studied (hatzivassiloglou and mckeown, 1997), <papid> P97-1023 </papid>but limited to adjectives, and only one sentiment could be assigned to each word.</citsent>
<aftsection>
<nextsent>yi et al (2003) pointed out that the multiple sentiment aspects in document should be extracted.
</nextsent>
<nextsent>this paper follows that approach, but exploits deeper analysis in order to avoid the analytic failures reported by nasukawa and yi (2003), which occurred when they used shallow parser and only addressed limited number of syntactic phenomena.in our in-depth approach described in the next section, two types of errors out of the four reported by nasukawa and yi (2003) were easily removed2.
</nextsent>
<nextsent>this section describes the sentiment units which are extracted from text, and their roles in the sentiment analysis and its applications.a sentiment unit consists of sentiment, predicate, its one or more arguments, and surface form.
</nextsent>
<nextsent>formally it is expressed as in figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3474">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results have shown that the precision of the sentiment polarity was much higher than for the conventional methods, and the sentiment units created by our system were less redundant and more informative than when using nave predicate-argumentstructures.
</prevsent>
<prevsent>even though we exploited many advantages of deep analysis, we could create sentiment analysis system at very low development cost, be cause many of the techniques for machine translation can be reused naturally when we regard the extraction of sentiment units as kind of translation.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
many techniques which have been studied for the purpose of machine translation, such as word sense disambiguation (dagan and itai, 1994; <papid> J94-4003 </papid>yarowsky, 1995), <papid> P95-1026 </papid>anaphora resolution (mitamura et al, 2002), and automatic pattern extraction from corpora (watanabe et al, 2003), can accelerate the further enhancement of sentiment analysis, or other nlp tasks.</citsent>
<aftsection>
<nextsent>therefore this work is the first step towards the integration of shallow and wide nlp, with deep nlp.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3475">
<title id=" C04-1071.xml">deeper sentiment analysis using machine translation technology </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results have shown that the precision of the sentiment polarity was much higher than for the conventional methods, and the sentiment units created by our system were less redundant and more informative than when using nave predicate-argumentstructures.
</prevsent>
<prevsent>even though we exploited many advantages of deep analysis, we could create sentiment analysis system at very low development cost, be cause many of the techniques for machine translation can be reused naturally when we regard the extraction of sentiment units as kind of translation.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
many techniques which have been studied for the purpose of machine translation, such as word sense disambiguation (dagan and itai, 1994; <papid> J94-4003 </papid>yarowsky, 1995), <papid> P95-1026 </papid>anaphora resolution (mitamura et al, 2002), and automatic pattern extraction from corpora (watanabe et al, 2003), can accelerate the further enhancement of sentiment analysis, or other nlp tasks.</citsent>
<aftsection>
<nextsent>therefore this work is the first step towards the integration of shallow and wide nlp, with deep nlp.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3476">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>note that our system participated in tsc3 is an automatic summarization system without user interaction by letting our system with user interaction select 12 best keywords regarding scoring by the system.
</prevsent>
<prevsent>moreover, we evaluated effectiveness of user interaction and that with user interaction attained both higher coverage and precision than that without user interaction.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
summarization system our multiple-document summarization system proposed in this paper is different from previ 1http://www.lr.pi.titech.ac.jp/tsc/index-en.htmlously proposed multiple-document summarization methods (see, e.g.,(barzilay et al, 1999), (<papid> P99-1071 </papid>mani and bloedorn, 1999), (goldstein et al, 2000), (<papid> W00-0405 </papid>ando et al, 2000), (<papid> W00-0409 </papid>lin and hovy, 2002), (<papid> P02-1058 </papid>nobata and sekine, 2002), (hirao et al, 2003)) in that: (1) our system can produce summary coping appropriately with each users summarization need by letting user select keywords reflecting users summarization need.</citsent>
<aftsection>
<nextsent>(2) the keywords are extracted automatically from document set to be summarized by calculating score to each noun contained in the document set.
</nextsent>
<nextsent>the formula to calculate scores consists ofnot only frequency of nouns and document frequency used in tf ? idf but also distribution of nouns in the document set and location of nounsin documents or the document set.
</nextsent>
<nextsent>the reason why such factors are used will be explained in the next section.
</nextsent>
<nextsent>(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3477">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>note that our system participated in tsc3 is an automatic summarization system without user interaction by letting our system with user interaction select 12 best keywords regarding scoring by the system.
</prevsent>
<prevsent>moreover, we evaluated effectiveness of user interaction and that with user interaction attained both higher coverage and precision than that without user interaction.
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
summarization system our multiple-document summarization system proposed in this paper is different from previ 1http://www.lr.pi.titech.ac.jp/tsc/index-en.htmlously proposed multiple-document summarization methods (see, e.g.,(barzilay et al, 1999), (<papid> P99-1071 </papid>mani and bloedorn, 1999), (goldstein et al, 2000), (<papid> W00-0405 </papid>ando et al, 2000), (<papid> W00-0409 </papid>lin and hovy, 2002), (<papid> P02-1058 </papid>nobata and sekine, 2002), (hirao et al, 2003)) in that: (1) our system can produce summary coping appropriately with each users summarization need by letting user select keywords reflecting users summarization need.</citsent>
<aftsection>
<nextsent>(2) the keywords are extracted automatically from document set to be summarized by calculating score to each noun contained in the document set.
</nextsent>
<nextsent>the formula to calculate scores consists ofnot only frequency of nouns and document frequency used in tf ? idf but also distribution of nouns in the document set and location of nounsin documents or the document set.
</nextsent>
<nextsent>the reason why such factors are used will be explained in the next section.
</nextsent>
<nextsent>(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3478">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>note that our system participated in tsc3 is an automatic summarization system without user interaction by letting our system with user interaction select 12 best keywords regarding scoring by the system.
</prevsent>
<prevsent>moreover, we evaluated effectiveness of user interaction and that with user interaction attained both higher coverage and precision than that without user interaction.
</prevsent>
</prevsection>
<citsent citstr=" W00-0409 ">
summarization system our multiple-document summarization system proposed in this paper is different from previ 1http://www.lr.pi.titech.ac.jp/tsc/index-en.htmlously proposed multiple-document summarization methods (see, e.g.,(barzilay et al, 1999), (<papid> P99-1071 </papid>mani and bloedorn, 1999), (goldstein et al, 2000), (<papid> W00-0405 </papid>ando et al, 2000), (<papid> W00-0409 </papid>lin and hovy, 2002), (<papid> P02-1058 </papid>nobata and sekine, 2002), (hirao et al, 2003)) in that: (1) our system can produce summary coping appropriately with each users summarization need by letting user select keywords reflecting users summarization need.</citsent>
<aftsection>
<nextsent>(2) the keywords are extracted automatically from document set to be summarized by calculating score to each noun contained in the document set.
</nextsent>
<nextsent>the formula to calculate scores consists ofnot only frequency of nouns and document frequency used in tf ? idf but also distribution of nouns in the document set and location of nounsin documents or the document set.
</nextsent>
<nextsent>the reason why such factors are used will be explained in the next section.
</nextsent>
<nextsent>(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3479">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>note that our system participated in tsc3 is an automatic summarization system without user interaction by letting our system with user interaction select 12 best keywords regarding scoring by the system.
</prevsent>
<prevsent>moreover, we evaluated effectiveness of user interaction and that with user interaction attained both higher coverage and precision than that without user interaction.
</prevsent>
</prevsection>
<citsent citstr=" P02-1058 ">
summarization system our multiple-document summarization system proposed in this paper is different from previ 1http://www.lr.pi.titech.ac.jp/tsc/index-en.htmlously proposed multiple-document summarization methods (see, e.g.,(barzilay et al, 1999), (<papid> P99-1071 </papid>mani and bloedorn, 1999), (goldstein et al, 2000), (<papid> W00-0405 </papid>ando et al, 2000), (<papid> W00-0409 </papid>lin and hovy, 2002), (<papid> P02-1058 </papid>nobata and sekine, 2002), (hirao et al, 2003)) in that: (1) our system can produce summary coping appropriately with each users summarization need by letting user select keywords reflecting users summarization need.</citsent>
<aftsection>
<nextsent>(2) the keywords are extracted automatically from document set to be summarized by calculating score to each noun contained in the document set.
</nextsent>
<nextsent>the formula to calculate scores consists ofnot only frequency of nouns and document frequency used in tf ? idf but also distribution of nouns in the document set and location of nounsin documents or the document set.
</nextsent>
<nextsent>the reason why such factors are used will be explained in the next section.
</nextsent>
<nextsent>(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3480">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>the formula to calculate scores consists ofnot only frequency of nouns and document frequency used in tf ? idf but also distribution of nouns in the document set and location of nounsin documents or the document set.
</prevsent>
<prevsent>the reason why such factors are used will be explained in the next section.
</prevsent>
</prevsection>
<citsent citstr=" W02-1907 ">
(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</citsent>
<aftsection>
<nextsent>the interactive summarization system has been introduced for the first time by (saggion and lapalme, 2002).<papid> J02-4005 </papid></nextsent>
<nextsent>the system proposed in(saggion and lapalme, 2002) <papid> J02-4005 </papid>is based on shallow syntactic and semantic analysis, conceptual identification and text re-generation, while, our system is based on statistical method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3484">
<title id=" C04-1144.xml">a multiple document summarization system with user interaction </title>
<section> feature of our multiple-document.  </section>
<citcontext>
<prevsection>
<prevsent>the reason why such factors are used will be explained in the next section.
</prevsent>
<prevsent>(3) our system deletes redundant adnominal verb phrases in sentences to reduce the number of characters in sentence.the dele table adnominal verb phrases are decided statistically by using entropy based on probability that verbs modify noun, etc. our previous method (sakai and masuyama, 2002) <papid> W02-1907 </papid>adjusted to multiple-document summarization so that more dele table adnominal verb phrases are recognized, is used in this system.</prevsent>
</prevsection>
<citsent citstr=" J02-4005 ">
the interactive summarization system has been introduced for the first time by (saggion and lapalme, 2002).<papid> J02-4005 </papid></citsent>
<aftsection>
<nextsent>the system proposed in(saggion and lapalme, 2002) <papid> J02-4005 </papid>is based on shallow syntactic and semantic analysis, conceptual identification and text re-generation, while, our system is based on statistical method.</nextsent>
<nextsent>keywords relevant document set to be summarized may be regarded as document set obtained by hypothetical query from the entire document set ? to be considered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3498">
<title id=" C02-1063.xml">hierarchical orderings of textual units </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more seriously (riloff, 1995) argues that the bag of words model ignores morphological and syntactical information which she found to be essential for solving some categorization tasks.
</prevsent>
<prevsent>an alternative to the vector space model are semantic spaces, which have been proposed as high dimensional format for representing relations of semantic proximity.
</prevsent>
</prevsection>
<citsent citstr=" P84-1062 ">
relying on sparse knowledge resources, they prove to be efficient in cognitive science (kintsch, 1998; landauer and du mais, 1997), computational linguistics (rieger, 1984; <papid> P84-1062 </papid>schutze, 1998), and information retrieval.although semantic spaces prove to be an alternative to the vector space model, they leave the question unanswered of how to explore and visualize similarities of signs mapped onto them.</citsent>
<aftsection>
<nextsent>in case that texts are represented as points in semantic space, this question refers to the exploration of their implicit, content based relations.
</nextsent>
<nextsent>several methods for solving this task have been proposed which range from simple lists via minimal spanning trees to cluster analysis as part of scatter/gahter algorithms (hearst and pedersen, 1996).
</nextsent>
<nextsent>representing signs environment in space by means of lists runs the risk of successively ordering semantically or thematically diverse units.
</nextsent>
<nextsent>obviously, lists neglect the poly-hierarchical structure of semantic spaces which may induce divergent thematic progressions starting from the same polysemous unit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3501">
<title id=" C02-1063.xml">hierarchical orderings of textual units </title>
<section> numerical text representation.  </section>
<citcontext>
<prevsection>
<prevsent>thus, in m2 the textual connotation of text is not only seen tobe structured on the basis of the criterion of similarity of lexical organization, but also by meansof genre specific features modeled as quantitative text characteristics.
</prevsent>
<prevsent>this approach follows (herdan, 1966), who programmatically asked,whether difference in style correlates with difference infrequency of use of linguistic forms.
</prevsent>
</prevsection>
<citsent citstr=" E99-1019 ">
see (wolters and kirsten, 1999) <papid> E99-1019 </papid>who, following this approach, already used pos frequency as source for genre classification, task which goes beyond the scope of the given paper.on this background compound text similarity measure can be derived as linear model: ?(x, y) = 3?</citsent>
<aftsection>
<nextsent>i=1 ii(x, y) ? [0, 1] (4)a. where 1(x, y) = ?(~x, ~y) models lexical semantics of texts x, according to m1;b. 2 uses the levenshtein metric for measuring the similarity of the text structure stings assigned to and y;c. and 3 measures, based on an euclidean metric, the similarity of texts with respect to the quantitative features enumerated above.i biases the contribution of these different dimensions of text representation.
</nextsent>
<nextsent>we yield good results for 1 = 0.9, 2 = 3 = 0.05.
</nextsent>
<nextsent>m3: finally, we experimented with text representation model resulting from the aggregation (i.e. weighted mean) of the vector representations of text in both spaces, i.e. vector and semantic space.
</nextsent>
<nextsent>this approach, which demands both spaces to have exactly the same dimensions and standardized coordinate values, follows the idea to reduce the noise inherent to both models: whether syntagmatic as in case of vector spaces, or paradigmatic as in case of semantic spaces.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3502">
<title id=" C02-1063.xml">hierarchical orderings of textual units </title>
<section> text linkage.  </section>
<citcontext>
<prevsection>
<prevsent>departing from ordinary list as well as cluster structures, we model the connotation of textas hierarchy, where each node represents single connoted text (and not set of texts as incase of agglomerative cluster analysis).
</prevsent>
<prevsent>in order to narrow down solution for this task weneed linguistic criterion, which bridges between the linguistic knowledge represented in semantic spaces and the task of connotative textlinkage.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
for this purpose we refer to the concept of lexical cohesion introduced by (halliday and hasan, 1976); see (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>marcu, 2000) who already use this concept for text segmentation.</citsent>
<aftsection>
<nextsent>according tothis approach, lexical cohesion results from reiterating words, which are semantically related on the basis of (un-)systematic relations (e.g.synonymy or hyponymy).
</nextsent>
<nextsent>unsystematic lexical cohesion results from patterns of contextual, paradigmatic similarity: ?[.
</nextsent>
<nextsent>] lexical items having similar patterns of collocation that is,tending to appear in similar context swill generate cohesive force if they occur in adjacent sentences.?
</nextsent>
<nextsent>(halliday and hasan, 1976, p. 286).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3503">
<title id=" C02-1063.xml">hierarchical orderings of textual units </title>
<section> text linkage.  </section>
<citcontext>
<prevsection>
<prevsent>departing from ordinary list as well as cluster structures, we model the connotation of textas hierarchy, where each node represents single connoted text (and not set of texts as incase of agglomerative cluster analysis).
</prevsent>
<prevsent>in order to narrow down solution for this task weneed linguistic criterion, which bridges between the linguistic knowledge represented in semantic spaces and the task of connotative textlinkage.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
for this purpose we refer to the concept of lexical cohesion introduced by (halliday and hasan, 1976); see (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997; <papid> J97-1003 </papid>marcu, 2000) who already use this concept for text segmentation.</citsent>
<aftsection>
<nextsent>according tothis approach, lexical cohesion results from reiterating words, which are semantically related on the basis of (un-)systematic relations (e.g.synonymy or hyponymy).
</nextsent>
<nextsent>unsystematic lexical cohesion results from patterns of contextual, paradigmatic similarity: ?[.
</nextsent>
<nextsent>] lexical items having similar patterns of collocation that is,tending to appear in similar context swill generate cohesive force if they occur in adjacent sentences.?
</nextsent>
<nextsent>(halliday and hasan, 1976, p. 286).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3506">
<title id=" C02-1063.xml">hierarchical orderings of textual units </title>
<section> text linkage.  </section>
<citcontext>
<prevsection>
<prevsent>looking fora mathematical model of this optimality criterion, minimal spanning trees (mst) drop out, since they only optimize direct node-to-nodesimilarities disregarding any path context.
</prevsent>
<prevsent>furthermore, whereas we expect to yield different trees modeling the connotations of different texts, msts ignore this aspect dependency since they focus on unique spanning tree ofthe underlying feature space.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
another candidate is given by dependency trees (rieger, 1984) <papid> P84-1062 </papid>which are equal to similarity trees (lin, 1998): <papid> P98-2127 </papid>forgiven root x, the nodes are inserted into its similarity tree (st) in descending order of their similarity to x, where the predecessor of any node is chosen to be the node already inserted, to which is most similar.</citsent>
<aftsection>
<nextsent>although sts already capture the aspect dependency induced by their varying roots, the path criterion is still not met.
</nextsent>
<nextsent>thus, we generalize the concept of st to that of cohesion tree as follows: first, we observe that the construction of sts uses two types of order relations: the first, let it call 1x, determines the order of the nodes inserted dependent on root x; the second, let it call 2y, varies with node to be inserted and determines its predecessor.
</nextsent>
<nextsent>next, in order to build cohesion trees out of this skeleton, we instantiate all relations 2y in way, which finds the path of minimal loss of cohesion when is attached to it.
</nextsent>
<nextsent>this is done with the help of distance measure which induces descending order of cohesion of paths: definition 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3507">
<title id=" C08-1070.xml">whats the date high accuracy interpretation of weekday names </title>
<section> the problem.  </section>
<citcontext>
<prevsection>
<prevsent>once we have the reference point determined, the interpretation of the offset from this reference point requires us to determine the magnitude and direction of offset.
</prevsent>
<prevsent>as noted above, in some cases the tense of the controlling verb will indicate the direction of offset; but prepositional attachment ambiguity can easily damage the reliability of such an approach, as demonstrated by the following minimal pair: (5) we can show you some pictures on monday.
</prevsent>
</prevsection>
<citsent citstr=" N06-1018 ">
(6) we can show you some pictures from monday.in example (5), the correct pp attachment is required in order to determine that monday is in the 1 in the literature, variety of different terms are used: (schilder and habel, 2001) call these expressions indexicals, and (han et al, 2006<papid> N06-1018 </papid>b) uses the term relative for what we call anaphoric references: in our terminology, both deictic and anaphorical expressions are relative.</citsent>
<aftsection>
<nextsent>2 this reference point is often referred to as the temporal focus or temporal anchor.
</nextsent>
<nextsent>scope of the verb group can show, allowing us to infer that the monday in question is in the future.example (6), on the other hand, is quite ambiguous and requires world knowledge in order to determine the correct attachment.
</nextsent>
<nextsent>we are interested, therefore, in determining whether some heuristic method might provide good results.
</nextsent>
<nextsent>in the rest of this paper, we focus on the determination of the direction of offset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3509">
<title id=" C08-1070.xml">whats the date high accuracy interpretation of weekday names </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated strategies for temporal focus tracking would likely be required in other genres.
</prevsent>
<prevsent>the literature contains number of approaches to the interpretation of weekday names, although weare not aware of any pre-existing direct comparison of these approaches.
</prevsent>
</prevsection>
<citsent citstr=" W01-1313 ">
3 filatova and hovy (2001) <papid> W01-1313 </papid>assign time stamps to clauses in which an event is mentioned.</citsent>
<aftsection>
<nextsent>as part of the overall process, they use heuristic for the interpretation of weekday names: if the day name in clause is the same as that of the temporal focus, then the temporal focus is used; 4 otherwise, they look for any signal words?
</nextsent>
<nextsent>or check the tense of the verb in the clause.
</nextsent>
<nextsent>an analogous approach is taken for the interpretation of month names.
</nextsent>
<nextsent>negri and marseglia (2005), in their rule-based system for temporal expression recognition and normalisation, use what they call context words?,such as following or later, to decide on the interpretation of weekday name.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3510">
<title id=" C08-1070.xml">whats the date high accuracy interpretation of weekday names </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>here, having identified the date march 30 2004(which happens to be tuesday), they then recognise the structure following + trigger?
</prevsent>
<prevsent>and reason that the friday is three days later.
</prevsent>
</prevsection>
<citsent citstr=" N07-1053 ">
3 although ahn et al (2007) <papid> N07-1053 </papid>compared their results with those presented by mani and wilson (2000), <papid> P00-1010 </papid>they went on to point out that, for variety of reasons, the numbers they provided were not really comparable.</citsent>
<aftsection>
<nextsent>4 filatova and hovy use the term reference point for what we call the temporal focus.
</nextsent>
<nextsent>554there have also been machine-learning approaches to the interpretation of temporal expressions.
</nextsent>
<nextsent>ahn et al (2005) describes system developed and tested on the ace 2004 tern testcorpus.
</nextsent>
<nextsent>using lexical features, such as the occurrence of last or earlier in context window of three words, their maximum entropy classifier picked the correct direction (backward?, same?,or forward?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3512">
<title id=" C08-1070.xml">whats the date high accuracy interpretation of weekday names </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>here, having identified the date march 30 2004(which happens to be tuesday), they then recognise the structure following + trigger?
</prevsent>
<prevsent>and reason that the friday is three days later.
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
3 although ahn et al (2007) <papid> N07-1053 </papid>compared their results with those presented by mani and wilson (2000), <papid> P00-1010 </papid>they went on to point out that, for variety of reasons, the numbers they provided were not really comparable.</citsent>
<aftsection>
<nextsent>4 filatova and hovy use the term reference point for what we call the temporal focus.
</nextsent>
<nextsent>554there have also been machine-learning approaches to the interpretation of temporal expressions.
</nextsent>
<nextsent>ahn et al (2005) describes system developed and tested on the ace 2004 tern testcorpus.
</nextsent>
<nextsent>using lexical features, such as the occurrence of last or earlier in context window of three words, their maximum entropy classifier picked the correct direction (backward?, same?,or forward?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3518">
<title id=" C08-1070.xml">whats the date high accuracy interpretation of weekday names </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we have investigated the problem of the interpretation of bare weekday names in texts, and presented new heuristic which extends baldwins (2002) approach.
</prevsent>
<prevsent>our evaluations on widely-available dataset show that our hybrid algorithm was the 559 best performing algorithm, achieving an accuracy of 95.91% with 15 errors out of 367 instances.
</prevsent>
</prevsection>
<citsent citstr=" W06-0902 ">
the algorithm is implemented within our dante system for temporal expression interpretation (dale and mazur, 2006; <papid> W06-0902 </papid>mazur and dale, 2007).</citsent>
<aftsection>
<nextsent>it seems quite possible that our heuristics take advantage of phenomena that are specific to newswire texts and other similar types of reportage.
</nextsent>
<nextsent>although these are precisely the kinds of texts where, in our own work, we need to provide fast processing of large volumes of text, it remains to be seen how these heuristics fare when faced with broader range of text types.
</nextsent>
<nextsent>in particular,other text types are likely to require more sophisticated approaches to temporal focus tracking than we have used here.
</nextsent>
<nextsent>also, we have not attempted to replicate here the machine learning approaches described in (ahn et al., 2005) and (ahn et al, 2007), <papid> N07-1053 </papid>nor hans use of constraint satisfaction problem methods (see (han et al, 2006<papid> N06-1018 </papid>a)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3522">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, head-finding rules are used to augment node labels with lexical heads.
</prevsent>
<prevsent>in this paper, we provide machinery to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose flexible notation for specifying these rules that would allow them to be shared by different models; second, we report on an experiment to see whether we can use expectation maximization to automatically fine-tune set of hand-written rules to particular corpus.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
most work in statistical parsing does not operate in the realm of parse trees as they appear in many treebanks, but rather on trees transformed via augmentation of their node labels, or some other transformation (johnson, 1998).<papid> J98-4004 </papid></citsent>
<aftsection>
<nextsent>this methodology is illustrated in figure 1.
</nextsent>
<nextsent>the information included in the node labels?
</nextsent>
<nextsent>augmentations may include lexical items, or node label suffix to indicate the node is an argument and not an adjunct; such extra information may be viewed as latent information, in that it is not directly present in the treebank parse trees, but maybe recovered by some means.
</nextsent>
<nextsent>the process of recovering this latent information has largely been limited to the hand-construction of heuristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3523">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>are performed by existing statistical parsers that we have examined.
</prevsent>
<prevsent>second, we explore novel use of expectation-maximization (dempster et al, 1977) that iteratively re estimates parsing model using the augmenting heuristics as starting point.
</prevsent>
</prevsection>
<citsent citstr=" P98-1091 ">
specifically, the em algorithm we use is variant of the inside-outside algorithm (baker, 1979; lari and young, 1990; hwa, 1998).<papid> P98-1091 </papid></citsent>
<aftsection>
<nextsent>the reestimation adjusts the models parameters in the augmented parse-tree space to maximize the likelihood of the observed (incomplete) data, in the hopes of finding better distribution over augmented parse trees (the complete data).
</nextsent>
<nextsent>the ultimate goal of this work is to minimize the human effort needed when adapting parsing model to new domain.
</nextsent>
<nextsent>2.1 head-lexicalization.
</nextsent>
<nextsent>many of the recent, successful statistical parser shave made use of lexical information or an implicit lexicalized grammar, both for english and, more recently, for other languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3524">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.
</prevsent>
<prevsent>in fact, nearly identical head-lexicalizations were used in the dis s(caughtvbd) np(boynn) det the nn boy advp(alsorb) rb also vp(caughtvbd) vbd caught np(ballnn) det the nn ball figure 2: simple lexicalized parse tree.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
criminative models described in (magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997), <papid> W97-0301 </papid>the lexicalized pcfg model sin (collins, 1999), the generative model in (char niak, 2000), <papid> A00-2018 </papid>the lexicalized tag extractor in (xia, 1999) and the stochastic lexicalized tag model sin (chiang, 2000; <papid> P00-1058 </papid>sarkar, 2001; <papid> N01-1023 </papid>chen and vijay shanker, 2000).</citsent>
<aftsection>
<nextsent>inducing lexicalized structure based on heads has two-pronged effect: it notonly allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies?
</nextsent>
<nextsent>both syntactic and lexical will be included in the model itself.
</nextsent>
<nextsent>for example, in figure 2, the nonterminal np(boynn) is dependent on vp(caughtvbd) and not the other way around.
</nextsent>
<nextsent>2.2 other tree transformations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3526">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.
</prevsent>
<prevsent>in fact, nearly identical head-lexicalizations were used in the dis s(caughtvbd) np(boynn) det the nn boy advp(alsorb) rb also vp(caughtvbd) vbd caught np(ballnn) det the nn ball figure 2: simple lexicalized parse tree.
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
criminative models described in (magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997), <papid> W97-0301 </papid>the lexicalized pcfg model sin (collins, 1999), the generative model in (char niak, 2000), <papid> A00-2018 </papid>the lexicalized tag extractor in (xia, 1999) and the stochastic lexicalized tag model sin (chiang, 2000; <papid> P00-1058 </papid>sarkar, 2001; <papid> N01-1023 </papid>chen and vijay shanker, 2000).</citsent>
<aftsection>
<nextsent>inducing lexicalized structure based on heads has two-pronged effect: it notonly allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies?
</nextsent>
<nextsent>both syntactic and lexical will be included in the model itself.
</nextsent>
<nextsent>for example, in figure 2, the nonterminal np(boynn) is dependent on vp(caughtvbd) and not the other way around.
</nextsent>
<nextsent>2.2 other tree transformations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3527">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.
</prevsent>
<prevsent>in fact, nearly identical head-lexicalizations were used in the dis s(caughtvbd) np(boynn) det the nn boy advp(alsorb) rb also vp(caughtvbd) vbd caught np(ballnn) det the nn ball figure 2: simple lexicalized parse tree.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
criminative models described in (magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997), <papid> W97-0301 </papid>the lexicalized pcfg model sin (collins, 1999), the generative model in (char niak, 2000), <papid> A00-2018 </papid>the lexicalized tag extractor in (xia, 1999) and the stochastic lexicalized tag model sin (chiang, 2000; <papid> P00-1058 </papid>sarkar, 2001; <papid> N01-1023 </papid>chen and vijay shanker, 2000).</citsent>
<aftsection>
<nextsent>inducing lexicalized structure based on heads has two-pronged effect: it notonly allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies?
</nextsent>
<nextsent>both syntactic and lexical will be included in the model itself.
</nextsent>
<nextsent>for example, in figure 2, the nonterminal np(boynn) is dependent on vp(caughtvbd) and not the other way around.
</nextsent>
<nextsent>2.2 other tree transformations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3528">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.
</prevsent>
<prevsent>in fact, nearly identical head-lexicalizations were used in the dis s(caughtvbd) np(boynn) det the nn boy advp(alsorb) rb also vp(caughtvbd) vbd caught np(ballnn) det the nn ball figure 2: simple lexicalized parse tree.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
criminative models described in (magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997), <papid> W97-0301 </papid>the lexicalized pcfg model sin (collins, 1999), the generative model in (char niak, 2000), <papid> A00-2018 </papid>the lexicalized tag extractor in (xia, 1999) and the stochastic lexicalized tag model sin (chiang, 2000; <papid> P00-1058 </papid>sarkar, 2001; <papid> N01-1023 </papid>chen and vijay shanker, 2000).</citsent>
<aftsection>
<nextsent>inducing lexicalized structure based on heads has two-pronged effect: it notonly allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies?
</nextsent>
<nextsent>both syntactic and lexical will be included in the model itself.
</nextsent>
<nextsent>for example, in figure 2, the nonterminal np(boynn) is dependent on vp(caughtvbd) and not the other way around.
</nextsent>
<nextsent>2.2 other tree transformations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3530">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents.
</prevsent>
<prevsent>in fact, nearly identical head-lexicalizations were used in the dis s(caughtvbd) np(boynn) det the nn boy advp(alsorb) rb also vp(caughtvbd) vbd caught np(ballnn) det the nn ball figure 2: simple lexicalized parse tree.
</prevsent>
</prevsection>
<citsent citstr=" N01-1023 ">
criminative models described in (magerman, 1995; <papid> P95-1037 </papid>ratnaparkhi, 1997), <papid> W97-0301 </papid>the lexicalized pcfg model sin (collins, 1999), the generative model in (char niak, 2000), <papid> A00-2018 </papid>the lexicalized tag extractor in (xia, 1999) and the stochastic lexicalized tag model sin (chiang, 2000; <papid> P00-1058 </papid>sarkar, 2001; <papid> N01-1023 </papid>chen and vijay shanker, 2000).</citsent>
<aftsection>
<nextsent>inducing lexicalized structure based on heads has two-pronged effect: it notonly allows statistical parsers to be sensitive to lexical information by including this information in the probability models dependencies, but it also determines which of all possible dependencies?
</nextsent>
<nextsent>both syntactic and lexical will be included in the model itself.
</nextsent>
<nextsent>for example, in figure 2, the nonterminal np(boynn) is dependent on vp(caughtvbd) and not the other way around.
</nextsent>
<nextsent>2.2 other tree transformations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3538">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> rule-based augmentation.  </section>
<citcontext>
<prevsection>
<prevsent>since nearly all treebanks have complex nonterminal alphabets, we need way of concisely specifying classes of labels.
</prevsent>
<prevsent>unfortunately, this will necessarily vary somewhat across treebanks: all we can define that is truly treebank-independent is the pattern, which matches any label.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for penn tree bank ii style annotation (marcus et al, 1993), <papid> J93-2004 </papid>in which nonterminal symbol is category together with zero or more functional tags, we adopt the following scheme: the atomic pattern matches any label with category or functional tag a; more over, we define boolean operators ?, ?, and ?.</citsent>
<aftsection>
<nextsent>thus np ? adv matches npsbj but not npadv.1 3.3 summary.
</nextsent>
<nextsent>using the structure pattern language and the label pattern language together, one can fully encode the head/argument rules used by xia (which resemble (5) above), and the family of rule sets used by black, magerman, collins, ratnaparkhi, and others (which resemble (6) above).
</nextsent>
<nextsent>in collins?
</nextsent>
<nextsent>version of the head rules, np and pp require special treatment, but these can be encoded in our notation as well.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3540">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> unsupervised learning of augmentations.  </section>
<citcontext>
<prevsection>
<prevsent>the parsing model we use is based on the stochastic tree-insertion grammar (tig) model described 1note that unlike the noncommutative union operator ?, the disjunction operator ? has no preference for its first argument.
</prevsent>
<prevsent>by chiang (2000).<papid> P00-1058 </papid></prevsent>
</prevsection>
<citsent citstr=" J95-4002 ">
tig (schabes and waters, 1995)<papid> J95-4002 </papid>is weakly-context free restriction of tree adjoining grammar (joshi and schabes, 1997), in which tree fragments called elementary trees are combined by two composition operations, substitution and adjunction (see figure 3).</citsent>
<aftsection>
<nextsent>in tig there are certain restrictions on the adjunction operation.
</nextsent>
<nextsent>chiangs model adds third composition operation called sister-adjunction (see figure 3), borrowed from d-tree substitution grammar (rambow et al, 1995).<papid> P95-1021 </papid>2 there is an important distinction between derived trees and derivation trees (see figure 3).</nextsent>
<nextsent>a derivation tree records the operations that are used to combine elementary trees into derived tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3541">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> unsupervised learning of augmentations.  </section>
<citcontext>
<prevsection>
<prevsent>tig (schabes and waters, 1995)<papid> J95-4002 </papid>is weakly-context free restriction of tree adjoining grammar (joshi and schabes, 1997), in which tree fragments called elementary trees are combined by two composition operations, substitution and adjunction (see figure 3).</prevsent>
<prevsent>in tig there are certain restrictions on the adjunction operation.</prevsent>
</prevsection>
<citsent citstr=" P95-1021 ">
chiangs model adds third composition operation called sister-adjunction (see figure 3), borrowed from d-tree substitution grammar (rambow et al, 1995).<papid> P95-1021 </papid>2 there is an important distinction between derived trees and derivation trees (see figure 3).</citsent>
<aftsection>
<nextsent>a derivation tree records the operations that are used to combine elementary trees into derived tree.
</nextsent>
<nextsent>thus there is many-to-one relationship between derivation trees and derived trees: every derivation tree specifies derived tree, but derived tree can be the result of several different derivations.the model can be trained directly on tig derivations if they are available, but corpora like the penn treebank have only derived trees.
</nextsent>
<nextsent>just as collins uses rules to identify heads and arguments and thereby lexicalize trees, chiang uses nearly the same rules to reconstruct derivations: each training example is broken into elementary trees, with each head child remaining attached to its parent, each argument broken into substitution node and an initial root, and each adjunct broken off as modifier auxiliary tree.
</nextsent>
<nextsent>however, in this experiment we view the derived trees in the treebank as incomplete data, and try to reconstruct the derivations (the complete data) using the inside-outside algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3542">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> unsupervised learning of augmentations.  </section>
<citcontext>
<prevsection>
<prevsent>there are several several backoff levels for each parameter class that are combined by deleted interpolation.
</prevsent>
<prevsent>let1, 2 and 3 be functions from full history contexts to less specific contexts at levels 1, 2 and 3, respectively, for some parameter class with three backoff levels (with level 1 using the most specific contexts).
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
smoothed estimates for parameters in this class are computed as follows: = 1e1 + (1 ? 1)(2e2 + (1 ? 2)e3) where ei is the estimate of p(x | i(y)) for some future context x, and the are computed by the formula found in (bikel et al, 1997), <papid> A97-1029 </papid>modified to use the multiplicative constant 5 found in the similar formula of (collins, 1999): = ( 1 ? di1 di ) ( 1 1 + 5ui/di ) (7) where di is the number of occurrences in training of the context i(y) (and d0 = 0), and ui is the number of unique outcomes for that context seen in training.</citsent>
<aftsection>
<nextsent>there are several ways one might incorporate this smoothing into the reestimation process, and wechose to depart as little as possible from the original smoothing method: in the e-step, we use the smoothed model, and after the m-step, we use the original formula (7) to recompute the smoothing weights based on the new counts computed fromthe e-step.
</nextsent>
<nextsent>while simple, this approach has two important consequences.
</nextsent>
<nextsent>first, since the formula forthe smoothing weights intentionally does not maximize the likelihood of the training data, each iteration of reestimation is not guaranteed to increase the 87.3 87.35 87.4 87.45 87.5 87.55 87.6 0 5 10 15 20f ea su reiteration figure 4: english, starting with full rule set likelihood of the training data.
</nextsent>
<nextsent>second, reestimation tends to increase the size of the model in memory, since smoothing gives nonzero expected counts tomany events which were unseen in training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3543">
<title id=" C02-1126.xml">recovering latent information in treebanks </title>
<section> unsupervised learning of augmentations.  </section>
<citcontext>
<prevsection>
<prevsent>the argument rules, however, were not changed.
</prevsent>
<prevsent>this rule set is supposed to represent the kind of rule set that someone with basic familiarity with english syntax might write down in few minutes.
</prevsent>
</prevsection>
<citsent citstr=" W00-1201 ">
the reestimated models seemed to improve on this simplified rule set when parsing section 00 (see figure 5); however, when we compared the 30th reestimated model with the initial model on section 23 (see figure 7), there was no improvement.the third experiment was on the chinese tree bank, starting with the same head rules used in(bikel and chiang, 2000).<papid> W00-1201 </papid></citsent>
<aftsection>
<nextsent>these rules were originally written by xia for grammar development, and although we have modified them for parsing, they have not received as much fine-tuning as the english rules have.
</nextsent>
<nextsent>we trained the model on sections 001?
</nextsent>
<nextsent>270 of the penn chinese treebank, and reestimated it on the same data, testing it at each iteration on sections 301325 (figure 6).
</nextsent>
<nextsent>we selected the 38th reestimated model for comparison with the initial model on sections 271300 (figure 7).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3545">
<title id=" C04-1035.xml">classifying ellipsis in dialogue a machine learning approach </title>
<section> corpus study.  </section>
<citcontext>
<prevsection>
<prevsent>(6) larna: were gonna find poison apple and know where that one is. charlotte: where?
</prevsent>
<prevsent>[kd1, 2371] 2.3 reliability.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
to evaluate the reliability of the annotation, we use the kappa coefficient (k) (carletta, 1996), <papid> J96-2004 </papid>which measures pairwise agreement between aset of coders making category judgements, correcting for expected chance agreement.</citsent>
<aftsection>
<nextsent>2 the agreement on the coding of the first sample of sluices was moderate (k = 52).3 there were important differences amongst sluice classes: the lowest agreement was on the annotation for why (k = 29), what (k = 32) and how (k = 32), which suggests that these categories are highly ambiguous.
</nextsent>
<nextsent>examination of the coincidence matrices shows that the largest confusions were between reprise and clarification in the case of what, and between direct and reprise for why and how.on the other hand, the agreement on classifying who was substantially higher (k = 71), with some disagreements between direct and reprise.agreement on the annotation of the 2nd sample was considerably higher although still not entirely convincing (k = 61).
</nextsent>
<nextsent>overall agreement was improved in all classes, except for2k = (a)p (e)/1p (e), where p(a) is the proportion of actual agreements and p(e) is the proportion of expected agreement by chance, which depends on the number of relative frequencies of the categories under test.
</nextsent>
<nextsent>the denominator is the total proportion less the proportion of chance expectation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3546">
<title id=" C02-1115.xml">a maximum entropy based word sense disambiguation system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of wsd consists in assigning the correct sense to words using an electronic dictionary as the source of word de ni tions.
</prevsent>
<prevsent>this is hard problem that is receivinga great deal of attention from the research com munity.currently, there are two main methodological approaches in this research area: knowledge based methods and corpus-based methods.
</prevsent>
</prevsection>
<citsent citstr=" N01-1011 ">
the former approach relies on previously acquired linguistic knowledge, and the latter uses techniques from statistics and machine learning to induce models of language usage from large samples of text (pedersen, 2001).<papid> N01-1011 </papid></citsent>
<aftsection>
<nextsent>learning can be supervised or unsupervised.
</nextsent>
<nextsent>with supervised  this paper has been partially supported by the spanish government (cicyt) under project number tic2000-0664-c02-02.
</nextsent>
<nextsent>learning, the actual status (here, sense label) for each piece of data in the training example is known, whereas with unsupervised learning the classi cation of the data in the training example is not known (manning and schutze, 1999).at senseval-2, researchers showed the latest contributions to wsd.
</nextsent>
<nextsent>some supervised systems competed in the spanish lexical sample task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3547">
<title id=" C02-1115.xml">a maximum entropy based word sense disambiguation system </title>
<section> description of features.  </section>
<citcontext>
<prevsection>
<prevsent>the new function drastically reduces the number of features, with minimum of degradation in evaluation results.
</prevsent>
<prevsent>at the same time, new features can be incorporated into the learning process with minimum impact on eciency.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
the set of features de ned for the training of the system is described in gure 1, and is based on the feature selection made by ng and lee (1996)<papid> P96-1006 </papid>and escudero et al (2000).</citsent>
<aftsection>
<nextsent>features are automatically de ned as explained before and depend on the data in the training corpus.
</nextsent>
<nextsent>these features are based on words, collocations, and pos tags in the local context.
</nextsent>
<nextsent>both \relaxed  and \non-relaxed  functions are used.
</nextsent>
<nextsent>figure 1: list of types of features  0: ambiguous-word shape  : words at positions 1, 2, 3  : pos-tags of words at positions 1, 2, 3  : lemmas of collocations at positions ( 2; 1), ( 1;+1), (+1;+2)  c: collocations at positions ( 2; 1), ( 1;+1), (+1;+2) km: lemmas of nouns at any position in context, occurring at least m% times with sense  : grammatical relation of the ambiguous word  : the word that the ambiguous word depends on  l: lemmas of content-words at positions 1, 2, 3 (\relaxed  de nition)  : content-words at positions 1, 2, 3 (\relaxed  de nition)  s, b, c, p, and : \relaxed  versions actually, each item in gure 1 groups several sets of features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3549">
<title id=" C02-1115.xml">a maximum entropy based word sense disambiguation system </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>the results obtained by these systems show that selecting best feature sets guarantees the success of the disambiguation method.
</prevsent>
<prevsent>as we work to improve the me method with deeper analysis of the feature selection strategy, we are also working to develop cooperative strategy between several other methods as well, both knowledge-based and corpus-based.future research will incorporate domain information as an additional information source for the system.
</prevsent>
</prevsection>
<citsent citstr=" W00-0804 ">
wordnet domains (magnini and strapparava, 2000) <papid> W00-0804 </papid>is an enrichment of wordnet with domain labels.</citsent>
<aftsection>
<nextsent>these attribute swill be incorporated into the learning of the system in the same way that features were incorporated, as described above, except that domain disambiguation will be evaluated as well; that is, wordnet senses (synsets) will be substituted for domains labels, thereby reducing the number of possible classes into which contexts can be classi ed.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3550">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this cannot be considered in the deterministic parsing methods.we propose an algorithm based on tournament model, in which the relative preferences are directly modeled by one-on one games in step-ladder tournament.
</prevsent>
<prevsent>in an evaluation experiment with kyoto text corpus version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method.
</prevsent>
</prevsection>
<citsent citstr=" W06-2920 ">
the shared tasks of multi-lingual dependency parsing took place at conll-2006 (buchholz and marsi, 2006) <papid> W06-2920 </papid>and conll-2007 (nivre et al,2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>many language-independent parsing algorithms were proposed there.
</nextsent>
<nextsent>the algorithms need to adapt to various dependency structure constraints according to target languages: projective vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted.
</nextsent>
<nextsent>eisner (1996) <papid> C96-1058 </papid>proposed cky-like o(n3) algorithm.</nextsent>
<nextsent>yamada and matsumoto (2003) proposed shift-reduce like o(n2) deterministic algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3551">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this cannot be considered in the deterministic parsing methods.we propose an algorithm based on tournament model, in which the relative preferences are directly modeled by one-on one games in step-ladder tournament.
</prevsent>
<prevsent>in an evaluation experiment with kyoto text corpus version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
the shared tasks of multi-lingual dependency parsing took place at conll-2006 (buchholz and marsi, 2006) <papid> W06-2920 </papid>and conll-2007 (nivre et al,2007).<papid> D07-1096 </papid></citsent>
<aftsection>
<nextsent>many language-independent parsing algorithms were proposed there.
</nextsent>
<nextsent>the algorithms need to adapt to various dependency structure constraints according to target languages: projective vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted.
</nextsent>
<nextsent>eisner (1996) <papid> C96-1058 </papid>proposed cky-like o(n3) algorithm.</nextsent>
<nextsent>yamada and matsumoto (2003) proposed shift-reduce like o(n2) deterministic algorithm.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3552">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many language-independent parsing algorithms were proposed there.
</prevsent>
<prevsent>the algorithms need to adapt to various dependency structure constraints according to target languages: projective vs. non-projective, head-initial vs. head-final, and single-rooted vs. multi-rooted.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
eisner (1996) <papid> C96-1058 </papid>proposed cky-like o(n3) algorithm.</citsent>
<aftsection>
<nextsent>yamada and matsumoto (2003) proposed shift-reduce like o(n2) deterministic algorithm.
</nextsent>
<nextsent>nivre et al (2003), nivre et al (2004) also proposed shift-reduce-like ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3553">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yomanai.(doesnt read .)hon-wo(books)kare-wa(he)(b) he doesnt read books.?
</prevsent>
<prevsent>kare-wa(he) figure 1: examples of japanese sentences.o(n) deterministic algorithm for projective languages.
</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
the model is enhanced for non-projectivelanguages by nivre and nilsson (2005).<papid> P05-1013 </papid></citsent>
<aftsection>
<nextsent>mcdonald et al (2005) <papid> P05-1012 </papid>proposed method based on search of maximum spanning trees employing thechu-liu-edmonds algorithm (hereafter cle al gorithm?)</nextsent>
<nextsent>(chu and liu, 1965; edmonds, 1967).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3554">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kare-wa(he) figure 1: examples of japanese sentences.o(n) deterministic algorithm for projective languages.
</prevsent>
<prevsent>the model is enhanced for non-projectivelanguages by nivre and nilsson (2005).<papid> P05-1013 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
mcdonald et al (2005) <papid> P05-1012 </papid>proposed method based on search of maximum spanning trees employing thechu-liu-edmonds algorithm (hereafter cle al gorithm?)</citsent>
<aftsection>
<nextsent>(chu and liu, 1965; edmonds, 1967).
</nextsent>
<nextsent>most japanese dependency parsers are based on bunsetsu units, which are similar concept to english base phrases.
</nextsent>
<nextsent>the constraints in japanese dependency structure are stronger than those in other languages.
</nextsent>
<nextsent>japanese dependency structure shave the following constraints: head-final, single head, single-rooted, connected, acyclic and projective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3555">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>directly depends on yomanai?
</prevsent>
<prevsent>in (b) but not in (a).in dependency parsing of japanese, deterministic algorithms outperform probabilistic cky methods.
</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
kudo and matsumoto (2002) <papid> W02-2016 </papid>applied the 361cascaded chunking algorithm (hereafter cc algorithm?)</citsent>
<aftsection>
<nextsent>to japanese dependency parsing.
</nextsent>
<nextsent>ya madas method (yamada and matsumoto, 2003) employed similar algorithm.
</nextsent>
<nextsent>sassano (2004) <papid> C04-1002 </papid>proposed linear-order shift-reduce-like algorithm (hereafter sr algorithm?), which is similar tonivres algorithm (nivre, 2003).</nextsent>
<nextsent>these deterministic algorithms are biased to select nearer candidate heads since they examine the candidates sequentially, and once they find plausible one they never consider further candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3556">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to japanese dependency parsing.
</prevsent>
<prevsent>ya madas method (yamada and matsumoto, 2003) employed similar algorithm.
</prevsent>
</prevsection>
<citsent citstr=" C04-1002 ">
sassano (2004) <papid> C04-1002 </papid>proposed linear-order shift-reduce-like algorithm (hereafter sr algorithm?), which is similar tonivres algorithm (nivre, 2003).</citsent>
<aftsection>
<nextsent>these deterministic algorithms are biased to select nearer candidate heads since they examine the candidates sequentially, and once they find plausible one they never consider further candidates.
</nextsent>
<nextsent>we experimented the cle algorithm with japanese dependency parsing, and found that the cle algorithm is comparable to or in some cases poorer than the deterministic algorithms in our experiments.
</nextsent>
<nextsent>actually, the cle algorithm is not suitable for some of the constraints in japanese dependency structures: head-final and projective.
</nextsent>
<nextsent>first, head-final means that dependency relation always goes from left to right.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3604">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>setsu improves accuracy.
</prevsent>
<prevsent>in kyoto text corpus version 4.0, coordination and apposition are annotated with different types of dependency relation.we did not use this information in parsing.
</prevsent>
</prevsection>
<citsent citstr=" D07-1064 ">
a simple extension is to include those dependency types.another extension is to employ coordination analyzer as separate process as proposed by shimbo and hara (2007).<papid> D07-1064 </papid></citsent>
<aftsection>
<nextsent>incorporating co-occurrence information will also improve the parsing accuracy.
</nextsent>
<nextsent>one usage ofsuch information is verb-noun co-occurrence information that would represent selectional preference for case-frame information.
</nextsent>
<nextsent>abekawa and okumura (2006) <papid> P06-1105 </papid>proposed reranking method of k-best dependency analyzer outputs using cooccurrence information.</nextsent>
<nextsent>we have already developed method to output k-best dependency trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3605">
<title id=" C08-1046.xml">japanese dependency parsing using a tournament model </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>incorporating co-occurrence information will also improve the parsing accuracy.
</prevsent>
<prevsent>one usage ofsuch information is verb-noun co-occurrence information that would represent selectional preference for case-frame information.
</prevsent>
</prevsection>
<citsent citstr=" P06-1105 ">
abekawa and okumura (2006) <papid> P06-1105 </papid>proposed reranking method of k-best dependency analyzer outputs using cooccurrence information.</citsent>
<aftsection>
<nextsent>we have already developed method to output k-best dependency trees.
</nextsent>
<nextsent>one of our future works is to test the reranking method using co-occurrence information on the best dependency trees.
</nextsent>
<nextsent>multilingual parsing is another goal.
</nextsent>
<nextsent>japanese is strict head-final language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3606">
<title id=" C04-1092.xml">automatic extraction of paraphrastic phrases from medium size corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in general, the left part of pattern is easier to acquire than the right part and some heuristics can be applied to infer the right boundary from the left one.
</prevsent>
<prevsent>the same method can be applied for argument acquisition: each argument can be acquired independently from the others since the argument structure of predicate in context is rarely complete.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
collins and singer (1999) <papid> W99-0613 </papid>demonstrate how two classifiers operating on disjoint features sets recognize named entities with very little supervision.</citsent>
<aftsection>
<nextsent>the method is interesting in that the analyst only needs to provide some seed examples to the system in order to learn relevant information.
</nextsent>
<nextsent>however, these classifiers must be made interactive in order not to diverge from the expected result, since each error is transmitted and amplified by subsequent processing stages.
</nextsent>
<nextsent>contrary to this approach, partially reproduced by duclaye et al.
</nextsent>
<nextsent>(2003) for paraphrase learning, we prefer slightly supervised method with clear interaction steps with the analyst during the acquisition process, to ensure the solution is converging.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3607">
<title id=" C04-1092.xml">automatic extraction of paraphrastic phrases from medium size corpora </title>
<section> similarity measures.  </section>
<citcontext>
<prevsection>
<prevsent>seed pattern selection automatic step paraphrase acquisition syntactic expansion semantic expansion semantic net corpus validation figure 1: outline of the acquisition process end-user input interaction with the end-user
</prevsent>
<prevsent>several studies have recently proposed measures to calculate the semantic proximity between words.
</prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
different measures have been proposed, which are not easy to evaluate (see (lin and pantel, 2002) <papid> C02-1144 </papid>for proposals).</citsent>
<aftsection>
<nextsent>the methods proposed so far are automatic or manual and generally imply the evaluation of word clusters in different contexts (a word cluster is close to another one if the words it contains are interchangeable in some linguistic contexts).
</nextsent>
<nextsent>budanitsky and hirst (2001) present the evaluation of 5 similarity measures based on the structure of wordnet.
</nextsent>
<nextsent>all the algorithms they examine are based on the hypernym hyponym relation which structures the classification of clusters inside wordnet (the synsets).
</nextsent>
<nextsent>they sometimes obtain unclear conclusions about the reason of the performances of the different algorithms (for example, comparing jiang and conraths measure (1997) with lins one (1998): it remains unclear, however, just why it performed so much better than lins measure, which is but different arithmetic combination of the same terms?).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3608">
<title id=" C04-1108.xml">improving chronological sentence ordering by precedence relation </title>
<section> sentence ordering.  </section>
<citcontext>
<prevsection>
<prevsent>they proposed two naive sentence ordering techniques such as majority ordering (examines most frequent orders in the original documents) and chronological ordering (orders sentence by the publication date).
</prevsent>
<prevsent>showing that using naive ordering algorithms does not produce satisfactory orderings, barzilay et al also investigates through experiments with humans, how to identify patterns of orderings that can improve the algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1069 ">
based on the experiments, they propose another algorithm that utilizes chronological ordering with topical segmentation to separate sentences referring to topic from ones referring to another.lapata (lapata, 2003) <papid> P03-1069 </papid>proposes another approach to information ordering based on probabilistic model that assumes the probability ofany given sentence is determined by its adjacent sentence and learns constraints on sentence order from corpus of domain specifictexts.</citsent>
<aftsection>
<nextsent>lapata estimates transitional probability between sentences by some attributes such as verbs (precedence relationships of verbs in the corpus), nouns (entity-based coherence by keeping track of the nouns) and dependencies (structure of sentences).
</nextsent>
<nextsent>2.2 improving chronological ordering.
</nextsent>
<nextsent>against the background of these studies, we propose the use of antecedence sentences to arrange sentences.
</nextsent>
<nextsent>let us consider an example shown in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3609">
<title id=" C04-1108.xml">improving chronological sentence ordering by precedence relation </title>
<section> sentence ordering.  </section>
<citcontext>
<prevsection>
<prevsent>hence, publication date (time) of each article turns out to be good estimator of resemblance relation (i.e., we observe trend or series of relevant events in time period), contiguity in time, and cause-effect relation (i.e., an event occurs as result of previous events).
</prevsent>
<prevsent>although resolving temporal expressions in sentences (e.g., yesterday, the next year, etc.)
</prevsent>
</prevsection>
<citsent citstr=" P00-1010 ">
(mani and wilson,2000; <papid> P00-1010 </papid>mani et al, 2003) <papid> N03-2019 </papid>may give more precise estimation of these relations, it is not an easy task.</citsent>
<aftsection>
<nextsent>for this reason we order sentences of each segment (cluster) by the chronological . .  . c . article #1 article #2 article #3 chronological order figure 3: background idea of ordering refinement by precedence relation.
</nextsent>
<nextsent>order, assigning time stamp for each sentence by its publication date (i.e., the date when the article was written).
</nextsent>
<nextsent>when there are sentences having the same time stamp, we elaborate the order on the basis of sentence position and sentence connectivity.
</nextsent>
<nextsent>we restore an original ordering if two sentences have the same time stamp and belong to the same article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3610">
<title id=" C04-1108.xml">improving chronological sentence ordering by precedence relation </title>
<section> sentence ordering.  </section>
<citcontext>
<prevsection>
<prevsent>hence, publication date (time) of each article turns out to be good estimator of resemblance relation (i.e., we observe trend or series of relevant events in time period), contiguity in time, and cause-effect relation (i.e., an event occurs as result of previous events).
</prevsent>
<prevsent>although resolving temporal expressions in sentences (e.g., yesterday, the next year, etc.)
</prevsent>
</prevsection>
<citsent citstr=" N03-2019 ">
(mani and wilson,2000; <papid> P00-1010 </papid>mani et al, 2003) <papid> N03-2019 </papid>may give more precise estimation of these relations, it is not an easy task.</citsent>
<aftsection>
<nextsent>for this reason we order sentences of each segment (cluster) by the chronological . .  . c . article #1 article #2 article #3 chronological order figure 3: background idea of ordering refinement by precedence relation.
</nextsent>
<nextsent>order, assigning time stamp for each sentence by its publication date (i.e., the date when the article was written).
</nextsent>
<nextsent>when there are sentences having the same time stamp, we elaborate the order on the basis of sentence position and sentence connectivity.
</nextsent>
<nextsent>we restore an original ordering if two sentences have the same time stamp and belong to the same article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3611">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we consider only the noun phrases referring to biomedical entities.
</prevsent>
<prevsent>the model reaches state-of-the art performance: 5669% precision and 54-67% recall on coref erent cases, and reasonable performance on different classes of associative cases.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
inspired by ge et al (1998) <papid> W98-1119 </papid>probabilistic model for pronoun resolution, we have developed model for resolution of non-pronominal anaphora in biomedical texts.</citsent>
<aftsection>
<nextsent>the probabilistic model results from simple decomposition process applied to conditional probability equation that involves several parameters (features).
</nextsent>
<nextsent>the decomposition makes use of bayes?
</nextsent>
<nextsent>theorem and independence assumptions,and aims to decrease the impact of data sparseness on the model, so that even small training corpora can be viable.
</nextsent>
<nextsent>the decomposed model can be understood as more sophisticated version of the naive-bayes algorithm, since we consider the dependence among some of the features instead of full independence as in naive bayes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3612">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>from this equation, we then selectively apply the chain rule to both numerator and denominator until we get to the following equation: = (c) (f |c) (f |c, a ) (d, dm|c, a , a ) (sr|c, a , a , d, dm) (bm, gp|c, a , a , d, dm, sr) (num|c, a , a ,d, dm, sr, bm, gp) (hm, hmm,mm|c, a ,f , d, dm, sr, bm, gp, num) (f ) (f |f ) (d, dm|f , a ) (sr|f , a , d, dm) (bm, gp|f , a , d, dm, sr) (num|f , a , d, dm, sr, bm, gp) (hm, hmm,mm|f , a , d, dm, sr, bm, gp, num) (2) following the decomposition, we eliminate some of the dependencies among the features that we consider unnecessary 3 . we consider that the.
</prevsent>
<prevsent>lexical features hm, hmm, andmm are not dependent on distance or dm, nor on sr, gp or num, so: (hm, hmm,mm|c, a , a , d, dm, sr, bm, gp, num) ? (hm, hmm,mm|c, a , a , bm) we model num as independent from d, dm, sr, bm, and gp, so: (num|c, a , a , d, dm, sr, bm, gp) ? (num|c, a , a ) we also assume the semantic features bm, and gp as independent from all features but c: (bm, gp|c, a , a , d, dm, sr) ? (bm, gp|c) we also assume sr to be independent of a and a : (sr|c, a , a , d, dm) ? (sr|c, d, dm) the final equation then becomes: (c|f , a , hm, hmm,mm,num, sr, bm, gp, d, dm) = (c) (f |c) (f |c, a ) (d, dm|c, a , a ) (sr|c, d, dm) (bm, gp|c) (num|c, a , a ) (hm, hmm,mm|c, a , a , bm) (f ) (f |f ) (d, dm|f , a ) (sr|d, dm) (bm, gp) (num|f , a ) (hm, hmm,mm|f , a , bm) (3)
</prevsent>
</prevsection>
<citsent citstr=" W05-1306 ">
there are very few biomedical corpora annotated with anaphora information, and all of them are built from paper abstracts (cohen et al, 2005), <papid> W05-1306 </papid>instead of full papers.</citsent>
<aftsection>
<nextsent>as anaphora is phenomenon that develops through text, we believe that short abstracts are not the best source to work with and decided to concentrate on full papers.
</nextsent>
<nextsent>in order to collect the statistics to train our model, we have manually annotated anaphoric relations between biomedical entities in 5 full-text articles (approx.
</nextsent>
<nextsent>33,300 words) 4 , which are part of the drosophila molecular biology literature.
</nextsent>
<nextsent>the corpus and annotation process are described in(gasperin et al, 2007).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3613">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>table 1: feature set before annotating anaphora, we have preprocessed the articles in order to (1) tag gene names,(2) identify all nps, and (3) classify the nps according to their domain type, which we call bio type.
</prevsent>
<prevsent>to tag all gene names in the corpus, we have applied the gene name recogniser developed by vlachos et al (2006).
</prevsent>
</prevsection>
<citsent citstr=" P06-4020 ">
to identify all nps, their sub constituents (head, modifiers, determiner) and broader pre- and post-modification patterns, we have used the rasp parser (briscoe et al, 2006).<papid> P06-4020 </papid></citsent>
<aftsection>
<nextsent>to classify the nps according to their type in biomedical terms, we have adopted the sequence ontology (so) 5 (eilbeck and lewis, 2004).
</nextsent>
<nextsent>so is fine-grained ontology, which contains the names of practically all entities that participate in genomic sequences, besides the relations among these entities (e.g. is-a, part-of, derived-from re lations).
</nextsent>
<nextsent>we derived from so seven bio types to be used to classify the entities in the text, namely: gene?, gene product?, part of gene?, part of product?, gene variant?, gene subtype?, and gene supertype?.
</nextsent>
<nextsent>we also created the biotype other-bio?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3614">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>the negative samples outnumber considerably the number of positive samples (annotated cases).
</prevsent>
<prevsent>table 2 presents the distribution of the cases among the classes of anaphora relations.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
we note that around 80% of the definite nps are anaphoric in our corpus, instead of the 50% presented in (vieira and poesio, 2000) <papid> J00-4003 </papid>for newspaper texts.</citsent>
<aftsection>
<nextsent>nearly all demonstrative nps (93%) are anaphoric.
</nextsent>
<nextsent>more than 70% of the proper name stake part in coreference relations, as they inherently refer to specific named entity, but nevertheless 5% of them take part in associative biotype relations, due to the fact that gene and the protein it synthesizes usually share the same name.
</nextsent>
<nextsent>44% of quantified nps take part in set-member relations,as they usually refer to more than one entity.
</nextsent>
<nextsent>finally 51% of indefinite nps are discourse new.to balance the ratio between positive and negative training samples, we have clustered the negative samples and kept only portion of each cluster, proportional to its size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3615">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>this way, small clusters (with less than 10 members), which 260 class/nps pn defnp demnp indefnp quantnp other np total co referent 689 429 70 40 54 396 1678 biotype 43 102 3 8 4 114 274 set-member 151 126 26 14 68 158 543 discourse new 63 107 0 72 38 156 436 none 873,731 table 2: training instances, according to anaphoric class and to np form are likely to represent noisy samples (similar to positive ones), are eliminated, and bigger clusters are shrunk; however the shape of the distribution of the negative samples is preserved.
</prevsent>
<prevsent>for example, our biggest cluster (feature values are: a =pn?, a =pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?,sr=none?, d=16 ?, dm=50 ?)
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al, 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>we compare the performance of both strategies in section 5.1 and show that ours is more effective.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>given the small size of our corpus, we did not holdout test set.
</nextsent>
<nextsent>instead, we have measured the average performance achieved by the model in 10 fold cross-validation setting, using the whole of the annotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3617">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>this way, small clusters (with less than 10 members), which 260 class/nps pn defnp demnp indefnp quantnp other np total co referent 689 429 70 40 54 396 1678 biotype 43 102 3 8 4 114 274 set-member 151 126 26 14 68 158 543 discourse new 63 107 0 72 38 156 436 none 873,731 table 2: training instances, according to anaphoric class and to np form are likely to represent noisy samples (similar to positive ones), are eliminated, and bigger clusters are shrunk; however the shape of the distribution of the negative samples is preserved.
</prevsent>
<prevsent>for example, our biggest cluster (feature values are: a =pn?, a =pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?,sr=none?, d=16 ?, dm=50 ?)
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al, 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>we compare the performance of both strategies in section 5.1 and show that ours is more effective.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>given the small size of our corpus, we did not holdout test set.
</nextsent>
<nextsent>instead, we have measured the average performance achieved by the model in 10 fold cross-validation setting, using the whole of the annotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3619">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> training.  </section>
<citcontext>
<prevsection>
<prevsent>this way, small clusters (with less than 10 members), which 260 class/nps pn defnp demnp indefnp quantnp other np total co referent 689 429 70 40 54 396 1678 biotype 43 102 3 8 4 114 274 set-member 151 126 26 14 68 158 543 discourse new 63 107 0 72 38 156 436 none 873,731 table 2: training instances, according to anaphoric class and to np form are likely to represent noisy samples (similar to positive ones), are eliminated, and bigger clusters are shrunk; however the shape of the distribution of the negative samples is preserved.
</prevsent>
<prevsent>for example, our biggest cluster (feature values are: a =pn?, a =pn?, hm=no?, hmm=no?, mm=no?, bm=yes?, gp=yes?, num=yes?,sr=none?, d=16 ?, dm=50 ?)
</prevsent>
</prevsection>
<citsent citstr=" W02-1040 ">
with 33,998 instances is reduced to 3,399 ? still considerably more numerous than any positive sample.other works have used different strategy to reduce the imbalance between positive and negative samples (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>strube et al, 2002), <papid> W02-1040 </papid>where only samples composed by negative antecedent that is closer than the annotated one are considered.</citsent>
<aftsection>
<nextsent>we compare the performance of both strategies in section 5.1 and show that ours is more effective.
</nextsent>
<nextsent>the higher the number of negative samples, the higher the precision of the resolution, but the lower the recall.
</nextsent>
<nextsent>given the small size of our corpus, we did not holdout test set.
</nextsent>
<nextsent>instead, we have measured the average performance achieved by the model in 10 fold cross-validation setting, using the whole of the annotated corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3620">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we believe that the lower overall performance for associative cases is due to the difficulty of selecting features that capture all aspects involved in associative relations.
</prevsent>
<prevsent>our set of features is clearly failing to cover some of these aspects, and deeper feature study should be the best way to boost the scores.
</prevsent>
</prevsection>
<citsent citstr=" W97-1301 ">
however, despite lower, these performance perfect relaxed class r p f co referent 56.3 54.7 55.5 69.4 67.4 68.3 biotype 28.5 35.0 31.4 31.2 37.9 34.2 set-member 35.4 38.2 36.7 38.5 41.5 40.0 discourse new 44.3 53.4 48.4 44.3 53.4 48.4 table 3: performance of the probabilistic model 261scores are higher that the ones from previous approaches for newspaper texts, which used for instance the wordnet (poesio et al, 1997) <papid> W97-1301 </papid>or the internet (bunescu, 2003) as source of semantic knowledge.</citsent>
<aftsection>
<nextsent>we have analysed our features and observed that the string matching features hm, hmm, andmm, the number agreement feature num, bio type matching bm, and distance in markables dmare the core features and achieve reasonable performance.
</nextsent>
<nextsent>however, a and aplay an important role, they increase the precision of corefer ent cases and boost considerably the performance of the associative ones.
</nextsent>
<nextsent>this is due to the different distribution of np types across the relations as shown is table 2.
</nextsent>
<nextsent>the remaining features focused on specific cases: gp improved biotype recall, by boosting the probability of biotype relation when anaphor or candidate had specific biotypes; and sr improved precision and recall of co referent cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3622">
<title id=" C08-1033.xml">statistical anaphora resolution in biomedical texts </title>
<section> results.  </section>
<citcontext>
<prevsection>
<prevsent>we reckon that the considerable drop on recall numbers for the associative cases would make the system less viable, while the low precision for discourse new cases shows that many anaphoric cases are left unresolved.
</prevsent>
<prevsent>we view our strategy, based on the clustering of negative samples and consecutive cluster size reduction, to be more effective at proportionally eliminating negative samples that are less frequent and that are more likely to be noisy.we compare our model to rule-based base line system that we have previously developed.
</prevsent>
</prevsection>
<citsent citstr=" W06-3316 ">
the baseline system (gasperin, 2006) <papid> W06-3316 </papid>for each anaphoric expression: 1) selects as co referent antecedent the closest preceding np that has the same head noun, same biotype and agrees in number with the anaphor, and 2) selects as associative antecedent the closest preceding np that has the same head noun, same biotype but disagrees in number with the anaphor, or that has the same head noun or modifier matching the anaphor head (orvice-versa) or matching modifiers, agrees in number but has different biotypes.</citsent>
<aftsection>
<nextsent>the baseline system does not distinguish between different typesof associative cases, although it aims to cover bio type and set-member cases.
</nextsent>
<nextsent>if no antecedent that matches these criteria is found, the anaphor is considered discourse new.
</nextsent>
<nextsent>column baseline?
</nextsent>
<nextsent>on table 5 shows the performance scores for the base line system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3630">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>location normalization is special application of word sense disambiguation (wsd).
</prevsent>
<prevsent>there is considerable research on wsd.
</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
knowledge-based work, such as (hirst, 1987; mcroy, 1992; <papid> J92-1001 </papid>ng and lee, 1996) <papid> P96-1006 </papid>used hand-coded rules or supervised machine learning based on annotated corpus to perform wsd.</citsent>
<aftsection>
<nextsent>recent work emphasizes corpus-based unsupervised approach (dagon and itai, 1994; yarowsky, 1992; <papid> C92-2070 </papid>yarowsky, 1995) <papid> P95-1026 </papid>that avoids the need for costly truthed training data.</nextsent>
<nextsent>location normalization is different from general wsd in that the selection restriction often used for wsd in many cases is not sufficient to distinguish the correct sense from the other candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3631">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>location normalization is special application of word sense disambiguation (wsd).
</prevsent>
<prevsent>there is considerable research on wsd.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
knowledge-based work, such as (hirst, 1987; mcroy, 1992; <papid> J92-1001 </papid>ng and lee, 1996) <papid> P96-1006 </papid>used hand-coded rules or supervised machine learning based on annotated corpus to perform wsd.</citsent>
<aftsection>
<nextsent>recent work emphasizes corpus-based unsupervised approach (dagon and itai, 1994; yarowsky, 1992; <papid> C92-2070 </papid>yarowsky, 1995) <papid> P95-1026 </papid>that avoids the need for costly truthed training data.</nextsent>
<nextsent>location normalization is different from general wsd in that the selection restriction often used for wsd in many cases is not sufficient to distinguish the correct sense from the other candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3632">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is considerable research on wsd.
</prevsent>
<prevsent>knowledge-based work, such as (hirst, 1987; mcroy, 1992; <papid> J92-1001 </papid>ng and lee, 1996) <papid> P96-1006 </papid>used hand-coded rules or supervised machine learning based on annotated corpus to perform wsd.</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
recent work emphasizes corpus-based unsupervised approach (dagon and itai, 1994; yarowsky, 1992; <papid> C92-2070 </papid>yarowsky, 1995) <papid> P95-1026 </papid>that avoids the need for costly truthed training data.</citsent>
<aftsection>
<nextsent>location normalization is different from general wsd in that the selection restriction often used for wsd in many cases is not sufficient to distinguish the correct sense from the other candidates.
</nextsent>
<nextsent>for example, in the sentence the white house is located in washington?, the selection restriction from the collocation located in?
</nextsent>
<nextsent>can only determine that washington?
</nextsent>
<nextsent>should be location name, but is not sufficient to decide the actual sense of this location.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3633">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is considerable research on wsd.
</prevsent>
<prevsent>knowledge-based work, such as (hirst, 1987; mcroy, 1992; <papid> J92-1001 </papid>ng and lee, 1996) <papid> P96-1006 </papid>used hand-coded rules or supervised machine learning based on annotated corpus to perform wsd.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
recent work emphasizes corpus-based unsupervised approach (dagon and itai, 1994; yarowsky, 1992; <papid> C92-2070 </papid>yarowsky, 1995) <papid> P95-1026 </papid>that avoids the need for costly truthed training data.</citsent>
<aftsection>
<nextsent>location normalization is different from general wsd in that the selection restriction often used for wsd in many cases is not sufficient to distinguish the correct sense from the other candidates.
</nextsent>
<nextsent>for example, in the sentence the white house is located in washington?, the selection restriction from the collocation located in?
</nextsent>
<nextsent>can only determine that washington?
</nextsent>
<nextsent>should be location name, but is not sufficient to decide the actual sense of this location.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3634">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> lexical grammar processing in.  </section>
<citcontext>
<prevsection>
<prevsent>sample profile avms involving the reference of locations are illustrated below.
</prevsent>
<prevsent> person profile 001  :: name: julian werner hill position: research chemist age: 91 birth-place:  locationprofile100  affiliation: dupont co. education: mit  location profile 100  :: name: st. louis state: missouri country: united states of america zipcode: 63101 lattitude : 90.191313 longitude: 38.634616 related_profiles:  person profile 001  several other applications such as question answering and classifying documents by location areas can also be enabled through locnz.
</prevsent>
</prevsection>
<citsent citstr=" M98-1015 ">
local context named entity tagging systems (krupka and hausman, 1998; <papid> M98-1015 </papid>srihari et al, 2000) <papid> A00-1034 </papid>attempt to tag information such as names of people, organizations, locations, time, etc. in running text.</citsent>
<aftsection>
<nextsent>in infoxtract, we combine maximum entropy model (maxent) and hidden markov model for ne tagging (shrihari et al,, 2000).
</nextsent>
<nextsent>the maximum entropy models incorporate local contextual evidence in handling ambiguity of information from location gazetteer.
</nextsent>
<nextsent>in the tipster location gazetteer used by infoxtract, there are lot of common words, such as i, a, june, friendship , etc. also, there is large overlap between person names and location names, such as clinton, jordan, etc. using maxent, systems learn under what situation word is location name, but it is very difficult to determine the correct sense of an ambiguous location name.
</nextsent>
<nextsent>if word can represent city or state at the same time, such as new york or washington, it is difficult to decide if it refers to city or state.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3635">
<title id=" C02-1127.xml">location normalization for information extraction </title>
<section> lexical grammar processing in.  </section>
<citcontext>
<prevsection>
<prevsent>sample profile avms involving the reference of locations are illustrated below.
</prevsent>
<prevsent> person profile 001  :: name: julian werner hill position: research chemist age: 91 birth-place:  locationprofile100  affiliation: dupont co. education: mit  location profile 100  :: name: st. louis state: missouri country: united states of america zipcode: 63101 lattitude : 90.191313 longitude: 38.634616 related_profiles:  person profile 001  several other applications such as question answering and classifying documents by location areas can also be enabled through locnz.
</prevsent>
</prevsection>
<citsent citstr=" A00-1034 ">
local context named entity tagging systems (krupka and hausman, 1998; <papid> M98-1015 </papid>srihari et al, 2000) <papid> A00-1034 </papid>attempt to tag information such as names of people, organizations, locations, time, etc. in running text.</citsent>
<aftsection>
<nextsent>in infoxtract, we combine maximum entropy model (maxent) and hidden markov model for ne tagging (shrihari et al,, 2000).
</nextsent>
<nextsent>the maximum entropy models incorporate local contextual evidence in handling ambiguity of information from location gazetteer.
</nextsent>
<nextsent>in the tipster location gazetteer used by infoxtract, there are lot of common words, such as i, a, june, friendship , etc. also, there is large overlap between person names and location names, such as clinton, jordan, etc. using maxent, systems learn under what situation word is location name, but it is very difficult to determine the correct sense of an ambiguous location name.
</nextsent>
<nextsent>if word can represent city or state at the same time, such as new york or washington, it is difficult to decide if it refers to city or state.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3637">
<title id=" C04-1013.xml">partially distribution free learning of regular languages from positive samples </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we discuss how this is related to other learning paradigms.
</prevsent>
<prevsent>we then present simple learning algorithm for regular languages, and self-contained proof that it learns according to this partially distribution free criterion.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
regular languages, especially generated by deterministic finite state automata are widely usedin natural language processing, for various different tasks (mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>efficient learning algorithms, that have some guarantees of correctness, would clearly be useful.
</nextsent>
<nextsent>existing algorithms for learning deterministic automata, such as (carrasco and oncina, 1994) have only guarantees of identification in the limit (gold, 1967), generally considered not to be good guide to practical utility.
</nextsent>
<nextsent>unforunately the prospects for learning according to the more useful pac-learning criterion are poor after the well known result of (kearns and valiant, 1989).distribution-free learning criteria require algorithms to learn for every possible combination of concept and distribution.
</nextsent>
<nextsent>under this worst case analysis many simple concept classes are unlearnable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3638">
<title id=" C04-1013.xml">partially distribution free learning of regular languages from positive samples </title>
<section> appropriateness.  </section>
<citcontext>
<prevsection>
<prevsent>when dealing with regular languages, for example, though the class of languages defined by deterministic automata is thesame as that defined by non-deterministic languages, the same is not true for their stochastic variants.
</prevsent>
<prevsent>additionally, one can have exponential blow-ups in the number of states whendeterminizing automata.
</prevsent>
</prevsection>
<citsent citstr=" P99-1070 ">
similarly, with context free languages, (abney et al, 1999) <papid> P99-1070 </papid>showed that converting between two parametrisations of models for stochastic context free languages are equivalent but that there are blow-ups in both directions.it is interesting to compare this to the pac learning with simple distributions model (de nis, 2001).</citsent>
<aftsection>
<nextsent>there, the class of distributions is limited to single distribution derived from algorithmic complexity theory.
</nextsent>
<nextsent>there are number of reasons why this is not appropriate.first there is computational issue: since kolmogorov complexity is not computable, sampling from the distribution is not possible, though lower bound on the probabilities canbe defined.
</nextsent>
<nextsent>secondly, there are very large constants in the sample complexity polynomial.
</nextsent>
<nextsent>finally and most importantly, there is no reason to think that in the real world, samples will be drawn from this distribution; in some sense it is the easiest distribution to learn from since it dominates every other distribution up to multiplicative factor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3639">
<title id=" C02-2002.xml">dynamic lexical acquisition in chinese sentence analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(easy-to-carry) are not entries in our dictionary.
</prevsent>
<prevsent>instead of adding them to the dictionary, we use templates to recognize them online.
</prevsent>
</prevsection>
<citsent citstr=" W00-1207 ">
the template that combines two individual characters to form verb may include conditions such as: ? none of the characters is subsumed by longer word; ? the joint probability of the characters being independent words in text is low; ? the internal structure of the new word conforms to the word formation rules of chinese ? the component characters have similar behavior in existing words ? etc. the details can be found in wu &amp; jiang (2000).<papid> W00-1207 </papid></citsent>
<aftsection>
<nextsent>currently we have 10 such templates, which are capable of identifying nouns, verbs, adjectives and adverbs of various lengths.
</nextsent>
<nextsent>1.2.
</nextsent>
<nextsent>proposing grammatical attributes.
</nextsent>
<nextsent>pos and sub-categorization information is crucial for the success of sentence analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3640">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>can be phrase.
</prevsent>
<prevsent>therefore, the number of rules that the model can learn is exponential in sentence length unless strict heuristics are used, which may limit the models effectiveness.
</prevsent>
</prevsection>
<citsent citstr=" D07-1079 ">
many other models translate dis contiguous phrases, and the size of their extracted rule sets is such pervasive problem that it is recurring topic in the literature (chiang, 2007; deneefe et al, 2007; <papid> D07-1079 </papid>simard et al, 2005).<papid> H05-1095 </papid></citsent>
<aftsection>
<nextsent>most decoder implementations assume that all model rules and parameters are known in advance.
</nextsent>
<nextsent>with very large models, computing all rules and parameters can be very slow.
</nextsent>
<nextsent>this is bottleneck in experimental settings where we wish to explore many model variants, and therefore presents real impediment to full exploration of their potential.
</nextsent>
<nextsent>we present solution to this problem.to fully motivate the discussion, we give concrete example of very large model, which we generate using simple techniques that are known to improve translation accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3641">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>can be phrase.
</prevsent>
<prevsent>therefore, the number of rules that the model can learn is exponential in sentence length unless strict heuristics are used, which may limit the models effectiveness.
</prevsent>
</prevsection>
<citsent citstr=" H05-1095 ">
many other models translate dis contiguous phrases, and the size of their extracted rule sets is such pervasive problem that it is recurring topic in the literature (chiang, 2007; deneefe et al, 2007; <papid> D07-1079 </papid>simard et al, 2005).<papid> H05-1095 </papid></citsent>
<aftsection>
<nextsent>most decoder implementations assume that all model rules and parameters are known in advance.
</nextsent>
<nextsent>with very large models, computing all rules and parameters can be very slow.
</nextsent>
<nextsent>this is bottleneck in experimental settings where we wish to explore many model variants, and therefore presents real impediment to full exploration of their potential.
</nextsent>
<nextsent>we present solution to this problem.to fully motivate the discussion, we give concrete example of very large model, which we generate using simple techniques that are known to improve translation accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3642">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a tera-scale translation model </section>
<citcontext>
<prevsection>
<prevsent>our results extend previous findings on the use of long phrases in translation, shed light on the source of improved performance in hierarchical phrase based models, and show that our tera-scale translation model outperforms strong baseline.
</prevsent>
<prevsent>we will focus on the hierarchical phrase-based model of chiang (2007).
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
it compares favorably 505 with conventional phrase-based translation (koehn et al, 2003) <papid> N03-1017 </papid>on chinese-english news translation (chiang, 2007).</citsent>
<aftsection>
<nextsent>we found that baseline system trained on 27 million words of news data is already quite strong, but we suspect that it would be possible to improve it using some simple techniques.
</nextsent>
<nextsent>add additional training data.
</nextsent>
<nextsent>our baseline already uses much of the available curated news data, but there is at least three times as much curated data available in the united nations proceedings.
</nextsent>
<nextsent>adding theun data gives us training corpus of 107 million words per language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3643">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a tera-scale translation model </section>
<citcontext>
<prevsection>
<prevsent>adding theun data gives us training corpus of 107 million words per language.
</prevsent>
<prevsent>change the word alignments.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
our baseline uses giza++ alignments (och and ney, 2003)<papid> J03-1002 </papid>symmetrized with the grow-diag-final-and heuristic (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>we replace these with the maximum entropy aligments of ayan and dorr (2006<papid> P06-1002 </papid>b).</nextsent>
<nextsent>they reported improvements of 1.6 bleu in chinese-english translation, though with much less training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3647">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a tera-scale translation model </section>
<citcontext>
<prevsection>
<prevsent>change the word alignments.
</prevsent>
<prevsent>our baseline uses giza++ alignments (och and ney, 2003)<papid> J03-1002 </papid>symmetrized with the grow-diag-final-and heuristic (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1002 ">
we replace these with the maximum entropy aligments of ayan and dorr (2006<papid> P06-1002 </papid>b).</citsent>
<aftsection>
<nextsent>they reported improvements of 1.6 bleu in chinese-english translation, though with much less training data.
</nextsent>
<nextsent>change the bilingual phrase extractionheuristic.
</nextsent>
<nextsent>our baseline uses tight heuristic, requiring aligned words at phrase edges.
</nextsent>
<nextsent>however,ayan and dorr (2006<papid> P06-1002 </papid>a) showed that loose heuristic, allowing unaligned words at the phrase edges,improved accuracy by 3.7 bleu with some alignments, again with much less training data.quadrupling the amount of training data predictably increases model size.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3655">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> translation by pattern matching.  </section>
<citcontext>
<prevsection>
<prevsent>this is much too slow for large texts.
</prevsent>
<prevsent>they solve this using an index data structure called suffix 506 baseline large rules extracted (millions) 195 19,300 extract time (cpu hours) 10.8 1,840 unique rules (millions) 67 6,600* extract file size (gb) 9.3 917* model size (gb) 6.1 604* table 1: extraction time and model sizes.
</prevsent>
</prevsection>
<citsent citstr=" N07-1062 ">
the model size reported is the size of the files containing an external prefix tree representation (zens and ney, 2007).<papid> N07-1062 </papid></citsent>
<aftsection>
<nextsent>*denotes estimated quantities.
</nextsent>
<nextsent>citation millions of rules simard et al (2005) (<papid> H05-1095 </papid>filtered) 4 chiang (2007) (filtered) 6 deneefe et al (2007) <papid> D07-1079 </papid>57 zens and ney (2007) <papid> N07-1062 </papid>225 this paper 6,600 table 2: model sizes in the literature.</nextsent>
<nextsent>array (manber and myers, 1993).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3663">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> translation by pattern matching.  </section>
<citcontext>
<prevsection>
<prevsent>thento look up the phrase uxv, we first find all occurrences of u, and all occurrences of v. we can then compute all cases where an occurrence of precedes and occurrence of in the same sentence.
</prevsent>
<prevsent>the complexity of this last step is linear in the number of occurrences of and v. if either or is very frequent, this is too slow.
</prevsent>
</prevsection>
<citsent citstr=" D07-1104 ">
lopez (2007), <papid> D07-1104 </papid>lopez (2008) solves this with series of empirically fast exact algorithms.</citsent>
<aftsection>
<nextsent>we briefly sketch the solution here; for details see lopez (2008, chapter 4).
</nextsent>
<nextsent>loss less pruning.
</nextsent>
<nextsent>for each phrase, we only search if we have already successfully found both its longest suffix and longest prefix.
</nextsent>
<nextsent>for example, if a, b, c, and are all words, then we only search for phrase abxcd if we have already found occurrences of phrases abxc and bxcd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3673">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented on chinese-english newswire translation.
</prevsent>
<prevsent>except where noted, each system was trained on 27 million words of newswire data, aligned with giza++ (och and ney, 2003)<papid> J03-1002 </papid> andsymmetrized with the grow-diag-final-and heuristic (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
in all experiments that follow, each system configuration was independently optimized on the nist 2003 chinese-english testset (919 sentences) using minimum error rate training (och, 2003) <papid> P03-1021 </papid>and tested on the nist 2005chinese-english task (1082 sentences).</citsent>
<aftsection>
<nextsent>optimization and measurement were done with the nist implementation of case-insensitive bleu 4n4r (papineni et al, 2002).<papid> P02-1040 </papid>4 4.1 baseline.</nextsent>
<nextsent>we compared translation by pattern matching with conventional exact model representation using external prefix trees (zens and ney, 2007).<papid> N07-1062 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3674">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>except where noted, each system was trained on 27 million words of newswire data, aligned with giza++ (och and ney, 2003)<papid> J03-1002 </papid> andsymmetrized with the grow-diag-final-and heuristic (koehn et al, 2003).<papid> N03-1017 </papid></prevsent>
<prevsent>in all experiments that follow, each system configuration was independently optimized on the nist 2003 chinese-english testset (919 sentences) using minimum error rate training (och, 2003) <papid> P03-1021 </papid>and tested on the nist 2005chinese-english task (1082 sentences).</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
optimization and measurement were done with the nist implementation of case-insensitive bleu 4n4r (papineni et al, 2002).<papid> P02-1040 </papid>4 4.1 baseline.</citsent>
<aftsection>
<nextsent>we compared translation by pattern matching with conventional exact model representation using external prefix trees (zens and ney, 2007).<papid> N07-1062 </papid></nextsent>
<nextsent>to make model computation efficient for the latter case, we followed the heuristic limits on phrase extraction used by chiang (2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3696">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a word count feature. </section>
<citcontext>
<prevsection>
<prevsent>to gain insight into the hierarchical model.
</prevsent>
<prevsent>hierarchical phrase-based translation is often reported to be better than conventional phrase based translation, but the actual reason for this isunknown.
</prevsent>
</prevsection>
<citsent citstr=" N06-1002 ">
it is often argued that the ability to translate dis contiguous phrases is important to modeling translation (chiang, 2007; simard et al, 2005; <papid> H05-1095 </papid>quirk and menezes, 2006), <papid> N06-1002 </papid>and it may be that this explains the results.</citsent>
<aftsection>
<nextsent>however, there is another hypothesis.
</nextsent>
<nextsent>the model can also translate phrases in the form ux or xu (a single contiguous unit and gap).
</nextsent>
<nextsent>if it learns that ux often translates as xu ? , then in addition to learning that translates as u?, it has also learned that switches places with neighboring phrase during translation.
</nextsent>
<nextsent>this is similar to lexicalized reordering in conventional phrase-based models (tillman, 2004; al-onaizan and papineni, 2006).6 if this is the real benefit of the hierarchical model, then the ability to translate dis contiguous phrases may be irrelevant.to tease apart these claims, we make the following distinction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3697">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a word count feature. </section>
<citcontext>
<prevsection>
<prevsent>we ran experiments varying both the number of contiguous subphrases and the number of gaps (ta 6this hypothesis was suggested independently in personal communications with several researchers, including chris callison-burch, chris dyer, alex fraser, and franz och.
</prevsent>
<prevsent>509 ble 4).
</prevsent>
</prevsection>
<citsent citstr=" P07-2045 ">
for comparison, we also include results of the phrase-based system moses (koehn et al, 2007) <papid> P07-2045 </papid>with and without lexicalized reordering.our results are consistent with those found elsewhere in the literature.</citsent>
<aftsection>
<nextsent>the strictest setting allowing no gaps replicates result in chiang (2007, table 7), with significantly worse accuracy than allothers.
</nextsent>
<nextsent>the most striking result is that the accuracy of moses with lexicalized reordering is indistinguishable from the accuracy of the full hierarchical system.
</nextsent>
<nextsent>both improve over non-lexicalizedmoses by about 1.4 bleu.
</nextsent>
<nextsent>the hierarchical emulation owes its performance only partially to lexicalized reordering.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3700">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a word count feature. </section>
<citcontext>
<prevsection>
<prevsent>there are several other useful approaches to scaling translation models.
</prevsent>
<prevsent>zens and ney (2007) <papid> N07-1062 </papid>remove constraints imposed by the size of main memory by using an external data structure.</prevsent>
</prevsection>
<citsent citstr=" D07-1103 ">
johnson et al (2007) <papid> D07-1103 </papid>substantially reduce model size with filtering method.</citsent>
<aftsection>
<nextsent>however, neither ofthese approaches addresses the preprocessing bottleneck.
</nextsent>
<nextsent>to our knowledge, the strand of research initiated by callison-burch et al (2005) and zhang and vogel (2005) and extended here is the first to do so.
</nextsent>
<nextsent>dyer et al (2008) <papid> W08-0333 </papid>address this bottleneck with promising approach based on parallel processing, showing reductions in real time that are linear in the number of cpus.</nextsent>
<nextsent>however, they do not reduce the overall cpu time.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3701">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a word count feature. </section>
<citcontext>
<prevsection>
<prevsent>however, neither ofthese approaches addresses the preprocessing bottleneck.
</prevsent>
<prevsent>to our knowledge, the strand of research initiated by callison-burch et al (2005) and zhang and vogel (2005) and extended here is the first to do so.
</prevsent>
</prevsection>
<citsent citstr=" W08-0333 ">
dyer et al (2008) <papid> W08-0333 </papid>address this bottleneck with promising approach based on parallel processing, showing reductions in real time that are linear in the number of cpus.</citsent>
<aftsection>
<nextsent>however, they do not reduce the overall cpu time.
</nextsent>
<nextsent>our techniques also benefit from parallel processing, but theyre duce overall cpu time, thus comparing favorably even in this scenario.8 moreover, our method works even with limited parallel processing.
</nextsent>
<nextsent>although we saw success with this approach,there are some interesting open problems.
</nextsent>
<nextsent>as discussed in 4.2, there are tradeoffs in the form of slower decoding and increased memory usage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3702">
<title id=" C08-1064.xml">tera scale translation models via pattern matching </title>
<section> a word count feature. </section>
<citcontext>
<prevsection>
<prevsent>it would also appear to limit our ability to use discriminative training methods, since these tend to be much slower thanthe analytical maximum likelihood estimate.
</prevsent>
<prevsent>discriminative methods are desirable for feature-rich models that we would like to explore with pattern matching.
</prevsent>
</prevsection>
<citsent citstr=" P07-1005 ">
for example, chan et al (2007) <papid> P07-1005 </papid>and carpuat and wu (2007) improve translation accuracy using discriminatively trained models with contextual features of source phrases.</citsent>
<aftsection>
<nextsent>their features are easy to obtain at runtime using our approach, which finds source phrases in context.
</nextsent>
<nextsent>however, to make their experiments tractable, they trained their discriminative models offline only forthe specific phrases of the test set.
</nextsent>
<nextsent>combining discriminative learning with our approach is an open problem.
</nextsent>
<nextsent>we showed that very large translation models present an interesting engineering challenge, and illustrated solution to this challenge using pattern matching algorithms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3703">
<title id=" C02-1155.xml">multidimensional text classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task is to classify documents into predefined set of categories (or classes) (lewis and ringuetee, 1994; eui-hong and karypis, 2000) where there are no structural relationships among these categories.
</prevsent>
<prevsent>many existing databases are organized in this type of flat structure, such as reuters newswire, ohsumed and trec.
</prevsent>
</prevsection>
<citsent citstr=" C92-2069 ">
to improve classification accuracy, variety of learning techniques are developed, including regression models (yang and chute, 1992), <papid> C92-2069 </papid>nearest neighbour classification (yang and liu, 1999), bayesian approaches (lewis and ringuetee, 1994; mccallum et al, 1998), decision trees (lewis and ringuetee 1994), neural networks (wiener et al,1995) and support vector machines (dumais and chen, 2000).</citsent>
<aftsection>
<nextsent>however, it is very difficult to browse or search documents in flat categories when there are large number of categories.
</nextsent>
<nextsent>as more efficient method, one possible natural extension to flat categories is to arrange documents in topic hierarchy instead of simple flat structure.
</nextsent>
<nextsent>when people organize extensive datasets into fine-grained classes, topic hierarchy is often employed to make the large collection of classes (categories) more manageable.
</nextsent>
<nextsent>this structure is known as category hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3704">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though propbank was not used for the training of disambiguation model, an hpsg parser achieved the accuracy competitive with existing studies on the task of identifying propbank annotations.
</prevsent>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</citsent>
<aftsection>
<nextsent>however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</nextsent>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3706">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though propbank was not used for the training of disambiguation model, an hpsg parser achieved the accuracy competitive with existing studies on the task of identifying propbank annotations.
</prevsent>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</citsent>
<aftsection>
<nextsent>however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</nextsent>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3708">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though propbank was not used for the training of disambiguation model, an hpsg parser achieved the accuracy competitive with existing studies on the task of identifying propbank annotations.
</prevsent>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</citsent>
<aftsection>
<nextsent>however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</nextsent>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3709">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though propbank was not used for the training of disambiguation model, an hpsg parser achieved the accuracy competitive with existing studies on the task of identifying propbank annotations.
</prevsent>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
</prevsection>
<citsent citstr=" P02-1042 ">
several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</citsent>
<aftsection>
<nextsent>however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</nextsent>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3710">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though propbank was not used for the training of disambiguation model, an hpsg parser achieved the accuracy competitive with existing studies on the task of identifying propbank annotations.
</prevsent>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
</prevsection>
<citsent citstr=" P03-1046 ">
several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</citsent>
<aftsection>
<nextsent>however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</nextsent>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3711">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recently, deep linguistic analysis has successfully been applied to real-world texts.
</prevsent>
<prevsent>several parser shave been implemented in various grammar formalisms and empirical evaluation has been re ported: lfg (riezler et al, 2002; <papid> P02-1035 </papid>cahill et al, 2002; burke et al, 2004), ltag (chiang, 2000), <papid> P00-1058 </papid>ccg (hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark et al., 2002; <papid> P02-1042 </papid>hockenmaier, 2003), <papid> P03-1046 </papid>and hpsg (miyaoet al, 2003; malouf and van noord, 2004).</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
however, their accuracy was still below the state-of-the art pcfg parsers (collins, 1999; charniak, 2000) <papid> A00-2018 </papid>in terms of the parseval score.</citsent>
<aftsection>
<nextsent>since deep parsers can output deeper representation of the structure of sentence, such as predicate argument structures,several studies reported the accuracy of predicate argument relations using treebank developed for each formalism.
</nextsent>
<nextsent>however, resources used for the evaluation were not available for other formalisms, and the results cannot be compared with each other.
</nextsent>
<nextsent>in this paper, we employ propbank (kingsburyand palmer, 2002) for the evaluation of the accuracy of hpsg parsing.
</nextsent>
<nextsent>in the propbank, semantic arguments of predicate and their semantic roles are manually annotated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3713">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the propbank, semantic arguments of predicate and their semantic roles are manually annotated.
</prevsent>
<prevsent>since the propbank hasbeen developed independently of any grammar formalisms, the results are comparable with other published results using the same test data.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
interestingly, several studies suggested that the identification of propbank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (gildea and hockenmaier, 2003; <papid> W03-1008 </papid>chen and rambow, 2003).<papid> W03-1006 </papid></citsent>
<aftsection>
<nextsent>they employed ccg (steedman, 2000) or ltag(schabes et al, 1988) <papid> C88-2121 </papid>parser to acquire syntac tic/semantic structures, which would be passed to statistical classifier as features.</nextsent>
<nextsent>that is, they used deep analysis as pre processor to obtain useful features for training probabilistic model or statistical classifier of semantic argument identifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3716">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the propbank, semantic arguments of predicate and their semantic roles are manually annotated.
</prevsent>
<prevsent>since the propbank hasbeen developed independently of any grammar formalisms, the results are comparable with other published results using the same test data.
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
interestingly, several studies suggested that the identification of propbank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (gildea and hockenmaier, 2003; <papid> W03-1008 </papid>chen and rambow, 2003).<papid> W03-1006 </papid></citsent>
<aftsection>
<nextsent>they employed ccg (steedman, 2000) or ltag(schabes et al, 1988) <papid> C88-2121 </papid>parser to acquire syntac tic/semantic structures, which would be passed to statistical classifier as features.</nextsent>
<nextsent>that is, they used deep analysis as pre processor to obtain useful features for training probabilistic model or statistical classifier of semantic argument identifier.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3718">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since the propbank hasbeen developed independently of any grammar formalisms, the results are comparable with other published results using the same test data.
</prevsent>
<prevsent>interestingly, several studies suggested that the identification of propbank annotations would require linguistically-motivated features that can be obtained by deep linguistic analysis (gildea and hockenmaier, 2003; <papid> W03-1008 </papid>chen and rambow, 2003).<papid> W03-1006 </papid></prevsent>
</prevsection>
<citsent citstr=" C88-2121 ">
they employed ccg (steedman, 2000) or ltag(schabes et al, 1988) <papid> C88-2121 </papid>parser to acquire syntac tic/semantic structures, which would be passed to statistical classifier as features.</citsent>
<aftsection>
<nextsent>that is, they used deep analysis as pre processor to obtain useful features for training probabilistic model or statistical classifier of semantic argument identifier.
</nextsent>
<nextsent>these results imply the superiority of deep linguistic analysis for this task.although the statistical approach seems reason able way for developing an accurate identifier of propbank annotations, this study aims at establishing method of directly comparing the outputs ofhpsg parsing with the propbank annotation in order to explicitly demonstrate the availability of deep parsers.
</nextsent>
<nextsent>that is, we do not apply statistical model nor machine learning to the post-processing of the output of hpsg parsing.
</nextsent>
<nextsent>by eliminating the effect of post-processing, we can directly evaluate the accuracy of deep linguistic analysis.section 2 introduces recent advances in deep linguistic analysis and the development of semantically annotated corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3720">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> deep linguistic analysis and.  </section>
<citcontext>
<prevsection>
<prevsent>section 3 describes the details of the implementation of an hpsg parser evaluated in this study.
</prevsent>
<prevsent>section 4 discusses problem in adopting propbank for the performance evaluation of deep linguistic parsers and proposes its solution.section 5 reports empirical evaluation of the accuracy of the hpsg parser.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
semantically annotated corpora riezler et al (2002) <papid> P02-1035 </papid>reported the successful application of hand-crafted lfg (bresnan, 1982) grammar to the parsing of the penn treebank (marcus et al, 1994) <papid> H94-1020 </papid>by exploiting various techniques for robust parsing.</citsent>
<aftsection>
<nextsent>the study was impressive because most researchers had believed that deep linguistic analysis of real-world text was impossible.
</nextsent>
<nextsent>their success owed much to consistent effort to maintain wide-coverage lfg grammar, as well as var svp have to choose this particular moment np vp vp np they np-1 didnt *-1 vp vp arg0-choose arg1-choosearg0-choose rel-choose figure 1: annotation of the propbank ious techniques for robust parsing.however, the manual development of wide coverage linguistic grammars is still difficult task.
</nextsent>
<nextsent>recent progress in deep linguistic analysis has mainly depended on the acquisition of lexicalized grammars from annotated corpora (xia, 1999; chenand vijay-shanker, 2000; chiang, 2000; <papid> P00-1058 </papid>hockenmaier and steedman, 2002<papid> P02-1043 </papid>a; cahill et al, 2002;frank et al, 2003; miyao et al, 2004).</nextsent>
<nextsent>this approach not only allows for the low-cost development of wide-coverage grammars, but also provides the training data for statistical modeling as by product.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3736">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> deep linguistic analysis and.  </section>
<citcontext>
<prevsection>
<prevsent>at the same time, following the great success of machine learning approaches in nlp, many research efforts are being devoted to developing various annotated corpora.
</prevsent>
<prevsent>notably, several projects are underway to annotate large corpora with semantic information such as semantic relations of words and coreferences.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
propbank (kingsbury and palmer, 2002) and framenet (baker et al, 1998) <papid> P98-1013 </papid>are large english corpora annotated with the semantic relations of words in sentence.</citsent>
<aftsection>
<nextsent>figure 1 shows an example of the annotation of the propbank.
</nextsent>
<nextsent>as the target text of the propbank is the same as the penn treebank, syntactic structure is given by the penn treebank.the propbank includes additional annotations representing predicate and its semantic arguments ina syntactic tree.
</nextsent>
<nextsent>for example, in figure 1, rel denotes predicate, choose?, and arg   represents its semantic arguments: they?
</nextsent>
<nextsent>for the 0th argument (i.e., subject) and this particular moment?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3737">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> deep linguistic analysis and.  </section>
<citcontext>
<prevsection>
<prevsent>similar to many methods of applying machine learning to nlp tasks, they first formulated the task as identifying in sentence each argument of given predicate.
</prevsent>
<prevsent>then, parameters of the identifier were learned from the annotated corpus.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
features of statistical model were defined as pattern on partial structure of the syntactic tree output by an automatic parser (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and jurafsky, 2002).<papid> J02-3001 </papid>several studies proposed the use of deep linguistic features, such as predicate-argument relations output by ccg parser (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and derivation trees output by an ltag parser (chen and rambow, 2003).<papid> W03-1006 </papid></citsent>
<aftsection>
<nextsent>both studies reported that the identification accuracy improved by introducing such deep linguistic features.
</nextsent>
<nextsent>although deep analysis has not outperformed pcfg parsers interms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations.
</nextsent>
<nextsent>however, these results do not directly reflect the performance of deep parsers.
</nextsent>
<nextsent>since these corpora provide deeper structure of sentence than surface parse trees, they would be suitable for the evaluation of deep parsers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3738">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> deep linguistic analysis and.  </section>
<citcontext>
<prevsection>
<prevsent>similar to many methods of applying machine learning to nlp tasks, they first formulated the task as identifying in sentence each argument of given predicate.
</prevsent>
<prevsent>then, parameters of the identifier were learned from the annotated corpus.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
features of statistical model were defined as pattern on partial structure of the syntactic tree output by an automatic parser (gildea and palmer, 2002; <papid> P02-1031 </papid>gildea and jurafsky, 2002).<papid> J02-3001 </papid>several studies proposed the use of deep linguistic features, such as predicate-argument relations output by ccg parser (gildea and hockenmaier, 2003) <papid> W03-1008 </papid>and derivation trees output by an ltag parser (chen and rambow, 2003).<papid> W03-1006 </papid></citsent>
<aftsection>
<nextsent>both studies reported that the identification accuracy improved by introducing such deep linguistic features.
</nextsent>
<nextsent>although deep analysis has not outperformed pcfg parsers interms of the accuracy of surface structure, these results are implicitly supporting the necessity of deep linguistic analysis for the recognition of semantic relations.
</nextsent>
<nextsent>however, these results do not directly reflect the performance of deep parsers.
</nextsent>
<nextsent>since these corpora provide deeper structure of sentence than surface parse trees, they would be suitable for the evaluation of deep parsers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3743">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> implementation of an hpsg parser.  </section>
<citcontext>
<prevsection>
<prevsent>by grammar extraction, we are able to obtain large lexicon together with complete derivation trees of hpsg, i.e, an hpsg treebank.
</prevsent>
<prevsent>the hpsg treebank can then be used as training data for the machine learning of the disambiguation model.
</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
following recent research about disambiguation models on linguistic grammars (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2002; <papid> P02-1035 </papid>clark and curran, 2003; <papid> W03-1013 </papid>miyao et al, 2003; malouf and van noord, 2004), we apply log-linear model or maximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>on hpsgderivations.</citsent>
<aftsection>
<nextsent>we represent an hpsg sign as tu ple
</nextsent>
<nextsent>    , where  is lexical sign of the head word, is part-of-speech, and is symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the penntreebank).
</nextsent>
<nextsent>given an hpsg schema  and the distance  between the headwords of the head/nonhead daughter constituents, each (binary) branching of an hpsg derivation is represented as tuple      fffifl fifl fffiffi , where  !
</nextsent>
<nextsent>?              ?      ?  ?  table 1: feature function templates used in the disambiguation model of hpsg parsing: for binary schema applications (top) and for unary ones (bot tom) denote head/non-head daughters.1 since an hpsg derivation is represented by set of b, probability of assigned to sentence is defined as follows:               ff fiffifl  !# $ %&amp;(  $*),+ ).-/)   0 12  is probability of sequence of lexical entries, and is defined as the product of unigram probabilities  )  3 )  , where  ) is lexical entry assigned to word 3 ) . we divided the probability into.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3744">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> implementation of an hpsg parser.  </section>
<citcontext>
<prevsection>
<prevsent>by grammar extraction, we are able to obtain large lexicon together with complete derivation trees of hpsg, i.e, an hpsg treebank.
</prevsent>
<prevsent>the hpsg treebank can then be used as training data for the machine learning of the disambiguation model.
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
following recent research about disambiguation models on linguistic grammars (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2002; <papid> P02-1035 </papid>clark and curran, 2003; <papid> W03-1013 </papid>miyao et al, 2003; malouf and van noord, 2004), we apply log-linear model or maximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>on hpsgderivations.</citsent>
<aftsection>
<nextsent>we represent an hpsg sign as tu ple
</nextsent>
<nextsent>    , where  is lexical sign of the head word, is part-of-speech, and is symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the penntreebank).
</nextsent>
<nextsent>given an hpsg schema  and the distance  between the headwords of the head/nonhead daughter constituents, each (binary) branching of an hpsg derivation is represented as tuple      fffifl fifl fffiffi , where  !
</nextsent>
<nextsent>?              ?      ?  ?  table 1: feature function templates used in the disambiguation model of hpsg parsing: for binary schema applications (top) and for unary ones (bot tom) denote head/non-head daughters.1 since an hpsg derivation is represented by set of b, probability of assigned to sentence is defined as follows:               ff fiffifl  !# $ %&amp;(  $*),+ ).-/)   0 12  is probability of sequence of lexical entries, and is defined as the product of unigram probabilities  )  3 )  , where  ) is lexical entry assigned to word 3 ) . we divided the probability into.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3747">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> implementation of an hpsg parser.  </section>
<citcontext>
<prevsection>
<prevsent>by grammar extraction, we are able to obtain large lexicon together with complete derivation trees of hpsg, i.e, an hpsg treebank.
</prevsent>
<prevsent>the hpsg treebank can then be used as training data for the machine learning of the disambiguation model.
</prevsent>
</prevsection>
<citsent citstr=" W03-1013 ">
following recent research about disambiguation models on linguistic grammars (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2002; <papid> P02-1035 </papid>clark and curran, 2003; <papid> W03-1013 </papid>miyao et al, 2003; malouf and van noord, 2004), we apply log-linear model or maximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>on hpsgderivations.</citsent>
<aftsection>
<nextsent>we represent an hpsg sign as tu ple
</nextsent>
<nextsent>    , where  is lexical sign of the head word, is part-of-speech, and is symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the penntreebank).
</nextsent>
<nextsent>given an hpsg schema  and the distance  between the headwords of the head/nonhead daughter constituents, each (binary) branching of an hpsg derivation is represented as tuple      fffifl fifl fffiffi , where  !
</nextsent>
<nextsent>?              ?      ?  ?  table 1: feature function templates used in the disambiguation model of hpsg parsing: for binary schema applications (top) and for unary ones (bot tom) denote head/non-head daughters.1 since an hpsg derivation is represented by set of b, probability of assigned to sentence is defined as follows:               ff fiffifl  !# $ %&amp;(  $*),+ ).-/)   0 12  is probability of sequence of lexical entries, and is defined as the product of unigram probabilities  )  3 )  , where  ) is lexical entry assigned to word 3 ) . we divided the probability into.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3748">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> implementation of an hpsg parser.  </section>
<citcontext>
<prevsection>
<prevsent>by grammar extraction, we are able to obtain large lexicon together with complete derivation trees of hpsg, i.e, an hpsg treebank.
</prevsent>
<prevsent>the hpsg treebank can then be used as training data for the machine learning of the disambiguation model.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
following recent research about disambiguation models on linguistic grammars (abney, 1997; <papid> J97-4005 </papid>johnson et al, 1999; <papid> P99-1069 </papid>riezler et al, 2002; <papid> P02-1035 </papid>clark and curran, 2003; <papid> W03-1013 </papid>miyao et al, 2003; malouf and van noord, 2004), we apply log-linear model or maximum entropy model (berger et al, 1996) <papid> J96-1002 </papid>on hpsgderivations.</citsent>
<aftsection>
<nextsent>we represent an hpsg sign as tu ple
</nextsent>
<nextsent>    , where  is lexical sign of the head word, is part-of-speech, and is symbol representing the structure of the sign (mostly corresponding to nonterminal symbols of the penntreebank).
</nextsent>
<nextsent>given an hpsg schema  and the distance  between the headwords of the head/nonhead daughter constituents, each (binary) branching of an hpsg derivation is represented as tuple      fffifl fifl fffiffi , where  !
</nextsent>
<nextsent>?              ?      ?  ?  table 1: feature function templates used in the disambiguation model of hpsg parsing: for binary schema applications (top) and for unary ones (bot tom) denote head/non-head daughters.1 since an hpsg derivation is represented by set of b, probability of assigned to sentence is defined as follows:               ff fiffifl  !# $ %&amp;(  $*),+ ).-/)   0 12  is probability of sequence of lexical entries, and is defined as the product of unigram probabilities  )  3 )  , where  ) is lexical entry assigned to word 3 ) . we divided the probability into.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3749">
<title id=" C04-1204.xml">deep linguistic analysis for the accurate identification of predicate argument relations </title>
<section> implementation of an hpsg parser.  </section>
<citcontext>
<prevsection>
<prevsent>in our probabilistic model, lexical entry templates are more fine-grained (as shown in section 5, grammar has more than 1,000 templates), while we used simple example here.
</prevsent>
<prevsent>s the window he np np vp arg0-broke arg1-broke broke rel-broke the window np vp arg1-broke broke rel-brokefigure 4: annotation of an ergative verb in the propbank the window np vp arg1-broke broke into pp np million pieces arg3-broke rel-broke figure 5: annotation of another usage of broke?
</prevsent>
</prevsection>
<citsent citstr=" P02-1036 ">
the other features are 0: head comp dfegd trans vb vp noun nns np c head comp d trans vb vp noun nns np c head comp die*d vb vp d nns np c head comp d vb vp d nns np c head comp die*d trans vb d noun nns h head comp d trans vb d noun nns h head comp dfegd vb d nns h head comp d vb d nns h given the hpsg treebank as training data, the model parameters + ) are efficiently estimated using dynamic programming algorithm for maximum entropy estimation (miyao and tsujii, 2002; geman and johnson, 2002).<papid> P02-1036 </papid></citsent>
<aftsection>
<nextsent>semantically annotated corpora our study aims toward the fair evaluation of deep linguistic parsers, thus we want to directly compare the output of hpsg parsing with hand-annotatedtest data.
</nextsent>
<nextsent>however, disagreements between the out put of hpsg parser and the propbank prevents us from direct comparison.
</nextsent>
<nextsent>in the propbank annotation, semantic arguments can occur in multiple syntactic realizations, as in the following example (figure 4).
</nextsent>
<nextsent>1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3757">
<title id=" C04-1164.xml">automated alignment and extraction of bilingual domain ontology for cross language domain specific applications </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>manual ontology merging using conventional editing tools without intelligent support is difficult, labor intensive and error prone.
</prevsent>
<prevsent>therefore, several systems and frameworks for supporting the knowledge engineer in the ontology merging task have recently been proposed (noy and musen 2000).
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
to avoid the reiteration in ontology construction, the algorithm of ontology merging (umls http://umlsks.nlm.nih.gov/) (langkilde and knight 1998) <papid> P98-1116 </papid>and ontology alignment (vossen and peters 1997) (weigard and hoppenbrouwers 1998) (asanoma 2001) were invested.</citsent>
<aftsection>
<nextsent>the final ontology is merged version of the original ontologies.
</nextsent>
<nextsent>the two original ontologies persist, with aligned links between them.
</nextsent>
<nextsent>alignment usually is performed when the ontologies cover domains that are complementary to each other.
</nextsent>
<nextsent>in the past, domain ontology was usually constructed manually according to the knowledge or experience of the experts or ontology engineers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3758">
<title id=" C02-1145.xml">building a largescale annotated chinese corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using 80,000 words from ctb-i as training data and the remaining 20,000 words as testing data, the maximum entropy segmenter achieved an accuracy of 91%, calculated by the f-measure, which combines precision and recall1.
</prevsent>
<prevsent>compared with industrial strength?
</prevsent>
</prevsection>
<citsent citstr=" W00-1207 ">
segment ers that have reported segmentation accuracy in the upper 90% range (wu and jiang 2000), <papid> W00-1207 </papid>this accuracy may seem to be relatively low.</citsent>
<aftsection>
<nextsent>there are two reasons for this.
</nextsent>
<nextsent>first, the industrial strength?
</nextsent>
<nextsent>segment ers usually go through several steps (name identification, number identification, to name few), which we did not do.
</nextsent>
<nextsent>second, 1 f-measure = (precision * recall * 2) / (precision + recall).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3759">
<title id=" C02-1145.xml">building a largescale annotated chinese corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another difference is that words in chinese are not as long as english words in terms of the number of characters or letters they have.
</prevsent>
<prevsent>still, some characters are useful predictors for the part-of-speech of the words they are components of.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
our pos tagger is essentially the maximum entropy tagger by ratnaparkhi (1996) <papid> W96-0213 </papid>retrained on the ctb-i data.</citsent>
<aftsection>
<nextsent>we used the same 80,000 words chunk that was used to train the segmenter and used the remaining 20,000 words for testing.
</nextsent>
<nextsent>our results show that the accuracy of this tagger is about 93% when tested on chinese data.
</nextsent>
<nextsent>considering the fact that our corpus is relatively small, this result is very promising.
</nextsent>
<nextsent>we expect that better accuracy will be achieved as more data become available.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3760">
<title id=" C02-1145.xml">building a largescale annotated chinese corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so we conducted an experiment and our results show that even with the apparent drawback of having to backtrack from the parses produced by the parser, the parser is still useful preprocessing tool that helps annotation substantially.
</prevsent>
<prevsent>we will discuss this result in the next subsection.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
1.3.3 training statistical parser in order to determine the usefulness of the parser as preprocessing tool, we used chiang parser (chiang 2000), <papid> P00-1058 </papid>originally developed for english, which was retrained on data from ctb-i.</citsent>
<aftsection>
<nextsent>we used 80,000 words of fully bracketed data for training and 10,000 words for testing.
</nextsent>
<nextsent>the parser obtains 73.9% labeled precision and 72.2% labeled recall.
</nextsent>
<nextsent>we then conducted an experiment to determine whether the use of parser as pre processor improves annotation speed.
</nextsent>
<nextsent>we randomly selected 13,469-word chunk of data form the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3761">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in trec-2001, where the distribution of question types reflected that of real user logs, 27% of the questions were requests for definitions.
</prevsent>
<prevsent>hence, techniques to handle this category of questions are very important.
</prevsent>
</prevsection>
<citsent citstr=" H01-1006 ">
we propose new method to answer definition questions, that combines and extends the technique of prager et al  (2001), <papid> H01-1006 </papid>prager et al  (2002), which relied on wordnet hypernyms, and that of joho et al  (2001), <papid> H01-1045 </papid>joho et al  (2002), which relied on manually crafted lexical patterns, sentence position, and word cooccurrence across candidate answers.</citsent>
<aftsection>
<nextsent>we train an svm (schlkopf and smola, 2002) on vectors whose attributes include the verdict of prager et al .s method, the attributes of joho et al , and additional phrasal attributes that we acquire automatically.
</nextsent>
<nextsent>the svm is then used to identify and rank 250-character snippets, each intended to contain stand-alone definition of given term, much as in trec qa tasks prior to 2003.
</nextsent>
<nextsent>in trec-2003, the answers to definition questions had to be lists of complementary snippets (voorhees, 2003), <papid> N03-2037 </papid>as opposed to single snippet definitions.</nextsent>
<nextsent>here, we focus on the pre-2003 task, for which trec data were publicly available during our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3764">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in trec-2001, where the distribution of question types reflected that of real user logs, 27% of the questions were requests for definitions.
</prevsent>
<prevsent>hence, techniques to handle this category of questions are very important.
</prevsent>
</prevsection>
<citsent citstr=" H01-1045 ">
we propose new method to answer definition questions, that combines and extends the technique of prager et al  (2001), <papid> H01-1006 </papid>prager et al  (2002), which relied on wordnet hypernyms, and that of joho et al  (2001), <papid> H01-1045 </papid>joho et al  (2002), which relied on manually crafted lexical patterns, sentence position, and word cooccurrence across candidate answers.</citsent>
<aftsection>
<nextsent>we train an svm (schlkopf and smola, 2002) on vectors whose attributes include the verdict of prager et al .s method, the attributes of joho et al , and additional phrasal attributes that we acquire automatically.
</nextsent>
<nextsent>the svm is then used to identify and rank 250-character snippets, each intended to contain stand-alone definition of given term, much as in trec qa tasks prior to 2003.
</nextsent>
<nextsent>in trec-2003, the answers to definition questions had to be lists of complementary snippets (voorhees, 2003), <papid> N03-2037 </papid>as opposed to single snippet definitions.</nextsent>
<nextsent>here, we focus on the pre-2003 task, for which trec data were publicly available during our work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3766">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we train an svm (schlkopf and smola, 2002) on vectors whose attributes include the verdict of prager et al .s method, the attributes of joho et al , and additional phrasal attributes that we acquire automatically.
</prevsent>
<prevsent>the svm is then used to identify and rank 250-character snippets, each intended to contain stand-alone definition of given term, much as in trec qa tasks prior to 2003.
</prevsent>
</prevsection>
<citsent citstr=" N03-2037 ">
in trec-2003, the answers to definition questions had to be lists of complementary snippets (voorhees, 2003), <papid> N03-2037 </papid>as opposed to single snippet definitions.</citsent>
<aftsection>
<nextsent>here, we focus on the pre-2003 task, for which trec data were publicly available during our work.
</nextsent>
<nextsent>we believe that this task is still interesting and of practical use.
</nextsent>
<nextsent>for example, list of single-snippet definitions accompanied by their source urls can be good starting point for users of search engines wishing to find definitions.
</nextsent>
<nextsent>single-snippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions; see radev and mckeown (1997).<papid> A97-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3767">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we believe that this task is still interesting and of practical use.
</prevsent>
<prevsent>for example, list of single-snippet definitions accompanied by their source urls can be good starting point for users of search engines wishing to find definitions.
</prevsent>
</prevsection>
<citsent citstr=" A97-1033 ">
single-snippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions; see radev and mckeown (1997).<papid> A97-1033 </papid></citsent>
<aftsection>
<nextsent>experiments indicate that our method clearly outperforms the techniques it builds upon in the task we considered.
</nextsent>
<nextsent>we sketch in section 6 how we plan to adapt our method to the post-2003 trec task.
</nextsent>
<nextsent>prager et al  (2001), <papid> H01-1006 </papid>prager et al  (2002) observe that definition questions can often be answered by hypernyms; for example, schizophrenia?</nextsent>
<nextsent>is mental illness?, where the latter is hypernym of the former in wordnet.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3771">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> previous techniques.  </section>
<citcontext>
<prevsection>
<prevsent>responding that an amphibian is an animal is less satisfactory than saying it is an animal that lives both on land and in water.
</prevsent>
<prevsent>prager et al  identify the best hypernyms by counting how often they co-occur with the definition term in two-sentence passages of the document collection.
</prevsent>
</prevsection>
<citsent citstr=" A00-1021 ">
they then short-list the passages that contain both the term and any of the best hypernyms, and rank them using measures similar to those of radev et al  (2000).<papid> A00-1021 </papid></citsent>
<aftsection>
<nextsent>more precisely, given term to define, they compute the level-adapted count (lac) of each of its hypernyms, defined as the number of two-sentence passages where the hypernym co-occurs with the term divided by the distance between the term and the hypernym in wordnets hierarchy.
</nextsent>
<nextsent>they then retain the hypernym with the largest lac, and all the hypernyms whose lac is within 20% margin from the largest one.
</nextsent>
<nextsent>to avoid very general hypernyms (e.g., {entity?, something?}), prager et al  discard the hypernyms of the highest 1 or 2 levels in wordnets trees, if the distance from the top of the tree to the definition term is up to 3 or 5, respectively; if the distance is longer, they discard the top three levels.
</nextsent>
<nextsent>this ceiling is raised gradually if no co-occurring hypernym is found.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3777">
<title id=" C04-1199.xml">learning to identify single snippet answers to definition questions </title>
<section> our method.  </section>
<citcontext>
<prevsection>
<prevsent>as result, the induced classifier is biased towards non-definitions, and, hence, most unseen vectors receive higher confidence scores for the category of non-definitions than for the category of definitions.
</prevsent>
<prevsent>we do not compare the two scores.
</prevsent>
</prevsection>
<citsent citstr=" W03-1209 ">
we pick the five vectors whose confidence score for the category of definitions is highest, and report the corresponding snippets; in effect, we use the svm as ranker, rather than classifier; see also ravichandran et al  (2003).<papid> W03-1209 </papid></citsent>
<aftsection>
<nextsent>the imbalance between the two categories can be reduced by considering (during both training and classification) only the first three snippets of each document, which discards mostly non-definitions.
</nextsent>
<nextsent>3.2.1 configuration 1: attributes of joho at al. in the first configuration of our learning-based approach, the attributes of the vectors are roughly those of joho et al : two numeric attributes for sn and wc (section 2), and binary attribute for each one of patterns (1)?(9) showing if the pattern is satisfied.
</nextsent>
<nextsent>we have also added binary attributes for the following manually crafted patterns, and numeric attribute for rk (section 3.1).
</nextsent>
<nextsent>(10) dp like qn e.g., antibiotics like amoxicillin?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3784">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, information extraction systems like those built in the darpa message understanding conferences (muc) have revealed that coreference resolution is such crucial component of an information extraction system that separate coreference task has been defined and evaluated in muc-6 (1995) and muc-7 (1998).
</prevsent>
<prevsent>there is long tradition of work on coreference resolution within computational linguistics.
</prevsent>
</prevsection>
<citsent citstr=" A88-1003 ">
many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (carter 1987; rich and luperfoy 1988; <papid> A88-1003 </papid>carbonell and brown 1988).</citsent>
<aftsection>
<nextsent>however, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (dagan and itai 1990; <papid> C90-3063 </papid>lappin and leass 1994; <papid> J94-4002 </papid>mitkov 1998; <papid> P98-2143 </papid>soon, ng and lim 2001; ng and cardie 2002), <papid> P02-1014 </papid>which was further motivated by the emergence of cheaper and more reliable corpus based nlp tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).</nextsent>
<nextsent>approaches to coreference resolution usually relyon set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. these factors can be either constraints?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3785">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is long tradition of work on coreference resolution within computational linguistics.
</prevsent>
<prevsent>many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (carter 1987; rich and luperfoy 1988; <papid> A88-1003 </papid>carbonell and brown 1988).</prevsent>
</prevsection>
<citsent citstr=" C90-3063 ">
however, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (dagan and itai 1990; <papid> C90-3063 </papid>lappin and leass 1994; <papid> J94-4002 </papid>mitkov 1998; <papid> P98-2143 </papid>soon, ng and lim 2001; ng and cardie 2002), <papid> P02-1014 </papid>which was further motivated by the emergence of cheaper and more reliable corpus based nlp tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).</citsent>
<aftsection>
<nextsent>approaches to coreference resolution usually relyon set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. these factors can be either constraints?
</nextsent>
<nextsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</nextsent>
<nextsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</nextsent>
<nextsent>while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3786">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is long tradition of work on coreference resolution within computational linguistics.
</prevsent>
<prevsent>many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (carter 1987; rich and luperfoy 1988; <papid> A88-1003 </papid>carbonell and brown 1988).</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
however, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (dagan and itai 1990; <papid> C90-3063 </papid>lappin and leass 1994; <papid> J94-4002 </papid>mitkov 1998; <papid> P98-2143 </papid>soon, ng and lim 2001; ng and cardie 2002), <papid> P02-1014 </papid>which was further motivated by the emergence of cheaper and more reliable corpus based nlp tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).</citsent>
<aftsection>
<nextsent>approaches to coreference resolution usually relyon set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. these factors can be either constraints?
</nextsent>
<nextsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</nextsent>
<nextsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</nextsent>
<nextsent>while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3787">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is long tradition of work on coreference resolution within computational linguistics.
</prevsent>
<prevsent>many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (carter 1987; rich and luperfoy 1988; <papid> A88-1003 </papid>carbonell and brown 1988).</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
however, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (dagan and itai 1990; <papid> C90-3063 </papid>lappin and leass 1994; <papid> J94-4002 </papid>mitkov 1998; <papid> P98-2143 </papid>soon, ng and lim 2001; ng and cardie 2002), <papid> P02-1014 </papid>which was further motivated by the emergence of cheaper and more reliable corpus based nlp tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).</citsent>
<aftsection>
<nextsent>approaches to coreference resolution usually relyon set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. these factors can be either constraints?
</nextsent>
<nextsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</nextsent>
<nextsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</nextsent>
<nextsent>while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3788">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is long tradition of work on coreference resolution within computational linguistics.
</prevsent>
<prevsent>many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (carter 1987; rich and luperfoy 1988; <papid> A88-1003 </papid>carbonell and brown 1988).</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
however, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (dagan and itai 1990; <papid> C90-3063 </papid>lappin and leass 1994; <papid> J94-4002 </papid>mitkov 1998; <papid> P98-2143 </papid>soon, ng and lim 2001; ng and cardie 2002), <papid> P02-1014 </papid>which was further motivated by the emergence of cheaper and more reliable corpus based nlp tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology).</citsent>
<aftsection>
<nextsent>approaches to coreference resolution usually relyon set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. these factors can be either constraints?
</nextsent>
<nextsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</nextsent>
<nextsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</nextsent>
<nextsent>while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3798">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</prevsent>
<prevsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>this paper proposes simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted english text.
</nextsent>
<nextsent>forgiven anaphor and all the preceding referring expressions as the antecedent candidates, common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge.
</nextsent>
<nextsent>then, according to the type of the anaphor, special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge.
</nextsent>
<nextsent>finally, simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates, based on the proximity principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3799">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</prevsent>
<prevsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>this paper proposes simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted english text.
</nextsent>
<nextsent>forgiven anaphor and all the preceding referring expressions as the antecedent candidates, common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge.
</nextsent>
<nextsent>then, according to the type of the anaphor, special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge.
</nextsent>
<nextsent>finally, simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates, based on the proximity principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3800">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>which discard invalid ones from the set of possible candidates (such as gender and number agreements, c-command constraints, semantic consistency), or preferences?
</prevsent>
<prevsent>which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity).
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
while number of approaches use similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (dagan and itai 1990) <papid> C90-3063 </papid>to decision trees (soon, ng and lim 2001; ng and cardie 2002) <papid> P02-1014 </papid>to pattern induced rules (ng and cardie 2002) <papid> P02-1014 </papid>to centering algorithms (grosz and sidner 1986; <papid> J86-3001 </papid>brennan, friedman and pollard 1987; strube 1998; <papid> P98-2204 </papid>tetreault 2001).<papid> J01-4003 </papid></citsent>
<aftsection>
<nextsent>this paper proposes simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted english text.
</nextsent>
<nextsent>forgiven anaphor and all the preceding referring expressions as the antecedent candidates, common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge.
</nextsent>
<nextsent>then, according to the type of the anaphor, special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge.
</nextsent>
<nextsent>finally, simple preference agent is used to choose an antecedent for the anaphor form the remaining antecedent candidates, based on the proximity principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3801">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> preprocessing: determination of.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present our conclusions.
</prevsent>
<prevsent>referring expressions the prerequisite for automatic coreference resolution is to obtain possible referring expressions in an input document.
</prevsent>
</prevsection>
<citsent citstr=" W00-1309 ">
in our system, the possible referring expressions are determined by pipeline of nlp components: ? tokenization and sentence segmentation ? named entity recognition ? part-of-speech tagging ? noun phrase chunking among them, named entity recognition, part of-speech tagging and noun phrase chunking apply the same hidden markov model (hmm) based engine with error-driven learning capability (zhou and su 2000).<papid> W00-1309 </papid></citsent>
<aftsection>
<nextsent>the named entity recognition component (zhou and su 2002) <papid> P02-1060 </papid>recognizes various types of muc-style named entities, that is, organization, location, person, date, time, money and percentage.</nextsent>
<nextsent>the hmm-based noun phrase chunking component (zhou and su 2000) <papid> W00-1309 </papid>determines various noun phrases based on the results of named entity recognition and part-of speech tagging.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3802">
<title id=" C04-1075.xml">a high performance coreference resolution system using a constraint based multi agent strategy </title>
<section> preprocessing: determination of.  </section>
<citcontext>
<prevsection>
<prevsent>referring expressions the prerequisite for automatic coreference resolution is to obtain possible referring expressions in an input document.
</prevsent>
<prevsent>in our system, the possible referring expressions are determined by pipeline of nlp components: ? tokenization and sentence segmentation ? named entity recognition ? part-of-speech tagging ? noun phrase chunking among them, named entity recognition, part of-speech tagging and noun phrase chunking apply the same hidden markov model (hmm) based engine with error-driven learning capability (zhou and su 2000).<papid> W00-1309 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1060 ">
the named entity recognition component (zhou and su 2002) <papid> P02-1060 </papid>recognizes various types of muc-style named entities, that is, organization, location, person, date, time, money and percentage.</citsent>
<aftsection>
<nextsent>the hmm-based noun phrase chunking component (zhou and su 2000) <papid> W00-1309 </papid>determines various noun phrases based on the results of named entity recognition and part-of speech tagging.</nextsent>
<nextsent>since coreference is symmetrical and transitive relation, it leads to simple partitioning of set of referring expressions and each partition forms coreference chain.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3818">
<title id=" C02-2013.xml">soat a semiautomatic domain ontology acquisition tool from chinese corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous works suggest that ontology acquisition is an iterative process which includes keyword collection as well as structure reorganization.
</prevsent>
<prevsent>the ontology will be revised, refined, and filled in detail during iteration.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
(noy and mcguinness 2001) for example (hearst 1992), <papid> C92-2082 </papid>in order to find hyponym of keyword, the human editor must observe sentences containing this keyword and its related hyponyms.</citsent>
<aftsection>
<nextsent>the editor then deduces rules for finding more hyponyms of this keyword.
</nextsent>
<nextsent>as such cycle ite rates, the editor refines the rules to obtain better quality pairs of keyword-hyponyms.
</nextsent>
<nextsent>in this work we try to speed up the above labor-intensive approach by designing acquisition rules that can be applied recursively.
</nextsent>
<nextsent>a human editor only has to verify the results of the acquisition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3819">
<title id=" C02-2013.xml">soat a semiautomatic domain ontology acquisition tool from chinese corpus </title>
<section> synonym: expressions that are synonymous.  </section>
<citcontext>
<prevsection>
<prevsent>discussion.
</prevsent>
<prevsent>li and thompson (1981) describe mandarin chinese as topic-prominent language in which the subject or the object is not as obvious as in other languages.
</prevsent>
</prevsection>
<citsent citstr=" W99-0621 ">
therefore, the highly precise shallow parsing result (munoz et al 1999) <papid> W99-0621 </papid>on nn and sv pairs in english is probably not applicable to chinese.</citsent>
<aftsection>
<nextsent>4.1 the experiment of extraction rate.
</nextsent>
<nextsent>to test the qualitative and quantitative performance of soat, we design two experiments.
</nextsent>
<nextsent>we construct three domain ontology prototypes for three different domains and corpora.
</nextsent>
<nextsent>table 5 shows the result in which the frequently asked questions (faqs) for stocks are taken from test sentences of the financial qa system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3820">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is design edas the first stage of complete vpe resolution system that is input free text, detects vpes, and proceeds to find the antecedents and resolve them.
</prevsent>
<prevsent>ellipsis is linguistic phenomenon that has received considerable attention, mostly focusing on its interpretation.
</prevsent>
</prevsection>
<citsent citstr=" E93-1025 ">
most work on ellipsis(fiengo and may, 1994; lappin, 1993; dalrymple et al, 1991; kehler, 1993; <papid> E93-1025 </papid>shieber et al, 1996) is aimed at discerning the procedures and the level of language processing at which ellipsis resolution takes place, or focuses on ambiguous and difficult cases.</citsent>
<aftsection>
<nextsent>the detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with, but taken as given.
</nextsent>
<nextsent>noisy or missing input, which is unavoidable in nlp applications, is not dealt with, and neither is focusing on specific domains or applications.
</nextsent>
<nextsent>it therefore becomes clear that robust, trainable approach is needed.
</nextsent>
<nextsent>an example of verb phrase ellipsis (vpe), which is detected by the presence of an auxiliary verb without verb phrase, is seen in example 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3821">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> identifying antecedents. for most cases of.  </section>
<citcontext>
<prevsection>
<prevsent>first, elided.
</prevsent>
<prevsent>verbs need to be found.
</prevsent>
</prevsection>
<citsent citstr=" J97-4002 ">
ellipsis, copying of the antecedent clause is enough for resolution (hardt, 1997).<papid> J97-4002 </papid></citsent>
<aftsection>
<nextsent>ambiguity exists, method for generating the full list of possible solutions, and suggesting the most likely one is needed.
</nextsent>
<nextsent>this paper describes the work done on the first stage, the detection of elliptical verbs.
</nextsent>
<nextsent>first, previous work done on tagged corpora will be summarised.
</nextsent>
<nextsent>then, new work on parsed corpora will be presented, showing the gains possible through sentence-level features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3822">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> resolving ambiguities. for cases where.  </section>
<citcontext>
<prevsection>
<prevsent>in previous work (nielsen, 2003a; nielsen, 2003b) we performed experiments on the british national corpus using variety of machine learning techniques.
</prevsent>
<prevsent>these earlier results are not directly comparable to hardts, due to the different corpora used.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the expanded set of results are summarised in table 1, for transformation based learning (tbl) (brill, 1995), <papid> J95-4004 </papid>gis based maximum entropy modelling (gis-maxent)2 (ratnaparkhi, 1998), l-bfgsbased maximum entropy modelling (l-bfgsmaxent)3 (malouf, 2002), <papid> W02-2018 </papid>decision tree learning (quinlan, 1993) and memory based learning (mbl) (daelemans et al, 2002).</citsent>
<aftsection>
<nextsent>algorithm recall precision f1 tbl 69.63 85.14 76.61 decision tree 60.93 79.39 68.94 mbl 72.58 71.50 72.04 gis-maxent 71.72 63.89 67.58 l-bfgs-maxent 71.93 80.58 76.01 table 1: comparison of algorithms for all of these experiments, the training features consisted of lexical forms and part of speech (pos) tags of the words in three wordforward/backward window of the auxiliary being tested.
</nextsent>
<nextsent>this context size was determined empirically to give optimum results, and will be used throughout this paper.
</nextsent>
<nextsent>l-bfgs-maxent uses gaussian prior smoothing optimized forthe bnc data, while gis-maxent has simple smoothing option available, but this deteriorates results and is not used.
</nextsent>
<nextsent>both maximum entropy models were experimented with to determine thresholds for accepting results as vpe; gis-maxent was set to 20% confidence threshold and l-bfgs-maxent to 35%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3823">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> resolving ambiguities. for cases where.  </section>
<citcontext>
<prevsection>
<prevsent>in previous work (nielsen, 2003a; nielsen, 2003b) we performed experiments on the british national corpus using variety of machine learning techniques.
</prevsent>
<prevsent>these earlier results are not directly comparable to hardts, due to the different corpora used.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
the expanded set of results are summarised in table 1, for transformation based learning (tbl) (brill, 1995), <papid> J95-4004 </papid>gis based maximum entropy modelling (gis-maxent)2 (ratnaparkhi, 1998), l-bfgsbased maximum entropy modelling (l-bfgsmaxent)3 (malouf, 2002), <papid> W02-2018 </papid>decision tree learning (quinlan, 1993) and memory based learning (mbl) (daelemans et al, 2002).</citsent>
<aftsection>
<nextsent>algorithm recall precision f1 tbl 69.63 85.14 76.61 decision tree 60.93 79.39 68.94 mbl 72.58 71.50 72.04 gis-maxent 71.72 63.89 67.58 l-bfgs-maxent 71.93 80.58 76.01 table 1: comparison of algorithms for all of these experiments, the training features consisted of lexical forms and part of speech (pos) tags of the words in three wordforward/backward window of the auxiliary being tested.
</nextsent>
<nextsent>this context size was determined empirically to give optimum results, and will be used throughout this paper.
</nextsent>
<nextsent>l-bfgs-maxent uses gaussian prior smoothing optimized forthe bnc data, while gis-maxent has simple smoothing option available, but this deteriorates results and is not used.
</nextsent>
<nextsent>both maximum entropy models were experimented with to determine thresholds for accepting results as vpe; gis-maxent was set to 20% confidence threshold and l-bfgs-maxent to 35%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3824">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> resolving ambiguities. for cases where.  </section>
<citcontext>
<prevsection>
<prevsent>l-bfgs-maxent uses gaussian prior smoothing optimized forthe bnc data, while gis-maxent has simple smoothing option available, but this deteriorates results and is not used.
</prevsent>
<prevsent>both maximum entropy models were experimented with to determine thresholds for accepting results as vpe; gis-maxent was set to 20% confidence threshold and l-bfgs-maxent to 35%.
</prevsent>
</prevsection>
<citsent citstr=" W99-0705 ">
mbl was used with its default settings.while tbl gave the best results, the software we used (lager, 1999) <papid> W99-0705 </papid>ran into memory problems and proved problematic with larger datasets.</citsent>
<aftsection>
<nextsent>decision trees, on the other hand, precision = no(correct ellipses found)no(all ellipses found) (2) f1 = 2?
</nextsent>
<nextsent>precisionrecallprecision+recall (3) 2downloadable from https://sourceforge.net/projects/maxent/ 3downloadable from http://www.nlplab.cn/zhangle/maxent toolkit.htmltend to oversimplify due to the very sparse nature of ellipsis, and produce single rule that classifies everything as non-vpe.
</nextsent>
<nextsent>this leaves maximum entropy and mbl for further experiments.
</nextsent>
<nextsent>3 corpus description.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3825">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> resolving ambiguities. for cases where.  </section>
<citcontext>
<prevsection>
<prevsent>a range of sections of the bnc, containing around 370k words4 with 645 samples of vpe was used as training data.
</prevsent>
<prevsent>the separate development data consists of around 74k words5 with 200 samples of vpe.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
the penn treebank (marcus et al, 1994) <papid> H94-1020 </papid>has more than hundred phrase labels, and number of empty categories, but uses coarser tagset.</citsent>
<aftsection>
<nextsent>a mixture of sections from the wall street journal and brown corpus were used.
</nextsent>
<nextsent>the training section6 consists of around 540k words and contains 522 samples of vpe.
</nextsent>
<nextsent>the development section7 consists of around 140k words and contains 150 samples of vpe.
</nextsent>
<nextsent>treebank to experiment with what gains are possible through the use of more complex data such as parse trees, the penn treebank is used for the second round of experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3827">
<title id=" C04-1157.xml">verb phrase ellipsis detection using automatically parsed text </title>
<section> experiments with automatically.  </section>
<citcontext>
<prevsection>
<prevsent>charniaks parser (2000) is combination probabilistic context free grammar and maximum entropy parser.
</prevsent>
<prevsent>it is trained on the penn tree bank, and achieves 90.1% recall and precision average for sentences of 40 words or less.
</prevsent>
</prevsection>
<citsent citstr=" P02-1018 ">
whilecharniaks parser does not generate empty category information, johnson (2002) <papid> P02-1018 </papid>has developed an algorithm that extracts patterns from the treebank which can be used to insert empty words + pos + features algorithm recall precision f1 recall precision f1 mbl 56.05 63.02 59.33 78.32 79.39 78.85 gis-maxent 40.00 57.03 47.02 65.06 68.64 66.80 l-bfgs-maxent 60.08 70.77 64.99 78.92 87.27 82.88 table 9: cross-validation on the treebank categories into the parsers output.</citsent>
<aftsection>
<nextsent>this program will be used in conjunction with char niaks parser.
</nextsent>
<nextsent>robust accurate statistical parsing (rasp)(briscoe and carroll, 2002) uses combination of statistical techniques and hand-craftedgrammar.
</nextsent>
<nextsent>rasp is trained on range of corpora, and uses more complex tagging system (claws-2), like that of the bnc.
</nextsent>
<nextsent>this parser, on our data, generated full parses for 70% of the sentences, partial parses for 28%, while 2% were not parsed, returning pos tags only.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3828">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the interpretation of coordinate structures directly affects the meaning of the text.
</prevsent>
<prevsent>addressing coordination ambiguities is fundamental to natural language understanding.
</prevsent>
</prevsection>
<citsent citstr=" P92-1003 ">
previous studies on coordination disambiguation suggested that con juncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-of speech matching, semantic similarities, and soforth (agarwal and boggess, 1992).<papid> P92-1003 </papid></citsent>
<aftsection>
<nextsent>semantic similarities are acquired from thesauri (kurohashi andnagao, 1994; <papid> J94-4001 </papid>resnik, 1999) or distributional similarity (chantree et al, 2005).</nextsent>
<nextsent>c ? 2008.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3829">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>addressing coordination ambiguities is fundamental to natural language understanding.
</prevsent>
<prevsent>previous studies on coordination disambiguation suggested that con juncts in coordinate structures have syntactic or semantic similarities, and dealt with coordination ambiguities using (sub-)string matching, part-of speech matching, semantic similarities, and soforth (agarwal and boggess, 1992).<papid> P92-1003 </papid></prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
semantic similarities are acquired from thesauri (kurohashi andnagao, 1994; <papid> J94-4001 </papid>resnik, 1999) or distributional similarity (chantree et al, 2005).</citsent>
<aftsection>
<nextsent>c ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>for instance, consider the following example: (1) eat caesar salad and italian pasta the above methods detect the similarity between salad and pasta using thesaurus or distributional similarity, and identify the coordinate structure that con joins salad and pasta.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3830">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>coordinate structures are supported by their surrounding dependency relations that provide selectional preferences.
</prevsent>
<prevsent>these relations implicitly work as similarities, and thus it is not necessary to use similarities explicitly.in this paper, we focus on japanese.
</prevsent>
</prevsection>
<citsent citstr=" D07-1032 ">
coordination disambiguation is integrated in fully lexicalized generative dependency parser (kawa hara and kurohashi, 2007).<papid> D07-1032 </papid></citsent>
<aftsection>
<nextsent>for the selectional preferences, we use lexical knowledge, such as case frames, which is extracted from large raw corpus.the remainder of this paper is organized as follows.
</nextsent>
<nextsent>section 2 summarizes previous work related to coordination disambiguation and its integration into parsing.
</nextsent>
<nextsent>section 3 briefly describes the back ground of this study.
</nextsent>
<nextsent>section 4 overviews our idea,and section 5 describes our model in detail.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3832">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarities are obtained from manually assigned semantic tags (agarwal and boggess, 1992), <papid> P92-1003 </papid>thesaurus (resnik, 1999) and distributional thesaurus (chantree et al, 2005).</prevsent>
<prevsent>other approaches used cooccurrence statistics.</prevsent>
</prevsection>
<citsent citstr=" P99-1081 ">
to determine the attachments of ambiguous coordinate noun phrases, goldberg (1999) <papid> P99-1081 </papid>applied cooccurrence-based probabilistic model, and nakov and hearst (2005) <papid> H05-1105 </papid>used web-based frequencies.</citsent>
<aftsection>
<nextsent>the performance of these methods ranges from 50% to 80%.
</nextsent>
<nextsent>of the above approaches, resnik (1999) and nakov and hearst (2005) <papid> H05-1105 </papid>considered the statistics of noun-noun modification.</nextsent>
<nextsent>for example, the coordinate structure ?((mail and securities) fraud)?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3833">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarities are obtained from manually assigned semantic tags (agarwal and boggess, 1992), <papid> P92-1003 </papid>thesaurus (resnik, 1999) and distributional thesaurus (chantree et al, 2005).</prevsent>
<prevsent>other approaches used cooccurrence statistics.</prevsent>
</prevsection>
<citsent citstr=" H05-1105 ">
to determine the attachments of ambiguous coordinate noun phrases, goldberg (1999) <papid> P99-1081 </papid>applied cooccurrence-based probabilistic model, and nakov and hearst (2005) <papid> H05-1105 </papid>used web-based frequencies.</citsent>
<aftsection>
<nextsent>the performance of these methods ranges from 50% to 80%.
</nextsent>
<nextsent>of the above approaches, resnik (1999) and nakov and hearst (2005) <papid> H05-1105 </papid>considered the statistics of noun-noun modification.</nextsent>
<nextsent>for example, the coordinate structure ?((mail and securities) fraud)?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3837">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the previously described methods focused on coordination disambiguation.
</prevsent>
<prevsent>some research hasbeen undertaken that integrated coordination disambiguation into parsing.several techniques have considered the characteristics of coordinate structures in generative or reranking parser.
</prevsent>
</prevsection>
<citsent citstr=" P06-1053 ">
dubey et al (2006) <papid> P06-1053 </papid>proposed an un lexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism.</citsent>
<aftsection>
<nextsent>hogan (2007) <papid> P07-1086 </papid>improved generative lexicalized parser by considering the symmetry between words in each conjunct.</nextsent>
<nextsent>as for reranking parser, charniak and johnson (2005) <papid> P05-1022 </papid>incorporated some features of syntactic parallelism in coordinate structures into their maxent reranking parser.nilsson et al tried to transform the tree representation of treebank into more suitable representation for data-driven dependency parsers (nilsson et al, 2006; <papid> P06-1033 </papid>nilsson et al, 2007).<papid> P07-1122 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3838">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>some research hasbeen undertaken that integrated coordination disambiguation into parsing.several techniques have considered the characteristics of coordinate structures in generative or reranking parser.
</prevsent>
<prevsent>dubey et al (2006) <papid> P06-1053 </papid>proposed an un lexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism.</prevsent>
</prevsection>
<citsent citstr=" P07-1086 ">
hogan (2007) <papid> P07-1086 </papid>improved generative lexicalized parser by considering the symmetry between words in each conjunct.</citsent>
<aftsection>
<nextsent>as for reranking parser, charniak and johnson (2005) <papid> P05-1022 </papid>incorporated some features of syntactic parallelism in coordinate structures into their maxent reranking parser.nilsson et al tried to transform the tree representation of treebank into more suitable representation for data-driven dependency parsers (nilsson et al, 2006; <papid> P06-1033 </papid>nilsson et al, 2007).<papid> P07-1122 </papid></nextsent>
<nextsent>one of their targets is the representation of coordinatestructures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3839">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dubey et al (2006) <papid> P06-1053 </papid>proposed an un lexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism.</prevsent>
<prevsent>hogan (2007) <papid> P07-1086 </papid>improved generative lexicalized parser by considering the symmetry between words in each conjunct.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
as for reranking parser, charniak and johnson (2005) <papid> P05-1022 </papid>incorporated some features of syntactic parallelism in coordinate structures into their maxent reranking parser.nilsson et al tried to transform the tree representation of treebank into more suitable representation for data-driven dependency parsers (nilsson et al, 2006; <papid> P06-1033 </papid>nilsson et al, 2007).<papid> P07-1122 </papid></citsent>
<aftsection>
<nextsent>one of their targets is the representation of coordinatestructures.
</nextsent>
<nextsent>they succeeded in improving deterministic parser, but failed for globally optimized discriminative parser.kurohashi and nagao proposed japanese parsing method that included coordinate structure detection (kurohashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
<nextsent>their method first detects coordinate structures in sentence, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3840">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dubey et al (2006) <papid> P06-1053 </papid>proposed an un lexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism.</prevsent>
<prevsent>hogan (2007) <papid> P07-1086 </papid>improved generative lexicalized parser by considering the symmetry between words in each conjunct.</prevsent>
</prevsection>
<citsent citstr=" P06-1033 ">
as for reranking parser, charniak and johnson (2005) <papid> P05-1022 </papid>incorporated some features of syntactic parallelism in coordinate structures into their maxent reranking parser.nilsson et al tried to transform the tree representation of treebank into more suitable representation for data-driven dependency parsers (nilsson et al, 2006; <papid> P06-1033 </papid>nilsson et al, 2007).<papid> P07-1122 </papid></citsent>
<aftsection>
<nextsent>one of their targets is the representation of coordinatestructures.
</nextsent>
<nextsent>they succeeded in improving deterministic parser, but failed for globally optimized discriminative parser.kurohashi and nagao proposed japanese parsing method that included coordinate structure detection (kurohashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
<nextsent>their method first detects coordinate structures in sentence, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3841">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>dubey et al (2006) <papid> P06-1053 </papid>proposed an un lexicalized pcfg parser that modified pcfg probabilities to condition the existence of syntactic parallelism.</prevsent>
<prevsent>hogan (2007) <papid> P07-1086 </papid>improved generative lexicalized parser by considering the symmetry between words in each conjunct.</prevsent>
</prevsection>
<citsent citstr=" P07-1122 ">
as for reranking parser, charniak and johnson (2005) <papid> P05-1022 </papid>incorporated some features of syntactic parallelism in coordinate structures into their maxent reranking parser.nilsson et al tried to transform the tree representation of treebank into more suitable representation for data-driven dependency parsers (nilsson et al, 2006; <papid> P06-1033 </papid>nilsson et al, 2007).<papid> P07-1122 </papid></citsent>
<aftsection>
<nextsent>one of their targets is the representation of coordinatestructures.
</nextsent>
<nextsent>they succeeded in improving deterministic parser, but failed for globally optimized discriminative parser.kurohashi and nagao proposed japanese parsing method that included coordinate structure detection (kurohashi and nagao, 1994).<papid> J94-4001 </papid></nextsent>
<nextsent>their method first detects coordinate structures in sentence, and then determines the dependency structure of the sentence under the constraints of the detected coordinate structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3844">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>their method correctly analyzed 97 out of 150 japanese sentences.
</prevsent>
<prevsent>kawahara and kurohashi (2007) <papid> D07-1032 </papid>integrated this method into generative parsing model.</prevsent>
</prevsection>
<citsent citstr=" D07-1064 ">
shimboand hara (2007) <papid> D07-1064 </papid>considered many features for coordination disambiguation and automatically optimized their weights, which were heuristic ally determined in kurohashi and nagao (1994), <papid> J94-4001 </papid>using discriminative learning model.a number of machine learning-based approaches to japanese parsing have been developed.</citsent>
<aftsection>
<nextsent>among them, the best parsers are the svm-based dependency analyzers (kudo and matsumoto, 2002; <papid> W02-2016 </papid>sassano, 2004).<papid> C04-1002 </papid></nextsent>
<nextsent>in particular, sassano added some features to improve his parser by enabling it to detect coordinate structures (sassano, 2004).<papid> C04-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3846">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kawahara and kurohashi (2007) <papid> D07-1032 </papid>integrated this method into generative parsing model.</prevsent>
<prevsent>shimboand hara (2007) <papid> D07-1064 </papid>considered many features for coordination disambiguation and automatically optimized their weights, which were heuristic ally determined in kurohashi and nagao (1994), <papid> J94-4001 </papid>using discriminative learning model.a number of machine learning-based approaches to japanese parsing have been developed.</prevsent>
</prevsection>
<citsent citstr=" W02-2016 ">
among them, the best parsers are the svm-based dependency analyzers (kudo and matsumoto, 2002; <papid> W02-2016 </papid>sassano, 2004).<papid> C04-1002 </papid></citsent>
<aftsection>
<nextsent>in particular, sassano added some features to improve his parser by enabling it to detect coordinate structures (sassano, 2004).<papid> C04-1002 </papid></nextsent>
<nextsent>however, the added features did not contribute to improving the parsing accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3847">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>kawahara and kurohashi (2007) <papid> D07-1032 </papid>integrated this method into generative parsing model.</prevsent>
<prevsent>shimboand hara (2007) <papid> D07-1064 </papid>considered many features for coordination disambiguation and automatically optimized their weights, which were heuristic ally determined in kurohashi and nagao (1994), <papid> J94-4001 </papid>using discriminative learning model.a number of machine learning-based approaches to japanese parsing have been developed.</prevsent>
</prevsection>
<citsent citstr=" C04-1002 ">
among them, the best parsers are the svm-based dependency analyzers (kudo and matsumoto, 2002; <papid> W02-2016 </papid>sassano, 2004).<papid> C04-1002 </papid></citsent>
<aftsection>
<nextsent>in particular, sassano added some features to improve his parser by enabling it to detect coordinate structures (sassano, 2004).<papid> C04-1002 </papid></nextsent>
<nextsent>however, the added features did not contribute to improving the parsing accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3851">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, sassano added some features to improve his parser by enabling it to detect coordinate structures (sassano, 2004).<papid> C04-1002 </papid></prevsent>
<prevsent>however, the added features did not contribute to improving the parsing accuracy.</prevsent>
</prevsection>
<citsent citstr=" D07-1063 ">
tamura et al (2007) <papid> D07-1063 </papid>learned not only standard modifier-head relations but also ancestor-descendant relations.</citsent>
<aftsection>
<nextsent>with this treatment, their method can indirectly improve the handling of coordinate structures in limited cases.
</nextsent>
<nextsent>3.1 japanese grammar.
</nextsent>
<nextsent>let us first briefly introduce japanese grammar.the structure of japanese sentence can be described well by the dependency relation betweenbunsetsus.
</nextsent>
<nextsent>a bunsetsu is basic unit of dependency, consisting of one or more content words andthe following zero or more function words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3862">
<title id=" C08-1054.xml">coordination disambiguation without any similarities </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this is because the heads of pre- and post-conjuncts sharea case marker and predicate, and thus they are essentially similar.
</prevsent>
<prevsent>our idea is related to the notion of distributional similarity.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
chantree et al (2005) applied the distributional similarity proposed by lin (1998) <papid> P98-2127 </papid>to coordination disambiguation.</citsent>
<aftsection>
<nextsent>lin extracted from corpus dependency triples of two words and the grammatical relationship between them, and considered that similar words are likely to have similar dependency relations.
</nextsent>
<nextsent>the difference between chantree et al (2005) and ours is that their method does not use the information of verbs in the sentence under consideration, but use only the cooccurrence information extracted from corpus.
</nextsent>
<nextsent>on the other hand, the disadvantage of our model is that it cannot consider the parallelism of conjuncts, which still seems to exist in especially strong coordinate structures.
</nextsent>
<nextsent>handling of such parallelism is an open question of our model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3864">
<title id=" C02-1006.xml">nlp and ir approaches to monolingual and multilingual link detection </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the digital era, how to assist users to deal with data explosion problem becomes emergent.
</prevsent>
<prevsent>news stories on the internet contain large amount of real-time and new information.
</prevsent>
</prevsection>
<citsent citstr=" C00-1024 ">
several attempts were made to extract information from news stories, e.g., multi-lingual multi-document summarization (chen and huang, 1999; chen and lin, 2000), <papid> C00-1024 </papid>topic detection and tracking (abbreviated as tdt hereafter, http://www.nist.gov/tdt), and so on.</citsent>
<aftsection>
<nextsent>of these, tdt, which is long-term project, proposed many diverse applications, e.g., story segmentation (greiff et al, 2000), topic tracking (levow et al, 2000; leek et al, 2002), topic detection (chen and ku, 2002) and link detection (allan et al, 2000).
</nextsent>
<nextsent>this paper will focus on the link detection application.
</nextsent>
<nextsent>the tdt link detection aims to determine whether two stories discuss the same topic.
</nextsent>
<nextsent>each story could discuss one or more than one topic, and the sizes of two stories compared may not be so comparable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3865">
<title id=" C02-1078.xml">a probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show the effectiveness of our method by way of experiments.
</prevsent>
<prevsent>anaphora resolution is crucial in natural language processing (nlp), specifically, discourseanalysis.
</prevsent>
</prevsection>
<citsent citstr=" C96-1079 ">
in the case of english, partially motivated by message understanding conferences (mucs) (grishman and sundheim, 1996), <papid> C96-1079 </papid>number of coreference resolution methods have been proposed.</citsent>
<aftsection>
<nextsent>in other languages such as japanese and spanish, anaphoric expressions are often omitted.
</nextsent>
<nextsent>ellipses related to obligatory cases are usually termed zero pronouns.
</nextsent>
<nextsent>since zero pronouns are not expressed in discourse, they have to be detected prior to identifying their antecedents.
</nextsent>
<nextsent>thus, although in english pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing japanese zero pronouns is different from general coreference resolution in english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3866">
<title id=" C02-1078.xml">a probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, although in english pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing japanese zero pronouns is different from general coreference resolution in english.
</prevsent>
<prevsent>for identifying anaphoric relations, existing methods are classified into two fundamental ap proaches: rule-based and statistical approaches.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
in rule-based approaches (grosz et al, 1995; <papid> J95-2003 </papid>hobbs, 1978; mitkov et al, 1998; nakaiwa and shirai, 1996; <papid> C96-2137 </papid>okumura and tamura, 1996; palomar et al, 2001; walker et al, 1994), anaphoric relations between anaphors and their antecedents are identified by way of handcrafted rules, which typically relyon syntactic structures, gender/number agreement, and selectional restrictions.</citsent>
<aftsection>
<nextsent>however, it is difficult to produce rules exhaustively, and rules that are developed for specific language are not necessarily effective for other languages.
</nextsent>
<nextsent>for example, gender/number agreement in english can not be applied to japanese.
</nextsent>
<nextsent>statistical approaches (aone and bennett, 1995; ge et al, 1998; <papid> W98-1119 </papid>kim and ehara,1995; soon et al, 2001) use statistical models produced based on corpora annotated with anaphoric relations.</nextsent>
<nextsent>however, only few attempts have been made in corpus-basedanaphora resolution for japanese zero pro nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3867">
<title id=" C02-1078.xml">a probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, although in english pleonastic pronouns have to be determined whether or not they are anaphoric expressions prior to resolution, the process of analyzing japanese zero pronouns is different from general coreference resolution in english.
</prevsent>
<prevsent>for identifying anaphoric relations, existing methods are classified into two fundamental ap proaches: rule-based and statistical approaches.
</prevsent>
</prevsection>
<citsent citstr=" C96-2137 ">
in rule-based approaches (grosz et al, 1995; <papid> J95-2003 </papid>hobbs, 1978; mitkov et al, 1998; nakaiwa and shirai, 1996; <papid> C96-2137 </papid>okumura and tamura, 1996; palomar et al, 2001; walker et al, 1994), anaphoric relations between anaphors and their antecedents are identified by way of handcrafted rules, which typically relyon syntactic structures, gender/number agreement, and selectional restrictions.</citsent>
<aftsection>
<nextsent>however, it is difficult to produce rules exhaustively, and rules that are developed for specific language are not necessarily effective for other languages.
</nextsent>
<nextsent>for example, gender/number agreement in english can not be applied to japanese.
</nextsent>
<nextsent>statistical approaches (aone and bennett, 1995; ge et al, 1998; <papid> W98-1119 </papid>kim and ehara,1995; soon et al, 2001) use statistical models produced based on corpora annotated with anaphoric relations.</nextsent>
<nextsent>however, only few attempts have been made in corpus-basedanaphora resolution for japanese zero pro nouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3868">
<title id=" C02-1078.xml">a probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, it is difficult to produce rules exhaustively, and rules that are developed for specific language are not necessarily effective for other languages.
</prevsent>
<prevsent>for example, gender/number agreement in english can not be applied to japanese.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
statistical approaches (aone and bennett, 1995; ge et al, 1998; <papid> W98-1119 </papid>kim and ehara,1995; soon et al, 2001) use statistical models produced based on corpora annotated with anaphoric relations.</citsent>
<aftsection>
<nextsent>however, only few attempts have been made in corpus-basedanaphora resolution for japanese zero pronouns.
</nextsent>
<nextsent>one of the reasons is that it is costly to produce sufficient volume of training corpora annotated with anaphoric relations.
</nextsent>
<nextsent>in addition, those above methods focused mainly on identifying antecedents, and few attempts have been made to detect zero pronouns.
</nextsent>
<nextsent>motivated by the above background, we propose probabilistic model for analyzing japanese zero pronouns combined with detection method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3869">
<title id=" C02-1078.xml">a probabilistic method for analyzing japanese anaphora integrating zero pronoun detection and resolution </title>
<section> a system for analyzing japanese </section>
<citcontext>
<prevsection>
<prevsent>surface cases related to zero pronouns (c), for which possible values are japanese case marker suffixes, ga (nominative), wo (ac cusative), and ni (dative).
</prevsent>
<prevsent>those values indicate which cases are omitted.
</prevsent>
</prevsection>
<citsent citstr=" P86-1031 ">
features for antecedents ? post-positional particles (p), which play crucial roles in resolving japanese zero pronouns (kameyama, 1986; <papid> P86-1031 </papid>walker et al, 1994).</citsent>
<aftsection>
<nextsent>distance (d), which denotes the distance (proximity) between zero pronoun and an antecedent candidate in an input text.
</nextsent>
<nextsent>inthe case where they occur in the same sentence, its value takes 0.
</nextsent>
<nextsent>in the case wherean antecedent occurs in sentences previous to the sentence including zero pronoun, its value takes n. ? constraint related to relative clauses (r),which denotes whether an antecedent is included in relative clause or not.
</nextsent>
<nextsent>in the case where it is included, the value of takes true, otherwise false.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3870">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1 in particular, we predict the fine-grained category of named entity, previously recognized, by simply estimating the plausibility of sentences where the entity to be classified is substituted with the ones contained in the training data, in our case, partially populated ontology.
</prevsent>
<prevsent>in most of the cases, ontologies are partially populated during the development phase and after that the annotation cost is practically negligible, making this method highly attractive in many applicative domains.
</prevsent>
</prevsection>
<citsent citstr=" W02-0816 ">
this allows us to define an instance-based learning approach for fine-grained 1 lexical substitution consists in identifying the most likely alternatives (substitutes) of target word given its context (mccarthy, 2002).<papid> W02-0816 </papid></citsent>
<aftsection>
<nextsent>265entity categorization that exploits the web to collect evidence of the new entities and does not require any labeled text for supervision, only partially populated ontology.
</nextsent>
<nextsent>therefore, it can be used in different domains and languages to enrich an existing ontology with new entities extracted from texts by named-entity recognition system and/or databases.we evaluated our method on the benchmark proposed by tanev and magnini (2006) to provide fair comparison with other approaches, and on general purpose ontology of people derived from wordnet (fellbaum, 1998) to perform more extensive evaluation.
</nextsent>
<nextsent>specifically, the experiments were designed to investigate the effectiveness of our approach at different levels of generality and with different amounts of training data.
</nextsent>
<nextsent>the results show that it significantly outperforms the base line methods and, where comparison is possible, other approaches and achieves good performance with small number of examples per category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3871">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> lexical substitutability and ontology.  </section>
<citcontext>
<prevsection>
<prevsent>the snippets play crucial role in our approach be cause we expect that they provide the features that characterize the category to which the entity belongs.
</prevsent>
<prevsent>thus, it is important to collect sufficiently large number of snippets to capture the features that allow fine-grained classification.
</prevsent>
</prevsection>
<citsent citstr=" W07-2029 ">
to estimate the correctness of each substitution, we calculate plausibility score using modified version of the lexical substitution algorithm introduced in giuliano et al (2007), <papid> W07-2029 </papid>that assigns higher scores to the substitutions that generate highly frequent sentences on the web.</citsent>
<aftsection>
<nextsent>in particular, this technique ranks given list of synonyms according to similarity metric based on the occurrences in the web 1t 5-gram corpus, 2which specify n-grams frequencies in large web sample.this technique achieved the state-of-the-art performance on the english lexical substitution task at semeval 2007 (mccarthy and navigli, 2007).<papid> W07-2009 </papid></nextsent>
<nextsent>finally, on the basis of these plausibility scores,the algorithm assigns the new instance to the category whose individuals show closer linguistic behavior (i.e., they can be substituted generating plausible statements).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3872">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> lexical substitutability and ontology.  </section>
<citcontext>
<prevsection>
<prevsent>thus, it is important to collect sufficiently large number of snippets to capture the features that allow fine-grained classification.
</prevsent>
<prevsent>to estimate the correctness of each substitution, we calculate plausibility score using modified version of the lexical substitution algorithm introduced in giuliano et al (2007), <papid> W07-2029 </papid>that assigns higher scores to the substitutions that generate highly frequent sentences on the web.</prevsent>
</prevsection>
<citsent citstr=" W07-2009 ">
in particular, this technique ranks given list of synonyms according to similarity metric based on the occurrences in the web 1t 5-gram corpus, 2which specify n-grams frequencies in large web sample.this technique achieved the state-of-the-art performance on the english lexical substitution task at semeval 2007 (mccarthy and navigli, 2007).<papid> W07-2009 </papid></citsent>
<aftsection>
<nextsent>finally, on the basis of these plausibility scores,the algorithm assigns the new instance to the category whose individuals show closer linguistic behavior (i.e., they can be substituted generating plausible statements).
</nextsent>
<nextsent>in this section, we describe the algorithmic and mathematical details of our approach.
</nextsent>
<nextsent>theinstance-based ontology population (ibop) algorithm is an instance-based supervised machine learning approach.
</nextsent>
<nextsent>3 the proposed algorithm is summarized as follows: step 1 for each candidate instance i, we collect the first snippets containing from the web.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3874">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>this approach is similar to the one proposed by melamed and resnik (2000) for similar hierarchical categorization task.
</prevsent>
<prevsent>5.2 accuracy.
</prevsent>
</prevsection>
<citsent citstr=" D07-1026 ">
table 1 shows micro- and macro-averaged results of the proposed method obtained on the tanev and magnini (2006) benchmark and compares them with the class-example (tanev and magnini, 2006), ible (giuliano and gliozzo, 2007), <papid> D07-1026 </papid>and class-word (cimiano and volker, 2005) methods,respectively.</citsent>
<aftsection>
<nextsent>table 2 shows micro- and macro averaged results of the proposed method obtained on the people ontology and compares them with the random and most frequent baseline methods.
</nextsent>
<nextsent>7 in both experiments, the ibop algorithm was trained on 20 examples per category and setting the parameter ? = 0 in equation 2.for the people ontology, we performed disaggregated evaluation, whose results are shown in table 3, while figure 2 shows the learning curve.the experiment was conducted setting the parameter ? = 0.
</nextsent>
<nextsent>system micro-f 1 macro-f 1 ibop 73 71 class-example 68 62 ible 57 47 class-word 42 33table 1: comparison of different ontology population techniques on the tanev and magnini (2006) benchmark.
</nextsent>
<nextsent>7 the most frequent category has been estimated on the training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3875">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our approach differs from theirs in that we do not learn patterns.thus, we do not require ad hoc strategies for generating patterns and estimating their reliability, crucial issue in these approaches as bad?
</prevsent>
<prevsent>patterns may extract wrong seeds instances that in turn may generate even more inaccurate patterns in the following iteration.
</prevsent>
</prevsection>
<citsent citstr=" C02-1130 ">
fleischman and hovy (2002) <papid> C02-1130 </papid>approached the ontology population problem as supervised classification task.</citsent>
<aftsection>
<nextsent>they compare different machine learning algorithms, providing instances in their context as training examples as well as more global semantic information derived from topic signature and wordnet.
</nextsent>
<nextsent>alfonseca and manandhar (2002) and cimiano and volker (2005) present similar approaches relying on the harris?
</nextsent>
<nextsent>distributional hypothesis andthe vector-space model.
</nextsent>
<nextsent>they assign particular instance represented by certain context vector to the concept corresponding to the most similar vector.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3876">
<title id=" C08-1034.xml">instance based ontology population exploiting named entity substitution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they assign particular instance represented by certain context vector to the concept corresponding to the most similar vector.
</prevsent>
<prevsent>contexts are represented using lexical syntactic features.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
knowitall (etzioni et al, 2005) uses search engine and semantic patterns (similar to those defined by hearst (1992)) <papid> C92-2082 </papid>to classify named entities on the web.</citsent>
<aftsection>
<nextsent>the approach uses simple techniques from the ontology learning field to perform extraction and then annotation.
</nextsent>
<nextsent>it also is able to perform very simple pattern induction, consisting of looking at words before and words after the occurrence of an example in the document.
</nextsent>
<nextsent>with pattern learning, knowitall becomes boot strapped learning system, where rules are used to learn new seeds, which in turn are used to learn new rules.a similar approach is used in c-pankow (cimi ano et al, 2005).
</nextsent>
<nextsent>compared to knowitall andc-pankow, our approach does not need handcrafted patterns as input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3878">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing f-scores on wall street journal parsing, and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs.
</prevsent>
<prevsent>in order to obtain the meaning of sentence automatically, it is necessary to have access to its syntactic analysis at some level of complexity.many nlp applications like translation, question answering, etc. might benefit from the availability of syntactic parses.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
probabilistic parser strained over labeled data have high accuracy on in domain data: lexicalized parsers get an f-score of up to 90.0% on wall street journal data (charniakand johnson (2005)<papid> P05-1022 </papid>s re-ranking parser), while recently, un lexicalized pcfgs have also been shown to perform much better than previously believed (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>however, the limited size of annotated training data results in many parameters of pcfg being badly estimated when ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>trained on annotated data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3879">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even with relatively small quantities of unlabeled training data, the re-estimated models show promising improvements in labeled bracketing f-scores on wall street journal parsing, and substantial benefit in acquiring the subcategorization preferences of low-frequency verbs.
</prevsent>
<prevsent>in order to obtain the meaning of sentence automatically, it is necessary to have access to its syntactic analysis at some level of complexity.many nlp applications like translation, question answering, etc. might benefit from the availability of syntactic parses.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
probabilistic parser strained over labeled data have high accuracy on in domain data: lexicalized parsers get an f-score of up to 90.0% on wall street journal data (charniakand johnson (2005)<papid> P05-1022 </papid>s re-ranking parser), while recently, un lexicalized pcfgs have also been shown to perform much better than previously believed (klein and manning, 2003).<papid> P03-1054 </papid></citsent>
<aftsection>
<nextsent>however, the limited size of annotated training data results in many parameters of pcfg being badly estimated when ? 2008.
</nextsent>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>trained on annotated data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3880">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>trained on annotated data.
</prevsent>
<prevsent>the zipfian nature of text corpus results in pcfg parameters related to the properties of specific words being especially badly estimated.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for instance, about 38% of verbs in the training sections of the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>occur only once ? the lexical properties of these verbs (such as their most common subcategorization frames ) cannot be represented accurately in model trained exclusively on the penn treebank.</citsent>
<aftsection>
<nextsent>the research reported here addresses this issue.
</nextsent>
<nextsent>we start with an un lexicalized pcfg trained on the ptb.
</nextsent>
<nextsent>we then re-estimate the parameters of this pcfg from raw text using an unsupervised estimation method based on the inside-outside algorithm (lari and young, 1990), an instance ofthe expectation maximization algorithm (dempster et al, 1977) for pcfg induction.
</nextsent>
<nextsent>there estimation improves f-score on the standard test section of the ptb significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3882">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples of such properties are: subcategorization frames of verbs and nouns, attachment preference of adverbs to sentential, verbal or nominal nodes, attachment preference of pps to verbal or nominal node, etc.the current research is related to semi supervised training paradigms like self-training these methods are currently being explored to im prove the performance of existing pcfg models by utilizing unlabeled data.
</prevsent>
<prevsent>for example, mccloskey et al (2006) achieve 1.1% improvement in labeled bracketing f-score by the use of unlabeled data to self-train the parser-reranker system from charniak and johnson (2005).<papid> P05-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
earlier research on inside-outside estimation of pcfg models has reported some positive results as well 193 (pereira and schabes, 1992; <papid> P92-1017 </papid>carroll and rooth, 1998; beil et al, 1999; <papid> P99-1035 </papid>imwalde, 2002).</citsent>
<aftsection>
<nextsent>in some of these cases, an initial model is derived by other means ? inside-outside is used to re estimate the initial model.
</nextsent>
<nextsent>however, many questions still remain open about its efficacy for pcfg re-estimation.
</nextsent>
<nextsent>grammars used previously have not been treebank grammars (for e.g., carroll androoth (1998) and beil et al (1999) <papid> P99-1035 </papid>used handcrafted grammars), hence these models could not be evaluated according to standardized evaluations in the parsing literature.</nextsent>
<nextsent>in the current work, weuse penn treebank based grammar; hence all reestimated grammars can be evaluated using standardized criteria.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3883">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>examples of such properties are: subcategorization frames of verbs and nouns, attachment preference of adverbs to sentential, verbal or nominal nodes, attachment preference of pps to verbal or nominal node, etc.the current research is related to semi supervised training paradigms like self-training these methods are currently being explored to im prove the performance of existing pcfg models by utilizing unlabeled data.
</prevsent>
<prevsent>for example, mccloskey et al (2006) achieve 1.1% improvement in labeled bracketing f-score by the use of unlabeled data to self-train the parser-reranker system from charniak and johnson (2005).<papid> P05-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1035 ">
earlier research on inside-outside estimation of pcfg models has reported some positive results as well 193 (pereira and schabes, 1992; <papid> P92-1017 </papid>carroll and rooth, 1998; beil et al, 1999; <papid> P99-1035 </papid>imwalde, 2002).</citsent>
<aftsection>
<nextsent>in some of these cases, an initial model is derived by other means ? inside-outside is used to re estimate the initial model.
</nextsent>
<nextsent>however, many questions still remain open about its efficacy for pcfg re-estimation.
</nextsent>
<nextsent>grammars used previously have not been treebank grammars (for e.g., carroll androoth (1998) and beil et al (1999) <papid> P99-1035 </papid>used handcrafted grammars), hence these models could not be evaluated according to standardized evaluations in the parsing literature.</nextsent>
<nextsent>in the current work, weuse penn treebank based grammar; hence all reestimated grammars can be evaluated using standardized criteria.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3885">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> un lexicalized treebank pcfg.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present evaluations of there estimated models, based on labeled bracketing measures and on the detection of subcategorization frames of verbs: there is 31.6% reduction in error for novel verbs and up to 8.97% reduction in overall subcategorization error.
</prevsent>
<prevsent>we build an un lexicalized pcfg from the standard training sections of the ptb.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
as is common(collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003; <papid> P03-1054 </papid>schmid, 2006), the treebank is first transformed in various ways, in order to give an accurate pcfg.</citsent>
<aftsection>
<nextsent>in our framework, treebank trees are augmented with extra features; the methodology involves constructing feature-constraint grammar from context-free treebank backbone grammar.
</nextsent>
<nextsent>the detailed methodology is described in deoskar and rooth (2008)1.
</nextsent>
<nextsent>a pcfg is trained on the transformed treebank, with these added features incorporated into the pcfgs non-terminal categories.
</nextsent>
<nextsent>the framework affords us the flexibility to stipulate the features to be incorporated in the pcfg categories, as parameters of the pcfg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3886">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> un lexicalized treebank pcfg.  </section>
<citcontext>
<prevsection>
<prevsent>finally, we present evaluations of there estimated models, based on labeled bracketing measures and on the detection of subcategorization frames of verbs: there is 31.6% reduction in error for novel verbs and up to 8.97% reduction in overall subcategorization error.
</prevsent>
<prevsent>we build an un lexicalized pcfg from the standard training sections of the ptb.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
as is common(collins, 1997; <papid> P97-1003 </papid>johnson, 1998; <papid> J98-4004 </papid>klein and manning, 2003; <papid> P03-1054 </papid>schmid, 2006), the treebank is first transformed in various ways, in order to give an accurate pcfg.</citsent>
<aftsection>
<nextsent>in our framework, treebank trees are augmented with extra features; the methodology involves constructing feature-constraint grammar from context-free treebank backbone grammar.
</nextsent>
<nextsent>the detailed methodology is described in deoskar and rooth (2008)1.
</nextsent>
<nextsent>a pcfg is trained on the transformed treebank, with these added features incorporated into the pcfgs non-terminal categories.
</nextsent>
<nextsent>the framework affords us the flexibility to stipulate the features to be incorporated in the pcfg categories, as parameters of the pcfg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3891">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> inside-outside re-estimation.  </section>
<citcontext>
<prevsection>
<prevsent>194 quency models such as i, rather than the relative frequency probability models they determine4.
</prevsent>
<prevsent>e 1 = i(c, 0 ) ... i+1 = i(c, i ) (1) 3.1 interleaved inside-outside.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
it is well-known that while lexicalization is useful, lexical parameters determined from the treebank are poorly estimated because of the sparseness of treebank data for particular words (e.g. hindle and rooth (1993)).<papid> J93-1005 </papid></citsent>
<aftsection>
<nextsent>gildea (2001) <papid> W01-0521 </papid>andbikel (2004) show that removing bilexical dependencies hardly hurts the performance of the collins model2 parser, although there is the benefit of lexicalization in the form of lexico-syntactic dependencies ? structures being conditioned on words.</nextsent>
<nextsent>on the other hand, structural parameters are comparatively well-estimated from treebanks since they are not keyed to particular words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3892">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> inside-outside re-estimation.  </section>
<citcontext>
<prevsection>
<prevsent>e 1 = i(c, 0 ) ... i+1 = i(c, i ) (1) 3.1 interleaved inside-outside.
</prevsent>
<prevsent>it is well-known that while lexicalization is useful, lexical parameters determined from the treebank are poorly estimated because of the sparseness of treebank data for particular words (e.g. hindle and rooth (1993)).<papid> J93-1005 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0521 ">
gildea (2001) <papid> W01-0521 </papid>andbikel (2004) show that removing bilexical dependencies hardly hurts the performance of the collins model2 parser, although there is the benefit of lexicalization in the form of lexico-syntactic dependencies ? structures being conditioned on words.</citsent>
<aftsection>
<nextsent>on the other hand, structural parameters are comparatively well-estimated from treebanks since they are not keyed to particular words.
</nextsent>
<nextsent>thus,it might be beneficial to use combination of supervised and unsupervised estimation for lexical parameters, while obtaining syntactic (structural) parameters solely by supervised estimation (i.e. from treebank).
</nextsent>
<nextsent>the experiments in this paper are based on this idea.
</nextsent>
<nextsent>in an un lexicalised pcfg like the one described in 2, it is easy to makethe distinction between structural parameters (non terminal rules) and lexical parameters (preterminal to terminal rules).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3893">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> inside-outside re-estimation.  </section>
<citcontext>
<prevsection>
<prevsent>for each iteration i, ire present models obtained by inside-outside estimation.
</prevsent>
<prevsent>d represent derived models obtained by performing transformation on i . the transformation combines.
</prevsent>
</prevsection>
<citsent citstr=" C04-1024 ">
the re-estimated model iand the smoothed tree4we use frequency-based notation because we use out of-the-box software bitpar (schmid, 2004) <papid> C04-1024 </papid>which implements inside-outside estimation ? bitpar reads infrequency models and converts them to relative frequency models.</citsent>
<aftsection>
<nextsent>we justify the use of the frequency-based notation by ensuring that all marginal frequencies in the treebank model are always preserved in all other models.
</nextsent>
<nextsent>bank model (hence represented as (c , t)).
</nextsent>
<nextsent>d 0 = smoothed treebank model 1 = i(c, 0 ) estimation step 1 = (c 1 , t) transformation step ... i+1 = i(c, i ) estimation step i+1 = (c i+1 , t) transformation step (2) the lexical parameters for the treebank model or the re-estimated models i are represented as t(w, ?, ?) or i (w, ?, ?), where is the terminal word, ? is the ptb-style pos tag, and ? is the sequence of additional features incorporated into the pos tag (the entries in our lexicon have the form w.?.?
</nextsent>
<nextsent>with an associated frequency).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3895">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> verbal subcategorization.  </section>
<citcontext>
<prevsection>
<prevsent>we also get an improved overall error reduction of 9.9% on testset ii for the larger training data, as compared to 8.97% previously.
</prevsent>
<prevsent>5.3 previous work.
</prevsent>
</prevsection>
<citsent citstr=" P91-1027 ">
while there has been substantial previous work on the task of sf acquisition from corpora (brent (1991); <papid> P91-1027 </papid>manning (1993); <papid> P93-1032 </papid>briscoe and carroll (1997); <papid> A97-1052 </papid>korhonen (2002), amongst others), we find that relatively few parsing-based evaluations are reported.</citsent>
<aftsection>
<nextsent>since their goal is to build probabilistic sf dictionaries, these systems are evaluated either against existing dictionaries, or on distributional similarity measures.
</nextsent>
<nextsent>most are evaluated on test sets of high-frequency verbs (unlike the present work),in order to gauge the effectiveness of the acquisition strategy.
</nextsent>
<nextsent>briscoe and carroll (1997) <papid> A97-1052 </papid>report token-based evaluation for seven verb types?</nextsent>
<nextsent>their system gets an average recall accuracy of 80.9% for these verbs (which appear to be high-frequencyverbs).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3896">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> verbal subcategorization.  </section>
<citcontext>
<prevsection>
<prevsent>we also get an improved overall error reduction of 9.9% on testset ii for the larger training data, as compared to 8.97% previously.
</prevsent>
<prevsent>5.3 previous work.
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
while there has been substantial previous work on the task of sf acquisition from corpora (brent (1991); <papid> P91-1027 </papid>manning (1993); <papid> P93-1032 </papid>briscoe and carroll (1997); <papid> A97-1052 </papid>korhonen (2002), amongst others), we find that relatively few parsing-based evaluations are reported.</citsent>
<aftsection>
<nextsent>since their goal is to build probabilistic sf dictionaries, these systems are evaluated either against existing dictionaries, or on distributional similarity measures.
</nextsent>
<nextsent>most are evaluated on test sets of high-frequency verbs (unlike the present work),in order to gauge the effectiveness of the acquisition strategy.
</nextsent>
<nextsent>briscoe and carroll (1997) <papid> A97-1052 </papid>report token-based evaluation for seven verb types?</nextsent>
<nextsent>their system gets an average recall accuracy of 80.9% for these verbs (which appear to be high-frequencyverbs).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3897">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> verbal subcategorization.  </section>
<citcontext>
<prevsection>
<prevsent>we also get an improved overall error reduction of 9.9% on testset ii for the larger training data, as compared to 8.97% previously.
</prevsent>
<prevsent>5.3 previous work.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
while there has been substantial previous work on the task of sf acquisition from corpora (brent (1991); <papid> P91-1027 </papid>manning (1993); <papid> P93-1032 </papid>briscoe and carroll (1997); <papid> A97-1052 </papid>korhonen (2002), amongst others), we find that relatively few parsing-based evaluations are reported.</citsent>
<aftsection>
<nextsent>since their goal is to build probabilistic sf dictionaries, these systems are evaluated either against existing dictionaries, or on distributional similarity measures.
</nextsent>
<nextsent>most are evaluated on test sets of high-frequency verbs (unlike the present work),in order to gauge the effectiveness of the acquisition strategy.
</nextsent>
<nextsent>briscoe and carroll (1997) <papid> A97-1052 </papid>report token-based evaluation for seven verb types?</nextsent>
<nextsent>their system gets an average recall accuracy of 80.9% for these verbs (which appear to be high-frequencyverbs).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3900">
<title id=" C08-1025.xml">reestimation of lexical parameters for treebank pcfgs </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>for instance, more weight should be given to the treebank model rather than the corpus model formid-to-high frequency words, by making the parameter ? insensitive to frequency.
</prevsent>
<prevsent>this methodology is relevant to the task ofdomain-adaption.
</prevsent>
</prevsection>
<citsent citstr=" W07-2202 ">
hara et al (2007) <papid> W07-2202 </papid>find that retraining model of hpsg lexical entry assignments is more critical for domain adaptation than re-training structural model alone.</citsent>
<aftsection>
<nextsent>our pcfg captures many of the important dependencies captured in framework like hpsg; in addition, we can use unlabeled data from new domain in an unsupervised fashion for re-estimating lexical parameters, an important consideration in domainadaption.
</nextsent>
<nextsent>preliminary experiments on this task us 199ing new york times unlabeled data with the ptb trained pcfg show promising results.
</nextsent>
<nextsent>acknowledgmentsi am grateful to mats rooth for extensive comments and guidance during the course of this research.
</nextsent>
<nextsent>the inside-outside re-estimation was conducted using the resources of the cornell university center for advanced computing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3901">
<title id=" C08-1078.xml">investigating statistical techniques for sentence level event classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for complex questions such as how many people were killed in baghdad in march?, qa systems often rely on event detection systems to identify all relevant events in set of documents before formulating an answer.
</prevsent>
<prevsent>more recently, much research in summarisation has focused on the use of phrasal concepts such as events to represent sentences in extractive summarisation systems.
</prevsent>
</prevsection>
<citsent citstr=" W04-1017 ">
specifically,(filatova and hatzivassiloglou, 2004) <papid> W04-1017 </papid>use event based features to represent sentences and shows that this approach improves the quality of the final summaries when compared with baseline bag-of words approach.</citsent>
<aftsection>
<nextsent>in this paper, we investigate the use of statistical methods for identifying the sentences in document that describe one or more instances of specified event type.
</nextsent>
<nextsent>we treat this task as text classification problem where each sentence in given document is either classified as containing an instance of the target event or not.
</nextsent>
<nextsent>we view this task as filtering step in larger pipeline nlp architecture (e.g. qa system) which helps speed up subsequent processing by removing irrelevant, non-event sentences.
</nextsent>
<nextsent>two event detection approaches are explored in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3902">
<title id=" C08-1078.xml">investigating statistical techniques for sentence level event classification </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>even though the task of identifying event mentions is not directly evaluated in ace, systems still need to identify them so that the various attributes and participants within the mention can be extracted.
</prevsent>
<prevsent>the algorithms evaluated in this paper can also be applied to the detection of event mentions that contain the ace events.
</prevsent>
</prevsection>
<citsent citstr=" W06-0901 ">
overall five sites participated in this task in 2005.the most similar work to that describe in this paper is detailed in (ahn, 2006), <papid> W06-0901 </papid>who treats the task of finding all event triggers (used to identify each event) as word classification task where the taskis to classify every term in document with label defined by 34 classes.</citsent>
<aftsection>
<nextsent>features used included various lexical, wordnet, dependency and related entity features.
</nextsent>
<nextsent>the ace 2005 multilingual corpus was annotated for entities, relations and events.
</nextsent>
<nextsent>it consists of articles originating from six difference sources including newswire (20%), broadcast news (20%), broadcast conversation (15%), weblog (15%), 2 an ace entity is an entity identified using guidelines outlined by ace entity detection and recognition task.
</nextsent>
<nextsent>usenet news groups (15%) and conversational telephone speech (15%).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3903">
<title id=" C08-1078.xml">investigating statistical techniques for sentence level event classification </title>
<section> event detection as classification.  </section>
<citcontext>
<prevsection>
<prevsent>with frequency greater than 2 in the training data were also used as feature.
</prevsent>
<prevsent>lexical information: the presence or absence of each part of speech (pos) tag and chunk tag was used as feature.
</prevsent>
</prevsection>
<citsent citstr=" P07-2009 ">
we use the maximum entropy pos tagger and chunker that are available with thec&c; toolkit (curran et al, 2007).<papid> P07-2009 </papid></citsent>
<aftsection>
<nextsent>the pos tagger uses the standard set of grammatical categories from the penn treebank and the chunker recognises the standard set of grammatical chunk tags: np, vp, pp, adjp, advp and so on.
</nextsent>
<nextsent>additional features: we added the following additional features to the feature vector: sentence length, sentence position, presence/absence of negative terms (e.g. no, not, didnt, dont, isnt, hasnt), presence/absence of modal terms (e.g.may, might, shall, should, must, will), look ahead feature that indicates whether the next sentence is an event sentence, look-back feature indicating whether or not the previous sentence is an event sentence and the presence/absence of atime-stamp.
</nextsent>
<nextsent>time-stamps were identified using inhouse software developed by the language technology group at the university of melbourne 3 . in the past, feature selection methods have been found to have positive effect on classification accuracy of text classification tasks.
</nextsent>
<nextsent>to examine the effects of such techniques on this task, we use information gain (ig) to reduce the number of features used by the classifier by factor of 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3904">
<title id=" C04-1165.xml">construction of an objective hierarchy of abstract concepts via directional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the relative proximity of words in the semantic map indicates their relative similarity.
</prevsent>
<prevsent>in previous research, word meanings have been statistically modeled based on syntactic information derived from corpus.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
hindle (1990) <papid> P90-1034 </papid>used noun-verb syntactic relations, and hatzivassiloglou and mckeown (1993) <papid> P93-1023 </papid>used coordinated adjective-adjective modifier pairs.</citsent>
<aftsection>
<nextsent>these methods are useful for the organization of words deep within hierarchy, but do not seem to provide solution for the top levels of the hierarchy.
</nextsent>
<nextsent>to find an objective hierarchical word structure, we utilize the complementary similarity measure (csm), which estimates one-to-many relation, such as superordinatesubordinate relations (hagita and sawaki 1995, yamamoto and umemura 2002).
</nextsent>
<nextsent>in this paper we propose an automated method for constructing adjective hierarchies by connecting strongly related abstract nouns in top-down fashion within semantic map, mainly using csm.
</nextsent>
<nextsent>we compare three hierarchical organizations of abstract nouns, according to csm, frequency (tf.csm) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3905">
<title id=" C04-1165.xml">construction of an objective hierarchy of abstract concepts via directional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the relative proximity of words in the semantic map indicates their relative similarity.
</prevsent>
<prevsent>in previous research, word meanings have been statistically modeled based on syntactic information derived from corpus.
</prevsent>
</prevsection>
<citsent citstr=" P93-1023 ">
hindle (1990) <papid> P90-1034 </papid>used noun-verb syntactic relations, and hatzivassiloglou and mckeown (1993) <papid> P93-1023 </papid>used coordinated adjective-adjective modifier pairs.</citsent>
<aftsection>
<nextsent>these methods are useful for the organization of words deep within hierarchy, but do not seem to provide solution for the top levels of the hierarchy.
</nextsent>
<nextsent>to find an objective hierarchical word structure, we utilize the complementary similarity measure (csm), which estimates one-to-many relation, such as superordinatesubordinate relations (hagita and sawaki 1995, yamamoto and umemura 2002).
</nextsent>
<nextsent>in this paper we propose an automated method for constructing adjective hierarchies by connecting strongly related abstract nouns in top-down fashion within semantic map, mainly using csm.
</nextsent>
<nextsent>we compare three hierarchical organizations of abstract nouns, according to csm, frequency (tf.csm) and an alternative similarity measure based on coefficient overlap, to estimate hyperonym relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3906">
<title id=" C04-1165.xml">construction of an objective hierarchy of abstract concepts via directional similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as kind of meaning repetition, or tautology.
</prevsent>
<prevsent>in this paper we define such abstract nouns that co-occur with adjectives as adjective hy peronyms.
</prevsent>
</prevsection>
<citsent citstr=" W00-0110 ">
we semi-automatically extracted from corpora 365 abstract nouns used as this kind of head noun, according to the procedures described in kanzaki et al (2000).<papid> W00-0110 </papid></citsent>
<aftsection>
<nextsent>we collected abstract nouns from two year worth of articles from the mainichi shinbun newspaper, and extracted adjectives co-occurring with abstract nouns in the manner of (a) above from 100 novels, 100 essays and 42 year worth of newspaper articles, including 11 year worth of mainichi shinbun articles, 10 year worth of nihon keizai shinbun (japanese economic newspaper) articles, 7 year worth of sangyoukinyuuryuutsu shinbun (an economic newspaper) articles, and 14 year worth of yomiuri shinbun articles.
</nextsent>
<nextsent>the total number of abstract noun types is 365, the number of adjective types is 10,525, and the total number of adjective tokens is 35,173.
</nextsent>
<nextsent>the maximum number of co-occurring adjectives forgiven abstract noun is 1,594.
</nextsent>
<nextsent>3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3907">
<title id=" C08-1062.xml">pnr2 ranking sentences with positive and negative reinforcement for query oriented update summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it should be noting that both (2) and (4) need to consider the influence from the sentences in the same and different collections.
</prevsent>
<prevsent>in this study, we made an attempt to capture the intuition that sentence receives positive influence from the sentences that correlate to it in the same collection, whereas sentence receives negative influence from the sentences that correlates to it in the different collection.?
</prevsent>
</prevsection>
<citsent citstr=" W04-3247 ">
we represent the sentences in or as text graph constructed using the same approach as was used in erkan and radev (2004<papid> W04-3247 </papid>a), erkan and radev (2004<papid> W04-3247 </papid>b).</citsent>
<aftsection>
<nextsent>different from the existing pagerank-like algorithms adopted in document summarization, we propose novel sentence ranking algorithm, called pnr2 (ranking with positive and negative reinforcement).
</nextsent>
<nextsent>while page rank models the positive mutual reinforcement among the sentences in the graph, pnr2 is capable of modeling both positive and negative reinforcement in the ranking process.
</nextsent>
<nextsent>the remainder of this paper is organized as follows.
</nextsent>
<nextsent>section 2 introduces the background of the work presented in this paper, including existing graph-based summarization models, descriptions of update summarization and time based ranking solutions with web graph and text graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3916">
<title id=" C08-1062.xml">pnr2 ranking sentences with positive and negative reinforcement for query oriented update summarization </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>erkan and radev (2004<papid> W04-3247 </papid>a and 2004b) represented the documents as weighted undirected graph by taking sentences as vertices and cosine similarity between sentences as the edge weight function.</prevsent>
<prevsent>an algorithm called lexrank, adapted from page rank, was applied to calculate sentence significance, which was then used as the criterion to rank and select summary sentences.</prevsent>
</prevsection>
<citsent citstr=" W04-3252 ">
meanwhile, mihalcea and tarau (2004) <papid> W04-3252 </papid>presented their page rank variation, called text rank, in the same year.</citsent>
<aftsection>
<nextsent>besides, they reported experimental comparison of three different graph-based sentence ranking algorithms obtained from positional power function, hits and page rank (mihalcea and tarau, 2005).
</nextsent>
<nextsent>both hits and page rank performed excellently.
</nextsent>
<nextsent>likewise, the use of page rank family was also very popular in event-based summarization approaches (leskovec et al, 2004; vanderwende et al, 2004; yoshioka and haraguchi, 2004; li et al., 2006).<papid> P06-1047 </papid></nextsent>
<nextsent>in contrast to conventional sentence based approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3917">
<title id=" C08-1062.xml">pnr2 ranking sentences with positive and negative reinforcement for query oriented update summarization </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>besides, they reported experimental comparison of three different graph-based sentence ranking algorithms obtained from positional power function, hits and page rank (mihalcea and tarau, 2005).
</prevsent>
<prevsent>both hits and page rank performed excellently.
</prevsent>
</prevsection>
<citsent citstr=" P06-1047 ">
likewise, the use of page rank family was also very popular in event-based summarization approaches (leskovec et al, 2004; vanderwende et al, 2004; yoshioka and haraguchi, 2004; li et al., 2006).<papid> P06-1047 </papid></citsent>
<aftsection>
<nextsent>in contrast to conventional sentence based approaches, newly emerged event-based approaches took event terms, such as verbs and action nouns and their associated named entities as graph nodes, and connected nodes according to their co-occurrence information or semantic dependency relations.
</nextsent>
<nextsent>they were able to provide finer text representation and thus could be in favor of sentence compression which was targeted to include more informative contents in fixed-length summary.
</nextsent>
<nextsent>nevertheless, these advantages lied on appropriately defining and selecting event terms.
</nextsent>
<nextsent>all above-mentioned representative work was concerned with generic summarization.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3918">
<title id=" C08-1062.xml">pnr2 ranking sentences with positive and negative reinforcement for query oriented update summarization </title>
<section> background and related work.  </section>
<citcontext>
<prevsection>
<prevsent>all above-mentioned representative work was concerned with generic summarization.
</prevsent>
<prevsent>later on, graph-based ranking algorithms were introduced in query-oriented summarization too when this new challenge became hot research topic recently.
</prevsent>
</prevsection>
<citsent citstr=" H05-1115 ">
for example, topic-sensitive version of page rank was proposed in (otterbacher et al, 2005).<papid> H05-1115 </papid></citsent>
<aftsection>
<nextsent>the same algorithm was followed by wan et al (2006) and lin et al (2007) who further investigated on its application in query-oriented update summarization.
</nextsent>
<nextsent>490 2.2 the duc 2007 update summarization.
</nextsent>
<nextsent>task description the duc 2007 update summarization pilot task is to create short (100 words) multi-document summaries under the assumption that the reader has already read some number of previous documents.
</nextsent>
<nextsent>each of 10 topics contains 25 documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3920">
<title id=" C08-1062.xml">pnr2 ranking sentences with positive and negative reinforcement for query oriented update summarization </title>
<section> experimental studies.  </section>
<citcontext>
<prevsection>
<prevsent>basic statistics of duc2007 update dataset as for the evaluation metric, it is difficult to come up with universally accepted method that can measure the quality of machine-generated summaries accurately and effectively.
</prevsent>
<prevsent>many literatures have addressed different methods for automatic evaluations other than human judges.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
among them, rouge5 (lin and hovy, 2003) <papid> N03-1020 </papid>is supposed to produce the most reliable scores in correspondence with human evaluations.</citsent>
<aftsection>
<nextsent>given the fact that judgments by humans are time consuming and labor-intensive, and more important, rouge has been officially adopted for the duc evaluations since 2005, like the other researchers, we also choose it as the evaluation criteria.
</nextsent>
<nextsent>in the following experiments, the sentences and the queries are all represented as the vectors of words.
</nextsent>
<nextsent>the relevance of sentence to the query is calculated by cosine similarity.
</nextsent>
<nextsent>notice that the word weights are normally measured by the document-level tf*idf scheme in conventional vector space models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3923">
<title id=" C02-2025.xml">the lingo redwoods treebank motivation and preliminary applications </title>
<section> a rich and dynamic treebank </section>
<citcontext>
<prevsection>
<prevsent>instead, the treebank records complete syntactosemantic analyses as defined by the lingo erg and provide tools to extract different types of linguistic information at varying granularity.
</prevsent>
<prevsent>the tree banking environment, building on the [incr tsdb()] profiling environment (oepen &amp; callmeier, 2000), presents annotators, one sentence at time, with the full set of analyses produced by the grammar.
</prevsent>
</prevsection>
<citsent citstr=" W97-1502 ">
using pre-existing tree comparison tool in the lkb (similar in kind to the sri cambridge treebanker; carter, 1997),<papid> W97-1502 </papid>annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar).</citsent>
<aftsection>
<nextsent>the tree selection tool presents users, who need little expert knowledge of the underlying grammar, with range of basic properties that distinguish competing analyses and that are relatively easy to judge.
</nextsent>
<nextsent>all disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic propagation into more recent profile obtained from re-running newer version of the grammar on the same corpus.
</nextsent>
<nextsent>important innovative research aspects in this approach to tree banking are (i) enabling users of the treebank to extract information of the type they need and to transform the available representation into form suited to their needs and (ii) the ability to update the treebank with an enhanced version of the grammar in an automated fashion, viz.
</nextsent>
<nextsent>by re-applying the disambiguating decisions on the corpus with an updated version of the grammar.depth of representation and transformation of information internally, the [incr tsdb()] database records analyses in three different formats, viz.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3924">
<title id=" C02-2025.xml">the lingo redwoods treebank motivation and preliminary applications </title>
<section> early experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>these lexical types ? the pre terminals in the derivation ? are essentially part-of-speech tags, but encode considerably finer-grained information about thewords.
</prevsent>
<prevsent>well-understood statistical part-of-speech tagging technology is sufficient for this approach.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
in order to use more information about the parse, we might examine the entire derivation of the string.most probabilistic parsing research ? including, forex ample, work by by collins (1997), <papid> P97-1003 </papid>and charniak (1997) ? is based on branching process models (harris, 1963).</citsent>
<aftsection>
<nextsent>the hpsg derivations that the treebank makes available can be viewed as just such branching process, anda stochastic model of the trees can be built as probabilistic context-free grammar (pcfg) model.
</nextsent>
<nextsent>abney (1997) <papid> J97-4005 </papid>notes important problems with the soundness ofthe approach when unification-based grammar is actually determining the derivations, motivating the use of log-linear models (agresti, 1990) for parse ranking that johnson and colleagues further developed (johnson, geman, canon, chi, &amp; riezler, 1999).</nextsent>
<nextsent>these models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax.nevertheless, the naive pcfg approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on buildinglog-linear models over hpsg signs (toutanova &amp; manning, 2002)).<papid> W02-2030 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3925">
<title id=" C02-2025.xml">the lingo redwoods treebank motivation and preliminary applications </title>
<section> early experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in order to use more information about the parse, we might examine the entire derivation of the string.most probabilistic parsing research ? including, forex ample, work by by collins (1997), <papid> P97-1003 </papid>and charniak (1997) ? is based on branching process models (harris, 1963).</prevsent>
<prevsent>the hpsg derivations that the treebank makes available can be viewed as just such branching process, anda stochastic model of the trees can be built as probabilistic context-free grammar (pcfg) model.</prevsent>
</prevsection>
<citsent citstr=" J97-4005 ">
abney (1997) <papid> J97-4005 </papid>notes important problems with the soundness ofthe approach when unification-based grammar is actually determining the derivations, motivating the use of log-linear models (agresti, 1990) for parse ranking that johnson and colleagues further developed (johnson, geman, canon, chi, &amp; riezler, 1999).</citsent>
<aftsection>
<nextsent>these models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax.nevertheless, the naive pcfg approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on buildinglog-linear models over hpsg signs (toutanova &amp; manning, 2002)).<papid> W02-2030 </papid></nextsent>
<nextsent>the learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3926">
<title id=" C02-2025.xml">the lingo redwoods treebank motivation and preliminary applications </title>
<section> early experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the hpsg derivations that the treebank makes available can be viewed as just such branching process, anda stochastic model of the trees can be built as probabilistic context-free grammar (pcfg) model.
</prevsent>
<prevsent>abney (1997) <papid> J97-4005 </papid>notes important problems with the soundness ofthe approach when unification-based grammar is actually determining the derivations, motivating the use of log-linear models (agresti, 1990) for parse ranking that johnson and colleagues further developed (johnson, geman, canon, chi, &amp; riezler, 1999).</prevsent>
</prevsection>
<citsent citstr=" W02-2030 ">
these models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax.nevertheless, the naive pcfg approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on buildinglog-linear models over hpsg signs (toutanova &amp; manning, 2002)).<papid> W02-2030 </papid></citsent>
<aftsection>
<nextsent>the learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them.
</nextsent>
<nextsent>we report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model.
</nextsent>
<nextsent>(we restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.)
</nextsent>
<nextsent>we examine four models: an hmm tagging model, simple pcfg, pcfg with grandparent annotation, and hybrid model that combines predictions from the pcfg and the tagger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3927">
<title id=" C02-2025.xml">the lingo redwoods treebank motivation and preliminary applications </title>
<section> early experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we can see that the parse ranking accuracy is decreasing quickly and more powerful models will be needed to achieve good accuracy for highly ambiguous sentences.despite several differences in corpus size and compo analyses sentences random combined ? 2 3824 25.81% 74.03% ? 5 1789 9.66% 59.64% ? 10 1027 5.33% 51.61% ? 20 525 3.03% 45.33% table 3: parse ranking accuracy by number of possible parses.
</prevsent>
<prevsent>sition, it is perhaps nevertheless useful to compare this work with other work on parse selection for unification based grammars.
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
johnson et al (1999) <papid> P99-1069 </papid>estimate stochastic unification based grammar (subg) using log-linear model.</citsent>
<aftsection>
<nextsent>the features they include in the model are not limited to production rule features but also adjunct and argument and other linguistically motivated features.
</nextsent>
<nextsent>on dataset of 540 sentences (total training and test set) from verb mobil corpus they report parse disambiguation accuracy of 58.7% given baseline accuracy for choosing at random of 9.7%.
</nextsent>
<nextsent>the random base line is much lower than ours for the full dataset, but it is comparable for the random baseline for sentences with more than 5 analyses.
</nextsent>
<nextsent>the accuracy of our combined model for these sentences is 59.64%, so the accuracies of the two models seem fairly similar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3928">
<title id=" C04-1131.xml">word sense disambiguation criteria a systematic study </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>wsd is usually performed by matching information from the context in which the word occurs with disambiguation knowledge source.
</prevsent>
<prevsent>our approach uses supervised machine-learning techniques to automatically acquire such disambiguation knowledge from sense-tagged corpora.
</prevsent>
</prevsection>
<citsent citstr=" W97-0323 ">
at present, this type of approach is widely used and seems to yield the best results (kilgarriff, rosenzweig, 2000; ng, 1997<papid> W97-0323 </papid>b).</citsent>
<aftsection>
<nextsent>information conveyed by context words (morphological form) is enriched with further annotations: part-of-speech tag, lemma, etc. each individual piece of information is called feature.
</nextsent>
<nextsent>a good feature should capture an important source of knowledge that is critical in determining the sense of the word to be disambiguated.
</nextsent>
<nextsent>the choice of the appropriate set of features is an important issue for wsd (bruce, wiebe, perdersen, 1996; ng, zelle, 1997; pedersen, 2001).
</nextsent>
<nextsent>thus, this paper describes the results of systematic and in-depth study of homogenous criteria (i.e. set of features) that can be used for wsd.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3932">
<title id=" C04-1131.xml">word sense disambiguation criteria a systematic study </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 classifiers.
</prevsent>
<prevsent>we have selected two complementary classifiers.
</prevsent>
</prevsection>
<citsent citstr=" W96-0208 ">
we have chosen the nave-bayes classifier (nb) for its simplicity and widespread use, as well as for its well-known state-of-the-art accuracy on supervised wsd (domingos, pazzani, 1997; mooney, 1996; <papid> W96-0208 </papid>ng, 1997<papid> W97-0323 </papid>a).</citsent>
<aftsection>
<nextsent>the nb classifier assumes the features are independent given the sense.
</nextsent>
<nextsent>during classification, it chooses the sense with the highest posterior probability.
</nextsent>
<nextsent>we have also selected decision list classifier (dl) which is similar to the classifier used by (yarowsky, 1994) for words having two senses, and extended for more senses by (golding, 1995).<papid> W95-0104 </papid></nextsent>
<nextsent>dl classifier is further developed in (audibert, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3938">
<title id=" C04-1131.xml">word sense disambiguation criteria a systematic study </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the nb classifier assumes the features are independent given the sense.
</prevsent>
<prevsent>during classification, it chooses the sense with the highest posterior probability.
</prevsent>
</prevsection>
<citsent citstr=" W95-0104 ">
we have also selected decision list classifier (dl) which is similar to the classifier used by (yarowsky, 1994) for words having two senses, and extended for more senses by (golding, 1995).<papid> W95-0104 </papid></citsent>
<aftsection>
<nextsent>dl classifier is further developed in (audibert, 2003).
</nextsent>
<nextsent>in dl, features are sorted in order of decreasing strength, where strength reflects feature reliability for decision-making.
</nextsent>
<nextsent>the dl classifier distinguishes itself clearly from the nb classifier as it does not combine the features, but bases its classifications solely on the single most reliable feature identified in the target context selected by the criteria.
</nextsent>
<nextsent>we will make use of this decision-making transparency several times in this article.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3941">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" P02-1021 ">
the task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (chang and schutze, 2006).furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (pakhomov,2002; <papid> P02-1021 </papid>gaudan et al, 2005; yu et al, 2006), providing sense inventories (lists of abbreviation def initions), training corpora (context information of full forms), and local definitions of abbreviations.</citsent>
<aftsection>
<nextsent>hence, abbreviation recognition plays key role in abbreviation management.
</nextsent>
<nextsent>numerous researchers have proposed variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (park and byrd,2001; <papid> W01-0516 </papid>wren and garner, 2002; liu and friedman, 2003; okazaki and ananiadou, 2006; zhou et al, 2006; jain et al, 2007).</nextsent>
<nextsent>schwartz and hearst (2003) implemented simple algorithm that finds the shortest expression containing all alpha numerical letters of an abbreviation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3942">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of abbreviation recognition, in which abbreviations and their expanded forms appearing in actual text are extracted, addresses the term variation problem caused by the increase in the number of abbreviations (chang and schutze, 2006).furthermore, abbreviation recognition is also crucial for disambiguating abbreviations (pakhomov,2002; <papid> P02-1021 </papid>gaudan et al, 2005; yu et al, 2006), providing sense inventories (lists of abbreviation def initions), training corpora (context information of full forms), and local definitions of abbreviations.</prevsent>
<prevsent>hence, abbreviation recognition plays key role in abbreviation management.</prevsent>
</prevsection>
<citsent citstr=" W01-0516 ">
numerous researchers have proposed variety of heuristics for recognizing abbreviation definitions, e.g., the use of initials, capitalizations, syllable boundaries, stop words, lengths of abbreviations, and co-occurrence statistics (park and byrd,2001; <papid> W01-0516 </papid>wren and garner, 2002; liu and friedman, 2003; okazaki and ananiadou, 2006; zhou et al, 2006; jain et al, 2007).</citsent>
<aftsection>
<nextsent>schwartz and hearst (2003) implemented simple algorithm that finds the shortest expression containing all alpha numerical letters of an abbreviation.
</nextsent>
<nextsent>adar (2004) presented four scoring rules to choose the most likely expanded form in multiple candidates.
</nextsent>
<nextsent>aoand takagi (2005) designed more detailed conditions for accepting or discarding candidates of abbreviation definitions.however, these studies have limitations in discovering an optimal combination of heuristic rules from manual observations of corpus.
</nextsent>
<nextsent>for example, when expressions tran scrip : tion factor 1 and thyroid transcription factor 1 are full-form candidates for the abbreviation ttf-1 1 , an algorithm should choose the latter expression over the shorter 1 in this paper, we use straight and ::::wavy underlines to represent correct and incorrect origins of abbreviation letters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3943">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this was probably because the training corpus did not include annotations on the exact origins of abbreviation letters but only pairs of abbreviations and fullforms.
</prevsent>
<prevsent>it was thus difficult to design effective features for abbreviation recognition and to reuse the knowledge obtained from the training processes.in this paper, we formalize the task of abbreviation recognition as sequential alignment problem, which finds the optimal alignment (origins of abbreviation letters) between two strings (abbrevi ation and full form).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
we design large amount of features that directly express the events where letters produce or do not produce abbreviations.preparing an aligned abbreviation corpus, we obtain the optimal combination of the features by using the maximum entropy framework (berger etal., 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>we report the remarkable improvements and conclude this paper.
</nextsent>
<nextsent>2.1 abbreviation alignment model.
</nextsent>
<nextsent>we express sentence as sequence of letters (x 1 , ..., l ), and an abbreviation candidate in the sentence as sequence of letters (y 1 , ..., m ).
</nextsent>
<nextsent>we define letter mapping = (i, j) to indicate that the abbreviation letter j is produced by the letter in the full form i. null mapping = (i, 0) indicates that the letter in the sentence i is unused to form the abbreviation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3945">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 alignment candidates.
</prevsent>
<prevsent>formula 1 requires sum over the possible alignments, which amounts to 2 lm for sentence (l letters) with an abbreviation (m letters).
</prevsent>
</prevsection>
<citsent citstr=" P06-1009 ">
it is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (mccallum et al., 2005; blunsom and cohn, 2006; <papid> P06-1009 </papid>shimbo and hara, 2007) <papid> D07-1064 </papid>or approximated by the n-best list of highly probable alignments (och and ney, 2002;<papid> P02-1038 </papid>liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 in table 2, set of curly brackets {} denotes list (array) rather than mathematical set.
</nextsent>
<nextsent>operators ? and ? present concatenation and cartesian product of lists.
</nextsent>
<nextsent>for instance, when = {a, b} and = {c, d}, ab = {a, b, c, d} and ab = {ac, ad, bc, bd}.
</nextsent>
<nextsent>660 investigate the effect of thyroid transcription factor 1 0 0 0 00 0 0 0 0 0 0 0 00 0 0 00 0 0 0 1 2 3 0 50 0 0 00 0 0 1 2 0 3 0 50 0 0 00 0 0 1 0 2 3 0 50 0 0 00 0 0 2 1 0 3 0 50 0 0 00 0 0 2 0 1 3 0 50 0 0 00 0 3 0 0 1 0 2 50 0 0 00 0 3 0 1 2 0 0 50 0 0 00 0 3 0 1 0 0 2 5 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3946">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 alignment candidates.
</prevsent>
<prevsent>formula 1 requires sum over the possible alignments, which amounts to 2 lm for sentence (l letters) with an abbreviation (m letters).
</prevsent>
</prevsection>
<citsent citstr=" D07-1064 ">
it is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (mccallum et al., 2005; blunsom and cohn, 2006; <papid> P06-1009 </papid>shimbo and hara, 2007) <papid> D07-1064 </papid>or approximated by the n-best list of highly probable alignments (och and ney, 2002;<papid> P02-1038 </papid>liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 in table 2, set of curly brackets {} denotes list (array) rather than mathematical set.
</nextsent>
<nextsent>operators ? and ? present concatenation and cartesian product of lists.
</nextsent>
<nextsent>for instance, when = {a, b} and = {c, d}, ab = {a, b, c, d} and ab = {ac, ad, bc, bd}.
</nextsent>
<nextsent>660 investigate the effect of thyroid transcription factor 1 0 0 0 00 0 0 0 0 0 0 0 00 0 0 00 0 0 0 1 2 3 0 50 0 0 00 0 0 1 2 0 3 0 50 0 0 00 0 0 1 0 2 3 0 50 0 0 00 0 0 2 1 0 3 0 50 0 0 00 0 0 2 0 1 3 0 50 0 0 00 0 3 0 0 1 0 2 50 0 0 00 0 3 0 1 2 0 0 50 0 0 00 0 3 0 1 0 0 2 5 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3947">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 alignment candidates.
</prevsent>
<prevsent>formula 1 requires sum over the possible alignments, which amounts to 2 lm for sentence (l letters) with an abbreviation (m letters).
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
it is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (mccallum et al., 2005; blunsom and cohn, 2006; <papid> P06-1009 </papid>shimbo and hara, 2007) <papid> D07-1064 </papid>or approximated by the n-best list of highly probable alignments (och and ney, 2002;<papid> P02-1038 </papid>liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 in table 2, set of curly brackets {} denotes list (array) rather than mathematical set.
</nextsent>
<nextsent>operators ? and ? present concatenation and cartesian product of lists.
</nextsent>
<nextsent>for instance, when = {a, b} and = {c, d}, ab = {a, b, c, d} and ab = {ac, ad, bc, bd}.
</nextsent>
<nextsent>660 investigate the effect of thyroid transcription factor 1 0 0 0 00 0 0 0 0 0 0 0 00 0 0 00 0 0 0 1 2 3 0 50 0 0 00 0 0 1 2 0 3 0 50 0 0 00 0 0 1 0 2 3 0 50 0 0 00 0 0 2 1 0 3 0 50 0 0 00 0 0 2 0 1 3 0 50 0 0 00 0 3 0 0 1 0 2 50 0 0 00 0 3 0 1 2 0 0 50 0 0 00 0 3 0 1 0 0 2 5 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3948">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> proposed method.  </section>
<citcontext>
<prevsection>
<prevsent>2.3 alignment candidates.
</prevsent>
<prevsent>formula 1 requires sum over the possible alignments, which amounts to 2 lm for sentence (l letters) with an abbreviation (m letters).
</prevsent>
</prevsection>
<citsent citstr=" P05-1057 ">
it is unrealistic to compute the partition factor of the formula directly; therefore, the factor has been computed by dynamic programing (mccallum et al., 2005; blunsom and cohn, 2006; <papid> P06-1009 </papid>shimbo and hara, 2007) <papid> D07-1064 </papid>or approximated by the n-best list of highly probable alignments (och and ney, 2002;<papid> P02-1038 </papid>liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>fortunately, we can prune alignments that are unlikely to present full forms, by introducing the natural assumptions for abbreviation definitions: 4 in table 2, set of curly brackets {} denotes list (array) rather than mathematical set.
</nextsent>
<nextsent>operators ? and ? present concatenation and cartesian product of lists.
</nextsent>
<nextsent>for instance, when = {a, b} and = {c, d}, ab = {a, b, c, d} and ab = {ac, ad, bc, bd}.
</nextsent>
<nextsent>660 investigate the effect of thyroid transcription factor 1 0 0 0 00 0 0 0 0 0 0 0 00 0 0 00 0 0 0 1 2 3 0 50 0 0 00 0 0 1 2 0 3 0 50 0 0 00 0 0 1 0 2 3 0 50 0 0 00 0 0 2 1 0 3 0 50 0 0 00 0 0 2 0 1 3 0 50 0 0 00 0 3 0 0 1 0 2 50 0 0 00 0 3 0 1 2 0 0 50 0 0 00 0 3 0 1 0 0 2 5 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3950">
<title id=" C08-1083.xml">a discriminative alignment model for abbreviation recognition </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>formulas 7and 8 are maximized by the orthant-wise limited memory quasi-newton (ow-lqn) method (an drew and gao, 2007) and the limited-memory bfgs (l-bfgs) method (nocedal, 1980) 5 .
</prevsent>
<prevsent>3.1 aligned abbreviation corpus.
</prevsent>
</prevsection>
<citsent citstr=" W02-0312 ">
the meds tract gold standard corpus (pustejovskyet al, 2002) <papid> W02-0312 </papid>was widely used for evaluating abbreviation recognition methods (schwartz and hearst, 2003; adar, 2004).</citsent>
<aftsection>
<nextsent>however, we cannot use this corpus for training the abbreviation alignment model, since it lacks annotations on the origins of abbreviation letters.
</nextsent>
<nextsent>in addition, the size of the corpus is insufficient for supervised machine learning method.
</nextsent>
<nextsent>therefore, we built our training corpus with1,000 scientific abstracts that were randomly chosen from the medline database.
</nextsent>
<nextsent>although the alignment model is independent of linguistic patterns for abbreviation definitions, in the corpus we found only three abbreviation definitions that were described without parentheses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3951">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>coreference resolution is the process of linking as cluster1 multiple expressions which refer to the same entities in document.
</prevsent>
<prevsent>in recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. aone and bennett (1995); mccarthy and lehnert (1995); soon et al.
</prevsent>
</prevsection>
<citsent citstr=" W02-1008 ">
(2001); ng and cardie (2002<papid> W02-1008 </papid>b)).</citsent>
<aftsection>
<nextsent>the main idea of most supervised learning approaches is to recast this task as binary classification problem.
</nextsent>
<nextsent>specifically, classifier is learned and then used to determine whether or not two nps in document are co-referring.
</nextsent>
<nextsent>clusters are formed by linking coreferential np pairs according to acer tain selection strategy.
</nextsent>
<nextsent>in this way, the identification of coreferential clusters in text is reduced to the identification of coreferential np pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3955">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the pronoun itself gives few clues for the reference determination.
</prevsent>
<prevsent>using such np pairs would have negative influence for rules learning and subsequent resolution.
</prevsent>
</prevsection>
<citsent citstr=" N01-1008 ">
so far, several efforts (harabagiu et al, 2001; <papid> N01-1008 </papid>ng and cardie, 2002<papid> W02-1008 </papid>a; ng and cardie, 2002<papid> W02-1008 </papid>b) have attempted to address this problem by discarding the hard?</citsent>
<aftsection>
<nextsent>pairs and select only those confident ones fromthe np-pair pool.
</nextsent>
<nextsent>nevertheless, this eliminating strategy still can not guarantee that the nps in confident?
</nextsent>
<nextsent>pairs bear necessary description information of their referents.
</nextsent>
<nextsent>in this paper, we present supervisedlearning-based approach to coreference resolution.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3964">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> baseline: the np-np based.  </section>
<citcontext>
<prevsection>
<prevsent>finally, conclusion is given in section 6.
</prevsent>
<prevsent>approach 2.1 framework description.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
we built baseline coreference resolution system, which adopts the common np-np based learning framework as employed in (soon et al, 2001).<papid> J01-4004 </papid></citsent>
<aftsection>
<nextsent>each instance in this approach takes the formof i{npj , npi}, which is associated with feature vector consisting of 18 features (f1 ? f18) as described in table 2.
</nextsent>
<nextsent>most of the features come from soon et al (2001)<papid> J01-4004 </papid>s system.</nextsent>
<nextsent>inspired by the work of (strube et al, 2002) <papid> W02-1040 </papid>and (yang et al, 2004), we use two features, strsim1 (f17) and strsim2 (f18), to measure the string-matchingdegree of npj and npi.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3966">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> baseline: the np-np based.  </section>
<citcontext>
<prevsection>
<prevsent>each instance in this approach takes the formof i{npj , npi}, which is associated with feature vector consisting of 18 features (f1 ? f18) as described in table 2.
</prevsent>
<prevsent>most of the features come from soon et al (2001)<papid> J01-4004 </papid>s system.</prevsent>
</prevsection>
<citsent citstr=" W02-1040 ">
inspired by the work of (strube et al, 2002) <papid> W02-1040 </papid>and (yang et al, 2004), we use two features, strsim1 (f17) and strsim2 (f18), to measure the string-matchingdegree of npj and npi.</citsent>
<aftsection>
<nextsent>given the following similarity function: str simlarity(str1, str2) = 100?
</nextsent>
<nextsent>|str1 ? str2|str1 strsim1 and strsim2 are computed using str similarity(snpj , snpi) and str similarity(snpi , snpj ), respectively.
</nextsent>
<nextsent>here snp is the token list of np, which is obtained by applying word stemming, stop word removal and acronym expansion to the original string as described in yang et al (2004)s work.
</nextsent>
<nextsent>during training, for each anaphor npj in given text, positive instance is generated by pairing npj with its closest antecedent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3969">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>a pipeline of nlp components is applied to pre-process an input raw text.
</prevsent>
<prevsent>among them, ne recognition, part-of-speech tagging and text chunking adopt the same hmm based engine with error-driven learning capability (zhou and su, 2002).
</prevsent>
</prevsection>
<citsent citstr=" W03-1307 ">
the ne recognition component trained on genia (shen et al, 2003) <papid> W03-1307 </papid>can recognize up to 23 common biomedical entity types with an overall performance of 66.1 measure (p=66.5% r=65.7%).</citsent>
<aftsection>
<nextsent>in addition, to remove the apparent non-anaphors (e.g., embedded proper nouns) in advance, heuristic based non-anaphoricity identification module is applied, which successfully removes 50.0% non anaphors with precision of 83.5% for our dataset.
</nextsent>
<nextsent>4.2 experiments and discussions.
</nextsent>
<nextsent>our experiments were done on first 100 documents from the annotated corpus, among them 70 for training and the other 30 for testing.
</nextsent>
<nextsent>throughout these experiments, default learning parameters were applied in the c5.0 algorithm.the recall and precision were calculated automatically according to the scoring scheme proposed by vilain et al (1995).<papid> M95-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3970">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 experiments and discussions.
</prevsent>
<prevsent>our experiments were done on first 100 documents from the annotated corpus, among them 70 for training and the other 30 for testing.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
throughout these experiments, default learning parameters were applied in the c5.0 algorithm.the recall and precision were calculated automatically according to the scoring scheme proposed by vilain et al (1995).<papid> M95-1005 </papid></citsent>
<aftsection>
<nextsent>in table 3 we compared the performance of different coreference resolution systems.
</nextsent>
<nextsent>the first line summarizes the results of the baseline system using traditional np-np based approach as described in section 2.
</nextsent>
<nextsent>using bf strategy,baseline obtains 80.3% recall and 77.5% precision.
</nextsent>
<nextsent>these results are better than the work by castano et al (2002) and yang et al (2004), which were also tested on the medline dataset and reported f-measure of about 74% and 69%, respectively.in the experiments, we evaluated another np np based system, allante.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3977">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it could barely increase, or even worse, reduces the measure when used together with the the other two features.
</prevsent>
<prevsent>to our knowledge, our work is the firstsupervised-learning based attempt to do coreference resolution by exploring the relationship between an np and coreferential clusters.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
in the heuristic salience-based algorithm for pronoun resolution, lappin and leass (1994) <papid> J94-4002 </papid>introduce procedure for identifying anaphoric ally linked np as cluster for which global salience value is computed as the sum of the salience values of its elements.</citsent>
<aftsection>
<nextsent>cardie and wagstaff (1999) <papid> W99-0611 </papid>have proposed an unsupervised approach which also incorporates cluster information into considera tion.</nextsent>
<nextsent>their approach uses hard constraints to preclude the link of an np to cluster mismatch ing the number, gender or semantic agreements,while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3978">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>to our knowledge, our work is the firstsupervised-learning based attempt to do coreference resolution by exploring the relationship between an np and coreferential clusters.
</prevsent>
<prevsent>in the heuristic salience-based algorithm for pronoun resolution, lappin and leass (1994) <papid> J94-4002 </papid>introduce procedure for identifying anaphoric ally linked np as cluster for which global salience value is computed as the sum of the salience values of its elements.</prevsent>
</prevsection>
<citsent citstr=" W99-0611 ">
cardie and wagstaff (1999) <papid> W99-0611 </papid>have proposed an unsupervised approach which also incorporates cluster information into considera tion.</citsent>
<aftsection>
<nextsent>their approach uses hard constraints to preclude the link of an np to cluster mismatch ing the number, gender or semantic agreements,while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection.
</nextsent>
<nextsent>besides, the idea of clustering can be seen in the research of cross-documentcoreference, where nps with high context similarity would be chained together based on certain clustering methods (bagga and biermann, 1998; gooi and allan, 2004).<papid> N04-1002 </papid></nextsent>
<nextsent>in this paper we have proposed supervisedlearning-based approach to coreference resolution.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3979">
<title id=" C04-1033.xml">an npc luster based approach to coreference resolution </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>cardie and wagstaff (1999) <papid> W99-0611 </papid>have proposed an unsupervised approach which also incorporates cluster information into considera tion.</prevsent>
<prevsent>their approach uses hard constraints to preclude the link of an np to cluster mismatch ing the number, gender or semantic agreements,while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection.</prevsent>
</prevsection>
<citsent citstr=" N04-1002 ">
besides, the idea of clustering can be seen in the research of cross-documentcoreference, where nps with high context similarity would be chained together based on certain clustering methods (bagga and biermann, 1998; gooi and allan, 2004).<papid> N04-1002 </papid></citsent>
<aftsection>
<nextsent>in this paper we have proposed supervisedlearning-based approach to coreference resolution.
</nextsent>
<nextsent>rather than mining the coreferential relationship between np pairs as in conventional approaches, our approach does resolution by exploring the relationships between an np and the coreferential clusters.
</nextsent>
<nextsent>compared to individual nps, coreferential clusters provide more information for rules learning and reference determination.
</nextsent>
<nextsent>in the paper, we first introduced the conventional np-np based approach and analyzed its limitation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3980">
<title id=" C04-1101.xml">combining linguistic features with weighted bayesian classifier for temporal reference processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(they solved the traffic problem of the city after the street bridge had been built?.
</prevsent>
<prevsent>temporal reference describes how events relate to one another, which is essential to natural language processing (nlp).
</prevsent>
</prevsection>
<citsent citstr=" P90-1016 ">
its major applications cover syntactic structural disambiguation (brent, 1990), <papid> P90-1016 </papid>information extraction and question answering (li, 2002), language generation and machine translation (dorr, 2002).</citsent>
<aftsection>
<nextsent>many researchers have attempted to characterize the nature of temporal reference in discourse.
</nextsent>
<nextsent>identifying temporal relations1 between two events de 1 the relations under examined include both intra-sentence and inter-.
</nextsent>
<nextsent>pends on combination of information resources.
</nextsent>
<nextsent>this information is provided by explicit tense and aspect markers, implicit event classes or discourse structures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3981">
<title id=" C04-1101.xml">combining linguistic features with weighted bayesian classifier for temporal reference processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pends on combination of information resources.
</prevsent>
<prevsent>this information is provided by explicit tense and aspect markers, implicit event classes or discourse structures.
</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
it has been used to explain semantics of temporal expressions (moens, 1988; webber, 1988), <papid> J88-2006 </papid>to constrain possible temporal interpretations (hitzeman, 1995; sing, 1997), or to generate appropriate temporally conjoined clauses (dorr, 2002).</citsent>
<aftsection>
<nextsent>the purpose of our work is to develop computational model, which automatically determines temporal relations in chinese.
</nextsent>
<nextsent>while temporal reference interpretation in english has been well studied, chinese has been rarely discussed.
</nextsent>
<nextsent>in our study, thirteen related features are identified from linguistic perspective.
</nextsent>
<nextsent>how to combine these features and how to map their combined effects to the corresponding relations are the critical issues to be addressed in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3983">
<title id=" C04-1101.xml">combining linguistic features with weighted bayesian classifier for temporal reference processing </title>
<section> linguistic background of temporal refer-.  </section>
<citcontext>
<prevsection>
<prevsent>tem poral/casual connective, such as after?, before?
</prevsent>
<prevsent>or because?, can supply explicit information about the temporal ordering of events.
</prevsent>
</prevsection>
<citsent citstr=" J88-2005 ">
passonneau (passon neau, 1988), <papid> J88-2005 </papid>brent (brent, 1990 <papid> P90-1016 </papid>and sing (sing, 1997) determined intra-sentential relations by accounting for temporal or causal connectives.</citsent>
<aftsection>
<nextsent>dorr and gaast erland (dorr, 2002), on the other hand, studied how to generate the sentences which reflect event temporal relations by selecting proper connecting words.
</nextsent>
<nextsent>however, temporal connectives can be ambiguous.
</nextsent>
<nextsent>for instance, when?
</nextsent>
<nextsent>clause permits many possible temporal relations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3986">
<title id=" C04-1101.xml">combining linguistic features with weighted bayesian classifier for temporal reference processing </title>
<section> combining linguistic features with machine.  </section>
<citcontext>
<prevsection>
<prevsent>obviously, it is not simple task to map the combined effects of the thirteen linguistic features to the corresponding relations.
</prevsent>
<prevsent>therefore, machine learning approach is proposed, which investigates how these features contribute to the task and how they should be combined.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
learning approach previous efforts in corpus-based nlp have incorporated machine learning methods to coordinate multiple linguistic features, for example, in accent restoration (yarowsky, 1994) <papid> P94-1013 </papid>and event classification (siegel, 1998).</citsent>
<aftsection>
<nextsent>temporal relation determination can be modeled as relation classification task.
</nextsent>
<nextsent>we formulate the thirteen temporal relations (see figure 1) as the classes to be decided by classifier.
</nextsent>
<nextsent>the classification process is to assign an event pair to one class according to their linguistic features.
</nextsent>
<nextsent>there existed numerous classification algorithms based upon supervised learning principle.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3987">
<title id=" C02-1159.xml">extending a broad coverage parser for a general nlp toolkit </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>with the rapid growth of real world applications for nlp systems, there is genuine demand for general toolkit from which programmers with no linguistic knowledge can build specific nlp systems.
</prevsent>
<prevsent>such toolkit should have parser that is general enough to be used across domains, and yet accurate enough for each specific application.
</prevsent>
</prevsection>
<citsent citstr=" H01-1046 ">
in this paper, we describe parser that extends broad-coverage parser, minipar (lin, 2001), <papid> H01-1046 </papid>with an adaptable shallow parser so as to achieve both generality and accuracy in handling domain specific nl problems.</citsent>
<aftsection>
<nextsent>we test this parser on our corpus and the results show that the accuracy is significantly higher than system that uses minipar alone.
</nextsent>
<nextsent>with the improvement of natural language processing (nlp) techniques, domains for nlp systems, especially those handling speech input, are rapidly growing.
</nextsent>
<nextsent>however, most computer programmers do not have enough linguistic knowledge to develop nlp systems.
</nextsent>
<nextsent>there is genuine demand for general toolkit from which programmers with no linguistic knowledge can rapidly build nlp systems that handle domain specific problems more accurately (alam, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3989">
<title id=" C02-1159.xml">extending a broad coverage parser for a general nlp toolkit </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore the toolkit should have broad coverage parser that has the accuracy of parser designed specifically for domain.
</prevsent>
<prevsent>one solution is to use an existing parser with relatively high accuracy.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
using existing parsers such as (charniak, 2000; <papid> A00-2018 </papid>collins, 1999) would eliminate the need to build parser from scratch.</citsent>
<aftsection>
<nextsent>however, there are two problems with such an approach.
</nextsent>
<nextsent>first, many parsers claim high precision in terms of the number of correctly parsed syntactic relations rather than sentences, whereas in commercial applications, the users are often concerned with the number of complete sentences that are parsed correctly.
</nextsent>
<nextsent>the precision might drop considerably using this standard.
</nextsent>
<nextsent>in addition, although many parsers are domain independent, they actually perform much better in the domains they are trained on or implemented in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3993">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>extractive systems generate summaries by extracting selected segments from the original document that are deemed to be most relevant.
</prevsent>
<prevsent>non extractive systems try to build an abstract representation model and re-generate the summary using this model and words found in the original document.
</prevsent>
</prevsection>
<citsent citstr=" E03-2013 ">
previous related work on extractive systems included the use of semantic tagging and co reference/lexical chains (saggion et al, 2003; <papid> E03-2013 </papid>barzilay and elhadad, 1997; <papid> W97-0703 </papid>azzam et al, 1998), lexical occur rence/structural statistics (mathis et al, 1973), discourse structure (marcu, 1998), cue phrases (luhn, 1958; paice, 1990; rau et al, 1994), positional indicators (edmunson, 1964) and other extraction methods (kui pec et al, 1995).</citsent>
<aftsection>
<nextsent>non-extractive systems are less common ? previous related work included reformulation of extracted models (mckeown et al, 1999), gist extraction (berger and mittal, 2000), machine translation-like approaches (witbrock and mittal, 1999) and generative models (de jong, 1982; radev and mckeown, 1998; <papid> J98-3005 </papid>fum et al, 1986; <papid> C86-1061 </papid>reihmer and hahn, 1988; rau et al, 1989).</nextsent>
<nextsent>a sentence-extraction system was decided for the fasil summa riser, with the capability to have phrase level extraction in the future.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3994">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>extractive systems generate summaries by extracting selected segments from the original document that are deemed to be most relevant.
</prevsent>
<prevsent>non extractive systems try to build an abstract representation model and re-generate the summary using this model and words found in the original document.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
previous related work on extractive systems included the use of semantic tagging and co reference/lexical chains (saggion et al, 2003; <papid> E03-2013 </papid>barzilay and elhadad, 1997; <papid> W97-0703 </papid>azzam et al, 1998), lexical occur rence/structural statistics (mathis et al, 1973), discourse structure (marcu, 1998), cue phrases (luhn, 1958; paice, 1990; rau et al, 1994), positional indicators (edmunson, 1964) and other extraction methods (kui pec et al, 1995).</citsent>
<aftsection>
<nextsent>non-extractive systems are less common ? previous related work included reformulation of extracted models (mckeown et al, 1999), gist extraction (berger and mittal, 2000), machine translation-like approaches (witbrock and mittal, 1999) and generative models (de jong, 1982; radev and mckeown, 1998; <papid> J98-3005 </papid>fum et al, 1986; <papid> C86-1061 </papid>reihmer and hahn, 1988; rau et al, 1989).</nextsent>
<nextsent>a sentence-extraction system was decided for the fasil summa riser, with the capability to have phrase level extraction in the future.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3995">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>non extractive systems try to build an abstract representation model and re-generate the summary using this model and words found in the original document.
</prevsent>
<prevsent>previous related work on extractive systems included the use of semantic tagging and co reference/lexical chains (saggion et al, 2003; <papid> E03-2013 </papid>barzilay and elhadad, 1997; <papid> W97-0703 </papid>azzam et al, 1998), lexical occur rence/structural statistics (mathis et al, 1973), discourse structure (marcu, 1998), cue phrases (luhn, 1958; paice, 1990; rau et al, 1994), positional indicators (edmunson, 1964) and other extraction methods (kui pec et al, 1995).</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
non-extractive systems are less common ? previous related work included reformulation of extracted models (mckeown et al, 1999), gist extraction (berger and mittal, 2000), machine translation-like approaches (witbrock and mittal, 1999) and generative models (de jong, 1982; radev and mckeown, 1998; <papid> J98-3005 </papid>fum et al, 1986; <papid> C86-1061 </papid>reihmer and hahn, 1988; rau et al, 1989).</citsent>
<aftsection>
<nextsent>a sentence-extraction system was decided for the fasil summa riser, with the capability to have phrase level extraction in the future.
</nextsent>
<nextsent>non-extractive systems were not likely to work as robustly and give the high quality results needed by the vpa to work as required.
</nextsent>
<nextsent>another advantage that extractive systems still pose is that in general they are more applicable to wider range of arbitrary domains and are more reliable than non extractive systems (teufel, 2003).
</nextsent>
<nextsent>the fasil summa riser uses named entities as an indication of the importance of every sentence, and performs anaphora resolution automatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3996">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>non extractive systems try to build an abstract representation model and re-generate the summary using this model and words found in the original document.
</prevsent>
<prevsent>previous related work on extractive systems included the use of semantic tagging and co reference/lexical chains (saggion et al, 2003; <papid> E03-2013 </papid>barzilay and elhadad, 1997; <papid> W97-0703 </papid>azzam et al, 1998), lexical occur rence/structural statistics (mathis et al, 1973), discourse structure (marcu, 1998), cue phrases (luhn, 1958; paice, 1990; rau et al, 1994), positional indicators (edmunson, 1964) and other extraction methods (kui pec et al, 1995).</prevsent>
</prevsection>
<citsent citstr=" C86-1061 ">
non-extractive systems are less common ? previous related work included reformulation of extracted models (mckeown et al, 1999), gist extraction (berger and mittal, 2000), machine translation-like approaches (witbrock and mittal, 1999) and generative models (de jong, 1982; radev and mckeown, 1998; <papid> J98-3005 </papid>fum et al, 1986; <papid> C86-1061 </papid>reihmer and hahn, 1988; rau et al, 1989).</citsent>
<aftsection>
<nextsent>a sentence-extraction system was decided for the fasil summa riser, with the capability to have phrase level extraction in the future.
</nextsent>
<nextsent>non-extractive systems were not likely to work as robustly and give the high quality results needed by the vpa to work as required.
</nextsent>
<nextsent>another advantage that extractive systems still pose is that in general they are more applicable to wider range of arbitrary domains and are more reliable than non extractive systems (teufel, 2003).
</nextsent>
<nextsent>the fasil summa riser uses named entities as an indication of the importance of every sentence, and performs anaphora resolution automatically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3997">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>an internet-based method that verified the list and filtered out likely spelling mistakes and non-existent names was then applied to this list, filtering out incorrectly spelt names and other features such as online chat nicknames (dalli, 2004).
</prevsent>
<prevsent>a list of over 592,000 proper names was thus obtained by this method with around 284,000 names being identified as male and 308,000 names identified as female.
</prevsent>
</prevsection>
<citsent citstr=" A00-1040 ">
the large size of this list contributed significantly to the ners resulting accuracy and compares favourably with previously compiled lists (stevenson and gaizauskas, 2000).<papid> A00-1040 </papid></citsent>
<aftsection>
<nextsent>3.4 anaphora resolution.
</nextsent>
<nextsent>extracting systems suffer from the problem of dangling anaphora in summaries.
</nextsent>
<nextsent>anaphora resolution is an effective way of reducing the incoherence in resulting summaries by replacing anaphors with references to the appropriate named entities (mitkov, 2002).
</nextsent>
<nextsent>this substitution has the direct effect of making the text less context sensitive and implicitly increases the formality of the text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3998">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>the major cohesion problem that still has not been fully addressed is the coherence of various events mentioned in the text.
</prevsent>
<prevsent>the anaphora re solver is aided by the gender categorised named entity classes, enabling it to perform better resolution over wide variety of names.
</prevsent>
</prevsection>
<citsent citstr=" C00-1031 ">
a simple linear model is adopted, where the system focuses mainly on nominal and clausal antecedents (cristea et al., 2000).<papid> C00-1031 </papid></citsent>
<aftsection>
<nextsent>the search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (mitkov, 1998) <papid> P98-2143 </papid>as empirical studies show that more than 85% of all cases are handled correctly with this window size (mitkov, 2002).</nextsent>
<nextsent>candidate antecedents being discarded after ten sentences have been processed without the presence of anaphora as suggested in (kameyama, 1997).<papid> W97-1307 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B3999">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>the anaphora re solver is aided by the gender categorised named entity classes, enabling it to perform better resolution over wide variety of names.
</prevsent>
<prevsent>a simple linear model is adopted, where the system focuses mainly on nominal and clausal antecedents (cristea et al., 2000).<papid> C00-1031 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
the search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (mitkov, 1998) <papid> P98-2143 </papid>as empirical studies show that more than 85% of all cases are handled correctly with this window size (mitkov, 2002).</citsent>
<aftsection>
<nextsent>candidate antecedents being discarded after ten sentences have been processed without the presence of anaphora as suggested in (kameyama, 1997).<papid> W97-1307 </papid></nextsent>
<nextsent>3.5 sentence ranking.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4000">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> summarisation techniques.  </section>
<citcontext>
<prevsection>
<prevsent>a simple linear model is adopted, where the system focuses mainly on nominal and clausal antecedents (cristea et al., 2000).<papid> C00-1031 </papid></prevsent>
<prevsent>the search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (mitkov, 1998) <papid> P98-2143 </papid>as empirical studies show that more than 85% of all cases are handled correctly with this window size (mitkov, 2002).</prevsent>
</prevsection>
<citsent citstr=" W97-1307 ">
candidate antecedents being discarded after ten sentences have been processed without the presence of anaphora as suggested in (kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>3.5 sentence ranking.
</nextsent>
<nextsent>after named entity recognition and anaphora resolution, the summa riser ranks the various sentences/phrases that it identifies and selects the best sentences to extract and put in the summary.
</nextsent>
<nextsent>the summa riser takes two parameters apart from the email text itself: preferred length and maximum length.
</nextsent>
<nextsent>typical lengths are 160 characters preferred with 640 characters maximum, which compares to the size mobile text message.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4001">
<title id=" C04-1143.xml">fasil email summarisation system </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>for recall and precision calculation, any sentence ranked ? 5 was defined as relevant.
</prevsent>
<prevsent>figure 7 shows the precision and recall values with 74% average precision and 71% average recall.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
0 0.5 1 1.5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 sample group value recall precision figure 7 summaries recall and precision utility-based evaluation was also used to obtain more intuitive results than those given by precision and recall using the methods reported in (jing et al, 1998; goldstein et al, 1999; radev et al, 2000).<papid> W00-0403 </papid></citsent>
<aftsection>
<nextsent>the average score of each summary was compared to the average score over infinity expected to be obtained by extracting combination of the first [1..n] sentences at random.
</nextsent>
<nextsent>the summary average score was also compared to the score obtained by an averaged pool of 3 human judges.
</nextsent>
<nextsent>figure 8 shows comparison between the summa riser performance and human performance, with the summar iser averaging at 86.5% of the human performance, ranging from 60% agreement to 100% agreement with the gold standard.
</nextsent>
<nextsent>0 0.5 1 1.5 2 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 sample group value summa riser utility gold standard utility figure 8 utility score comparison in figure 8 random extraction system is expected to get score of 1 averaged across an infinite amount of runs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4002">
<title id=" C04-1128.xml">detection of question answer pairs in email conversations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, number of emails can be used in scheduling meeting, and search for information on the meeting may retrieve all the intermediate emails,thus hindering ones access to the required information.
</prevsent>
<prevsent>access to required information could be dramatically improved by querying summaries of email conversations while summarization of email conversations seems natural way to improve upon current methods of email management, research on email summarization is in early stages.
</prevsent>
</prevsection>
<citsent citstr=" N04-4027 ">
consider an example summary of thread of email conversation produced by sentence extraction based email thread summarization system developed at columbia (rambow et al, 2004) <papid> N04-4027 </papid>shown in figure 1.</citsent>
<aftsection>
<nextsent>while this summary does include an answer to the first question, it does not include answers to the two questions posed subsequently even though the answers are present in the thread.
</nextsent>
<nextsent>this example demonstrates one of the inadequacies of sentence extraction based summarization modules: namely, the absence of discourse segments that would have made the summaries more readable and complete.
</nextsent>
<nextsent>a summarization module that includes answers to questions posed in extractive summaries, then, becomes very useful.further, questions are natural means of resolving any issue.
</nextsent>
<nextsent>this is especially so of email conversations through which most of our issues, whether professional or personal, get resolved.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4006">
<title id=" C04-1128.xml">detection of question answer pairs in email conversations </title>
<section> automatic answer detection.  </section>
<citcontext>
<prevsection>
<prevsent>we consider segment to be question segment if sentence in that segment has been highlighted as question.
</prevsent>
<prevsent>similarly, we consider segment to be an answer segment if sentence in that segment has been paired with question to form question and answer pair.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
the kappa statistic (carletta, 1996) <papid> J96-2004 </papid>for identifying question segments is 0.68, and for linking question and answer segments given question segment is 0.81.</citsent>
<aftsection>
<nextsent>4.2 features.
</nextsent>
<nextsent>for each question segment in an email message, we make list of candidate answer segments.
</nextsent>
<nextsent>this is basically just list of original (content that is not quoted from past emails)4 segments in all the messages in the thread subsequent to the message of the question segment.
</nextsent>
<nextsent>let the thread in consideration be called t, the container message of the question segment be called mq, the container message of the candidate answer segment be called ma, the question segment be called q, and the candidate answer segment be called a. for each question and candidate answer pair, we compute the following sets of features: 4.2.1 some standard features(a) number of nonstop words in segment and segment a;(b) cosine similarity5 and euclidean distance6 between segment and a; 4.2.2 features derived from the structure of the thread (c) the number of intermediate messages between mq and ma in t;(d) the ratio of the number of messages in sent ear 4while people do use quoted material to respond to specific segments of past emails, phenomenon more common is discussion lists, because the occurrence of such is rare in the acm corpus we decided not to use them as feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4007">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the experimental results indicate that our proposed approach outperforms the baseline system, and is better than current state-of-the-art adaptation techniques.
</prevsent>
<prevsent>dependency parsing aims to build the dependency relations between words in sentence.
</prevsent>
</prevsection>
<citsent citstr=" D07-1096 ">
there are many supervised learning methods for training high-performance dependency parsers(nivre et al, 2007), <papid> D07-1096 </papid>if given sufficient labeled data.</citsent>
<aftsection>
<nextsent>however, the performance of parsers declines when we are in the situation that parser is trained in one source?
</nextsent>
<nextsent>domain but is to parse the sentences in second target?
</nextsent>
<nextsent>domain.
</nextsent>
<nextsent>there are two tasks(daume iii, 2007) for the domain adaptation problem.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4008">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important characteristic of parsing adaptation is that the parsers perform much better for shorter dependencies than for longer ones (the score at length is much higher than the scores at length  ).
</prevsent>
<prevsent>in this paper, we propose an approach by using the information on shorter dependencies in auto parsed target data to help parse longer distance words for adapting parser.
</prevsent>
</prevsection>
<citsent citstr=" D07-1111 ">
compared with the adaptation methods of sagae and tsujii (2007) <papid> D07-1111 </papid>and reichart and rappoport (2007), <papid> P07-1078 </papid>our approach uses the information on word pairs in auto-parsed data instead of using the whole sentences as newly labeled data for training new parsers.</citsent>
<aftsection>
<nextsent>it is difficult to detect reliable parsed sentences, but we can find relative reliable parsed word pairs according to dependency length.
</nextsent>
<nextsent>the experimental results show that our approach significantly outperforms base line system and current state of the art techniques.
</nextsent>
<nextsent>in dependency parsing, we assign head-dependent relations between words in sentence.
</nextsent>
<nextsent>a simple example is shown in figure 1, where the arc between and hat indicates that hat is the head of a. current statistical dependency parsers perform better if the dependency lengthes are shorter (mc donald and nivre, 2007).<papid> D07-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4010">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an important characteristic of parsing adaptation is that the parsers perform much better for shorter dependencies than for longer ones (the score at length is much higher than the scores at length  ).
</prevsent>
<prevsent>in this paper, we propose an approach by using the information on shorter dependencies in auto parsed target data to help parse longer distance words for adapting parser.
</prevsent>
</prevsection>
<citsent citstr=" P07-1078 ">
compared with the adaptation methods of sagae and tsujii (2007) <papid> D07-1111 </papid>and reichart and rappoport (2007), <papid> P07-1078 </papid>our approach uses the information on word pairs in auto-parsed data instead of using the whole sentences as newly labeled data for training new parsers.</citsent>
<aftsection>
<nextsent>it is difficult to detect reliable parsed sentences, but we can find relative reliable parsed word pairs according to dependency length.
</nextsent>
<nextsent>the experimental results show that our approach significantly outperforms base line system and current state of the art techniques.
</nextsent>
<nextsent>in dependency parsing, we assign head-dependent relations between words in sentence.
</nextsent>
<nextsent>a simple example is shown in figure 1, where the arc between and hat indicates that hat is the head of a. current statistical dependency parsers perform better if the dependency lengthes are shorter (mc donald and nivre, 2007).<papid> D07-1013 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4011">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the experimental results show that our approach significantly outperforms base line system and current state of the art techniques.
</prevsent>
<prevsent>in dependency parsing, we assign head-dependent relations between words in sentence.
</prevsent>
</prevsection>
<citsent citstr=" D07-1013 ">
a simple example is shown in figure 1, where the arc between and hat indicates that hat is the head of a. current statistical dependency parsers perform better if the dependency lengthes are shorter (mc donald and nivre, 2007).<papid> D07-1013 </papid></citsent>
<aftsection>
<nextsent>here the length of the dependency from word i to word j is simply equal to |i ? j|.
</nextsent>
<nextsent>figure 2 shows the results (f 1 113 the boy saw red hat . figure 1: an example for dependency relations.
</nextsent>
<nextsent>20 30 40 50 60 70 80 90 100 0 2 4 6 8 10 12 14 16 18 20 f1 dependency length same domain diff domain figure 2: the scores relative to dependency length.
</nextsent>
<nextsent>samedomain?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4015">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the labeled data was from the wall street journal, the development data was from biomedical abstracts, and the testing datawas from chemical abstracts and parent-child dialogues.
</prevsent>
<prevsent>additionally, large unlabeled corpus was provided.
</prevsent>
</prevsection>
<citsent citstr=" D07-1119 ">
the systems by sagae and tsujii (2007),<papid> D07-1111 </papid>attardi et al (2007), <papid> D07-1119 </papid>and dredze et al (2007) <papid> D07-1112 </papid>performed top three in the shared task.</citsent>
<aftsection>
<nextsent>sagae and tsujii (2007) <papid> D07-1111 </papid>presented procedure similar to single iteration of co-training.</nextsent>
<nextsent>firstly, they trained two parsers on labeled source data.then the two parsers were used to parse the sentences in unlabeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4016">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the labeled data was from the wall street journal, the development data was from biomedical abstracts, and the testing datawas from chemical abstracts and parent-child dialogues.
</prevsent>
<prevsent>additionally, large unlabeled corpus was provided.
</prevsent>
</prevsection>
<citsent citstr=" D07-1112 ">
the systems by sagae and tsujii (2007),<papid> D07-1111 </papid>attardi et al (2007), <papid> D07-1119 </papid>and dredze et al (2007) <papid> D07-1112 </papid>performed top three in the shared task.</citsent>
<aftsection>
<nextsent>sagae and tsujii (2007) <papid> D07-1111 </papid>presented procedure similar to single iteration of co-training.</nextsent>
<nextsent>firstly, they trained two parsers on labeled source data.then the two parsers were used to parse the sentences in unlabeled data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4022">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> motivation and prior work.  </section>
<citcontext>
<prevsection>
<prevsent>they declared that it was difficult to significantly improve performance on any test domain beyond that of state-of-the-art parser.
</prevsent>
<prevsent>their error analysis suggested that the primary cause of loss from adaptation is because of differences inthe annotation guidelines.
</prevsent>
</prevsection>
<citsent citstr=" P06-1043 ">
without specific knowledge of the target domains annotation standards, significant improvement can not be made.reichart and rappoport (2007) <papid> P07-1078 </papid>studied self training method for domain adaptation (the wsj data and the brown data) of phrase-based parsers.mcclosky et al (2006) <papid> P06-1043 </papid>presented successful instance of parsing with self-training by using re ranker.</citsent>
<aftsection>
<nextsent>both of them used the whole sentences as newly labeled data for adapting the parsers, while our approach uses the information on word pairs.
</nextsent>
<nextsent>chen et al (2008) presented an approach byusing the information of adjacent words for in domain parsing.
</nextsent>
<nextsent>as figure 2 shows, the score curves of same domain (in-domain) parsing anddiffdomain (out-domain) parsing are quite different.
</nextsent>
<nextsent>our work focuses on parsing adaptation and is based on the fact that current parsers perform much better for shorter dependencies than for longer ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4023">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> the parsing approach.  </section>
<citcontext>
<prevsection>
<prevsent>length is relatively shorter than length + 1, where can be any number.
</prevsent>
<prevsent>in this paper, we choose the model described bynivre (2003) as our parsing model.
</prevsent>
</prevsection>
<citsent citstr=" W06-2933 ">
it is deterministic parser and works quite well in the shared task of conll2006(nivre et al, 2006).<papid> W06-2933 </papid></citsent>
<aftsection>
<nextsent>3.1 the parsing model.
</nextsent>
<nextsent>the nivre (2003) model is shift-reduce type algorithm, which uses stack to store processed tokens and queue to store remaining input tokens.
</nextsent>
<nextsent>it can perform dependency parsing in o(n) time.
</nextsent>
<nextsent>the dependency parsing tree is built from atomic actions in left-to-right pass over the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4030">
<title id=" C08-1015.xml">learning reliable information for dependency parsing adaptation </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>4 more detailed information can be found at http://www.icl.pku.edu.
</prevsent>
<prevsent>5 more detailed information can be found at http://rocling.iis.sinica.edu.tw/ckip/index.htm 117 num of words unknown word rate strain 17983 stest 1829 9.73 ttest 1783 30.79ckip 140k stest 1829 11.42 ttest 1783 8.63pfr 123k stest 1829 8.58 ttest 1783 15.64 table 2: the information of the datasets closer to source domain and the ckip data was closer to target domain.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
to assign pos tags for the unlabeled data, we used the package tnt (brants, 2000) <papid> A00-1031 </papid>to train pos tagger on training data.</citsent>
<aftsection>
<nextsent>because the pfr data and the ctb used different pos standards, we did not use the pos tags in the pfr data.we measured the quality of the parser by the unlabeled attachment score (uas), i.e., the percentage of tokens with correct head.
</nextsent>
<nextsent>we also reported the accuracy of root.
</nextsent>
<nextsent>in the following content, ours refers to our proposed approach.
</nextsent>
<nextsent>the baseline system refers to the basic parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4037">
<title id=" C02-1107.xml">example based speech intention understanding and its application to incar spoken dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as method of speech intention understanding, kimura et al has proposed rule-based approach (kimura et al, 1998).
</prevsent>
<prevsent>they have defined 52 kinds of utterance intentions, and constructed rules for inferring the intention from each utterance by taking account of the intentions of the last utterances, verb, an aspect of the input utterance, and so on.
</prevsent>
</prevsection>
<citsent citstr=" W01-1603 ">
the huge work for constructing the rules, however, cannot help depending on lot of hands, and it is also difficult to modify the rules.on the other hand, technique for tagging dialogue acts has been proposed so far (araki etal., 2001).<papid> W01-1603 </papid></citsent>
<aftsection>
<nextsent>for the purpose of concretely determining the operations to be done by the system,the intention to be inferred should be more detailed than the level of dialogue act tags such as yes-no question?
</nextsent>
<nextsent>and wh question?.this paper proposes method of understanding speeches intentions based on lot of dialogue examples.
</nextsent>
<nextsent>the method uses the corpus in which the utterance intention has given to each sentence in advance.
</nextsent>
<nextsent>we have defined the utterance intention tags by extending an annotation scheme of dialogue act tags, called jdtag (jdri, 2000), and arrived at 78 kinds of tags presently.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4038">
<title id=" C02-1107.xml">example based speech intention understanding and its application to incar spoken dialogue system </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 experimental data.
</prevsent>
<prevsent>an in-car speech dialogue corpus which has been constructed at ciair (kawaguchi et al, 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 weight coefficient ? c u a y recall precision figure 4: relation between the weight coefficient ? and the accuracy (?
</prevsent>
</prevsection>
<citsent citstr=" C02-1136 ">
= 0.3) 2001), was used as corpus with intention tags, and analyzed based on japanese dependency grammar (matsubara et al, 2002).<papid> C02-1136 </papid></citsent>
<aftsection>
<nextsent>that is, the intention tags were assigned manually into all sentences in 412 dialogues about restaurant search recorded on the corpus.
</nextsent>
<nextsent>the intentions2-gram probability was learned from the sentences of 174 dialogues in them.
</nextsent>
<nextsent>the standard for assigning the intention tags was established by extending the decision tree proposed as dialogue tag scheme (jdri, 2000).
</nextsent>
<nextsent>consequently, 78 kinds of intention tags were prepared in all(38 kinds are for driver utterances).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4042">
<title id=" C04-1023.xml">a support system for revising titles to stimulate the lay readers interest in technical achievements </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, technical specialists are not necessarily good at composing appealing titles, because it isnt clear what sort of titles will stimulate the interest of lay readers in the technology.
</prevsent>
<prevsent>in the field of nlp and linguistics, there arefew researches which help the specialists compose appealing titles for lay readers.
</prevsent>
</prevsection>
<citsent citstr=" W97-0705 ">
several researches have been reported on title generation (jin and hauptmann, 2000) (berger and mittal, 2000) and readability of texts (minel et al., 1997) (<papid> W97-0705 </papid>hartley and sydes, 1997) (inui et al,2003).<papid> W03-1602 </papid></citsent>
<aftsection>
<nextsent>however, the researches on title generation focus on generating very compact summary of the document rather than composing an appealing title.
</nextsent>
<nextsent>the previous researches on readability mainly see it as comprehensibility rather than interestingness.
</nextsent>
<nextsent>in this regard, our previous study (senda and sinohara, 2002) <papid> C02-1133 </papid>clarified what sort of content and wording in titles are effective in stimulating lay readers?</nextsent>
<nextsent>interest in the technology by an analysis of parallel corpus of japanese technical paper titles and japanese newspapers headlines.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4043">
<title id=" C04-1023.xml">a support system for revising titles to stimulate the lay readers interest in technical achievements </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, technical specialists are not necessarily good at composing appealing titles, because it isnt clear what sort of titles will stimulate the interest of lay readers in the technology.
</prevsent>
<prevsent>in the field of nlp and linguistics, there arefew researches which help the specialists compose appealing titles for lay readers.
</prevsent>
</prevsection>
<citsent citstr=" W03-1602 ">
several researches have been reported on title generation (jin and hauptmann, 2000) (berger and mittal, 2000) and readability of texts (minel et al., 1997) (<papid> W97-0705 </papid>hartley and sydes, 1997) (inui et al,2003).<papid> W03-1602 </papid></citsent>
<aftsection>
<nextsent>however, the researches on title generation focus on generating very compact summary of the document rather than composing an appealing title.
</nextsent>
<nextsent>the previous researches on readability mainly see it as comprehensibility rather than interestingness.
</nextsent>
<nextsent>in this regard, our previous study (senda and sinohara, 2002) <papid> C02-1133 </papid>clarified what sort of content and wording in titles are effective in stimulating lay readers?</nextsent>
<nextsent>interest in the technology by an analysis of parallel corpus of japanese technical paper titles and japanese newspapers headlines.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4044">
<title id=" C04-1023.xml">a support system for revising titles to stimulate the lay readers interest in technical achievements </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, the researches on title generation focus on generating very compact summary of the document rather than composing an appealing title.
</prevsent>
<prevsent>the previous researches on readability mainly see it as comprehensibility rather than interestingness.
</prevsent>
</prevsection>
<citsent citstr=" C02-1133 ">
in this regard, our previous study (senda and sinohara, 2002) <papid> C02-1133 </papid>clarified what sort of content and wording in titles are effective in stimulating lay readers?</citsent>
<aftsection>
<nextsent>interest in the technology by an analysis of parallel corpus of japanese technical paper titles and japanese newspapers headlines.
</nextsent>
<nextsent>the study categorized the effective content and wording of the titles into the following three key points.key point 1 (for wording) instead of technical terms, use synonymous plain term seven where the plain term is not synonymous with the technical term in precise sense.
</nextsent>
<nextsent>key point 2 (for content) describe what the technology is for, rather than what the technology does.key point 3 (for content) describe the advantages of the technology, rather than the method of realizing the technology.our next goal is to enable inexperienced authors to compose title according to these key points.
</nextsent>
<nextsent>to this end, we developed support system for revising titles.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4045">
<title id=" C04-1023.xml">a support system for revising titles to stimulate the lay readers interest in technical achievements </title>
<section> support tools provided in the.  </section>
<citcontext>
<prevsection>
<prevsent>the pull down menu options are organized by technical fields.
</prevsent>
<prevsent>users can search the clues for revising the title of the draft version from these search boxes as well as by scrolling through the window.the role of this parallel corpus is basically the same as the one of example-basedtranslation aid (ebta)?
</prevsent>
</prevsection>
<citsent citstr=" C92-4203 ">
(sato, 1992) (<papid> C92-4203 </papid>furugori and takeda, 1993) (kumano and tanaka,1998).</citsent>
<aftsection>
<nextsent>ebta researches have shown that parallel translation examples (pairs of source text and its translation equivalent) are very helpful for translators to translate the similar text be cause parallel translation examples give them useful clues for translation.
</nextsent>
<nextsent>from the viewpoint that paper titles and newspapers headlines for related technologies are also regarded as parallel translation examples (pairs of text for specialists and its translation equivalent for layreaders) describing newly-developed technologies, the our database is expected to be helpful for the user of the wizard.
</nextsent>
<nextsent>3.2 technical terms checker.
</nextsent>
<nextsent>the author should use comprehensible term forlay readers in order to compose the titles meeting the key point 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4047">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.
</prevsent>
<prevsent>* research fellow of the japan society for the promotion of science (jsps) erence resolution and pronoun resolution, which have been studied for many years (e.g. soon et al.
</prevsent>
</prevsection>
<citsent citstr=" P05-1020 ">
(2001); mitkov (2002); ng (2005)).<papid> P05-1020 </papid></citsent>
<aftsection>
<nextsent>isozaki and hirao (2003) <papid> W03-1024 </papid>and iida et al (2006) <papid> P06-1079 </papid>focused on zero pronoun resolution assuming perfect pre-detection of zero pronouns.</nextsent>
<nextsent>however, we consider that zero pronoun detection and resolution have tight relation and should not be handled independently.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4048">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>* research fellow of the japan society for the promotion of science (jsps) erence resolution and pronoun resolution, which have been studied for many years (e.g. soon et al.
</prevsent>
<prevsent>(2001); mitkov (2002); ng (2005)).<papid> P05-1020 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-1024 ">
isozaki and hirao (2003) <papid> W03-1024 </papid>and iida et al (2006) <papid> P06-1079 </papid>focused on zero pronoun resolution assuming perfect pre-detection of zero pronouns.</citsent>
<aftsection>
<nextsent>however, we consider that zero pronoun detection and resolution have tight relation and should not be handled independently.
</nextsent>
<nextsent>our proposed model aims not only to resolve zero pronouns but to detect zero pronouns.
</nextsent>
<nextsent>zero pronouns are not expressed in text andhave to be detected prior to identifying their antecedents.
</nextsent>
<nextsent>seki et al (2002) <papid> C02-1078 </papid>proposed probabilistic model for zero pronoun detection andres olution that uses hand-crafted case frames.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4049">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>* research fellow of the japan society for the promotion of science (jsps) erence resolution and pronoun resolution, which have been studied for many years (e.g. soon et al.
</prevsent>
<prevsent>(2001); mitkov (2002); ng (2005)).<papid> P05-1020 </papid></prevsent>
</prevsection>
<citsent citstr=" P06-1079 ">
isozaki and hirao (2003) <papid> W03-1024 </papid>and iida et al (2006) <papid> P06-1079 </papid>focused on zero pronoun resolution assuming perfect pre-detection of zero pronouns.</citsent>
<aftsection>
<nextsent>however, we consider that zero pronoun detection and resolution have tight relation and should not be handled independently.
</nextsent>
<nextsent>our proposed model aims not only to resolve zero pronouns but to detect zero pronouns.
</nextsent>
<nextsent>zero pronouns are not expressed in text andhave to be detected prior to identifying their antecedents.
</nextsent>
<nextsent>seki et al (2002) <papid> C02-1078 </papid>proposed probabilistic model for zero pronoun detection andres olution that uses hand-crafted case frames.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4050">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our proposed model aims not only to resolve zero pronouns but to detect zero pronouns.
</prevsent>
<prevsent>zero pronouns are not expressed in text andhave to be detected prior to identifying their antecedents.
</prevsent>
</prevsection>
<citsent citstr=" C02-1078 ">
seki et al (2002) <papid> C02-1078 </papid>proposed probabilistic model for zero pronoun detection andres olution that uses hand-crafted case frames.</citsent>
<aftsection>
<nextsent>in order to alleviate the sparseness of hand-craftedcase frames, kawahara and kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from large corpus.
</nextsent>
<nextsent>they use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots.
</nextsent>
<nextsent>however, since the frequency is shown to be good clue for syntactic and case structure analysis (kawahara and kurohashi, 2006), <papid> N06-1023 </papid>we consider the frequency also can benefit zero pronoun detection.</nextsent>
<nextsent>therefore we propose probabilistic model for zero anaphora resolution that fully utilizes case frames.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4051">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to alleviate the sparseness of hand-craftedcase frames, kawahara and kurohashi (2004) introduced wide-coverage case frames to zero pronoun detection that are automatically constructed from large corpus.
</prevsent>
<prevsent>they use the case frames as selectional restriction for zero pronoun resolution, but do not utilize the frequency of each example of case slots.
</prevsent>
</prevsection>
<citsent citstr=" N06-1023 ">
however, since the frequency is shown to be good clue for syntactic and case structure analysis (kawahara and kurohashi, 2006), <papid> N06-1023 </papid>we consider the frequency also can benefit zero pronoun detection.</citsent>
<aftsection>
<nextsent>therefore we propose probabilistic model for zero anaphora resolution that fully utilizes case frames.
</nextsent>
<nextsent>this model directly considers the frequency and estimates case assignments for overt case components and antecedents of zero pronoun simultaneously.
</nextsent>
<nextsent>in addition, our model directly links each zero pronoun to an entity, while most existing models link it to certain mention of an entity.
</nextsent>
<nextsent>in our model, mentions and zero pronouns are treated similarly and all of them are linked to corresponding entities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4052">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>de (locative) shop, bookstore, site ? ?
</prevsent>
<prevsent>[ct:facility]:0.40, [ct:location]:0.39, ? ?
</prevsent>
</prevsection>
<citsent citstr=" N07-1010 ">
the coreference model proposed by luo (2007) <papid> N07-1010 </papid>and that proposed by yang et al (2008).</citsent>
<aftsection>
<nextsent>due to this characteristic, our model can utilize information beyond mention and easily consider salience (the importance of an entity).
</nextsent>
<nextsent>case frames describe what kinds of cases each predicate has and what kinds of nouns can fill these case slots.
</nextsent>
<nextsent>we construct case frames from large raw corpus by using the method proposed by kawahara and kurohashi (2002), <papid> C02-1122 </papid>and use themfor case structure analysis and zero anaphora res olution.</nextsent>
<nextsent>this section shows how to construct the case frames.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4053">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> construction of case frames.  </section>
<citcontext>
<prevsection>
<prevsent>due to this characteristic, our model can utilize information beyond mention and easily consider salience (the importance of an entity).
</prevsent>
<prevsent>case frames describe what kinds of cases each predicate has and what kinds of nouns can fill these case slots.
</prevsent>
</prevsection>
<citsent citstr=" C02-1122 ">
we construct case frames from large raw corpus by using the method proposed by kawahara and kurohashi (2002), <papid> C02-1122 </papid>and use themfor case structure analysis and zero anaphora res olution.</citsent>
<aftsection>
<nextsent>this section shows how to construct the case frames.
</nextsent>
<nextsent>2.1 basic method.
</nextsent>
<nextsent>after large corpus is parsed by japanese parser, case frames are constructed from modifier-head examples in the resulting parses.
</nextsent>
<nextsent>the problems ofcase frame construction are syntactic and semantic ambiguities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4055">
<title id=" C08-1097.xml">a fully lexicalized probabilistic model for japanese zero anaphora resolution </title>
<section> zero anaphora resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>(wo) = 1) (prius) and uses the higher value.
</prevsent>
<prevsent>3.3 salience score.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
previous works reported the usefulness of salience for anaphora resolution (lappin and leass, 1994; <papid> J94-4002 </papid>mitkov et al, 2002).</citsent>
<aftsection>
<nextsent>in order to consider salience of an entity, we introduce salience score, which is calculated by the following set of simple rules: ? +2 : mentioned with topical marker wa?.
</nextsent>
<nextsent>+1 : mentioned without topical marker wa?.
</nextsent>
<nextsent>+0.5 : assigned to zero pronoun.
</nextsent>
<nextsent>0.7 : beginning of each sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4058">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> information extraction.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash.
</prevsent>
<prevsent>most ie systems use some form of extraction patterns to recognize and extract relevant information.
</prevsent>
</prevsection>
<citsent citstr=" P98-1067 ">
many techniques have been developed to generate extraction patterns for new domain automatically, including palka (kim &amp; moldovan, 1993), auto slog (riloff, 1993), crystal (soderland et al ., 1995), rapier (califf, 1998), srv (freitag, 1998), <papid> P98-1067 </papid>meta-bootstrapping (riloff &amp; jones, 1999), and exdisco (yangarber et al , 2000).<papid> C00-2136 </papid></citsent>
<aftsection>
<nextsent>for this work, we will use autoslog-ts (riloff, 1996b) to generate ie patterns for the plane crash domain.autoslog-ts is derivative of auto slog that automatically generates extraction patterns by gathering statistics from corpus of relevant texts (within the domain) and irrelevant texts (outside the domain).each extraction pattern represents linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of prepositional phrase.
</nextsent>
<nextsent>for example, the following patterns could extract vehicles involved in plane crash: ? subject  crashed?, hijacked  direct-object ?, and wreckage of  np ?.
</nextsent>
<nextsent>we trained autoslog-ts using ap news stories about plane crashes as the relevant text, and ap news stories that do not mention plane crashes as the irrelevant texts.
</nextsent>
<nextsent>autoslog-ts generates listof extraction patterns, ranked according to their association with the domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4059">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> information extraction.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we will focus on the domain of plane crashes and will try to extract descriptions of the vehicle involved in the crash, victims of the crash, and the location of the crash.
</prevsent>
<prevsent>most ie systems use some form of extraction patterns to recognize and extract relevant information.
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
many techniques have been developed to generate extraction patterns for new domain automatically, including palka (kim &amp; moldovan, 1993), auto slog (riloff, 1993), crystal (soderland et al ., 1995), rapier (califf, 1998), srv (freitag, 1998), <papid> P98-1067 </papid>meta-bootstrapping (riloff &amp; jones, 1999), and exdisco (yangarber et al , 2000).<papid> C00-2136 </papid></citsent>
<aftsection>
<nextsent>for this work, we will use autoslog-ts (riloff, 1996b) to generate ie patterns for the plane crash domain.autoslog-ts is derivative of auto slog that automatically generates extraction patterns by gathering statistics from corpus of relevant texts (within the domain) and irrelevant texts (outside the domain).each extraction pattern represents linguistic expression that can extract noun phrases from one of three syntactic positions: subject, direct object, or object of prepositional phrase.
</nextsent>
<nextsent>for example, the following patterns could extract vehicles involved in plane crash: ? subject  crashed?, hijacked  direct-object ?, and wreckage of  np ?.
</nextsent>
<nextsent>we trained autoslog-ts using ap news stories about plane crashes as the relevant text, and ap news stories that do not mention plane crashes as the irrelevant texts.
</nextsent>
<nextsent>autoslog-ts generates listof extraction patterns, ranked according to their association with the domain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4060">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> cross-language projection.  </section>
<citcontext>
<prevsection>
<prevsent>one solution to this nlp-resource disparity isto transfer linguistic resources, tools, and do main knowledge from resource-rich languages to resource-impoverished ones.
</prevsent>
<prevsent>in recent years, there has been burst of projects based on this paradigm.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
yarowsky et al  (2001) <papid> H01-1035 </papid>developed cross-language projection models for part-of-speech tags, base noun phrases, named-entity tags, and morphological analysis (lemmatization) for four languages.</citsent>
<aftsection>
<nextsent>resnik et al  (2001) developed related models for projecting dependency parsers from english to chinese.
</nextsent>
<nextsent>there has also been extensive work on thecross-language transfer and development of ontologies and wordnets (e.g., (atserias et al , 1997)).
</nextsent>
<nextsent>3.2 mechanics of projection.
</nextsent>
<nextsent>the cross-language projection methodology employed in this paper is based on yarowsky et al  (2001), <papid> H01-1035 </papid>with one important exception.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4063">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> cross-language projection.  </section>
<citcontext>
<prevsection>
<prevsent>sentence align the parallel corpus.1.
</prevsent>
<prevsent>2. word-align the parallel corpus using the.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
giza++ system (och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>3.
</nextsent>
<nextsent>transfer english ie annotations and noun-.
</nextsent>
<nextsent>phrase boundaries to french via the mechanism described in yarowsky et al  (2001), <papid> H01-1035 </papid>yielding annotated sentence pairs as illustrated in figure 1.</nextsent>
<nextsent>jected annotations (described in section 4).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4066">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> transformation-based learning.  </section>
<citcontext>
<prevsection>
<prevsent>phrase boundaries to french via the mechanism described in yarowsky et al  (2001), <papid> H01-1035 </papid>yielding annotated sentence pairs as illustrated in figure 1.</prevsent>
<prevsent>jected annotations (described in section 4).</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
we used transformation-based learning (tbl) (brill, 1995) <papid> J95-4004 </papid>to learn information extraction rules for french.</citsent>
<aftsection>
<nextsent>tbl is well-suited for this task because it uses rule templates as the basis for learning, which can be easily modeled after english extraction patterns.
</nextsent>
<nextsent>however, information extraction systems typically relyon shallow parser to identify syntactic elements (e.g., subjects and direct objects) and verb1this is trivial because each sentence has numbered anchor preserved by the mt system.
</nextsent>
<nextsent>constructions (e.g., passive vs. active voice).
</nextsent>
<nextsent>ourhope was that the rules learned by tbl would be applicable to new french texts without the need for french parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4067">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> experiments and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>would constitute match.
</prevsent>
<prevsent>we used different tools to identify noun phrases in english and french.
</prevsent>
</prevsection>
<citsent citstr=" N01-1006 ">
for english, we applied the base noun phrase chunker supplied with the fntbl toolkit (ngai &amp; florian, 2001).<papid> N01-1006 </papid></citsent>
<aftsection>
<nextsent>in french, we ran part-of-speech tagger (cucerzan &amp; yarowsky, 2000) <papid> P00-1035 </papid>and applied regular-expression heuristics to detect the heads of noun phrases.</nextsent>
<nextsent>we measured agreement rates among our human annotators to assess the difficulty of the ie task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4068">
<title id=" C02-1070.xml">inducing information extraction systems for new languages via cross language projection </title>
<section> experiments and evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we used different tools to identify noun phrases in english and french.
</prevsent>
<prevsent>for english, we applied the base noun phrase chunker supplied with the fntbl toolkit (ngai &amp; florian, 2001).<papid> N01-1006 </papid></prevsent>
</prevsection>
<citsent citstr=" P00-1035 ">
in french, we ran part-of-speech tagger (cucerzan &amp; yarowsky, 2000) <papid> P00-1035 </papid>and applied regular-expression heuristics to detect the heads of noun phrases.</citsent>
<aftsection>
<nextsent>we measured agreement rates among our human annotators to assess the difficulty of the ie task.
</nextsent>
<nextsent>we computed pairwise agreement scores among our 3english annotators and among our 3 french annotators.
</nextsent>
<nextsent>the exact-word-match scores ranged from 16-31% for french and 24-27% for english.
</nextsent>
<nextsent>these relatively low numbers suggest that the exact-word match criterion is too strict.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4069">
<title id=" C04-1153.xml">learning greek verb complements addressing the class imbalance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>of the event (its context, location etc.).
</prevsent>
<prevsent>in previous work in automatic complement adjunct distinction, buchholz (1998) uses memory based learning on the part-of-speech tagged and phrase structured part of the wall street journal with generalization accuracy of 91.6% and she includes verb subcategorization information in her data.
</prevsent>
</prevsection>
<citsent citstr=" W01-0715 ">
merlo and leybold (2001) <papid> W01-0715 </papid>use decision trees to distinguish prepositional arguments from prepositional modifiers.</citsent>
<aftsection>
<nextsent>they incorporate semantic verb class, preposition and noun cluster information and reach an accuracy of 86.5% with training set of 3692 and test set of 400 instances.
</nextsent>
<nextsent>aldezabal et al.
</nextsent>
<nextsent>(2002) work on basque.
</nextsent>
<nextsent>they apply mutual information and fishers exact test to verb-case pairs (a case is any type of argument) which were obtained from partially parsed newspaper corpus of 1.3 million words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4070">
<title id=" C04-1153.xml">learning greek verb complements addressing the class imbalance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many researchers have attempted to distinguish complements from adjuncts as prerequisite for identifying verb subcategorization frames: sarkar 1 this work was supported by the eu project inspire (ist 2001-32746).
</prevsent>
<prevsent>and zeman (2000) use treebank and iteratively reduce the size of the candidate frame to filter out adjuncts.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
briscoe and carroll (1997) <papid> A97-1052 </papid>and korhonen et al (2000) <papid> W00-1325 </papid>use grammar and sophisticated parsing tool for argument-adjunct distinction.</citsent>
<aftsection>
<nextsent>in this paper we address the issue of comple ment-adjunct distinction in modern greek (mg) texts using well-known machine learning techniques (instance based learning, nave bayes, and decision trees) and minimal resources.
</nextsent>
<nextsent>we make use of input that is automatically annotated only up to the phrase level, where the verb dependents are not identified.
</nextsent>
<nextsent>therefore, significant disproportion between the number of complements and non complements (adjuncts and non-dependents) arises among the candidates (complements being significantly fewer).
</nextsent>
<nextsent>this disproportion causes significant drop in the minority (or positive) class (i.e. complements) prediction accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4071">
<title id=" C04-1153.xml">learning greek verb complements addressing the class imbalance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many researchers have attempted to distinguish complements from adjuncts as prerequisite for identifying verb subcategorization frames: sarkar 1 this work was supported by the eu project inspire (ist 2001-32746).
</prevsent>
<prevsent>and zeman (2000) use treebank and iteratively reduce the size of the candidate frame to filter out adjuncts.
</prevsent>
</prevsection>
<citsent citstr=" W00-1325 ">
briscoe and carroll (1997) <papid> A97-1052 </papid>and korhonen et al (2000) <papid> W00-1325 </papid>use grammar and sophisticated parsing tool for argument-adjunct distinction.</citsent>
<aftsection>
<nextsent>in this paper we address the issue of comple ment-adjunct distinction in modern greek (mg) texts using well-known machine learning techniques (instance based learning, nave bayes, and decision trees) and minimal resources.
</nextsent>
<nextsent>we make use of input that is automatically annotated only up to the phrase level, where the verb dependents are not identified.
</nextsent>
<nextsent>therefore, significant disproportion between the number of complements and non complements (adjuncts and non-dependents) arises among the candidates (complements being significantly fewer).
</nextsent>
<nextsent>this disproportion causes significant drop in the minority (or positive) class (i.e. complements) prediction accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4072">
<title id=" C04-1166.xml">from controlled document authoring to interactive document normalization </title>
<section> document normalization.  </section>
<citcontext>
<prevsection>
<prevsent>in some cases, the distinction between technical writers and technical translators has started to blur, so as to minimize the time and efforts needed to obtain multilingual documents.
</prevsent>
<prevsent>the paradigm of translation for monolinguals introduced by kay in 1973 (kay, 1997)1 led the way to new conception of the authoring task, which first materialized with systems involving human disambiguation (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C90-3048 ">
(boitet, 1989; somerset al, 1990)).<papid> C90-3048 </papid></citsent>
<aftsection>
<nextsent>a related paradigm emerged in the 90s (hartley and paris, 1997), whereby technical author is responsible for providing the content of document and generation system produces multilingual versions of it.
</nextsent>
<nextsent>updating documents is then done by updating the document content, and only some post editing may take place instead of full translation by human translator.
</nextsent>
<nextsent>systems implementing this paradigm range from template-based multilingual document 1this is re edition of the original article.
</nextsent>
<nextsent>figure 1: architecture of mda system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the wysiwym (what you see is what you meant) approach (power and scott, 1998).<papid> P98-2173 </papid>2 anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4073">
<title id=" C04-1166.xml">from controlled document authoring to interactive document normalization </title>
<section> document normalization.  </section>
<citcontext>
<prevsection>
<prevsent>updating documents is then done by updating the document content, and only some post editing may take place instead of full translation by human translator.
</prevsent>
<prevsent>systems implementing this paradigm range from template-based multilingual document 1this is re edition of the original article.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
figure 1: architecture of mda system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the wysiwym (what you see is what you meant) approach (power and scott, 1998).<papid> P98-2173 </papid>2 anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language.</citsent>
<aftsection>
<nextsent>the underlying content representation is then used to generate the text of the document in as many languages as the system supports.
</nextsent>
<nextsent>the mda (multilingual document authoring) system (dymetman et al, 2000; <papid> C00-1036 </papid>brunet al, 2000) follows the wysiwym approach, but puts strong emphasis on the well-formedness of document semantic content.</nextsent>
<nextsent>more particularly, document content can be specified in terms of communicative goals, allowing the selection of messages which are contrastive within the modelled class of documents in no more steps than is needed to identify predefined communicative goal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4074">
<title id=" C04-1166.xml">from controlled document authoring to interactive document normalization </title>
<section> document normalization.  </section>
<citcontext>
<prevsection>
<prevsent>figure 1: architecture of mda system creation to systems presenting the user with the evolving text of the document (often called the feedback or control text) in her language, following from the wysiwym (what you see is what you meant) approach (power and scott, 1998).<papid> P98-2173 </papid>2 anchors (or active zones) in the text of the evolving document allow the user to specify further its semantics by making choices presented to her in her language.</prevsent>
<prevsent>the underlying content representation is then used to generate the text of the document in as many languages as the system supports.</prevsent>
</prevsection>
<citsent citstr=" C00-1036 ">
the mda (multilingual document authoring) system (dymetman et al, 2000; <papid> C00-1036 </papid>brunet al, 2000) follows the wysiwym approach, but puts strong emphasis on the well-formedness of document semantic content.</citsent>
<aftsection>
<nextsent>more particularly, document content can be specified in terms of communicative goals, allowing the selection of messages which are contrastive within the modelled class of documents in no more steps than is needed to identify predefined communicative goal.
</nextsent>
<nextsent>figure 1 illustrates the architecture of mda system.
</nextsent>
<nextsent>a mda grammar specifies the possible content representations of document in terms of trees of typed semantic objects in formalism inspired from definite clause grammars (pereira and warren, 1980).
</nextsent>
<nextsent>2we have done review of these systems in (max, 2003a) in which we have identified and compared five families of approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4076">
<title id=" C04-1166.xml">from controlled document authoring to interactive document normalization </title>
<section> discussion and perspectives.  </section>
<citcontext>
<prevsection>
<prevsent>but because the expert might have already resolved some under specifications and thus identified subparts that should belong to the solution, this information should be taken into account while re analyzing the document, which is not the case in the current implementation.
</prevsent>
<prevsent>if the solution has to be present in the list of candidates returned, it should be as close to the top of the list as possible, so that the first choices for each under specification represent the actual best choices.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
to this end, we intend to implement second-pass analysis that would rerank the candidates produced by fuzzy inverted generation by computing text similarities over short passages such as those proposed in (hatzivassiloglou et al, 1999).<papid> W99-0625 </papid></citsent>
<aftsection>
<nextsent>these techniques were much harder to implement during the search in the virtual space of documents produced by the document model, because partial content representations are not actual texts.
</nextsent>
<nextsent>many thanks to marc dymetman, who supervised my work at xerox research centre europe and who originally came up with the concept of fuzzy inverted generation.
</nextsent>
<nextsent>many thanks also to christian boitet, my university phd supervisor, and to anne-lise bully, cedric leray and abdelkhalek rherad for their programming work on the interface of the presented system.
</nextsent>
<nextsent>this work was funded by phd grant from anrt and xrce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4077">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our conano tool allows for efficient, interactive annotation of connectives,scopes and relations.
</prevsent>
<prevsent>this intermediate result is exported to odonnells rst tool?,which facilitates completing the tree structure.
</prevsent>
</prevsection>
<citsent citstr=" P97-1013 ">
a number of approaches tackling the difficult problem of automatic discourse parsing have been proposed in recent years (e.g., (sumita et al , 1992) (marcu, 1997), (<papid> P97-1013 </papid>schilder, 2002)).they differ in their orientation toward symbolic or statistical information, but they all ? quite naturally ? share the assumption that the lexical connectives or discourse markers arethe primary source of information for constructing rhetorical tree automatically.</citsent>
<aftsection>
<nextsent>the density of discourse markers in text depends on its genre (e.g., commentaries tend to have more than narratives), but in general, it is clear that only portion of the relations holding in textis lexically signalled.1 furthermore, it is well known that discourse markers are often ambigu ous; for example, the english but can, in terms of (mann, thompson, 1988), signal any of the relations antithesis, contrast, and concession.accordingly, automatic discourse parsing focusing on connectives is bound to have its limitations.
</nextsent>
<nextsent>1in our corpus of newspaper commentaries (stede, 2004), <papid> W04-0213 </papid>we found that 35% of the coherence relations are signalled by connective.</nextsent>
<nextsent>our position is that progress in discourse parsing relies on the one hand on more thorough understanding of the underlying issues, and on the other hand on the availability of human-annotated corpora, which can serve asa resource for in-depth studies of discourse structural phenomena, and also for training statistical analysis programs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4078">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a number of approaches tackling the difficult problem of automatic discourse parsing have been proposed in recent years (e.g., (sumita et al , 1992) (marcu, 1997), (<papid> P97-1013 </papid>schilder, 2002)).they differ in their orientation toward symbolic or statistical information, but they all ? quite naturally ? share the assumption that the lexical connectives or discourse markers arethe primary source of information for constructing rhetorical tree automatically.</prevsent>
<prevsent>the density of discourse markers in text depends on its genre (e.g., commentaries tend to have more than narratives), but in general, it is clear that only portion of the relations holding in textis lexically signalled.1 furthermore, it is well known that discourse markers are often ambigu ous; for example, the english but can, in terms of (mann, thompson, 1988), signal any of the relations antithesis, contrast, and concession.accordingly, automatic discourse parsing focusing on connectives is bound to have its limita tions.</prevsent>
</prevsection>
<citsent citstr=" W04-0213 ">
1in our corpus of newspaper commentaries (stede, 2004), <papid> W04-0213 </papid>we found that 35% of the coherence relations are signalled by connective.</citsent>
<aftsection>
<nextsent>our position is that progress in discourse parsing relies on the one hand on more thorough understanding of the underlying issues, and on the other hand on the availability of human-annotated corpora, which can serve asa resource for in-depth studies of discourse structural phenomena, and also for training statistical analysis programs.
</nextsent>
<nextsent>two examples of such corpora are the rst tree corpus by(marcu et al , 1999) <papid> W99-0307 </papid>for english and the potsdam commentary corpus (stede, 2004) <papid> W04-0213 </papid>forgerman.</nextsent>
<nextsent>producing such resources is labour intensive task that requires time, trained annotators, and clearly specified guidelines on what relation to choose under which circumstances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4079">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1in our corpus of newspaper commentaries (stede, 2004), <papid> W04-0213 </papid>we found that 35% of the coherence relations are signalled by connective.</prevsent>
<prevsent>our position is that progress in discourse parsing relies on the one hand on more thorough understanding of the underlying issues, and on the other hand on the availability of human-annotated corpora, which can serve asa resource for in-depth studies of discourse structural phenomena, and also for training statistical analysis programs.</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
two examples of such corpora are the rst tree corpus by(marcu et al , 1999) <papid> W99-0307 </papid>for english and the potsdam commentary corpus (stede, 2004) <papid> W04-0213 </papid>forgerman.</citsent>
<aftsection>
<nextsent>producing such resources is labour intensive task that requires time, trained annotators, and clearly specified guidelines on what relation to choose under which circumstances.
</nextsent>
<nextsent>nonetheless, rhetorical analysis remains to be in part rather subjective process (see section2).
</nextsent>
<nextsent>in order to eventually arrive at more objective, comparable results, our proposal is to split the annotation process into two parts: 1.
</nextsent>
<nextsent>annotation of connectives, their scopes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4082">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> annotation of the remaining (unsignalled).  </section>
<citcontext>
<prevsection>
<prevsent>annotation of connectives, their scopes.
</prevsent>
<prevsent>(the two related textual units), and ? optionally ? the signalled relation
</prevsent>
</prevsection>
<citsent citstr=" W04-2703 ">
relations between larger segments step 1 is inspired by work done for english in the penn discourse treebank2 (miltsakaki et al , 2004).<papid> W04-2703 </papid></citsent>
<aftsection>
<nextsent>in our two-step scenario, it is the easier part of the whole task in that connectives can be quite clearly identified, their scopes are often (but not always, see below) transparent, and the coherence relation is often clear.
</nextsent>
<nextsent>we see the result of step 1 as corpus resource in its own right (it can be used for training statistical classifiers, for instance) and at the same time as the input for step 2, which fills the gaps?: now annotators have to decide how the set of small trees produced in step 1 is best arranged in one complete tree, which involves assigning 2http://www.cis.upenn.edu/pdtb/relations to instances without any lexical signals and also making more complicated scope judgements across larger spans of text ? the more subjective and also more time-consuming step.3 our approach is as follows.
</nextsent>
<nextsent>to speed up the annotation process in step 1, we have developed an xml format and dedicated analysis tool called conano, which will be introduced in section 4.
</nextsent>
<nextsent>conano can export the annotated text in the rs3?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4083">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> annotation of the remaining (unsignalled).  </section>
<citcontext>
<prevsection>
<prevsent>sdrt) are computed and the discourse structure grows step by step.
</prevsent>
<prevsent>this view is taken for instance in sdrt (asher, lascarides, 2003), which places emphasis on the notion of right frontier?
</prevsent>
</prevsection>
<citsent citstr=" J03-4002 ">
(also discussed recently by (webber et al , 2003)).<papid> J03-4002 </papid></citsent>
<aftsection>
<nextsent>however, when we trained two (experienced)students to annotate the 171 newspaper commentaries of the potsdam commentary corpus (stede, 2004) <papid> W04-0213 </papid>and upon completion of the task asked them about their experiences, very different picture emerged.</nextsent>
<nextsent>both annotators agreed that strict left-to-right approach is highly im practical, because the intended argumentative structure of the text often becomes clear onlyin retrospect, after reflecting the possible contributions of the segments to the larger scheme.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4086">
<title id=" C04-1061.xml">machine assisted rhetorical structure annotation </title>
<section> annotation of the remaining (unsignalled).  </section>
<citcontext>
<prevsection>
<prevsent>3this assessment of relative difficulty does not carry over to pdtb, where the annotations are more complex than in our step 1 but do not go as far as building rhetorical structures.thus they very soon settled on bottom-up ap proach: first, mark the transparent cases, inwhich connective undoubtedly signals relation between two segments.4 then, see how the resulting pieces fit together into structure that mirrors the argument presented.
</prevsent>
<prevsent>the annotators used rst tool (odonnell,1997), which worked reasonably well for the purpose.
</prevsent>
</prevsection>
<citsent citstr=" W02-1704 ">
however, since we also have in our group an xml-based lexicon of german connectives at our disposal (berger et al , 2002), <papid> W02-1704 </papid>why not use this resource to speed up the first phase of the annotation?</citsent>
<aftsection>
<nextsent>scope sin our definition of connective?, we largely follow (pasch et al , 2003) (a substantial catalogue and classification of german connectives), who require them to take two arguments that can potentially be full clauses and that semantically denote two-place relations between eventualities (but they need not always be spelled out as clauses).
</nextsent>
<nextsent>from the syntactic viewpoint, they area rather inhomogeneous group consisting of subordinating and coordinating conjunctions, some prepositions, and number of sententence ad verbials.
</nextsent>
<nextsent>we refer to the two related units as an internal?
</nextsent>
<nextsent>and an external?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4089">
<title id=" C02-1041.xml">automatic semantic grouping in a spoken language user interface toolkit </title>
<section> sluitk.  </section>
<citcontext>
<prevsection>
<prevsent>synonym sets and other linguistic devices, and stored in semantic frame table (sft).
</prevsent>
<prevsent>the sft becomes comprehensive database of all the possible commands user could request system to do.
</prevsent>
</prevsection>
<citsent citstr=" P91-1024 ">
it has the same function as the database of parallel translations in an example-based machine translation system (sumita and iida, 1991).<papid> P91-1024 </papid></citsent>
<aftsection>
<nextsent>attaching the slui to the back end applications.
</nextsent>
<nextsent>4.
</nextsent>
<nextsent>when the slui enabled system is. released, user may enter an nl sentence, which is translated into semantic frame by the system.
</nextsent>
<nextsent>the sft is then searched for an equivalent frame.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4090">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantically equivalent condition 2.
</prevsent>
<prevsent>substitutable in some context the most critical issue in developing such knowledge is ensuring the coverage of the paraphrase phenomena.
</prevsent>
</prevsection>
<citsent citstr=" W07-1425 ">
to attain this coverage, we have proposed strategy for dividing paraphrase phenomena into the following two classes (fujita et al, 2007)<papid> W07-1425 </papid>: (<papid> W07-1425 </papid>1) non-productive (idiosyncratic) paraphrases a. burst into tears ? cried b. comfort ? console (barzilay and mckeown, 2001) (<papid> P01-1008 </papid>2) productive paraphrases a. be in our favor ? be favorable to us b. show sharp decrease ? decrease sharply (fujita et al, 2007)<papid> W07-1425 </papid>typical examples of non-productive paraphrases are lexical paraphrases such as those shown in (1) and idiomatic paraphrases of literal phrases (e.g., kick the bucket?</citsent>
<aftsection>
<nextsent>die?).
</nextsent>
<nextsent>knowledge of this class of paraphrases should be stored static ally, because they cannot be represented with abstractpatterns.
</nextsent>
<nextsent>on the other hand, productive paraphrase is one having degree of regularity, as exhibited by the examples in (2).
</nextsent>
<nextsent>it is therefore reasonable to represent them with set of general patterns such as those shown in (3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4092">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantically equivalent condition 2.
</prevsent>
<prevsent>substitutable in some context the most critical issue in developing such knowledge is ensuring the coverage of the paraphrase phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P01-1008 ">
to attain this coverage, we have proposed strategy for dividing paraphrase phenomena into the following two classes (fujita et al, 2007)<papid> W07-1425 </papid>: (<papid> W07-1425 </papid>1) non-productive (idiosyncratic) paraphrases a. burst into tears ? cried b. comfort ? console (barzilay and mckeown, 2001) (<papid> P01-1008 </papid>2) productive paraphrases a. be in our favor ? be favorable to us b. show sharp decrease ? decrease sharply (fujita et al, 2007)<papid> W07-1425 </papid>typical examples of non-productive paraphrases are lexical paraphrases such as those shown in (1) and idiomatic paraphrases of literal phrases (e.g., kick the bucket?</citsent>
<aftsection>
<nextsent>die?).
</nextsent>
<nextsent>knowledge of this class of paraphrases should be stored static ally, because they cannot be represented with abstractpatterns.
</nextsent>
<nextsent>on the other hand, productive paraphrase is one having degree of regularity, as exhibited by the examples in (2).
</nextsent>
<nextsent>it is therefore reasonable to represent them with set of general patterns such as those shown in (3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4096">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they are tackling the problem of how precisely paraphrase knowledge can be acquired, although they have tended to notice that it is hard to acquire paraphrase knowledge that ensures full coverage of the various paraphrase phenomena from existing text corpora alone.
</prevsent>
<prevsent>to date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus.several alignment techniques have been proposed to acquire paraphrase knowledge from par allel/comparable corpora, imitating the techniques devised for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1003 ">
multiple translations of the same text (barzilay and mckeown, 2001), <papid> P01-1008 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (bannard and callison-burch, 2005) have been utilized.</citsent>
<aftsection>
<nextsent>unfortunately, this approach produces only low coverage because the size of the par allel/comparable corpora is limited.
</nextsent>
<nextsent>in the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (harris, 1968) has been adopted.
</nextsent>
<nextsent>the similarity of two expressions, computed from this hypothesis, is called distributional similarity.
</nextsent>
<nextsent>the essence of this measure is summarized as follows:feature representation: to compute the similarity, given expressions are first mapped to certain feature representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4097">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they are tackling the problem of how precisely paraphrase knowledge can be acquired, although they have tended to notice that it is hard to acquire paraphrase knowledge that ensures full coverage of the various paraphrase phenomena from existing text corpora alone.
</prevsent>
<prevsent>to date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus.several alignment techniques have been proposed to acquire paraphrase knowledge from par allel/comparable corpora, imitating the techniques devised for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" W04-3219 ">
multiple translations of the same text (barzilay and mckeown, 2001), <papid> P01-1008 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (bannard and callison-burch, 2005) have been utilized.</citsent>
<aftsection>
<nextsent>unfortunately, this approach produces only low coverage because the size of the par allel/comparable corpora is limited.
</nextsent>
<nextsent>in the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (harris, 1968) has been adopted.
</nextsent>
<nextsent>the similarity of two expressions, computed from this hypothesis, is called distributional similarity.
</nextsent>
<nextsent>the essence of this measure is summarized as follows:feature representation: to compute the similarity, given expressions are first mapped to certain feature representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4098">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>they are tackling the problem of how precisely paraphrase knowledge can be acquired, although they have tended to notice that it is hard to acquire paraphrase knowledge that ensures full coverage of the various paraphrase phenomena from existing text corpora alone.
</prevsent>
<prevsent>to date, two streams of research have evolved: one acquires paraphrase knowledge from parallel/comparable corpora, while the other uses the regular corpus.several alignment techniques have been proposed to acquire paraphrase knowledge from par allel/comparable corpora, imitating the techniques devised for machine translation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1051 ">
multiple translations of the same text (barzilay and mckeown, 2001), <papid> P01-1008 </papid>corresponding articles from multiple news sources (barzilay and lee, 2003; <papid> N03-1003 </papid>quirk et al, 2004; <papid> W04-3219 </papid>dolan et al, 2004), <papid> C04-1051 </papid>and bilingual corpus (bannard and callison-burch, 2005) have been utilized.</citsent>
<aftsection>
<nextsent>unfortunately, this approach produces only low coverage because the size of the par allel/comparable corpora is limited.
</nextsent>
<nextsent>in the second stream, i.e., paraphrase acquisition from the regular corpus, the distributional hypothesis (harris, 1968) has been adopted.
</nextsent>
<nextsent>the similarity of two expressions, computed from this hypothesis, is called distributional similarity.
</nextsent>
<nextsent>the essence of this measure is summarized as follows:feature representation: to compute the similarity, given expressions are first mapped to certain feature representations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4100">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the similarity of two expressions, computed from this hypothesis, is called distributional similarity.
</prevsent>
<prevsent>the essence of this measure is summarized as follows:feature representation: to compute the similarity, given expressions are first mapped to certain feature representations.
</prevsent>
</prevsection>
<citsent citstr=" W05-1202 ">
expressions that co-occur with the given expression, such as adjacent words (barzilay and mckeown,2001; <papid> P01-1008 </papid>lin and pantel, 2001), and modi fiers/modifiees (yamamoto, 2002; weeds et al., 2005), <papid> W05-1202 </papid>have so far been examined.feature weighting: to precisely compute the similarity, the weight for each feature is adjusted.</citsent>
<aftsection>
<nextsent>point-wise mutual information (lin, 1998)<papid> P98-2127 </papid>and relative feature focus (geffet and da gan, 2004) are well-known examples.</nextsent>
<nextsent>feature comparison measures: to convert two feature sets into scalar value, several measures have been proposed, such as cosine, lins measure (lin, 1998)<papid> P98-2127 </papid>, kullback-leibler (kl) divergence and its variants.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4101">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the essence of this measure is summarized as follows:feature representation: to compute the similarity, given expressions are first mapped to certain feature representations.
</prevsent>
<prevsent>expressions that co-occur with the given expression, such as adjacent words (barzilay and mckeown,2001; <papid> P01-1008 </papid>lin and pantel, 2001), and modi fiers/modifiees (yamamoto, 2002; weeds et al., 2005), <papid> W05-1202 </papid>have so far been examined.feature weighting: to precisely compute the similarity, the weight for each feature is adjusted.</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
point-wise mutual information (lin, 1998)<papid> P98-2127 </papid>and relative feature focus (geffet and da gan, 2004) are well-known examples.</citsent>
<aftsection>
<nextsent>feature comparison measures: to convert two feature sets into scalar value, several measures have been proposed, such as cosine, lins measure (lin, 1998)<papid> P98-2127 </papid>, kullback-leibler (kl) divergence and its variants.</nextsent>
<nextsent>while most researchers extract fully-lexicalizedpairs of words or word sequences only, two algorithms collect template-like knowledge using dependency parsers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4103">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>while most researchers extract fully-lexicalizedpairs of words or word sequences only, two algorithms collect template-like knowledge using dependency parsers.
</prevsent>
<prevsent>dirt (lin and pantel, 2001) collects pairs of paths in dependency parses that connect two nominal entities.
</prevsent>
</prevsection>
<citsent citstr=" W04-3206 ">
tease (szpektor et al., 2004) <papid> W04-3206 </papid>discovers dependency sub-parses from theweb, based on sets of representative entities forgiven lexical item.</citsent>
<aftsection>
<nextsent>the output of these systems contains the variable slots as shown in (4).
</nextsent>
<nextsent>(4) a. wrote ? is the author of b. solves ? deals with (lin and pantel, 2001) the knowledge in (4) falls between that in (1), which is fully lexicalized, and that in (3), whichis almost fully abstracted.
</nextsent>
<nextsent>as way of enriching such template-like knowledge, pantel et al(2007) <papid> N07-1071 </papid>proposed the notion of inferential selectional preference and collected expressions that would fill those slots.as mentioned in section 1, the aim of the studies reviewed here is to collect paraphrase knowledge.</nextsent>
<nextsent>thus, they need not to take the grammatical ity of expressions into account.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4104">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the output of these systems contains the variable slots as shown in (4).
</prevsent>
<prevsent>(4) a. wrote ? is the author of b. solves ? deals with (lin and pantel, 2001) the knowledge in (4) falls between that in (1), which is fully lexicalized, and that in (3), whichis almost fully abstracted.
</prevsent>
</prevsection>
<citsent citstr=" N07-1071 ">
as way of enriching such template-like knowledge, pantel et al(2007) <papid> N07-1071 </papid>proposed the notion of inferential selectional preference and collected expressions that would fill those slots.as mentioned in section 1, the aim of the studies reviewed here is to collect paraphrase knowledge.</citsent>
<aftsection>
<nextsent>thus, they need not to take the grammatical ity of expressions into account.
</nextsent>
<nextsent>2.2 generating paraphrase instances.
</nextsent>
<nextsent>representing productive paraphrases with set of general patterns makes them maintainable and attains higher coverage of the paraphrase phenomena.
</nextsent>
<nextsent>from the transformation grammar (har 226 ris, 1957), this approach has been adopted by many researchers (melcuk and polgue`re, 1987;jacquemin, 1999; <papid> P99-1044 </papid>fujita et al, 2007)<papid> W07-1425 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4105">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>2.2 generating paraphrase instances.
</prevsent>
<prevsent>representing productive paraphrases with set of general patterns makes them maintainable and attains higher coverage of the paraphrase phenomena.
</prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
from the transformation grammar (har 226 ris, 1957), this approach has been adopted by many researchers (melcuk and polgue`re, 1987;jacquemin, 1999; <papid> P99-1044 </papid>fujita et al, 2007)<papid> W07-1425 </papid></citsent>
<aftsection>
<nextsent>an important issue arises when such pattern is used to generate instances of paraphrases by replacing its variables with specific words.
</nextsent>
<nextsent>this involves assessing the grammaticality of two expressions in addition to their semantic equivalence and substitutability.as post-generation assessment of automatically generated productive paraphrases, we have applied distributional similarity measures (fujitaand sato, 2008).
</nextsent>
<nextsent>our findings from series of empirical experiments are summarized as follows: ? search engines are useful for retrieving the contextual features of predicate phrases despite some limitations (kilgarriff, 2007).
</nextsent>
<nextsent>distributional similarity measures produce tolerable level of performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4111">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, it may not work well for dealing with general predicate phrases because it is hard to enumerate all phrases to determine the weights of features w(?, f).
</prevsent>
<prevsent>we thus simply adopted the co-occurrence frequency of the phrase and the feature as in (fujita and sato, 2008).
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
skew divergence the skew divergence, variant of kl divergence, was proposed in (lee, 1999) <papid> P99-1004 </papid>based on an insight: the substitutability of one word for another need not be symmetrical.</citsent>
<aftsection>
<nextsent>the divergence is given by the following formula: skew (t, s) = (p p + (1?
</nextsent>
<nextsent>?)p ) , where s and t are the probability distributions of features for the given original and substituted words and t, respectively.
</nextsent>
<nextsent>0 ? ?
</nextsent>
<nextsent>1 is parameter for approximating kl divergence d. the score can be recast into similarity score via, for example, the following function (fujita and sato, 2008): par skew (st) = exp (d skew (t, s)) . this measure offers an advantage: the weight for each feature is determined theoretically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4115">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> proposed probabilistic model.  </section>
<citcontext>
<prevsection>
<prevsent>both expressions are grammatical based on the characteristics of the existing measures reviewed in section 2.3, we propose probabilistic model.
</prevsent>
<prevsent>let and be the source and target predicate phrase, respectively.
</prevsent>
</prevsection>
<citsent citstr=" P05-1074 ">
assuming that is grammatical, the degree to which the above conditions are satisfied is formalized as conditional probability (t|s), as in (bannard and callison burch, 2005).<papid> P05-1074 </papid></citsent>
<aftsection>
<nextsent>then, assuming that and areparadigmatic (i.e., paraphrases) and thus do not co occur, the proposed model is derived as follows: (t|s) = ? ff (t|f)p (f |s) = ? ff (f |t)p (t) (f) (f |s) = (t) ? ff (f |t)p (f |s) (f) , where denotes set of features.
</nextsent>
<nextsent>the first factor (t) is called the grammaticality factor because it quantifies the degree to which condition 3 is satisfied, except that we assume that the given is grammatical.
</nextsent>
<nextsent>the second factor ? ff (f |t)p (f |s) (f) (sim(s, t), hereafter), on the other hand, is called the similarity factor because it approximates the degree to which conditions 1 and 2 are satisfied by summing up the overlap of the features of two expressions and t.the characteristics and advantages of the proposed model are summarized as follows: 1) asymmetric.
</nextsent>
<nextsent>2) grammaticality is assessed by (t).3) no heuristic is introduced.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4117">
<title id=" C08-1029.xml">a probabilistic model for measuring grammaticality and similarity of automatically generated paraphrases of predicate phrases </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this result is quite natural because mds cannot verify the collocation between content words in those cases where number of function words appear between them.
</prevsent>
<prevsent>on the other hand, cfds with = 3 could verify this as result of treating the sequence of function words as single node.as mentioned in a1, however, more sophisticated language model must enhance the proposed model.
</prevsent>
</prevsection>
<citsent citstr=" P07-1010 ">
one way of obtaining suitable granularity of nodes is to introduce latent classes, such as the semi-markov class model (okanohara and tsujii,2007).<papid> P07-1010 </papid></citsent>
<aftsection>
<nextsent>the existence of many orthographic variants of both the content and function words may prevent us from accurately estimating the gram maticality.
</nextsent>
<nextsent>we plan to normalize these variations by using several existing resources such as the japanese functional expression dictionary (mat suyoshi, 2008).
</nextsent>
<nextsent>a3: contrary to our expectations, the huge web corpus did not offer any advantage over the newspaper corpus: mainichi always produced better results than webcp when it was combined with the grammaticality factor or when mod was used.we can speculate that morphological and dependency parsers produce errors when features are extracted, because they are tuned to newspaper articles.
</nextsent>
<nextsent>likewise, (f |s) and (f |t) may involve noise even though they are estimated using relatively clean parts of web text that are retrieved by querying phrase candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4118">
<title id=" C04-1149.xml">integrating crosslingually relevant news articles and monolingual web documents in bilingual lexicon acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>how ever, the sizes as well as the domain of existing parallel/comparative corpora are limited, whileit is very expensive to manually collect paral lel/comparative corpora.
</prevsent>
<prevsent>therefore, it is quite important to overcome this resource scarcity bottleneck in corpus-based translation knowledge acquisition research.
</prevsent>
</prevsection>
<citsent citstr=" E03-1023 ">
in order to solve this problem, we proposed an approach of taking bilingual news articles on web news sites as source for translation knowledge acquisition (utsuro et al, 2003).<papid> E03-1023 </papid></citsent>
<aftsection>
<nextsent>in the case of web news sites in japan, japanese as well as english news articles are updated everyday.
</nextsent>
<nextsent>although most of those bilingual news articles are not parallel even if they are from the same site, certain portion of those bilingual news articles share their contents or at least report quite relevant topics.
</nextsent>
<nextsent>this characteristic is quite important for the purpose of translation knowledge acquisition.
</nextsent>
<nextsent>utsuro et al (2003) <papid> E03-1023 </papid>showed that it is possible to acquire translation knowledge of domain specific named entities, event expressions, and collocational expressions from the collection of bilingual news articles on web news sites.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4129">
<title id=" C04-1149.xml">integrating crosslingually relevant news articles and monolingual web documents in bilingual lexicon acquisition </title>
<section> re-estimating bilingual term.  </section>
<citcontext>
<prevsection>
<prevsent>in the similar way, for each japanese term in the japanese translation candidates, japanese pages which contain the japanese term are collected througha japanese search engine.
</prevsent>
<prevsent>then, texts contained in those english and japanese pages are extracted and are regarded as comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
here, standard technique of estimating bilingual term correspondences from comparable corpora (e.g., fung and yee (1998) <papid> P98-1069 </papid>andrapp (1999)) <papid> P99-1067 </papid>is employed.</citsent>
<aftsection>
<nextsent>contextual similarity between the target english term and the japanese translation candidate is measured across languages, and all the japanese translation candidates are re-ranked according to the contextual similarities.
</nextsent>
<nextsent>3.2 filtering by hits of search en-.
</nextsent>
<nextsent>ginesbefore re-estimating bilingual term correspondences using monolingual web documents, we assume there exists certain correlation between hits of the english term te and the japanese term tj returned by search engines.
</nextsent>
<nextsent>depending on the hits h(te) of te , we restrict the hits h(tj ) of tj to be within the range of lower bound hl and an upper bound hu : hl   h(tj ) ? hu as search engines, we used altavista(http://www.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4131">
<title id=" C04-1149.xml">integrating crosslingually relevant news articles and monolingual web documents in bilingual lexicon acquisition </title>
<section> re-estimating bilingual term.  </section>
<citcontext>
<prevsection>
<prevsent>in the similar way, for each japanese term in the japanese translation candidates, japanese pages which contain the japanese term are collected througha japanese search engine.
</prevsent>
<prevsent>then, texts contained in those english and japanese pages are extracted and are regarded as comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
here, standard technique of estimating bilingual term correspondences from comparable corpora (e.g., fung and yee (1998) <papid> P98-1069 </papid>andrapp (1999)) <papid> P99-1067 </papid>is employed.</citsent>
<aftsection>
<nextsent>contextual similarity between the target english term and the japanese translation candidate is measured across languages, and all the japanese translation candidates are re-ranked according to the contextual similarities.
</nextsent>
<nextsent>3.2 filtering by hits of search en-.
</nextsent>
<nextsent>ginesbefore re-estimating bilingual term correspondences using monolingual web documents, we assume there exists certain correlation between hits of the english term te and the japanese term tj returned by search engines.
</nextsent>
<nextsent>depending on the hits h(te) of te , we restrict the hits h(tj ) of tj to be within the range of lower bound hl and an upper bound hu : hl   h(tj ) ? hu as search engines, we used altavista(http://www.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4136">
<title id=" C04-1149.xml">integrating crosslingually relevant news articles and monolingual web documents in bilingual lexicon acquisition </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>previous works on bilingual term correspondence estimation from comparable corpora controlled experimental evaluation in various ways in order to reduce this computational complexity.
</prevsent>
<prevsent>for example, rapp (1999) <papid> P99-1067 </papid>filtered out bilingual term pairs with low monolingual frequencies (those below 100 times), while fung and yee (1998) <papid> P98-1069 </papid>restricted candidate bilingual term pairs to be pairs of the most frequent 118 unknown words.</prevsent>
</prevsection>
<citsent citstr=" C02-1011 ">
cao and li (2002) <papid> C02-1011 </papid>restricted candidate bilingual compound term pairs by consulting seed bilingual lexicon and requiring their constituent words to be translation of each other across languages.</citsent>
<aftsection>
<nextsent>on the other hand, in the framework of bilingual term correspondences estimation of this paper, the computational complexity of enumerating translation candidates can be easily avoided with the help of cross-language retrieval of relevant news texts.furthermore, unlike cao and li (2002), <papid> C02-1011 </papid>bilingual term correspondences for compound terms are not restricted to compositional translation.</nextsent>
<nextsent>in the framework of bilingual lexicon acquisition from cross-lingually relevant news articles on the web, it has been relatively harder to reliably estimate bilingual term correspondences for low frequency terms.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4138">
<title id=" C04-1179.xml">framenet based semantic parsing using maximum entropy models </title>
<section> maximum entropy.  </section>
<citcontext>
<prevsection>
<prevsent>it is not surprising that there is dependency between each constituents role in sentence and sentence level features reflecting this dependency improve the performance.
</prevsent>
<prevsent>in this paper, we extend our previous work (kfh) by adopting sentence level features even for frame element identification.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
me models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence, but otherwise is as uniform as possible (berger et al 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>we model the probability of class given vector of features according to the me formulation below: )],(exp[1)|( 0 xcfzxcp ii ix ?=?= here xz is normalization constant, ),( xcfi is feature function which maps each class and vector element to binary value, is the total number of feature functions, and i?
</nextsent>
<nextsent>is weight for the feature function.
</nextsent>
<nextsent>the final classification is just the class with the highest probability given its feature vector and the model.
</nextsent>
<nextsent>it is important to note that the feature functions described here are not equivalent to the subset conditional distributions that are used in &amp; js model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4139">
<title id=" C04-1179.xml">framenet based semantic parsing using maximum entropy models </title>
<section> model.  </section>
<citcontext>
<prevsection>
<prevsent>we thank dan gildea for providing the following data set: training (36,993 sentences / 75,548 frame elements), development (4,000 sentences / 8,167 frame elements), and held our test sets (3,865 sentences / 7,899 frame elements).
</prevsent>
<prevsent>we train the me models using the gis algorithm (darroch and rat cliff, 1972) as implemented in the yasmet me package (och, 2002).
</prevsent>
</prevsection>
<citsent citstr=" E03-1055 ">
for testing, we use the yasmet me tagger (bender et al 2003) <papid> E03-1055 </papid>to perform the viterbi search for choosing the most probable tag sequence for sentence using the probabilities from training.</citsent>
<aftsection>
<nextsent>feature weights are smoothed using gaussian priors with mean 0 (chen and rosenfeld, 1999).
</nextsent>
<nextsent>the standard deviation of this distribution and the number of gis iterations for training are optimized on development set for each experiment.
</nextsent>
<nextsent>table 4 shows the performance for test set.
</nextsent>
<nextsent>the evaluation is done for individual frame elements.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4140">
<title id=" C04-1014.xml">modeling of long distance context dependency </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimentation shows that about 20% of errors can be corrected by using mi-trigram model compared with the pure word trigram model.
</prevsent>
<prevsent>language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language and has been successfully applied to many domains.
</prevsent>
</prevsection>
<citsent citstr=" H90-1056 ">
among all the language modeling approaches, ngram models have been most widely used in speech recognition (jelinek 1990; gale and church 1990; <papid> H90-1056 </papid>brown et al  1992; <papid> J92-4003 </papid>yang et al  1996) and other applications.</citsent>
<aftsection>
<nextsent>while ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks, they have obvious deficiencies.
</nextsent>
<nextsent>for instance, ngram models can only capture the short-distance dependency within an words window where currently the largest practical for natural language is three.
</nextsent>
<nextsent>in the meantime, it is found that there always exist many preferred relationships between words.
</nextsent>
<nextsent>two highly associated word pairs are not only/but also?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4141">
<title id=" C04-1014.xml">modeling of long distance context dependency </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimentation shows that about 20% of errors can be corrected by using mi-trigram model compared with the pure word trigram model.
</prevsent>
<prevsent>language modeling is the attempt to characterize, capture and exploit the regularities and constraints in natural language and has been successfully applied to many domains.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
among all the language modeling approaches, ngram models have been most widely used in speech recognition (jelinek 1990; gale and church 1990; <papid> H90-1056 </papid>brown et al  1992; <papid> J92-4003 </papid>yang et al  1996) and other applications.</citsent>
<aftsection>
<nextsent>while ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks, they have obvious deficiencies.
</nextsent>
<nextsent>for instance, ngram models can only capture the short-distance dependency within an words window where currently the largest practical for natural language is three.
</nextsent>
<nextsent>in the meantime, it is found that there always exist many preferred relationships between words.
</nextsent>
<nextsent>two highly associated word pairs are not only/but also?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4142">
<title id=" C08-1020.xml">hybrid processing for grammar and style checking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems are typically equipped with amodel of target well-formedness.
</prevsent>
<prevsent>the main problem, when applied to the task of error checking is that the sentences that are the focus of grammar checker are ideally outside the scope of the grammar.
</prevsent>
</prevsection>
<citsent citstr=" P98-1086 ">
to address this problem, grammar-based checkers typically employ robustness techniques (ravin, 1988; jensen et al, 1993; douglas, 1995;menzel, 1998; heinecke et al, 1998).<papid> P98-1086 </papid></citsent>
<aftsection>
<nextsent>the addition of robustness features, while inevitable for grammar-based approach, has the disadvantage of considerably slowing down runtime performance.another issue with purely grammar-based checking is related to the scarce distribution of actual errors: thus, most effort is spent on the processing of perfectly impeccable utterances.
</nextsent>
<nextsent>finally, since coverage of real-world grammars is never perfect,these system also have difficulty to distinguish be 153 tween extra grammatical and truly ungrammaticalsentences.
</nextsent>
<nextsent>conversely, since grammars often over generate, successful parse does not guarantee wellformedness either.
</nextsent>
<nextsent>one of the two major robustness techniques used in the context of grammar-based language checking are constraint relaxation (see e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4144">
<title id=" C08-1020.xml">hybrid processing for grammar and style checking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>butto the best of our knowledge, the application of hybrid processing to grammar and style checking has not been previously investigated.in this paper, we present an implemented prototype of hybrid grammar and style checking system for german and english, called checkpoint.
</prevsent>
<prevsent>as the baseline shallow system we have takenan industrial strength grammar and controlled language style checker, which is based on the flag technology.
</prevsent>
</prevsection>
<citsent citstr=" W07-1219 ">
the deep processing platform used in the project is the pet parser (callmeier, 2000) operating on wide-coverage english and german hpsg grammars, the english resource grammar (erg) (copestake and flickinger, 2000) and the german grammar (gg) (muller and kasper, 2000; crysmann, 2005; crysmann, 2007), <papid> W07-1219 </papid>respectively.</citsent>
<aftsection>
<nextsent>the erg and the gg have been developed for over15 years and have already been used as deep processing engines in the heart-of-gold hybrid processing platform.
</nextsent>
<nextsent>we have developed an approach for the selective application of deep processing based on the error hypotheses of the shallow system.
</nextsent>
<nextsent>error detection in the deep system follows amal-rule approach.
</nextsent>
<nextsent>in order to compare the benefits of the selective application of deep processing with its non selective application, we have developed two scenarios: one parallel and one integrated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4145">
<title id=" C08-1020.xml">hybrid processing for grammar and style checking </title>
<section> error models.  </section>
<citcontext>
<prevsection>
<prevsent>4000 sentences from the english corpus were presented to the parser, of which 86.8%could be parsed with the erg, and of these, the annotators found an intended analysis for 2500 sentences, including some which correctly used mal rules.
</prevsent>
<prevsent>from these annotations, customised parse selection model was computed and then used in parsing all of the corpus, this time recording onlythe one analysis determined to be most likely according to this model.
</prevsent>
</prevsection>
<citsent citstr=" W08-1002 ">
we also compared accuracy of error detection based on this new model with the accuracy of pre-existing parse-selection model trained on tourism data for logon, and confirmed that the new model indeed improved over the old one.for german, we have not created specific statistical model yet, but, instead, we have used an existing parse selection model (crysmann, 2008) <papid> W08-1002 </papid>and combined it with some heuristics which enable us to select the best error hypothesis.</citsent>
<aftsection>
<nextsent>the heuristics check for each parsed sentence whether there is an analysis containing no mal-rule.
</nextsent>
<nextsent>if there is one and this is not ranked as the best parse, it is moved to the first position in the parse list.
</nextsent>
<nextsent>as result, we can eliminate high percentage of false alarms.
</nextsent>
<nextsent>we have evaluated the english and the german versions of checkpoint against the corpora described in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4146">
<title id=" C04-1190.xml">semi supervised training of a kernel pca based model for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although empirical results with supervised kpca models demonstrate significantly better accuracy compared to the state-of-the-art achieved by either nave bayes or maximum entropy models on senseval-2 data, we identify specific sparse data conditions under which supervised kpca models deteriorate to essentially most-frequent-sense predictor.
</prevsent>
<prevsent>we discuss the potential of kpca for leveraging unannotated data for partially-unsupervised training to address these issues, leading to composite model that combines both the supervised and semi-supervised models.
</prevsent>
</prevsection>
<citsent citstr=" P04-1081 ">
wu et al (2004) <papid> P04-1081 </papid>propose an efficient and accurate new supervised learning model for word sense disambiguation (wsd), that exploits nonlinear kernel principal component analysis (kpca) technique to make predictions implicitly based on generalizations over featurecombinations.</citsent>
<aftsection>
<nextsent>experiments performed on the senseval word sense disambiguation method is capable of outperforming other widely used wsd models including nave bayes, maximum entropy, and svm models.
</nextsent>
<nextsent>despite the excellent performance of the supervisedkpca-based wsd model on average, though, our further error analysis investigations have suggested certain limitations.
</nextsent>
<nextsent>in particular, the supervised kpca-basedmodel often appears to perform poorly when it encounters target words whose contexts are highly dissimilar to those of any previously seen instances in the training set.
</nextsent>
<nextsent>empirically, the supervised kpca-based model nearly always disambiguates target words of this kindto the most frequent sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4147">
<title id=" C04-1190.xml">semi supervised training of a kernel pca based model for word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we then discuss how data sparseness affects the model, and propose anew semi-supervised model that takes advantage of unlabeled data, along with composite model that combines both the supervised and semi-supervised models.finally, details of the experimental setup and comparative results are given.
</prevsent>
<prevsent>the long history of wsd research includes numerous statistically trained methods; space only permits us to summarize few key points here.
</prevsent>
</prevsection>
<citsent citstr=" W96-0208 ">
nave bayes models (e.g., mooney (1996), <papid> W96-0208 </papid>chodorow et al (1999), pedersen(2001), yarowsky and florian (2002)) as well as maximum entropy models (e.g., dang and palmer (2002), <papid> W02-0813 </papid>klein and manning (2002)) <papid> W02-1002 </papid>in particular have shown large degree of success for wsd, and have established challenging state-of-the-art benchmarks.</citsent>
<aftsection>
<nextsent>the senseval series of evaluations facilitates comparing the strengths and weaknesses of various wsd models on common datasets, with senseval-1 (kilgarriff and rosenzweig, 1999), senseval-2 (kilgarriff, 2001), and senseval-3 held in 1998, 2001, and 2004 respectively.
</nextsent>
<nextsent>our baseline wsd model is supervised learning model that also makes use of kernel principal component analysis (kpca), proposed by (scholkopf et al, 1998) as generalization of pca.
</nextsent>
<nextsent>kpca has been successfully applied in many areas such as de-noising of images of hand-written digits (mika et al, 1999) and modeling the distribution of non-linear datasets in the context of shape modelling for real objects (active shape models) (twin ing and taylor, 2001).
</nextsent>
<nextsent>in this section, we first review the theory of kpca and explanation of why it is suited for wsd applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4148">
<title id=" C04-1190.xml">semi supervised training of a kernel pca based model for word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we then discuss how data sparseness affects the model, and propose anew semi-supervised model that takes advantage of unlabeled data, along with composite model that combines both the supervised and semi-supervised models.finally, details of the experimental setup and comparative results are given.
</prevsent>
<prevsent>the long history of wsd research includes numerous statistically trained methods; space only permits us to summarize few key points here.
</prevsent>
</prevsection>
<citsent citstr=" W02-0813 ">
nave bayes models (e.g., mooney (1996), <papid> W96-0208 </papid>chodorow et al (1999), pedersen(2001), yarowsky and florian (2002)) as well as maximum entropy models (e.g., dang and palmer (2002), <papid> W02-0813 </papid>klein and manning (2002)) <papid> W02-1002 </papid>in particular have shown large degree of success for wsd, and have established challenging state-of-the-art benchmarks.</citsent>
<aftsection>
<nextsent>the senseval series of evaluations facilitates comparing the strengths and weaknesses of various wsd models on common datasets, with senseval-1 (kilgarriff and rosenzweig, 1999), senseval-2 (kilgarriff, 2001), and senseval-3 held in 1998, 2001, and 2004 respectively.
</nextsent>
<nextsent>our baseline wsd model is supervised learning model that also makes use of kernel principal component analysis (kpca), proposed by (scholkopf et al, 1998) as generalization of pca.
</nextsent>
<nextsent>kpca has been successfully applied in many areas such as de-noising of images of hand-written digits (mika et al, 1999) and modeling the distribution of non-linear datasets in the context of shape modelling for real objects (active shape models) (twin ing and taylor, 2001).
</nextsent>
<nextsent>in this section, we first review the theory of kpca and explanation of why it is suited for wsd applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4149">
<title id=" C04-1190.xml">semi supervised training of a kernel pca based model for word sense disambiguation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we then discuss how data sparseness affects the model, and propose anew semi-supervised model that takes advantage of unlabeled data, along with composite model that combines both the supervised and semi-supervised models.finally, details of the experimental setup and comparative results are given.
</prevsent>
<prevsent>the long history of wsd research includes numerous statistically trained methods; space only permits us to summarize few key points here.
</prevsent>
</prevsection>
<citsent citstr=" W02-1002 ">
nave bayes models (e.g., mooney (1996), <papid> W96-0208 </papid>chodorow et al (1999), pedersen(2001), yarowsky and florian (2002)) as well as maximum entropy models (e.g., dang and palmer (2002), <papid> W02-0813 </papid>klein and manning (2002)) <papid> W02-1002 </papid>in particular have shown large degree of success for wsd, and have established challenging state-of-the-art benchmarks.</citsent>
<aftsection>
<nextsent>the senseval series of evaluations facilitates comparing the strengths and weaknesses of various wsd models on common datasets, with senseval-1 (kilgarriff and rosenzweig, 1999), senseval-2 (kilgarriff, 2001), and senseval-3 held in 1998, 2001, and 2004 respectively.
</nextsent>
<nextsent>our baseline wsd model is supervised learning model that also makes use of kernel principal component analysis (kpca), proposed by (scholkopf et al, 1998) as generalization of pca.
</nextsent>
<nextsent>kpca has been successfully applied in many areas such as de-noising of images of hand-written digits (mika et al, 1999) and modeling the distribution of non-linear datasets in the context of shape modelling for real objects (active shape models) (twin ing and taylor, 2001).
</nextsent>
<nextsent>in this section, we first review the theory of kpca and explanation of why it is suited for wsd applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4150">
<title id=" C04-1190.xml">semi supervised training of a kernel pca based model for word sense disambiguation </title>
<section> supervised kpca baseline model.  </section>
<citcontext>
<prevsection>
<prevsent>instead we use kernel function to implicitly define the nonlinear mapping; in this respect kpca is similar to support vector machines (scholkopf et al, 1998).
</prevsent>
<prevsent>compared with other common analysis techniques, kpca has several advantages: ? as with other kernel methods it inherently takes combinations of predictive features into account when optimizing dimensionality reduction.
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
for natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., kudo and matsumoto (2003)).<papid> P03-1004 </papid></citsent>
<aftsection>
<nextsent>we can select suitable kernel function according to the task we are dealing with and the knowledge we have about the task.
</nextsent>
<nextsent>another advantage of kpca is that it is good at dealing with input data with very high dimensionality, condition where kernel methods excel.
</nextsent>
<nextsent>nonlinear principal components (diamantaras and kung, 1996) may be defined as follows.
</nextsent>
<nextsent>suppose we are given training set of pairs (xt, ct) where the observed vectors xt ? rn in an n-dimensional input space represent the context of the target word being disambiguated, and the correct class ct represents the sense of the word, for = 1, ..,m . suppose ? is anon linear mapping from the input space rn to the feature space . without loss of generality we assume the vectors are centered vectors in the feature space, i.e., mt=1 ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4155">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the unsupervised methods the attachment decision is based on information derived from large corpora of raw text.
</prevsent>
<prevsent>the text may be automatically processed (e.g. by shallow parsing) but not manually disambiguated.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
the most prominent unsupervised methods are the lexical association score by hindle and rooth(1993) <papid> J93-1005 </papid>and the cooccurrence values by ratnaparkhi (1998).<papid> P98-2177 </papid></citsent>
<aftsection>
<nextsent>they resulted in up to 82% correct attachments for set of around 3000 test cases from the penn treebank.
</nextsent>
<nextsent>pantel and lin (2000) <papid> P00-1014 </papid>increased the training corpus, added acollocation database and thesaurus which improved the accuracy to 84%.</nextsent>
<nextsent>in contrast, the supervised methods are based on information that the program learns from manually disambiguated cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4156">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the unsupervised methods the attachment decision is based on information derived from large corpora of raw text.
</prevsent>
<prevsent>the text may be automatically processed (e.g. by shallow parsing) but not manually disambiguated.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
the most prominent unsupervised methods are the lexical association score by hindle and rooth(1993) <papid> J93-1005 </papid>and the cooccurrence values by ratnaparkhi (1998).<papid> P98-2177 </papid></citsent>
<aftsection>
<nextsent>they resulted in up to 82% correct attachments for set of around 3000 test cases from the penn treebank.
</nextsent>
<nextsent>pantel and lin (2000) <papid> P00-1014 </papid>increased the training corpus, added acollocation database and thesaurus which improved the accuracy to 84%.</nextsent>
<nextsent>in contrast, the supervised methods are based on information that the program learns from manually disambiguated cases.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4157">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most prominent unsupervised methods are the lexical association score by hindle and rooth(1993) <papid> J93-1005 </papid>and the cooccurrence values by ratnaparkhi (1998).<papid> P98-2177 </papid></prevsent>
<prevsent>they resulted in up to 82% correct attachments for set of around 3000 test cases from the penn treebank.</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
pantel and lin (2000) <papid> P00-1014 </papid>increased the training corpus, added acollocation database and thesaurus which improved the accuracy to 84%.</citsent>
<aftsection>
<nextsent>in contrast, the supervised methods are based on information that the program learns from manually disambiguated cases.
</nextsent>
<nextsent>these cases 1this research was supported by the swiss national science foundation under grant 12-54106.98.are usually extracted from treebank.
</nextsent>
<nextsent>supervised methods are as varied as the back off approach by collins and brooks (1995) <papid> W95-0103 </papid>and the transformation-based approach by brill and resnik (1994).<papid> C94-2195 </papid></nextsent>
<nextsent>back-off scored 84% correct attachments and outperformed the transformation-based approach (80%).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4158">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the supervised methods are based on information that the program learns from manually disambiguated cases.
</prevsent>
<prevsent>these cases 1this research was supported by the swiss national science foundation under grant 12-54106.98.are usually extracted from treebank.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
supervised methods are as varied as the back off approach by collins and brooks (1995) <papid> W95-0103 </papid>and the transformation-based approach by brill and resnik (1994).<papid> C94-2195 </papid></citsent>
<aftsection>
<nextsent>back-off scored 84% correct attachments and outperformed the transformation-based approach (80%).
</nextsent>
<nextsent>even better results were reported by stetina and na gao (1997) <papid> W97-0109 </papid>who used the wordnet thesaurus with supervised learner and achieved 88% ac curacy.</nextsent>
<nextsent>all these accuracy figures were reported for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4159">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the supervised methods are based on information that the program learns from manually disambiguated cases.
</prevsent>
<prevsent>these cases 1this research was supported by the swiss national science foundation under grant 12-54106.98.are usually extracted from treebank.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
supervised methods are as varied as the back off approach by collins and brooks (1995) <papid> W95-0103 </papid>and the transformation-based approach by brill and resnik (1994).<papid> C94-2195 </papid></citsent>
<aftsection>
<nextsent>back-off scored 84% correct attachments and outperformed the transformation-based approach (80%).
</nextsent>
<nextsent>even better results were reported by stetina and na gao (1997) <papid> W97-0109 </papid>who used the wordnet thesaurus with supervised learner and achieved 88% ac curacy.</nextsent>
<nextsent>all these accuracy figures were reported for english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4160">
<title id=" C02-1004.xml">combining unsupervised and supervised methods for pp attachment disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>supervised methods are as varied as the back off approach by collins and brooks (1995) <papid> W95-0103 </papid>and the transformation-based approach by brill and resnik (1994).<papid> C94-2195 </papid></prevsent>
<prevsent>back-off scored 84% correct attachments and outperformed the transformation-based approach (80%).</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
even better results were reported by stetina and na gao (1997) <papid> W97-0109 </papid>who used the wordnet thesaurus with supervised learner and achieved 88% ac curacy.</citsent>
<aftsection>
<nextsent>all these accuracy figures were reported for english.
</nextsent>
<nextsent>we have evaluated both unsupervised and supervised methods for pp attachment disambiguation in german.
</nextsent>
<nextsent>this work was constrained by the availability of only small german treebank (10,000 sentences).
</nextsent>
<nextsent>under this constraint we found that an intertwined combination of using information from unsupervised and supervised learning leads to the best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4166">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other requisite classes of names include: biological agents causing disease, such as viruses and bac teria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment.
</prevsent>
<prevsent>1.1 generalized names.
</prevsent>
</prevsection>
<citsent citstr=" C96-1071 ">
names of these kinds, generalized names (gns), dier from conventional proper names (pns)that have been studied extensively in the literature, e.g., as part of the traditional named entity (ne) categorization task, which evolved out of the muc ne evaluation, (wakao et al, 1996; <papid> C96-1071 </papid>bikel et al, 1997; <papid> A97-1029 </papid>borthwick et al, 1998;<papid> W98-1118 </papid>collins and singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>the three mainstream ne kinds are location, person, and organization, and much research has centered on these \classical  kinds of proper names.on the other hand, the vast eld of terminology has traditionally dealt with identifying single and multi-word domain-speci expressions, for various nlp tasks, and recent years have seen growing convergence between the two elds.
</nextsent>
<nextsent>in fact, good identi cation of names of both kinds is essential for ie in general.
</nextsent>
<nextsent>in ife-bio, for example, the text: national veterinary services director dr. gideon bruckner said no cases of mad cow disease have been found in south africa.exhibits more than one problem of name identi cation and classi cation.
</nextsent>
<nextsent>we focus on generalized names, which pose numerous challenges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4168">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other requisite classes of names include: biological agents causing disease, such as viruses and bac teria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment.
</prevsent>
<prevsent>1.1 generalized names.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
names of these kinds, generalized names (gns), dier from conventional proper names (pns)that have been studied extensively in the literature, e.g., as part of the traditional named entity (ne) categorization task, which evolved out of the muc ne evaluation, (wakao et al, 1996; <papid> C96-1071 </papid>bikel et al, 1997; <papid> A97-1029 </papid>borthwick et al, 1998;<papid> W98-1118 </papid>collins and singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>the three mainstream ne kinds are location, person, and organization, and much research has centered on these \classical  kinds of proper names.on the other hand, the vast eld of terminology has traditionally dealt with identifying single and multi-word domain-speci expressions, for various nlp tasks, and recent years have seen growing convergence between the two elds.
</nextsent>
<nextsent>in fact, good identi cation of names of both kinds is essential for ie in general.
</nextsent>
<nextsent>in ife-bio, for example, the text: national veterinary services director dr. gideon bruckner said no cases of mad cow disease have been found in south africa.exhibits more than one problem of name identi cation and classi cation.
</nextsent>
<nextsent>we focus on generalized names, which pose numerous challenges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4170">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other requisite classes of names include: biological agents causing disease, such as viruses and bac teria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment.
</prevsent>
<prevsent>1.1 generalized names.
</prevsent>
</prevsection>
<citsent citstr=" W98-1118 ">
names of these kinds, generalized names (gns), dier from conventional proper names (pns)that have been studied extensively in the literature, e.g., as part of the traditional named entity (ne) categorization task, which evolved out of the muc ne evaluation, (wakao et al, 1996; <papid> C96-1071 </papid>bikel et al, 1997; <papid> A97-1029 </papid>borthwick et al, 1998;<papid> W98-1118 </papid>collins and singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>the three mainstream ne kinds are location, person, and organization, and much research has centered on these \classical  kinds of proper names.on the other hand, the vast eld of terminology has traditionally dealt with identifying single and multi-word domain-speci expressions, for various nlp tasks, and recent years have seen growing convergence between the two elds.
</nextsent>
<nextsent>in fact, good identi cation of names of both kinds is essential for ie in general.
</nextsent>
<nextsent>in ife-bio, for example, the text: national veterinary services director dr. gideon bruckner said no cases of mad cow disease have been found in south africa.exhibits more than one problem of name identi cation and classi cation.
</nextsent>
<nextsent>we focus on generalized names, which pose numerous challenges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4171">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other requisite classes of names include: biological agents causing disease, such as viruses and bac teria; vectors|organisms or animals capable of transmitting infection; and possibly names of drugs, used in treatment.
</prevsent>
<prevsent>1.1 generalized names.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
names of these kinds, generalized names (gns), dier from conventional proper names (pns)that have been studied extensively in the literature, e.g., as part of the traditional named entity (ne) categorization task, which evolved out of the muc ne evaluation, (wakao et al, 1996; <papid> C96-1071 </papid>bikel et al, 1997; <papid> A97-1029 </papid>borthwick et al, 1998;<papid> W98-1118 </papid>collins and singer, 1999).<papid> W99-0613 </papid></citsent>
<aftsection>
<nextsent>the three mainstream ne kinds are location, person, and organization, and much research has centered on these \classical  kinds of proper names.on the other hand, the vast eld of terminology has traditionally dealt with identifying single and multi-word domain-speci expressions, for various nlp tasks, and recent years have seen growing convergence between the two elds.
</nextsent>
<nextsent>in fact, good identi cation of names of both kinds is essential for ie in general.
</nextsent>
<nextsent>in ife-bio, for example, the text: national veterinary services director dr. gideon bruckner said no cases of mad cow disease have been found in south africa.exhibits more than one problem of name identi cation and classi cation.
</nextsent>
<nextsent>we focus on generalized names, which pose numerous challenges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4173">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> nomen: the learning algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>this makes discovering names in text an interesting research problem in its own right.
</prevsent>
<prevsent>the following section introduces the learning algorithm; section 3 compares our approach to related prior work; section 4 presents an evaluation of results; we conclude with discussion of evaluation and current work, in section 5.
</prevsent>
</prevsection>
<citsent citstr=" C00-2136 ">
nomen is based on bootstrapping approach,similar in essence to that employed in (yangar beret al, 2000).<papid> C00-2136 </papid></citsent>
<aftsection>
<nextsent>1 the algorithm is trained on large corpus of medical text, as described in section 4.
</nextsent>
<nextsent>2.1 pre-processing.
</nextsent>
<nextsent>a large text corpus is passed through zoner, tokenizer/lemmatizer, and part-of-speech (pos) tagger.
</nextsent>
<nextsent>the zoner is rule-based program to extract textual content from the mailing-list messages, i.e., stripping headers and footers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4179">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> candidate acquisition: compute a.  </section>
<citcontext>
<prevsection>
<prevsent>the nomen algorithm builds on some ideasin previous research.
</prevsent>
<prevsent>initially, ne classi cation centered on supervised methods, statistically learning from tagged corpora, using bayesian learning, me, etc., (wakao et al, 1996; <papid> C96-1071 </papid>bikel et al, 1997; <papid> A97-1029 </papid>borthwick et al, 1998).<papid> W98-1118 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
(cucerzan and yarowsky., 1999) <papid> W99-0612 </papid>present an unsupervised algorithms for learning proper names.</citsent>
<aftsection>
<nextsent>autoslog-ts, (rilo and jones, 1999), learns \concepts  (general nps) for lling slotsin events, which in principle can include generalized names.
</nextsent>
<nextsent>the algorithm does not use competing evidence.
</nextsent>
<nextsent>it uses syntactic heuristics which mark whole noun phrases as candidate instances, whereas nomen also attempts to learn names that appear as modi ers within np.
</nextsent>
<nextsent>4 note, this means that the algorithm is unlikely to learn candidate which occurs only once in the corpus.it can happen if the unique occurrence is anked by accepted patterns on both sides.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4180">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> candidate acquisition: compute a.  </section>
<citcontext>
<prevsection>
<prevsent>4 note, this means that the algorithm is unlikely to learn candidate which occurs only once in the corpus.it can happen if the unique occurrence is anked by accepted patterns on both sides.
</prevsent>
<prevsent>in the area of ne learning, (lp) 2 , (ciravegna, 2001), is recent high-performance, supervised algorithm that learns contextual surface-based rules separately for the left and the right side of an instance in text.
</prevsent>
</prevsection>
<citsent citstr=" C96-2212 ">
separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insucient evidence by removing uncertain items, and preventing them from polluting the set of good seeds.research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (justeson and katz, 1995), the value/nc-value method, (frantzi et al, 2000).separately, the problem of classi cation or clustering is addressed in, e.g., (ushioda, 1996) (<papid> C96-2212 </papid>strzalkowski and wang, 1996) <papid> C96-2157 </papid>presents an algorithm for learning \universal concepts,  which in principle includes both pns and generic nps|a step toward our notion of generalized names.</citsent>
<aftsection>
<nextsent>the \spotter  proceeds itera tively from handful of seeds and learns names in single category.
</nextsent>
<nextsent>dl-cotrain, (collins and singer, 1999),<papid> W99-0613 </papid>learns capitalized proper name nes from syntactically analyzed corpus.</nextsent>
<nextsent>this allows the rules to use deeper, longer-range dependencies, which are dicult to express with surface-level information alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4181">
<title id=" C02-1154.xml">unsupervised learning of generalized names </title>
<section> candidate acquisition: compute a.  </section>
<citcontext>
<prevsection>
<prevsent>4 note, this means that the algorithm is unlikely to learn candidate which occurs only once in the corpus.it can happen if the unique occurrence is anked by accepted patterns on both sides.
</prevsent>
<prevsent>in the area of ne learning, (lp) 2 , (ciravegna, 2001), is recent high-performance, supervised algorithm that learns contextual surface-based rules separately for the left and the right side of an instance in text.
</prevsent>
</prevsection>
<citsent citstr=" C96-2157 ">
separating the two sides allows the learner to accept weaker rules, and several correction phases compensate in cases of insucient evidence by removing uncertain items, and preventing them from polluting the set of good seeds.research in automatic terminology acquisition initially focused more on the problem of identi cation and statistical methods for this task, e.g., (justeson and katz, 1995), the value/nc-value method, (frantzi et al, 2000).separately, the problem of classi cation or clustering is addressed in, e.g., (ushioda, 1996) (<papid> C96-2212 </papid>strzalkowski and wang, 1996) <papid> C96-2157 </papid>presents an algorithm for learning \universal concepts,  which in principle includes both pns and generic nps|a step toward our notion of generalized names.</citsent>
<aftsection>
<nextsent>the \spotter  proceeds itera tively from handful of seeds and learns names in single category.
</nextsent>
<nextsent>dl-cotrain, (collins and singer, 1999),<papid> W99-0613 </papid>learns capitalized proper name nes from syntactically analyzed corpus.</nextsent>
<nextsent>this allows the rules to use deeper, longer-range dependencies, which are dicult to express with surface-level information alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4190">
<title id=" C04-1193.xml">acquiring an ontology for a fundamental vocabulary </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the research is part of project to construct fundamental vocabulary knowledge-base of japanese: resource that will include rich syntactic and semantic descriptions of the core vocabulary ofjapanese.
</prevsent>
<prevsent>in this paper we describe the automatic acquisition of thesaurus from the dictionary definition sentences.
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
the basic method has long pedigree (copestake, 1990; tsurumaru et al, 1991; rigau et al, 1997).<papid> P97-1007 </papid></citsent>
<aftsection>
<nextsent>the main difference from earlier work is that we use mono-stratal grammar (head-driven phrase structure grammar: pollard and sag (1994)) where the syntax and semantics are represented in the same structure.
</nextsent>
<nextsent>our extraction can thus be done directly on the semantic output of the parser.
</nextsent>
<nextsent>in the first stage, we extract the thesaurus backbone of our ontology, consisting mainly of hypernym links, although other links are also extracted (e.g., domain).
</nextsent>
<nextsent>we also link our some of this research was done while the second author was visiting the ntt communication science laboratories extracted thesaurus to an existing ontology of japanese: the goi-taikei ontology (ikehara etal., 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4192">
<title id=" C04-1193.xml">acquiring an ontology for a fundamental vocabulary </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>this was rewritten to the explanation: tool for inserting and removing screws?.
</prevsent>
<prevsent>2.2 the hinoki treebank.
</prevsent>
</prevsection>
<citsent citstr=" W02-1210 ">
in order to produce semantic representations we are using an open source hpsg grammar of japanese: jacy (siegel and bender, 2002),<papid> W02-1210 </papid>which we have extended to cover the dictionary definition sentences (bond et al, 2004).</citsent>
<aftsection>
<nextsent>we have tree banked 23,000 sentences using the [incr tsdb()] profiling environment (oepen and carroll, 2000) and used them to train parse ranking model for the pet parser (callmeier, 2002) to selectively rank the parser output.
</nextsent>
<nextsent>these tools, and the grammar, are available from the deep linguistic processing with hpsg initiative (delph-in: http://www.delph-in.
</nextsent>
<nextsent>net/).we use this parser to parse the defining sentences into full meaning representation using minimal recur sion semantics (mrs: copestake et al (2001)).<papid> P01-1019 </papid></nextsent>
<nextsent>in this section we present our work on creating an ontology.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4193">
<title id=" C04-1193.xml">acquiring an ontology for a fundamental vocabulary </title>
<section> resources.  </section>
<citcontext>
<prevsection>
<prevsent>we have tree banked 23,000 sentences using the [incr tsdb()] profiling environment (oepen and carroll, 2000) and used them to train parse ranking model for the pet parser (callmeier, 2002) to selectively rank the parser output.
</prevsent>
<prevsent>these tools, and the grammar, are available from the deep linguistic processing with hpsg initiative (delph-in: http://www.delph-in.
</prevsent>
</prevsection>
<citsent citstr=" P01-1019 ">
net/).we use this parser to parse the defining sentences into full meaning representation using minimal recur sion semantics (mrs: copestake et al (2001)).<papid> P01-1019 </papid></citsent>
<aftsection>
<nextsent>in this section we present our work on creating an ontology.
</nextsent>
<nextsent>past research on knowledge acquisition from definition sentences in japanese has primarily dealt with the task of automatically generating hierarchical structures.
</nextsent>
<nextsent>tsurumaru et al (1991) developed system for automatic thesaurus construction based on information derived from analysis of the terminal clauses of definition sentences.
</nextsent>
<nextsent>it was successful in classifying hyponym, meronym, and synonym relationships between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4198">
<title id=" C04-1193.xml">acquiring an ontology for a fundamental vocabulary </title>
<section> ontology extraction.  </section>
<citcontext>
<prevsection>
<prevsent>if we have parser thatcan produce mrs, and machine readable dictionary for that language, the knowledge acquisition system can easily be ported.
</prevsent>
<prevsent>the second reason is that we can go on to use the parser and acquisition system to acquire knowledge from non-dictionary sources.
</prevsent>
</prevsection>
<citsent citstr=" C04-1093 ">
fujii and ishikawa (2004) <papid> C04-1093 </papid>have shown how it is possible to identify definitions semi automatically, however these sources are not as standard as dictionaries and thus harder to parse using only regular expressions.</citsent>
<aftsection>
<nextsent>the third reason is that we can more easily acquire knowledge beyond simple hypernyms, for example, identifying synonyms through common definition patterns as proposed by tsuchiya et al (2001).
</nextsent>
<nextsent>the final reason is that we are ultimately interested in language understanding, and thus wish to develop parser.
</nextsent>
<nextsent>any effort spent in building and refining regular expressions is not reusable, while creating and improving grammar has intrinsic value.
</nextsent>
<nextsent>3.1 the extraction process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4202">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the relation between noun and its indispensable entity is parallel to that between verb and its arguments or obligatory cases.
</prevsent>
<prevsent>in this paper, we call indispensable entities of nouns obligatory cases.indirect anaphora resolution needs comprehensive information or dictionary of obligatory cases of nouns.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
in case of verbs, syntactic structures such as subject/object/pp in english or case markers such as ga, wo, ni in japanese can be utilize das strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (briscoe and car roll, 1997; <papid> A97-1052 </papid>kawahara and kurohashi, 2002).<papid> C02-1122 </papid></citsent>
<aftsection>
<nextsent>(kawahara and kurohashi, 2004) then utilized the automatically constructed case frames to japanese zero pronoun resolution.on the other hand, in case of nouns, obligatory cases of noun nh appear, in most cases, in the single form of noun phrase nh of nm?
</nextsent>
<nextsent>in english, or nm no nh?
</nextsent>
<nextsent>in japanese.
</nextsent>
<nextsent>this single form can express several obligatory cases, and furthermore optional cases, for example, rugby no coach?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4203">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the relation between noun and its indispensable entity is parallel to that between verb and its arguments or obligatory cases.
</prevsent>
<prevsent>in this paper, we call indispensable entities of nouns obligatory cases.indirect anaphora resolution needs comprehensive information or dictionary of obligatory cases of nouns.
</prevsent>
</prevsection>
<citsent citstr=" C02-1122 ">
in case of verbs, syntactic structures such as subject/object/pp in english or case markers such as ga, wo, ni in japanese can be utilize das strong clue to distinguish several obligatory cases and adjuncts (and adverbs), which makes it feasible to construct case frames from large corpora automatically (briscoe and car roll, 1997; <papid> A97-1052 </papid>kawahara and kurohashi, 2002).<papid> C02-1122 </papid></citsent>
<aftsection>
<nextsent>(kawahara and kurohashi, 2004) then utilized the automatically constructed case frames to japanese zero pronoun resolution.on the other hand, in case of nouns, obligatory cases of noun nh appear, in most cases, in the single form of noun phrase nh of nm?
</nextsent>
<nextsent>in english, or nm no nh?
</nextsent>
<nextsent>in japanese.
</nextsent>
<nextsent>this single form can express several obligatory cases, and furthermore optional cases, for example, rugby no coach?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4204">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or nm no nh?
</prevsent>
<prevsent>phrases to distinguish obligatory case examples and others.
</prevsent>
</prevsection>
<citsent citstr=" C96-1084 ">
work which addressed indirect anaphora in english texts so far restricts relationships to small, relatively well-defined set, mainly part-of relation like the above example (2), and utilizedhand-crafted heuristic rules or hand-crafted lexical knowledge such as wordnet (hahn et al, 1996; <papid> C96-1084 </papid>vieira and poesio, 2000; <papid> J00-4003 </papid>strube and hahn, 1999).<papid> J99-3001 </papid></citsent>
<aftsection>
<nextsent>(poesio et al, 2002) proposed method of acquiring lexical knowledge from nh of nm?
</nextsent>
<nextsent>phrases, but again concentrated on part-of relation.
</nextsent>
<nextsent>in case of japanese text analysis, (murata et al., 1999) <papid> W99-0206 </papid>proposed method of utilizing nm no nh?</nextsent>
<nextsent>phrases for indirect anaphora resolution of diverse relationships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4205">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or nm no nh?
</prevsent>
<prevsent>phrases to distinguish obligatory case examples and others.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
work which addressed indirect anaphora in english texts so far restricts relationships to small, relatively well-defined set, mainly part-of relation like the above example (2), and utilizedhand-crafted heuristic rules or hand-crafted lexical knowledge such as wordnet (hahn et al, 1996; <papid> C96-1084 </papid>vieira and poesio, 2000; <papid> J00-4003 </papid>strube and hahn, 1999).<papid> J99-3001 </papid></citsent>
<aftsection>
<nextsent>(poesio et al, 2002) proposed method of acquiring lexical knowledge from nh of nm?
</nextsent>
<nextsent>phrases, but again concentrated on part-of relation.
</nextsent>
<nextsent>in case of japanese text analysis, (murata et al., 1999) <papid> W99-0206 </papid>proposed method of utilizing nm no nh?</nextsent>
<nextsent>phrases for indirect anaphora resolution of diverse relationships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4206">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or nm no nh?
</prevsent>
<prevsent>phrases to distinguish obligatory case examples and others.
</prevsent>
</prevsection>
<citsent citstr=" J99-3001 ">
work which addressed indirect anaphora in english texts so far restricts relationships to small, relatively well-defined set, mainly part-of relation like the above example (2), and utilizedhand-crafted heuristic rules or hand-crafted lexical knowledge such as wordnet (hahn et al, 1996; <papid> C96-1084 </papid>vieira and poesio, 2000; <papid> J00-4003 </papid>strube and hahn, 1999).<papid> J99-3001 </papid></citsent>
<aftsection>
<nextsent>(poesio et al, 2002) proposed method of acquiring lexical knowledge from nh of nm?
</nextsent>
<nextsent>phrases, but again concentrated on part-of relation.
</nextsent>
<nextsent>in case of japanese text analysis, (murata et al., 1999) <papid> W99-0206 </papid>proposed method of utilizing nm no nh?</nextsent>
<nextsent>phrases for indirect anaphora resolution of diverse relationships.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4207">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(poesio et al, 2002) proposed method of acquiring lexical knowledge from nh of nm?
</prevsent>
<prevsent>phrases, but again concentrated on part-of relation.
</prevsent>
</prevsection>
<citsent citstr=" W99-0206 ">
in case of japanese text analysis, (murata et al., 1999) <papid> W99-0206 </papid>proposed method of utilizing nm no nh?</citsent>
<aftsection>
<nextsent>phrases for indirect anaphora resolution of diverse relationships.
</nextsent>
<nextsent>however, they basically used all nm no nh?
</nextsent>
<nextsent>phrases from corpora, just excluding some pre-fixed stop words.
</nextsent>
<nextsent>they confessed that an accurate analysis of nm no nhphrases is necessary for the further improvement of indirect anaphora resolution.as response to these problems and following the work in (kurohashi and sakai, 1999), <papid> P99-1062 </papid>we propose method to construct japanese nominal case frames from large corpora, based on an accurate analysis of nm no nh?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4208">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, they basically used all nm no nh?
</prevsent>
<prevsent>phrases from corpora, just excluding some pre-fixed stop words.
</prevsent>
</prevsection>
<citsent citstr=" P99-1062 ">
they confessed that an accurate analysis of nm no nhphrases is necessary for the further improvement of indirect anaphora resolution.as response to these problems and following the work in (kurohashi and sakai, 1999), <papid> P99-1062 </papid>we propose method to construct japanese nominal case frames from large corpora, based on an accurate analysis of nm no nh?</citsent>
<aftsection>
<nextsent>phrases using an ordinary dictionary and thesaurus.to examine the practical usefulness of the constructed nominal case frames, we also built system of indirect anaphora resolution based on the case frames.
</nextsent>
<nextsent>first of all, we briefly introduce ntt semantic feature dictionary employed in this paper.
</nextsent>
<nextsent>ntt semantic feature dictionary consists of asemantic feature tree, whose 3,000 nodes are semantic features, and nominal dictionary containing about 300,000 nouns, each of which isgiven one or more appropriate semantic features.
</nextsent>
<nextsent>the main purpose of using this dictionary is to calculate the similarity between two words.suppose the word and have semantic feature sx and sy, respectively, their depth is dx and dy in the semantic tree, and the depth of their lowest (most specific) common node is dc,the similarity between and y, sim(x, y), is calculated as follows: sim(x, y) = (dc ? 2)/(dx + dy).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4210">
<title id=" C04-1174.xml">automatic construction of nominal case frames and its application to indirect anaphora resolution </title>
<section> indirect anaphora resolution.  </section>
<citcontext>
<prevsection>
<prevsent>[company] company, corporation, ? ?
</prevsent>
<prevsent>to examine the practical usefulness of the constructed nominal case frames, we built preliminary system of indirect anaphora resolution based on the case frames.
</prevsent>
</prevsection>
<citsent citstr=" J94-4001 ">
an input sentence is parsed using the japanese parser, knp (kurohashi and nagao,1994).<papid> J94-4001 </papid></citsent>
<aftsection>
<nextsent>then, from the beginning of the sentence, each noun is analyzed.
</nextsent>
<nextsent>when hasmore than one case frame, the process of antecedent estimation (stated in the next para graph) is performed for each case frame, and thecase frame with the highest similarity score (de scribed below) and assignments of antecedents to the case frame are selected as final result.
</nextsent>
<nextsent>for each case slot of the target case frame ofx, its antecedent is estimated.
</nextsent>
<nextsent>a possible antecedent in the target sentence and the previous two sentences is checked.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4211">
<title id=" C02-1086.xml">implicit ambiguity resolution using incremental clustering in koreantoenglish cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in translation approach, either queries or documents are translated.
</prevsent>
<prevsent>though document translation is possible when high quality machine translation systems are available (kwon et al 1997; oard and hackett, 1997), it is not very practical.
</prevsent>
</prevsection>
<citsent citstr=" P99-1029 ">
query translation methods (hull and grefenstette, 1996; davis, 1996; eichmann et al 1998; yang et al 1998; jang et al 1999; <papid> P99-1029 </papid>chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical.</citsent>
<aftsection>
<nextsent>many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries.
</nextsent>
<nextsent>in order to achieve high performance clir using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms.
</nextsent>
<nextsent>one way of resolving query ambiguities is to use the statistics, such as mutual information (church and hanks, 1990), <papid> J90-1003 </papid>to measure associations of query terms, on the basis of existing corpora (jang et al 1999).<papid> P99-1029 </papid></nextsent>
<nextsent>document clusters, widely adopted in various applications such as browsing and viewing of document results (hearst and pedersen, 1996) or topic detection (allan et al 1998), also reflect the association of terms and documents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4212">
<title id=" C02-1086.xml">implicit ambiguity resolution using incremental clustering in koreantoenglish cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries.
</prevsent>
<prevsent>in order to achieve high performance clir using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
one way of resolving query ambiguities is to use the statistics, such as mutual information (church and hanks, 1990), <papid> J90-1003 </papid>to measure associations of query terms, on the basis of existing corpora (jang et al 1999).<papid> P99-1029 </papid></citsent>
<aftsection>
<nextsent>document clusters, widely adopted in various applications such as browsing and viewing of document results (hearst and pedersen, 1996) or topic detection (allan et al 1998), also reflect the association of terms and documents.
</nextsent>
<nextsent>lee et al (2001) showed that incorporating document re-ranking method based on document clusters into the vector space retrieval achieved the significant improvement in monolingual ir, as it contributed to resolving ambiguities caused by polysemous query terms.
</nextsent>
<nextsent>the noise or ambiguity produced by dictionary-based query translation in clir is much larger than the polysemous ambiguities in monolingual ir.
</nextsent>
<nextsent>for example, korean term ???[eun-haeng]?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4215">
<title id=" C04-1182.xml">analysis and detection of reading miscues for interactive literacy tutors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a key component of this program is the interactive book, which combines real-time multilingual speech recognition, facial animation, and natural language understanding capabilities to teach children to read and comprehend text.
</prevsent>
<prevsent>within the context of this reading program, hagen et al  (2003) demonstrated an initial speech recognition system that provides real-time reading tracking for children.
</prevsent>
</prevsection>
<citsent citstr=" N04-4007 ">
this work was later extended by hagen et al  (2004) <papid> N04-4007 </papid>to incorporate improved acoustic and language modeling strategies.</citsent>
<aftsection>
<nextsent>when tested on 106 children (ages 9-11) who were asked to read one of number of short age-appropriate stories, final system word error rate of 8.0% was demonstrated.
</nextsent>
<nextsent>while reporting raw word error rate is useful for comparison purposes to prior research, we point out that it does not provide any diagnostic information which can be used to understand factors that contribute to speech recognition error within such childrens literacy tutor programs.
</nextsent>
<nextsent>therefore, this paper extends our earlier work in two important ways.
</nextsent>
<nextsent>first, in order to understand where future improvements can be obtained, we provide novel event?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4225">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propensity of words to appear together in texts, also known as their distributional similarity is an important part of natural language processing (nlp): the need to determine semantic related ness?
</prevsent>
<prevsent>between two lexically expressed concepts is problem that pervades much of [nlp].?
</prevsent>
</prevsection>
<citsent citstr=" J06-1003 ">
(budanitsky and hirst 2006) <papid> J06-1003 </papid>2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commons attri bution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc sa/3.0/).
</nextsent>
<nextsent>some rights reserved.
</nextsent>
<nextsent>such requirements are evident in word sense disambiguation (wsd) (patwardhan et-al 2003), spelling correction (budanitsky and hirst 2006) <papid> J06-1003 </papid>and lexical chaining (morris and hirst 1991).</nextsent>
<nextsent>as well as measuring the co-occurrence of word-pairs, it is also considered useful to extend these measures to calculate the likelihood of sets of words to appear together.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4229">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>sec 281 tion 4 describes and discusses the results of the evaluation techniques used, one for evaluating word-pair similarities, which compares with previous work and human judgements, and three for evaluating word-group similarities.
</prevsent>
<prevsent>section 5 outlines the conclusions drawn from these experiments and section 6 discusses further work.
</prevsent>
</prevsection>
<citsent citstr=" D07-1061 ">
the most commonly used similarity measures are based on the wordnet lexical database (eg budanitsky and hirst 2006, <papid> J06-1003 </papid>hughes and ramage 2007) <papid> D07-1061 </papid>and number of such measures have been made publicly available (pedersen et-al 2004).</citsent>
<aftsection>
<nextsent>the problem with such methods is that they are confined to measuring words in terms of the lexical connections manually assigned to them.
</nextsent>
<nextsent>language is evolving continuously and the continual maintenance and updating of such databases is highly labour intensive, therefore, such lexical resources can never be fully up-to-date.
</nextsent>
<nextsent>in addition, the lexical connections made between words and concepts do not cover all possible relations between words.
</nextsent>
<nextsent>an important relationship between words is distributional similarity and budanitsky and hirst (2006) <papid> J06-1003 </papid>conclude that the capture of these non-classical?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4231">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this distributional similarity measure calculates the propensity of words to appear together, regardless of part-of-speech or grammatical functions.
</prevsent>
<prevsent>we assert that the world-wide-web can be used to capture distributional similarity, and is less likely to suffer from problems of coverage, found in smaller corpora.
</prevsent>
</prevsection>
<citsent citstr=" J03-3001 ">
the web as corpus has been successfully used for many areas in nlp (kilgarriff and grefenstette 2003) <papid> J03-3001 </papid>such as wsd (mihalcea and moldovan 1999), <papid> P99-1020 </papid>obtaining frequencies for bigrams (keller and lapata 2003) <papid> J03-3005 </papid>and noun compound bracketing (nakov and hearst 2005).<papid> W05-0603 </papid></citsent>
<aftsection>
<nextsent>such reliance on web search engine results does come with caveats, the most important (in this context) being that the reported hit counts may not be entirely trustworthy (kil garriff 2007).
</nextsent>
<nextsent>strube and ponzettos (2006) use the wikipedia database, which includes taxonomy of categories, and they adapt well established semantic relatedness measures originally developed for wordnet?.
</nextsent>
<nextsent>they achieve correlation coefficient of 0.48 with human judgments, which is stated as being higher than google-only and wordnet only based measure for the largest of their test datasets (the 353 word-pairs of 353-tc) chen et-al (2006) <papid> P06-1127 </papid>use the snippets returned from web-searches, in order to perform word similarity measurement that captures new usag es?</nextsent>
<nextsent>of ever evolving, live?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4232">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this distributional similarity measure calculates the propensity of words to appear together, regardless of part-of-speech or grammatical functions.
</prevsent>
<prevsent>we assert that the world-wide-web can be used to capture distributional similarity, and is less likely to suffer from problems of coverage, found in smaller corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1020 ">
the web as corpus has been successfully used for many areas in nlp (kilgarriff and grefenstette 2003) <papid> J03-3001 </papid>such as wsd (mihalcea and moldovan 1999), <papid> P99-1020 </papid>obtaining frequencies for bigrams (keller and lapata 2003) <papid> J03-3005 </papid>and noun compound bracketing (nakov and hearst 2005).<papid> W05-0603 </papid></citsent>
<aftsection>
<nextsent>such reliance on web search engine results does come with caveats, the most important (in this context) being that the reported hit counts may not be entirely trustworthy (kil garriff 2007).
</nextsent>
<nextsent>strube and ponzettos (2006) use the wikipedia database, which includes taxonomy of categories, and they adapt well established semantic relatedness measures originally developed for wordnet?.
</nextsent>
<nextsent>they achieve correlation coefficient of 0.48 with human judgments, which is stated as being higher than google-only and wordnet only based measure for the largest of their test datasets (the 353 word-pairs of 353-tc) chen et-al (2006) <papid> P06-1127 </papid>use the snippets returned from web-searches, in order to perform word similarity measurement that captures new usag es?</nextsent>
<nextsent>of ever evolving, live?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4233">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this distributional similarity measure calculates the propensity of words to appear together, regardless of part-of-speech or grammatical functions.
</prevsent>
<prevsent>we assert that the world-wide-web can be used to capture distributional similarity, and is less likely to suffer from problems of coverage, found in smaller corpora.
</prevsent>
</prevsection>
<citsent citstr=" J03-3005 ">
the web as corpus has been successfully used for many areas in nlp (kilgarriff and grefenstette 2003) <papid> J03-3001 </papid>such as wsd (mihalcea and moldovan 1999), <papid> P99-1020 </papid>obtaining frequencies for bigrams (keller and lapata 2003) <papid> J03-3005 </papid>and noun compound bracketing (nakov and hearst 2005).<papid> W05-0603 </papid></citsent>
<aftsection>
<nextsent>such reliance on web search engine results does come with caveats, the most important (in this context) being that the reported hit counts may not be entirely trustworthy (kil garriff 2007).
</nextsent>
<nextsent>strube and ponzettos (2006) use the wikipedia database, which includes taxonomy of categories, and they adapt well established semantic relatedness measures originally developed for wordnet?.
</nextsent>
<nextsent>they achieve correlation coefficient of 0.48 with human judgments, which is stated as being higher than google-only and wordnet only based measure for the largest of their test datasets (the 353 word-pairs of 353-tc) chen et-al (2006) <papid> P06-1127 </papid>use the snippets returned from web-searches, in order to perform word similarity measurement that captures new usag es?</nextsent>
<nextsent>of ever evolving, live?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4234">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this distributional similarity measure calculates the propensity of words to appear together, regardless of part-of-speech or grammatical functions.
</prevsent>
<prevsent>we assert that the world-wide-web can be used to capture distributional similarity, and is less likely to suffer from problems of coverage, found in smaller corpora.
</prevsent>
</prevsection>
<citsent citstr=" W05-0603 ">
the web as corpus has been successfully used for many areas in nlp (kilgarriff and grefenstette 2003) <papid> J03-3001 </papid>such as wsd (mihalcea and moldovan 1999), <papid> P99-1020 </papid>obtaining frequencies for bigrams (keller and lapata 2003) <papid> J03-3005 </papid>and noun compound bracketing (nakov and hearst 2005).<papid> W05-0603 </papid></citsent>
<aftsection>
<nextsent>such reliance on web search engine results does come with caveats, the most important (in this context) being that the reported hit counts may not be entirely trustworthy (kil garriff 2007).
</nextsent>
<nextsent>strube and ponzettos (2006) use the wikipedia database, which includes taxonomy of categories, and they adapt well established semantic relatedness measures originally developed for wordnet?.
</nextsent>
<nextsent>they achieve correlation coefficient of 0.48 with human judgments, which is stated as being higher than google-only and wordnet only based measure for the largest of their test datasets (the 353 word-pairs of 353-tc) chen et-al (2006) <papid> P06-1127 </papid>use the snippets returned from web-searches, in order to perform word similarity measurement that captures new usag es?</nextsent>
<nextsent>of ever evolving, live?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4235">
<title id=" C08-1036.xml">using web search results to measure word group similarity </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>such reliance on web search engine results does come with caveats, the most important (in this context) being that the reported hit counts may not be entirely trustworthy (kil garriff 2007).
</prevsent>
<prevsent>strube and ponzettos (2006) use the wikipedia database, which includes taxonomy of categories, and they adapt well established semantic relatedness measures originally developed for wordnet?.
</prevsent>
</prevsection>
<citsent citstr=" P06-1127 ">
they achieve correlation coefficient of 0.48 with human judgments, which is stated as being higher than google-only and wordnet only based measure for the largest of their test datasets (the 353 word-pairs of 353-tc) chen et-al (2006) <papid> P06-1127 </papid>use the snippets returned from web-searches, in order to perform word similarity measurement that captures new usag es?</citsent>
<aftsection>
<nextsent>of ever evolving, live?
</nextsent>
<nextsent>languages.
</nextsent>
<nextsent>a double checking model is utilized which combines the number of occurrences of the first word in the snippets of the second word, and vice-versa.
</nextsent>
<nextsent>this work achieves correlation coefficient of 0.85 with the miller and charles (1998) dataset of 28 word-pairs, but to achieve the best results, 600 700 snippets are required for each word-pair, requiring extra text-processing and searching.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4237">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when large quantity of training data is available, tuning can be effectively realized by machine learning.
</prevsent>
<prevsent>in recent years, machine learning has attracted attention in the field of automatic text summarization.
</prevsent>
</prevsection>
<citsent citstr=" P98-1009 ">
aone et al (1998) <papid> P98-1009 </papid>and kupiec et al (1995) employed bayesian classifiers, mani et al (1998), nomoto et al (1997), lin (1999), and okumura etal.</citsent>
<aftsection>
<nextsent>(1999) used decision tree learning.
</nextsent>
<nextsent>how ever, most machine learning methods overfit the training data when many features are given.
</nextsent>
<nextsent>therefore, we need to select features carefully.
</nextsent>
<nextsent>support vector machines (svms) (vapnik, 1995) is robust even when the number of features is large.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4238">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, we need to select features carefully.
</prevsent>
<prevsent>support vector machines (svms) (vapnik, 1995) is robust even when the number of features is large.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
therefore, svms have shown good performance for text categorization (joachims, 1998), chunking (kudo and matsumoto, 2001), <papid> N01-1025 </papid>and dependency structure analysis (kudo and matsumoto, 2000).<papid> W00-1303 </papid>in this paper, we present an important sentence extraction technique based on svms.</citsent>
<aftsection>
<nextsent>we verified the technique against the text summarization challenge (tsc) (fukushima and okumura, 2001) corpus.
</nextsent>
<nextsent>based on support vector machines 2.1 support vector machines (svms).
</nextsent>
<nextsent>svm is supervised learning algorithm for 2 class problems.
</nextsent>
<nextsent>training data is given by (x 1 , 1 ), ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4239">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, we need to select features carefully.
</prevsent>
<prevsent>support vector machines (svms) (vapnik, 1995) is robust even when the number of features is large.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
therefore, svms have shown good performance for text categorization (joachims, 1998), chunking (kudo and matsumoto, 2001), <papid> N01-1025 </papid>and dependency structure analysis (kudo and matsumoto, 2000).<papid> W00-1303 </papid>in this paper, we present an important sentence extraction technique based on svms.</citsent>
<aftsection>
<nextsent>we verified the technique against the text summarization challenge (tsc) (fukushima and okumura, 2001) corpus.
</nextsent>
<nextsent>based on support vector machines 2.1 support vector machines (svms).
</nextsent>
<nextsent>svm is supervised learning algorithm for 2 class problems.
</nextsent>
<nextsent>training data is given by (x 1 , 1 ), ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4242">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> important sentence extraction.  </section>
<citcontext>
<prevsection>
<prevsent>we use g(x) the distance from the hyper plane to to rank the sentences.
</prevsent>
<prevsent>2.3 features.
</prevsent>
</prevsection>
<citsent citstr=" C96-2166 ">
we define the boolean features discussed below that are associated with sentence i by taking past studies into account (zechner, 1996; <papid> C96-2166 </papid>no bata et al, 2001; hirao et al, 2001; nomoto and matsumoto, 1997).</citsent>
<aftsection>
<nextsent>we use 410 boolean variables for each i .where = (x[1], ? ?
</nextsent>
<nextsent>, x[410]).
</nextsent>
<nextsent>a real-valued feature normalized between 0 and 1 is represented by 10 boolean variables.
</nextsent>
<nextsent>each variable corresponds to an internal [i/10,(i + 1)/10) wherei = 0 to 9.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4243">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> important sentence extraction.  </section>
<citcontext>
<prevsection>
<prevsent>kw ) begins at 0 is not the beginning position of word in kw.
</prevsent>
<prevsent>named entities x[r]=1 (1r8) indicates that certain named entity class appears in i . the number of.
</prevsent>
</prevsection>
<citsent citstr=" C00-2167 ">
named entity classes is 8 (sekine and eriguchi, 2000), <papid> C00-2167 </papid>e.g., person, location.</citsent>
<aftsection>
<nextsent>we use isozakis ne recognizer (isozaki, 2001).<papid> P01-1041 </papid></nextsent>
<nextsent>conjunctionsx[r]=1 (9r61) if and only if certain conjunction is used in the sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4244">
<title id=" C02-1053.xml">extracting important sentences with support vector machines </title>
<section> important sentence extraction.  </section>
<citcontext>
<prevsection>
<prevsent>named entities x[r]=1 (1r8) indicates that certain named entity class appears in i . the number of.
</prevsent>
<prevsent>named entity classes is 8 (sekine and eriguchi, 2000), <papid> C00-2167 </papid>e.g., person, location.</prevsent>
</prevsection>
<citsent citstr=" P01-1041 ">
we use isozakis ne recognizer (isozaki, 2001).<papid> P01-1041 </papid></citsent>
<aftsection>
<nextsent>conjunctionsx[r]=1 (9r61) if and only if certain conjunction is used in the sentence.
</nextsent>
<nextsent>the number of conjunctions is 53.
</nextsent>
<nextsent>functional wordsx[r]=1 (62r234) if and only if certain functional word such as ga, ha, and ta is used in the sentence.
</nextsent>
<nextsent>the number of functional words is 173.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4245">
<title id=" C02-1144.xml">concept discovery from text </title>
<section> word similarity.  </section>
<citcontext>
<prevsection>
<prevsent>the sample size counterbalances the quadratic running time of average-link to make buckshot efficient: o(ktn + nlogn).
</prevsent>
<prevsent>the parameters and are usually considered to be small numbers.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
following (lin 1998), <papid> P98-2127 </papid>we represent each word by feature vector.</citsent>
<aftsection>
<nextsent>each feature corresponds to context in which the word occurs.
</nextsent>
<nextsent>for example, threaten with __?
</nextsent>
<nextsent>is context.
</nextsent>
<nextsent>if the word handgun occurred in this context, the context is feature of handgun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4246">
<title id=" C02-1144.xml">concept discovery from text </title>
<section> evaluation methodology.  </section>
<citcontext>
<prevsection>
<prevsent>an example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (steinbach, karypis, and kumar 2000).
</prevsent>
<prevsent>however, maximum purity is trivially achieved when each element forms its own cluster.
</prevsent>
</prevsection>
<citsent citstr=" P99-1005 ">
an example of the second approach evaluates the clusters by using them to smooth probability distributions (lee and pereira 1999).<papid> P99-1005 </papid></citsent>
<aftsection>
<nextsent>like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered.
</nextsent>
<nextsent>let be set of clusters and be the answer key.
</nextsent>
<nextsent>we define the editing distance, dist(c, a), as the number of operations required to make consistent with a. we say that is consistent with if there is one to one mapping between clusters in and the classes in such that for each cluster in c, all elements of belong to the same class in a. we allow two editing operations: ? merge two clusters; and ? move an element from one cluster to another.
</nextsent>
<nextsent>let be the baseline clustering where each element is its own cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4247">
<title id=" C02-1144.xml">concept discovery from text </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>we smooth the probabilities by assuming that all siblings are equally likely given the parent.
</prevsent>
<prevsent>a class is then defined as the maximal sub hierarchy with probability less than threshold (we used e-2).
</prevsent>
</prevsection>
<citsent citstr=" C94-1079 ">
we used minipar 1 (lin 1994), <papid> C94-1079 </papid>broad coverage english parser, to parse about 1gb (144m words) of newspaper text from the trec collection (1988 ap newswire, 1989-90 la times, and 1991 san jose mercury) at speed of about 500 words/second on piii-750 with 512mb memory.</citsent>
<aftsection>
<nextsent>we collected the frequency counts of the grammatical relationships (contexts) output by minipar and used them to compute the pointwise mutual information values from section 3.
</nextsent>
<nextsent>the test set is constructed by intersecting the words in wordnet with the nouns in the corpus whose total mutual information with all of its contexts exceeds threshold m. since wordnet has low coverage of proper names, we removed all capitalized nouns.
</nextsent>
<nextsent>we constructed two test sets: s13403 consisting of 13403 words (m = 250) and s3566 consisting of 3566 words (m = 3500).
</nextsent>
<nextsent>we then removed from the answer classes the words that did not occur in the test sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4248">
<title id=" C02-1137.xml">a new probabilistic model for title generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>language.
</prevsent>
<prevsent>therefore, title-word-document-word translation?
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
probability p(tw|dw) can be learned from the training corpus using statistical translation model (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>word ordering component p(t)/p({twt}).
</nextsent>
<nextsent>there are two terms in this component, namely p(t) and p({twt}).
</nextsent>
<nextsent>as already used by the old framework for title generation, p(t) can be estimated using ngram statistical language model (clarkson &amp; rosenfeld, 1997).
</nextsent>
<nextsent>the term p({twt}), by assuming the independence between words tw, can be written as the product of the occurring probability of each tw in t, i.e. ? ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4249">
<title id=" C04-1189.xml">hitiqa towards analytical question answering </title>
<section> text framing.  </section>
<citcontext>
<prevsection>
<prevsent>we used both setups under different conditions: the generic frames were used with trec document collection to measure impact of ir precision on qa accuracy (small et al, 2004).
</prevsent>
<prevsent>the domain-adapted frames were used for sessions with intelligence analysts working with the wmd domain (see below).
</prevsent>
</prevsection>
<citsent citstr=" C96-2157 ">
currently, the adaptation process includes manual tuning followed by corpus bootstrapping using an unsupervised learning method (strzalkowski &amp; wang, 1996).<papid> C96-2157 </papid></citsent>
<aftsection>
<nextsent>we generally relyon bbns identifinder for extraction of basic entities, and use bootstrapping to define additional entity types as well as to assign roles to attributes.
</nextsent>
<nextsent>the version of hitiqa reported here and used by analysts during the evaluation has been adapted to the weapons of mass destruction nonproliferation domain (wmd domain, henceforth).
</nextsent>
<nextsent>figure 3 contains an example passage from this dataset.
</nextsent>
<nextsent>in the wmd domain, the typed frames were mapped onto wmdtransfer 3-role frame, and two 2-role frames wmdtreaty and wmddevelop.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4250">
<title id=" C04-1189.xml">hitiqa towards analytical question answering </title>
<section> text framing.  </section>
<citcontext>
<prevsection>
<prevsent>figure 3: text passage from the wmd domain data hitiqa frames define top-down constraints on how to interpret given text passage, which is quite different from muc4 template filling task 4 muc, the message understanding conference, funded by.
</prevsent>
<prevsent>darpa, involved the evaluation of information extraction systems applied to common task.
</prevsent>
</prevsection>
<citsent citstr=" M98-1007 ">
(humphreys et al, 1998).<papid> M98-1007 </papid></citsent>
<aftsection>
<nextsent>what were trying to do here is to fit?
</nextsent>
<nextsent>a frame over text passage.
</nextsent>
<nextsent>this also means that multiple frames can be associated with text passage, or to be exact, with cluster of passages.
</nextsent>
<nextsent>since most of the passages that undergo the framing process are part of some cluster of very similar passages, the added redundancy helps to reinforce the most salient features for extraction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4251">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation.
</prevsent>
<prevsent>we demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen wsj text.we believe this is major step towards wide coverage semantic interpretation, one of the key objectives of the field of nlp.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the levels of accuracy and robustness recently achieved by statistical parsers (e.g. collins (1999),charniak (2000)) <papid> A00-2018 </papid>have led to their use in number of nlp applications, such as question-answering(pasca and harabagiu, 2001), machine translation (charniak et al, 2003), sentence simplification (carroll et al, 1999), and linguists search engine (resnik and elkiss, 2003).</citsent>
<aftsection>
<nextsent>such parsers typically return phrase-structure trees in the styleof the penn treebank, but without traces and co indexation.
</nextsent>
<nextsent>however, the usefulness of this output is limited, since the underlying meaning (as represented in predicate-argument structure or logical form) is difficult to reconstruct from such skeletal parse trees.in this paper we demonstrate how wide coverage statistical parser using combinatory categorial grammar (ccg) can be used to generate semantic representations.
</nextsent>
<nextsent>there are number of advantages to using ccg for this task.
</nextsent>
<nextsent>first, ccg provides surface compositional?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4252">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>second, ccg isa lexicalised grammar, and only uses small number of semantically transparent combinatory rules to combine ccg categories.
</prevsent>
<prevsent>hence providing compositional semantics for ccg simply amounts to assigning semantic representations to the lexical entries and interpreting the combinatory rules.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
and third, there exist highly accurate, efficient and robust ccg parsers which can be used directly for this task (clark and curran, 2004<papid> P04-1014 </papid>b; hockenmaier, 2003).the existing ccg parsers deliver predicate argument structures, but not semantic representations that can be used for inference.</citsent>
<aftsection>
<nextsent>the present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use in various nlp applications that require semantic in terpretation.we show how to construct first-order representations from ccg derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen wsj text.
</nextsent>
<nextsent>the only other deep parser we are aware of to achieve such levels of robustness for the wsj is kaplan et al (2004).<papid> N04-1013 </papid></nextsent>
<nextsent>the use of the ?-calculusis integral to our method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4254">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and third, there exist highly accurate, efficient and robust ccg parsers which can be used directly for this task (clark and curran, 2004<papid> P04-1014 </papid>b; hockenmaier, 2003).the existing ccg parsers deliver predicate argument structures, but not semantic representations that can be used for inference.</prevsent>
<prevsent>the present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use in various nlp applications that require semantic in terpretation.we show how to construct first-order representations from ccg derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen wsj text.</prevsent>
</prevsection>
<citsent citstr=" N04-1013 ">
the only other deep parser we are aware of to achieve such levels of robustness for the wsj is kaplan et al (2004).<papid> N04-1013 </papid></citsent>
<aftsection>
<nextsent>the use of the ?-calculusis integral to our method.
</nextsent>
<nextsent>however, first-order representations are simply used as proof-of-concept; we could have used drss (kamp and reyle, 1993)or some other representation more tailored to the application in hand.there is some existing work with similar motivation to ours.
</nextsent>
<nextsent>briscoe and carroll (2002) generate underspecified semantic representations from their robust parser.
</nextsent>
<nextsent>toutanova et al (2002) and kaplan et al (2004) <papid> N04-1013 </papid>combine statistical methods with linguistically motivated grammar formalism (hpsg and lfg respectively) in an attempt to achieve levels of robustness and accuracy comparable to the penn treebank parsers (which kaplan et al do achieve).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4256">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>so abc is equivalent to  ab  c. the order of arguments in the predication is wrapped?, consistent with the facts of reflexive binding.
</prevsent>
<prevsent>2some details of the derivation and of the semantics ofnoun phrases are suppressed, since these are developed bewhile the proliferation of surface constituents allowed by ccg adds to derivational ambiguity (since the constituent taxpayers 15 million to install is also allowed in the non-coordinate sentence it could cost taxpayers 15 million to install), previous workhas shown that standard techniques from the statistical parsing literature can be used for practical wide coverage parsing with state-of-the-art performance.
</prevsent>
</prevsection>
<citsent citstr=" P02-1042 ">
a number of statistical parsers have recently been developed for ccg (clark et al, 2002; <papid> P02-1042 </papid>hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark and curran,2004<papid> P04-1014 </papid>b).</citsent>
<aftsection>
<nextsent>all of these parsers use grammar derived from ccgbank (hockenmaier and steedman,2002<papid> P02-1043 </papid>a; hockenmaier, 2003), treebank of normal form ccg derivations derived semi-automatically from the penn treebank.</nextsent>
<nextsent>in this paper we use the clark and curran (2004<papid> P04-1014 </papid>b) parser, which uses loglinear model of normal-form derivations to select an analysis.the parser takes pos tagged sentence as in put with set of lexical categories assigned toeach word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4257">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent>so abc is equivalent to  ab  c. the order of arguments in the predication is wrapped?, consistent with the facts of reflexive binding.
</prevsent>
<prevsent>2some details of the derivation and of the semantics ofnoun phrases are suppressed, since these are developed bewhile the proliferation of surface constituents allowed by ccg adds to derivational ambiguity (since the constituent taxpayers 15 million to install is also allowed in the non-coordinate sentence it could cost taxpayers 15 million to install), previous workhas shown that standard techniques from the statistical parsing literature can be used for practical wide coverage parsing with state-of-the-art performance.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
a number of statistical parsers have recently been developed for ccg (clark et al, 2002; <papid> P02-1042 </papid>hockenmaier and steedman, 2002<papid> P02-1043 </papid>b; clark and curran,2004<papid> P04-1014 </papid>b).</citsent>
<aftsection>
<nextsent>all of these parsers use grammar derived from ccgbank (hockenmaier and steedman,2002<papid> P02-1043 </papid>a; hockenmaier, 2003), treebank of normal form ccg derivations derived semi-automatically from the penn treebank.</nextsent>
<nextsent>in this paper we use the clark and curran (2004<papid> P04-1014 </papid>b) parser, which uses loglinear model of normal-form derivations to select an analysis.the parser takes pos tagged sentence as in put with set of lexical categories assigned toeach word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4288">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> the parser.  </section>
<citcontext>
<prevsection>
<prevsent> np.
</prevsent>
<prevsent>forgiven sentence, the automatically extracted grammar can produce very large number of derivations.
</prevsent>
</prevsection>
<citsent citstr=" W03-1013 ">
clark and curran (2003) <papid> W03-1013 </papid>and clark and curran (2004<papid> P04-1014 </papid>b) describe how packed chart can be used to efficiently represent the derivation space, and also efficient algorithms for finding the most probable derivation.</citsent>
<aftsection>
<nextsent>the parser uses log-linear model over normal-form derivations.3 features are defined in terms of the local trees in the derivation, including lexical head information and word-word dependencies.
</nextsent>
<nextsent>the normal-form derivations in ccgbank provide the gold standard training data.
</nextsent>
<nextsent>forgiven sentence, the output of the parser is set of syntactic dependencies corresponding to the3a normal-form derivation is one which only uses type raising and function composition when necessary.
</nextsent>
<nextsent>most probable derivation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4300">
<title id=" C04-1180.xml">wide coverage semantic representations from a ccg parser </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>in future work we will investigate using underspecified semantic representations.
</prevsent>
<prevsent>the utility of our system for nlp application swill be tested by integration with an existing open domain question-answering system (leidner et al, 2003).we will also investigate the construction of treebank of semantic representations derived automatically from ccgbank.
</prevsent>
</prevsection>
<citsent citstr=" C02-1105 ">
previous work, such as liakata and pulman (2002) <papid> C02-1105 </papid>and cahill et al (2003), has attempted to generate semantic representations from the penn treebank.</citsent>
<aftsection>
<nextsent>cahill et al use translation of the treebank to lfg f-structures and quasi logical forms.
</nextsent>
<nextsent>an advantage of our approach isthat our system for constructing semantic representations, whatever semantic formalism is used, can be applied directly to the derivations in ccgbank.
</nextsent>
<nextsent>acknowledgements this research was partially supported by epsrc grant gr/m96889.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4301">
<title id=" C04-1124.xml">detecting multiword verbs in the english sublanguage of medline abstracts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>et al (2003) briefly presented method to find domain-specific verbs by filtering the verbs in stop list, at the same time, using the co-occurrence of verb and specific terms in the text.
</prevsent>
<prevsent>in our experiment, the domain specific verbs are determined through the comparison between different corpora in different domains,or through genre analysis of the sublanguage dominated corpus.the second problem is how to determine multiword verbs (mwvs).
</prevsent>
</prevsection>
<citsent citstr=" W02-2001 ">
here we do not make differences between the more detailed classification of multiword verbs, especially the verb particle constructions and verb-preposition constructions (baldwin and villavicencio, 2002).<papid> W02-2001 </papid></citsent>
<aftsection>
<nextsent>as subcategory of multiword expressions (sag et al,2002), mwvs raise the complexity of our processing.
</nextsent>
<nextsent>because some mwvs share the same verb head but lead to different semantic interpretations, like result in and result from, considering only verb heads in the processing is obviously not sufficient.a goodie system should deal with such mwvs automatically and appropriately.the third problem is that there is need to investigate the inflectional and derivational forms of the verbs.
</nextsent>
<nextsent>an ie system may have to deal with aset of patterns, in which the inflectional and derivational forms of the verbs should be taken into account.
</nextsent>
<nextsent>for example, in biomedical texts, the verb interact defines binary relation between two substances, whereas its nominal ization morpheme in pattern such as the interaction of ... with ... also constructs such relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4302">
<title id=" C04-1124.xml">detecting multiword verbs in the english sublanguage of medline abstracts </title>
<section> tokeniser, pos tagger and chunker.  </section>
<citcontext>
<prevsection>
<prevsent>tokeniser: following the whit espace delimited token isation discipline, the tokeniser determines the segmentation of the non-lexicalentries such as tokens with non-alphabet characters or abbreviations.
</prevsent>
<prevsent>after token isation, the sentence boundaries are determined as well.?
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
pos tagger: the maximum entropy pos tagger developed by ratnaparkhi (ratnaparkhi,1996) <papid> W96-0213 </papid>and the rule-based pos tagger developed by brill (brill, 1994) are trained with1200 abstracts extracted from the genia corpus, which achieve accuracies of 97.97% and 98.06% respectively, when testing on the rest 800 abstract of the genia corpus.</citsent>
<aftsection>
<nextsent>since our test corpus is directly extracted from the pos tagged genia corpus v3.0p, we do not have to apply the process of token isation and pos tagging.?
</nextsent>
<nextsent>chunker: in this experiment, unlike the traditional statistical method for collocation extraction, where sentences are treated as word sequences (manning and schutze, 2002), shallow chunking process is first carried out.
</nextsent>
<nextsent>then, sentences in our test corpus are treated as chunk sequences.
</nextsent>
<nextsent>up to now, the chunker consists of two parts, both utilize wordnet 1.7.13 (fellbaum, 1999) as the lexical resource for the lemmatization, i.e., as the verb and noun stemmer.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4303">
<title id=" C04-1124.xml">detecting multiword verbs in the english sublanguage of medline abstracts </title>
<section> mwv extraction.  </section>
<citcontext>
<prevsection>
<prevsent>purely syntactic processing of mwes requires specific linguistic knowledge across the different domains of language, such as semantic ontology (piao et al, 3http://www.cogsci.princeton.edu/wn/index.shtml 4presented in pairs of matching parentheses.
</prevsent>
<prevsent>2003).
</prevsent>
</prevsection>
<citsent citstr=" W03-1807 ">
purely statistical processing over generates the mwe candidates (gael dias, 2002), and is not sensitive enough to the mwe candidates with low frequencies (piao et al, 2003).<papid> W03-1807 </papid></citsent>
<aftsection>
<nextsent>it is practical in some cases for hybrid syntactic-statistical system to pre-define set of mwe pattern rules, then use statistical techniques to filter proper candidates.
</nextsent>
<nextsent>but it lacks the flexibility to obtain comprehensive coverage of possible mwe candidates, especially when mwv is non-contiguous in our case.
</nextsent>
<nextsent>in addition, it also suffers from the problem of over generation, if the pre-defined syntactic pattern occurs rarely in the corpus.
</nextsent>
<nextsent>sag et al (2002) indicated that it is very necessary to find the balance between the two methods in hybrid systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4307">
<title id=" C04-1124.xml">detecting multiword verbs in the english sublanguage of medline abstracts </title>
<section> mwv extraction.  </section>
<citcontext>
<prevsection>
<prevsent>this point of view is taken into account in our approach.
</prevsent>
<prevsent>3.2 extraction of contiguous mwv.
</prevsent>
</prevsection>
<citsent citstr=" W03-1809 ">
candidates number of works about mwv extraction from corpora are based on the output of pos tagger and chunker (baldwin and villavicencio, 2002; <papid> W02-2001 </papid>bannard et al, 2003), <papid> W03-1809 </papid>or the output of aparser (mccarthy et al, 2003).<papid> W03-1810 </papid></citsent>
<aftsection>
<nextsent>these works extracted mainly the verb+particle structures.
</nextsent>
<nextsent>similar to those works, the mwv extraction in our experiment is also based on the chunking output.
</nextsent>
<nextsent>but, since mwvs has various pos tag patterns, it is not practical to assign each pattern an according syntactic rule.
</nextsent>
<nextsent>therefore variation of finite state automaton is considered in our approach for the extraction of mwvs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4308">
<title id=" C04-1124.xml">detecting multiword verbs in the english sublanguage of medline abstracts </title>
<section> mwv extraction.  </section>
<citcontext>
<prevsection>
<prevsent>this point of view is taken into account in our approach.
</prevsent>
<prevsent>3.2 extraction of contiguous mwv.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
candidates number of works about mwv extraction from corpora are based on the output of pos tagger and chunker (baldwin and villavicencio, 2002; <papid> W02-2001 </papid>bannard et al, 2003), <papid> W03-1809 </papid>or the output of aparser (mccarthy et al, 2003).<papid> W03-1810 </papid></citsent>
<aftsection>
<nextsent>these works extracted mainly the verb+particle structures.
</nextsent>
<nextsent>similar to those works, the mwv extraction in our experiment is also based on the chunking output.
</nextsent>
<nextsent>but, since mwvs has various pos tag patterns, it is not practical to assign each pattern an according syntactic rule.
</nextsent>
<nextsent>therefore variation of finite state automaton is considered in our approach for the extraction of mwvs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4318">
<title id=" C04-1018.xml">playing the telephone game determining the hierarchical structure of perspective and speech expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, facts, events, and opinions are filtered by the point ofview of the writer and other sources.
</prevsent>
<prevsent>unfortunately, this filtering of information through multiple sources (and multiple points of view) complicates the natural language interpretation process because the reader (human or machine) must take into account the biases introduced by this indirection.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
it is important for understanding both newswire and narrative text (wiebe, 1994), <papid> J94-2004 </papid>therefore, to appropriately recognize expressions of point of view, and to associate them with their direct and indirect sources.</citsent>
<aftsection>
<nextsent>this paper introduces two kinds of expression that can filter information.
</nextsent>
<nextsent>first, we define perspective expression to be the minimal span of text that denotes the presence of an explicit opinion, evaluation, emotion, speculation, belief, sentiment, etc.1 private state is the general term typically used 1note that implicit expressions of perspective, i.e. wiebe et to refer to these mental and emotional states that cannot be directly observed or verified (quirk et al,1985).
</nextsent>
<nextsent>further, we define the source of perspective expression to be the experiencer of that private state, that is, the person or entity whose opinion or emotion is being conveyed in the text.
</nextsent>
<nextsent>second,speech expressions simply convey the words of an other individual ? and by the choice of words, the reporter filters the original sources intent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4320">
<title id=" C04-1018.xml">playing the telephone game determining the hierarchical structure of perspective and speech expressions </title>
<section> 1810 </section>
<citcontext>
<prevsection>
<prevsent>as shown in table 2, however, we find that sentences with multiple non-writer pses (i.e. sentences that contain 3 or more total pses)comprise significant portion (29.98%) of our corpus.
</prevsent>
<prevsent>an advantage over our work, however, is that bethard et al (2004) do not require separate solutions to pse identification and the identification of their direct sources.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
automatic identification of sources has also been addressed indirectly by gildea and jurafskys (2002) <papid> J02-3001 </papid>work on semantic role identification in that finding sources often corresponds to finding the filler of the agent role for verbs.</citsent>
<aftsection>
<nextsent>their methods then might be used to identify sources and associate them with pses that are verbs or portions of verb phrases.
</nextsent>
<nextsent>whether their work will also apply to pses that are realized as other parts of speech is an open question.
</nextsent>
<nextsent>wiebe (1994), <papid> J94-2004 </papid>studies methods to track the change of point of view?</nextsent>
<nextsent>in narrative text (fiction).that is, the writer?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4322">
<title id=" C04-1018.xml">playing the telephone game determining the hierarchical structure of perspective and speech expressions </title>
<section> 1810 </section>
<citcontext>
<prevsection>
<prevsent>3.2 resources.
</prevsent>
<prevsent>we relyon variety of resources to generate our features.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
the corpus (see section 4) is distributed with annotations for sentence breaks, tokenization, and part of speech information automatically generated by the gate toolkit (cunningham et al, 2002).<papid> P02-1022 </papid>8 for parsing we use the collins (1999) parser.9 for partial parses, we employ cass (abney, 1997).</citsent>
<aftsection>
<nextsent>finally, we use simple finite-state recognizer to identify (possibly nested) quoted phrases.for classifier construction, we use the ind package (buntine, 1993) to train decision trees (we usethe mml tree style, minimum message length criterion with bayesian smoothing).
</nextsent>
<nextsent>the data for these experiments come from version 1.1 of the nrrc corpus (wiebe et al, 2002).10.
</nextsent>
<nextsent>the corpus consists of 535 newswire documents (mostlyfrom the fbis), of which we used 66 (1375 sen tences) for developing the heuristics and features, while keeping the remaining 469 (9808 sentences) blind (used for 10-fold cross-validation).
</nextsent>
<nextsent>although the nrrc corpus provides annotations for all pses, it does not provide annotations to denote directly their hierarchical structure within 8gates sentences sometimes extend across paragraph boundaries, which seems never to be warranted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4323">
<title id=" C04-1018.xml">playing the telephone game determining the hierarchical structure of perspective and speech expressions </title>
<section> data description.  </section>
<citcontext>
<prevsection>
<prevsent>although the nrrc corpus provides annotations for all pses, it does not provide annotations to denote directly their hierarchical structure within 8gates sentences sometimes extend across paragraph boundaries, which seems never to be warranted.
</prevsent>
<prevsent>inaccurately joining sentences has the effect of adding more noise to our problem, so we split gates sentences at paragraph boundaries, and introduce writer pses for the newly created sentences.
</prevsent>
</prevsection>
<citsent citstr=" H01-1014 ">
9we convert the parse to dependency format that makes some of our features simpler using method similar to the one described in xia and palmer (2001).<papid> H01-1014 </papid></citsent>
<aftsection>
<nextsent>we also employ method from adam lopez at the university of maryland to find grammatical relationships between words (subject, object, etc.).
</nextsent>
<nextsent>10the original corpus is available at http: //nrrc.mitre.org/nrrc/docs_data/mpqa_ 04/approval_mpqa.htm.
</nextsent>
<nextsent>code and data used in our experiments are available at http://www.cs.cornell.
</nextsent>
<nextsent>edu/ebreck/breck04playing/.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4324">
<title id=" C04-1018.xml">playing the telephone game determining the hierarchical structure of perspective and speech expressions </title>
<section> results and discussion.  </section>
<citcontext>
<prevsection>
<prevsent>other sources of error include erroneous sentence boundary detection, parenthetical statements (which the parser does not treat correctly for our purposes) and other parse errors, partial quotations, as well as some errors in the annotation.
</prevsent>
<prevsent>examining the learned trees is difficult because of their size, but looking at one tree to depth three 14p   0.01, using an approximate randomization test with 9,999 trials.
</prevsent>
</prevsection>
<citsent citstr=" J93-3001 ">
see (eisner, 1996, page 17) and (chinchor et al, 1993, <papid> J93-3001 </papid>pages 430-433) for descriptions of this method.</citsent>
<aftsection>
<nextsent>15using the same test as above,   0.01, except for the performance on sentences with more than 5 pses, because of the small amount of data, where   0.02.reveals fairly intuitive model.
</nextsent>
<nextsent>ignoring the probabilities, the tree decides pse parent is the parent of pse target if and only if pse parent is the writers pse (and pse target is not in quotation marks), or if pse parent is the word said.?
</nextsent>
<nextsent>for all the trees learned, the root feature was either the writer pse test or the partial-parse-based domination feature.
</nextsent>
<nextsent>we have presented the concept of perspective and speech expressions, and argued that determining their hierarchical structure is important for natural language understanding of perspective.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4325">
<title id=" C04-1037.xml">optimizing disambiguation in swahili </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in ambiguity resolution of natural language, both explicit linguistic information and probability calculation have been used as basic approaches.
</prevsent>
<prevsent>in early experiments usually only one strategy was applied, so that ambiguity resolution was performed either with the help of linguistic rules, or through probability calculation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2228 ">
advanced approaches make use of both strategies, and they differ mainly in what kind of role each of these two methods has in the system (wilks and stevenson 1998, <papid> P98-2228 </papid>stevenson and wilks 2001).<papid> J01-3001 </papid></citsent>
<aftsection>
<nextsent>sources of structured data, such as the wordnet (miller 1990; resnik 1998b; banerjee and pedersen 2002), have also been made use of.
</nextsent>
<nextsent>it is commonly known that the more comprehensive the description of language is, the more ambiguous is the interpretation of individual words1.
</nextsent>
<nextsent>ambiguity occurs between word classes, between variously inflected word forms, and above all, between various meanings of word.
</nextsent>
<nextsent>a fairly large number2 of words in different word categories have more than one clearly distinguished meaning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4326">
<title id=" C04-1037.xml">optimizing disambiguation in swahili </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in ambiguity resolution of natural language, both explicit linguistic information and probability calculation have been used as basic approaches.
</prevsent>
<prevsent>in early experiments usually only one strategy was applied, so that ambiguity resolution was performed either with the help of linguistic rules, or through probability calculation.
</prevsent>
</prevsection>
<citsent citstr=" J01-3001 ">
advanced approaches make use of both strategies, and they differ mainly in what kind of role each of these two methods has in the system (wilks and stevenson 1998, <papid> P98-2228 </papid>stevenson and wilks 2001).<papid> J01-3001 </papid></citsent>
<aftsection>
<nextsent>sources of structured data, such as the wordnet (miller 1990; resnik 1998b; banerjee and pedersen 2002), have also been made use of.
</nextsent>
<nextsent>it is commonly known that the more comprehensive the description of language is, the more ambiguous is the interpretation of individual words1.
</nextsent>
<nextsent>ambiguity occurs between word classes, between variously inflected word forms, and above all, between various meanings of word.
</nextsent>
<nextsent>a fairly large number2 of words in different word categories have more than one clearly distinguished meaning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4327">
<title id=" C04-1037.xml">optimizing disambiguation in swahili </title>
<section> problem of semantic generalisation.  </section>
<citcontext>
<prevsection>
<prevsent>even though classes in swahili are only in exceptional cases semantically  pure , the class membership often provides sufficient information for disambiguation, either by direct selection or, more often, by exclusion of reading.
</prevsent>
<prevsent>the grades of animacy (e.g. human, animal, vegetation) are an example of useful semantic groupings, which can be used in generalising disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" C90-2067 ">
another useful feature, actually belonging to syntax, is the division of verbs into categories according to their argument structure (e.g. sv, svo, svoo) neural networks have been used successfully for identifying clusters of co-occurrence of words and their accompanying tags (veronis and ide 1990; <papid> C90-2067 </papid>sussna 1993; resnik 1998a).</citsent>
<aftsection>
<nextsent>research results, carried out with the self-organizing map (kohonen 1995) on semantic clustering of verbs and their arguments in swahili, are very promising, and useful generalizations have been found (ng ang 2003).13 these findings can be encoded into the morphological parser and used in writing semantic disambiguation rules.
</nextsent>
<nextsent>it sometimes happens that linguistic disambiguation rules cannot be written.
</nextsent>
<nextsent>particularly problematic is the noun of the class 9/10 in object position without qualifiers, many of which would help in disambiguation.
</nextsent>
<nextsent>in this noun class there are no features in nouns for determining whether the word is in singular or plural14.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4328">
<title id=" C04-1037.xml">optimizing disambiguation in swahili </title>
<section> problem of semantic generalisation.  </section>
<citcontext>
<prevsection>
<prevsent>the experiments with the som algorithm indicate that it is possible to find significant relationships between adjacent words on the one hand and between words and tags on the other.
</prevsent>
<prevsent>such information can then be encoded in the morphological dictionary and used in generalising disambiguation rules.
</prevsent>
</prevsection>
<citsent citstr=" A97-1011 ">
ambiguity resolution can be enhanced further by constructing explicit dependencies between constituent parts of sentence (jrvinen and tapanainen 1997; tapanainen and jrvinen 1997; <papid> A97-1011 </papid>tapanainen 1999) or by making use of parse tree bank of the type of wordnet (hirst and onge 1998).</citsent>
<aftsection>
<nextsent>10 acknowledgements.
</nextsent>
<nextsent>thanks go to ling soft and kimmo koskenniemi for allowing me to use the two level compiler for handling morphological analysis and to conn exor and pasi tapanainen for prividing access to cg-2 for writing disambiguation rules.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4329">
<title id=" C02-1083.xml">a methodology for terminology based knowledge acquisition and integration </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides term recognition, term clustering is an indispensable component in knowledge management process (see figure 2).
</prevsent>
<prevsent>since terminological opacity and polysemy are very common in molecular biology, term clustering is essential for the semantic integration of terms, the construction of domain ontology and for choosing the appropriate semantic information.
</prevsent>
</prevsection>
<citsent citstr=" C96-2212 ">
the atc method is based on ushio das ami (average mutual information)-hierarchical clustering method (ushioda, 1996).<papid> C96-2212 </papid></citsent>
<aftsection>
<nextsent>our implementation uses parallel symmetric processing for high speed clustering and is built on the c/nc-value results.
</nextsent>
<nextsent>as input, we use co-occurrences of automatically recognised terms and their contexts, and the output is dendrogram of hierarchical term clusters (like thesaurus).
</nextsent>
<nextsent>the calculated term cluster information is stored in lilfes (see below) and combined with predefined ontology according to the term classes automatically assigned.
</nextsent>
<nextsent>1.3 lilfes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4330">
<title id=" C04-1104.xml">subcategorization acquisition and evaluation for chinese verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides, simple application of the acquired lexicon to pcfg parser indicates great potentialities of subcategorization information in the fields of nlp.
</prevsent>
<prevsent>credits this research is sponsored by national natural science foundation (grant no. 60373101 and 603750 19), and high-tech research and development program (grant no. 2002aa117010-09).
</prevsent>
</prevsection>
<citsent citstr=" P91-1027 ">
since (brent 1991) <papid> P91-1027 </papid>there have been considerable amount of researches focusing on verb lexicons with respective subcategorization information specified both in the field of traditional linguistics and that of computational linguistics.</citsent>
<aftsection>
<nextsent>as for the former, subcategory theories illustrating the syntactic behaviors of verbal predicates are now much more systemically improved, e.g.
</nextsent>
<nextsent>(korhonen 2001).
</nextsent>
<nextsent>and for auto-acquisition and relevant application, researchers have made great achievements not only in english, e.g.
</nextsent>
<nextsent>(briscoe and carroll 1997), (<papid> A97-1052 </papid>korhonen 2003), but also in many other languages, such as germany (schulte im walde 2002), czech (sarkar and zeman 2000), <papid> C00-2100 </papid>and portuguese (gamallo et. al 2002).<papid> W02-0905 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4331">
<title id=" C04-1104.xml">subcategorization acquisition and evaluation for chinese verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(korhonen 2001).
</prevsent>
<prevsent>and for auto-acquisition and relevant application, researchers have made great achievements not only in english, e.g.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
(briscoe and carroll 1997), (<papid> A97-1052 </papid>korhonen 2003), but also in many other languages, such as germany (schulte im walde 2002), czech (sarkar and zeman 2000), <papid> C00-2100 </papid>and portuguese (gamallo et. al 2002).<papid> W02-0905 </papid></citsent>
<aftsection>
<nextsent>however, relevant theoretical researches on chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and few papers on manual acquisition or prescriptive design ment of syntactic patterns.
</nextsent>
<nextsent>due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such harmony that satisfies the description granularity for scf (han and zhao 2004).
</nextsent>
<nextsent>the only auto-acquisition work for chinese scf made by (han and zhao 2004) describes the pre definition of 152 general frames for all verbs in chinese, but that experiment is not based on real corpus.
</nextsent>
<nextsent>after observing and analyzing quantity of subcategory phenomena in real chinese corpus in the peoples daily (jan.~june, 1998), we removed from han &amp; zhaos pre definition 15 scfs that are actually similar derivants of others, and then with this foundation and linguistic rules from (zhao 2002) as heuristic information we generated scf hypotheses from the corpus of peoples daily (jan.~june, 1998), and statistically filtered the hypotheses into chinese verb scf lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4332">
<title id=" C04-1104.xml">subcategorization acquisition and evaluation for chinese verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(korhonen 2001).
</prevsent>
<prevsent>and for auto-acquisition and relevant application, researchers have made great achievements not only in english, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C00-2100 ">
(briscoe and carroll 1997), (<papid> A97-1052 </papid>korhonen 2003), but also in many other languages, such as germany (schulte im walde 2002), czech (sarkar and zeman 2000), <papid> C00-2100 </papid>and portuguese (gamallo et. al 2002).<papid> W02-0905 </papid></citsent>
<aftsection>
<nextsent>however, relevant theoretical researches on chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and few papers on manual acquisition or prescriptive design ment of syntactic patterns.
</nextsent>
<nextsent>due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such harmony that satisfies the description granularity for scf (han and zhao 2004).
</nextsent>
<nextsent>the only auto-acquisition work for chinese scf made by (han and zhao 2004) describes the pre definition of 152 general frames for all verbs in chinese, but that experiment is not based on real corpus.
</nextsent>
<nextsent>after observing and analyzing quantity of subcategory phenomena in real chinese corpus in the peoples daily (jan.~june, 1998), we removed from han &amp; zhaos pre definition 15 scfs that are actually similar derivants of others, and then with this foundation and linguistic rules from (zhao 2002) as heuristic information we generated scf hypotheses from the corpus of peoples daily (jan.~june, 1998), and statistically filtered the hypotheses into chinese verb scf lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4334">
<title id=" C04-1104.xml">subcategorization acquisition and evaluation for chinese verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(korhonen 2001).
</prevsent>
<prevsent>and for auto-acquisition and relevant application, researchers have made great achievements not only in english, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-0905 ">
(briscoe and carroll 1997), (<papid> A97-1052 </papid>korhonen 2003), but also in many other languages, such as germany (schulte im walde 2002), czech (sarkar and zeman 2000), <papid> C00-2100 </papid>and portuguese (gamallo et. al 2002).<papid> W02-0905 </papid></citsent>
<aftsection>
<nextsent>however, relevant theoretical researches on chinese verbs are generally limited to case grammar, valency, some semantic computation theories, and few papers on manual acquisition or prescriptive design ment of syntactic patterns.
</nextsent>
<nextsent>due to irrelevant initial motivations, syntactic and semantic generalizabilities of the consequent outputs are not in such harmony that satisfies the description granularity for scf (han and zhao 2004).
</nextsent>
<nextsent>the only auto-acquisition work for chinese scf made by (han and zhao 2004) describes the pre definition of 152 general frames for all verbs in chinese, but that experiment is not based on real corpus.
</nextsent>
<nextsent>after observing and analyzing quantity of subcategory phenomena in real chinese corpus in the peoples daily (jan.~june, 1998), we removed from han &amp; zhaos pre definition 15 scfs that are actually similar derivants of others, and then with this foundation and linguistic rules from (zhao 2002) as heuristic information we generated scf hypotheses from the corpus of peoples daily (jan.~june, 1998), and statistically filtered the hypotheses into chinese verb scf lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4335">
<title id=" C04-1104.xml">subcategorization acquisition and evaluation for chinese verbs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.2 filtering methods.
</prevsent>
<prevsent>in researches of subcategorization acquisition, statistical methods for hypothesis filtering mainly include the bht, the log likelihood ratio (llr), the t-test and the mle, and the most popular one is the bht.
</prevsent>
</prevsection>
<citsent citstr=" J93-2002 ">
since (brent 1993) <papid> J93-2002 </papid>began to use the method, most researchers have agreed that the bht results in better precision and recall with scf hypotheses of high, medium and low frequencies.</citsent>
<aftsection>
<nextsent>only (korhonen 2001) reports 11.9% total performance of the mle better than the bht.
</nextsent>
<nextsent>therefore, we applied the two statistical methods in our present experiment.
</nextsent>
<nextsent>this subsection chiefly illustrates the expressions of our methods and definitions of parameters in them, while performance comparison of the two will be introduced in section 3.
</nextsent>
<nextsent>when applying the bht method, it is necessary to determine the probability of the primitive event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4345">
<title id=" C08-1055.xml">generation of referring expressions managing structural ambiguities </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>licensed under the creative commons attribution-noncommercial-share alike 3.0 unported.
</prevsent>
<prevsent>some rights reserved.
</prevsent>
</prevsection>
<citsent citstr=" P04-1052 ">
webber, 1998; krahmer and theune, 2002; siddharthan and copestake, 2004)).<papid> P04-1052 </papid></citsent>
<aftsection>
<nextsent>this is an important limitation, for example because ambiguities can be introduced in the step from properties to language descriptions.
</nextsent>
<nextsent>such surface ambiguities?
</nextsent>
<nextsent>take center stage in this paper.
</nextsent>
<nextsent>more specifically, we shall be investigating situations where they lead to referential ambiguity, that is, un clarity as to what the intended referent of referring expression is. example 1: consider scenario in which there are sheep and goats along with other animals, grazing in meadow; some of the sheep and goats are black while others are either brown or yellow.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4346">
<title id=" C08-1055.xml">generation of referring expressions managing structural ambiguities </title>
<section> applying results to gre.  </section>
<citcontext>
<prevsection>
<prevsent>disjunction is required whenever there is no conjunction of atomic properties that sets the elements of aset of referents apart from all the other objects in the domain.
</prevsent>
<prevsent>recall example 1 (from 1), where the aim is to single out the black sheep and black goats from the rest of the animals.
</prevsent>
</prevsection>
<citsent citstr=" W00-1416 ">
this task cannot be performed by simple conjunction (i.e., of the form the x?, where contains adjectives and nouns only), so dis junctions become unavoidable.various proposals have been made for allowing gre algorithms to produce referring expressions of this kind (stone, 2000; <papid> W00-1416 </papid>van deemter, 2002; gardent, 2002; horacek, 2004).</citsent>
<aftsection>
<nextsent>here we take as our starting point the approach of (gatt, 2007) (henceforth gatts algorithm with partitioning or gap).
</nextsent>
<nextsent>gap isthe only algorithm that produces dd in disjunctive normal form (dnf) while also guaranteeing that every part?
</nextsent>
<nextsent>of the partition contains noun.
</nextsent>
<nextsent>the dnf takes the form: 1 ? 2 ...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4347">
<title id=" C02-1042.xml">using knowledge to facilitate factoid answer pinpointing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>early qa systems used simple filtering technique, question word density within fixed n-word window, to pinpoint answers.
</prevsent>
<prevsent>robust though this may be, the window method is not accurate enough.
</prevsent>
</prevsection>
<citsent citstr=" A00-1041 ">
in response, factoid question answering systems have evolved into two types: ? use-knowledge: extract query words from the input question, perform ir against the source corpus, possibly segment resulting documents, identify set of segments containing likely answers, apply set of heuristics that each consults different source of knowledge to score each candidate, rank them, and select the best (harabagiu et al , 2001; hovy et al , 2001; srihari and li, 2000; abney et al , 2000).<papid> A00-1041 </papid></citsent>
<aftsection>
<nextsent>use-the-web: extract query words from the question, perform ir against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s).
</nextsent>
<nextsent>then, for justification, locate examples of the answers in the source corpus (brill et al , 2001; buchholz, 2001).
</nextsent>
<nextsent>of course, these techniques can be combined: the popularity ratings from use-the-web can also be applied as filtering criterion (clarke et al ., 2001), or the knowledge resource heuristics can filter the web results.
</nextsent>
<nextsent>however, simply going to the web without using further knowledge (brill et al , 2001) may return the webs majority opinions on astrology, the killers of jfk, the cancerous effects of microwave ovens, etc.fun but not altogether trustworthy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4348">
<title id=" C02-1042.xml">using knowledge to facilitate factoid answer pinpointing </title>
<section> knowledge used for pinpointing.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 type 3: surface pattern matching.
</prevsent>
<prevsent>often qtarget answers are expressed using rather stereotypical words or phrases.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
for example, the year of birth of person is typically expressed using one of these phrases:  name  was born in  birth year   name  ( birth year ? death year ) we have developed method to learn such patterns automatically from text on the web (ravichandran and hovy, 2002).<papid> P02-1006 </papid></citsent>
<aftsection>
<nextsent>we have added into the qa typology the patterns for appropriate qtargets (qtargets with closed-list answers, such as planets, require no patterns).
</nextsent>
<nextsent>where some qa systems use such patterns exclusively (soubbotin and soubbotin, 2001) or partially (wang et al , 2001; lee et al , 2001), we employ them as an additional source of evidence for the answer.
</nextsent>
<nextsent>preliminary results on for range of qtargets, using the trec-10 questions and the trec corpus, are: question type (qtarget) number of questions mrr on trec docs birth year 8 0.47875 inventors 6 0.16667 discoverers 4 0.1250 definitions 102 0.3445 why-famous 3 0.6666 locations 16 0.75 3.4 type 4: expected numerical ranges.
</nextsent>
<nextsent>quantity-targeting questions are often underspecified and relyon culturally shared cooperativeness rules and/or world knowledge: q: how many people live in chile?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4349">
<title id=" C02-1042.xml">using knowledge to facilitate factoid answer pinpointing </title>
<section> knowledge used for pinpointing.  </section>
<citcontext>
<prevsection>
<prevsent>while wordnets definition wordnet: washington the capital of the united states?
</prevsent>
<prevsent>directly provides the answer to the matcher, it also allows their module to focus its search on passages containing washington?, capital?, and united states?, and the matcher to pick good motivating passage in the source corpus.
</prevsent>
</prevsection>
<citsent citstr=" C02-1026 ">
clearly, this capability can be extended to include (definitional and other) information provided by other sources, including encyclopedias and the web (lin 2002).<papid> C02-1026 </papid></citsent>
<aftsection>
<nextsent>3.8 type 8: semantic relation matching.
</nextsent>
<nextsent>so far, we have considered individual words and groups of words.
</nextsent>
<nextsent>but often this is insufficient to accurately score an answer.
</nextsent>
<nextsent>as also noted in (buchholz, 2001), pinpointing can be improved significantly by matching semantic relations among constituents: q: who killed lee harvey oswald?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4350">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we focus on the accuracy of these two systems as function of processing time and corpus size.
</prevsent>
<prevsent>the natural language processing (nlp) community has recently seen growth in corpus-based methods.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as machine translation (och and ney 2002), <papid> P02-1038 </papid>information extraction (etzioni et al 2004), and question answering (brill et al 2001).</citsent>
<aftsection>
<nextsent>in the last decade, we have seen an explosion in the amount of available digital text resources.
</nextsent>
<nextsent>it is estimated that the internet contains hundreds of tera bytes of text data, most of which is in an unstructured format.
</nextsent>
<nextsent>yet, many nlp algorithms tap into only megabytes or gigabytes of this information.
</nextsent>
<nextsent>in this paper, we make step towards acquiring semantic knowledge from tera bytes of data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4351">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yet, many nlp algorithms tap into only megabytes or gigabytes of this information.
</prevsent>
<prevsent>in this paper, we make step towards acquiring semantic knowledge from tera bytes of data.
</prevsent>
</prevsection>
<citsent citstr=" N04-1041 ">
we present an algorithm for extracting is-a relations, designed for the tera scale, and compare it to state of the art method that employs deep analysis of text (pantel and ravichandran 2004).<papid> N04-1041 </papid></citsent>
<aftsection>
<nextsent>we show that by simply utilizing more data on this task, we can achieve similar performance to linguistically rich approach.
</nextsent>
<nextsent>the current state of the art cooccurrence model requires an estimated 10 years just to parse 1tb corpus (see table 1).
</nextsent>
<nextsent>instead of using syntactically motivated co-occurrence approach as above, our system uses lexico-syntactic rules.
</nextsent>
<nextsent>in particular, it finds lexico-pos patterns by making modifications to the basic edit distance algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4353">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>previous approaches to extracting is-a relations fall under two categories: pattern-based and co occurrence-based approaches.
</prevsent>
<prevsent>2.1 pattern-based approaches.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
marti hearst (1992) <papid> C92-2082 </papid>was the first to use pat tern-based approach to extract hyponym relations from raw corpus.</citsent>
<aftsection>
<nextsent>she used an iterative process to semi-automatically learn patterns.
</nextsent>
<nextsent>however, corpus of 20mb words yielded only 400 examples.
</nextsent>
<nextsent>our pattern-based algorithm is very similar to the one used by hearst.
</nextsent>
<nextsent>she uses seed examples to manually discover her patterns whearas we use minimal edit distance algorithm to automatically discover the patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4355">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>our pattern-based algorithm is very similar to the one used by hearst.
</prevsent>
<prevsent>she uses seed examples to manually discover her patterns whearas we use minimal edit distance algorithm to automatically discover the patterns.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
771riloff and shepherd (1997) <papid> W97-0313 </papid>used semiautomatic method for discovering similar words using few seed examples by using pattern-based techniques and human supervision.</citsent>
<aftsection>
<nextsent>berland and charniak (1999) <papid> P99-1008 </papid>used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations.</nextsent>
<nextsent>they reported an accuracy of about 55% precision on corpus of 100,000 words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4356">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>she uses seed examples to manually discover her patterns whearas we use minimal edit distance algorithm to automatically discover the patterns.
</prevsent>
<prevsent>771riloff and shepherd (1997) <papid> W97-0313 </papid>used semiautomatic method for discovering similar words using few seed examples by using pattern-based techniques and human supervision.</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
berland and charniak (1999) <papid> P99-1008 </papid>used similar pattern-based techniques and other heuristics to extract meronymy (part-whole) relations.</citsent>
<aftsection>
<nextsent>they reported an accuracy of about 55% precision on corpus of 100,000 words.
</nextsent>
<nextsent>girju et al (2003) improved upon berland and charniak work using machine learning filter.
</nextsent>
<nextsent>mann (2002) <papid> W02-1111 </papid>and fleischman et al (2003) <papid> P03-1001 </papid>used part of speech patterns to extract subset of hyponym relations involving proper nouns.</nextsent>
<nextsent>our pattern-based algorithm differs from these approaches in two ways.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4357">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>they reported an accuracy of about 55% precision on corpus of 100,000 words.
</prevsent>
<prevsent>girju et al (2003) improved upon berland and charniak work using machine learning filter.
</prevsent>
</prevsection>
<citsent citstr=" W02-1111 ">
mann (2002) <papid> W02-1111 </papid>and fleischman et al (2003) <papid> P03-1001 </papid>used part of speech patterns to extract subset of hyponym relations involving proper nouns.</citsent>
<aftsection>
<nextsent>our pattern-based algorithm differs from these approaches in two ways.
</nextsent>
<nextsent>we learn lexico-pos patterns in an automatic way.
</nextsent>
<nextsent>also, the patterns are learned with the specific goal of scaling to the tera scale (see table 2).
</nextsent>
<nextsent>2.2 co-occurrence-based approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4358">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>they reported an accuracy of about 55% precision on corpus of 100,000 words.
</prevsent>
<prevsent>girju et al (2003) improved upon berland and charniak work using machine learning filter.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
mann (2002) <papid> W02-1111 </papid>and fleischman et al (2003) <papid> P03-1001 </papid>used part of speech patterns to extract subset of hyponym relations involving proper nouns.</citsent>
<aftsection>
<nextsent>our pattern-based algorithm differs from these approaches in two ways.
</nextsent>
<nextsent>we learn lexico-pos patterns in an automatic way.
</nextsent>
<nextsent>also, the patterns are learned with the specific goal of scaling to the tera scale (see table 2).
</nextsent>
<nextsent>2.2 co-occurrence-based approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4359">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>also, the patterns are learned with the specific goal of scaling to the tera scale (see table 2).
</prevsent>
<prevsent>2.2 co-occurrence-based approaches.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
the second class of algorithms uses cooccurrence statistics (hindle 1990, <papid> P90-1034 </papid>lin 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>these systems mostly employ clustering algorithms to group words according to their meanings in text.
</nextsent>
<nextsent>assuming the distributional hypothesis (harris 1985), words that occur in similar grammatical contexts are similar in meaning.
</nextsent>
<nextsent>curran and moens (2002) <papid> P02-1030 </papid>experimented with corpus size and complexity of proximity features in building automatic thesauri.</nextsent>
<nextsent>cbc (clustering by commit tee) proposed by pantel and lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4360">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>also, the patterns are learned with the specific goal of scaling to the tera scale (see table 2).
</prevsent>
<prevsent>2.2 co-occurrence-based approaches.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the second class of algorithms uses cooccurrence statistics (hindle 1990, <papid> P90-1034 </papid>lin 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>these systems mostly employ clustering algorithms to group words according to their meanings in text.
</nextsent>
<nextsent>assuming the distributional hypothesis (harris 1985), words that occur in similar grammatical contexts are similar in meaning.
</nextsent>
<nextsent>curran and moens (2002) <papid> P02-1030 </papid>experimented with corpus size and complexity of proximity features in building automatic thesauri.</nextsent>
<nextsent>cbc (clustering by commit tee) proposed by pantel and lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4361">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>these systems mostly employ clustering algorithms to group words according to their meanings in text.
</prevsent>
<prevsent>assuming the distributional hypothesis (harris 1985), words that occur in similar grammatical contexts are similar in meaning.
</prevsent>
</prevsection>
<citsent citstr=" P02-1030 ">
curran and moens (2002) <papid> P02-1030 </papid>experimented with corpus size and complexity of proximity features in building automatic thesauri.</citsent>
<aftsection>
<nextsent>cbc (clustering by commit tee) proposed by pantel and lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses.
</nextsent>
<nextsent>however, such clustering algorithms fail to name their classes.
</nextsent>
<nextsent>caraballo (1999) <papid> P99-1016 </papid>was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.</nextsent>
<nextsent>recently, pantel and ravichandran (2004) <papid> N04-1041 </papid>extended this approach by making use of all syntactic dependency features for each noun.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4362">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> relevant work.  </section>
<citcontext>
<prevsection>
<prevsent>cbc (clustering by commit tee) proposed by pantel and lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses.
</prevsent>
<prevsent>however, such clustering algorithms fail to name their classes.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
caraballo (1999) <papid> P99-1016 </papid>was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters.</citsent>
<aftsection>
<nextsent>recently, pantel and ravichandran (2004) <papid> N04-1041 </papid>extended this approach by making use of all syntactic dependency features for each noun.</nextsent>
<nextsent>much of the research discussed above takes similar approach of searching text for simple surface or lexico-syntactic patterns in bottom-up approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4368">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> scalable pattern-based approach.  </section>
<citcontext>
<prevsection>
<prevsent>for example, consider the following 2 sentences: 1) platinum is precious metal.
</prevsent>
<prevsent>2) molybdenum is metal.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
applying pos tagger (brill 1995) <papid> J95-4004 </papid>gives the following output: surface platinum is precious metal . pos nnp vbz dt jj nn . surface molybdenum is metal . pos nnp vbz dt nn . very good pattern to generalize from the alignment of these two strings would be surface is metal . pos nnp . we use the following notation to denote this alignment: _nnp is (*s*) metal.?, where _nnp represents the pos tag nnp?.</citsent>
<aftsection>
<nextsent>to perform such alignments we introduce two wild card operators, skip (*s*) and wild card (*g*).
</nextsent>
<nextsent>the skip operator represents 0 or 1 instance of any word (similar to the \w* pattern in perl), while the wild card operator represents exactly 1 instance of any word (similar to the \w+ pattern in perl).
</nextsent>
<nextsent>4.1 algorithm.
</nextsent>
<nextsent>we present an algorithm for learning patterns at multiple levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4369">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> scalable pattern-based approach.  </section>
<citcontext>
<prevsection>
<prevsent>from the information extraction point of view neither of these patterns is very useful.
</prevsent>
<prevsent>we need to find patterns with relatively high occurrence and high precision.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
we apply the log likelihood principle (dunning 1993) <papid> J93-1003 </papid>to compute this score.</citsent>
<aftsection>
<nextsent>the top 15 patterns according to this metric are listed in table 3 (we omit the pos variations for visibility).
</nextsent>
<nextsent>some of these patterns are similar to the ones discovered by hearst (1992) <papid> C92-2082 </papid>while other patterns are similar to the ones used by fleischman et al (2003).<papid> P03-1001 </papid></nextsent>
<nextsent>4.3 time complexity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4373">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>news ~0.5gb, ap newswire ~2gb, new york times ~2gb, reuters ~0.8gb, wall street journal ~1.2gb, and various online news website ~1.5gb.
</prevsent>
<prevsent>for our experiments, we extract from this corpus six datasets of different sizes: 1.5mb, 15 mb, 150 mb, 1.5gb, 6gb and 15gb.
</prevsent>
</prevsection>
<citsent citstr=" C94-1079 ">
for the co-occurrence model, we used minipar (lin 1994), <papid> C94-1079 </papid>broad coverage parser, to parse each data set.</citsent>
<aftsection>
<nextsent>we collected the frequency counts of the grammatical relationships (contexts) output by minipar and used them to compute the pointwise mutual information vectors described in section 3.1.
</nextsent>
<nextsent>for the pattern-based approach, we use brills. pos tagger (1995) to tag each dataset.
</nextsent>
<nextsent>5.2 precision.
</nextsent>
<nextsent>we performed manual evaluation to estimate the precision of both systems on each dataset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4375">
<title id=" C04-1111.xml">towards tera scale semantic acquisition </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>it is possible that semantic resources richer than wordnet willen able them to break the current quality ceilings.
</prevsent>
<prevsent>both statistical and symbolic nlp systems can make use of such semantic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" H01-1052 ">
with the increased size of the web, more and more training data is becoming available, and as banko and brill (2001) <papid> H01-1052 </papid>showed, even rather simple learning algorithms can perform well when given enough data.</citsent>
<aftsection>
<nextsent>in this light, we see an interesting need to develop fast, robust, and scalable methods to mine semantic information from the web.
</nextsent>
<nextsent>this paper compares and contrasts two methods for extracting is-a relations from corpora.
</nextsent>
<nextsent>we presented novel pattern-based algorithm, scalable to the tera scale, which outperforms its more informed syntactical co-occurrence counterpart on very small and very large data.
</nextsent>
<nextsent>albeit possible to successfully apply linguisti cally-light but data-rich approaches to some nlp applications, merely reporting these results often fails to yield insights into the underlying theories of language at play.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4376">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4378">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" C96-1006 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4379">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4381">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" W96-0107 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4382">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4383">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this demonstrates that our method could reduce the cost for making translation dictionaries.
</prevsent>
<prevsent>translation dictionaries used in multilingual natural language processing such as machine translation have been made manually, but great deal of labor is required for this work and it is difficult to keep the description of the dictionaries consistent.
</prevsent>
</prevsection>
<citsent citstr=" P98-2191 ">
therefore, researches of extracting translation pairs from parallel corpora automatically be come active recently (gale and church, 1991; <papid> H91-1026 </papid>kajiand aizono, 1996; <papid> C96-1006 </papid>tanaka and iwasaki, 1996; <papid> C96-2098 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>fung, 1997; melamed, 1997; <papid> P97-1063 </papid>sato and nakanishi, 1998).<papid> P98-2191 </papid>this paper proposes learning and extracting method of bilingual word sequence correspondences from non-aligned parallel corpora with support vector machines (svms) (vapnik, 1999).</citsent>
<aftsection>
<nextsent>svms are ones of large margin classifiers (smola et al, 2000) which are based on the strategy where margins between separating boundary and vectors of which elements express the features of training samples is maximized.
</nextsent>
<nextsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</nextsent>
<nextsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</nextsent>
<nextsent>therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4384">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, svms havehiger ability of the generalization than other learning models such as the decision trees and rarely cause over-fit for training samples.
</prevsent>
<prevsent>in addition, by using kernel functions, they can learn non-linear separating boundary and dependencies between the features.
</prevsent>
</prevsection>
<citsent citstr=" W00-1303 ">
therefore, svms have been recently used for the natural language processing such as text categorization (joachims, 1998; taira and haruno, 1999), chunk identification (kudo and matsumoto, 2000<papid> W00-1303 </papid>b), dependency structure analysis (kudo and matsumoto, 2000<papid> W00-1303 </papid>a).the method proposed in this paper does notre quire aligned parallel corpora which do not exist toomany at present.</citsent>
<aftsection>
<nextsent>therefore, without limiting applicable domains, word sequence correspondences can been extracted.
</nextsent>
<nextsent>svms are binary classifiers which linearly separated dimension vectors to two classes.
</nextsent>
<nextsent>each vector represents the sample which has features.
</nextsent>
<nextsent>it is distinguished whether given sample ~x = (x1, x2, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4399">
<title id=" C02-1020.xml">extracting word sequence correspondences with support vector machines </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>with difference from our method, there have been researches which are based on the assumption of the sentence alignments for parallel corpora (gale and church, 1991; <papid> H91-1026 </papid>kitamura and matsumoto, 1996; <papid> W96-0107 </papid>melamed, 1997).<papid> P97-1063 </papid></prevsent>
<prevsent>(gale and church, 1991) <papid> H91-1026 </papid>has used the 2 statistics as the correspondence level of the word pairs and has showed that it was more effective than the mutual information.</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
(kitamura and matsumoto, 1996) <papid> W96-0107 </papid>has used the dice coefficient (kay and roschesen, 1993) <papid> J93-1006 </papid>which was weighted by the logarithm of the frequency of the word pair as the table 2: examples of translation pairs extracted by our method japanese english  </citsent>
<aftsection>
<nextsent>   chairman of special program committee    officially retired as
</nextsent>
<nextsent> fffiflffi  !  would like to say an official farewell 30 #  $!% &amp;  () my thirty years of experience * +,  -fl . / % sharpen up on my golf table 3: precision rate and recall rate when each kind of features is removed feature num.
</nextsent>
<nextsent>outputs corrects precision (%) recall (%) (1a) 94,511 891 686 77.0 (4.1) 68.6 (0.4) (1b) 94,511 1,058 719 68.0 (13.1) 71.9 (+2.9) (1) 189,022 1,237 756 61.1 (20.0) 75.6 (+6.6) (2a) 1 742 611 82.3 (+1.3) 61.1 (7.9) (2b) 1 755 600 79.5 (1.6) 60.0 (9.0) (2) 2 489 404 82.6 (+1.5) 40.4 (28.6) (3a) 4 846 685 81.0 (0.1) 68.5 (0.5) (3b) 4 834 660 79.1 (1.9) 66.0 (3.0) (3) 8 840 661 78.7 (2.4) 66.1 (2.9) (4a) 1,009 814 668 82.1 (+1.0) 66.8 (2.2) (4b) 890 855 698 81.6 (+0.6) 69.8 (+0.8) (4) 1,899 838 689 82.2 (+1.1) 68.9 (0.1) (5a) 1,009 844 683 80.9 (0.2) 68.3 (0.7) (5b) 890 851 688 80.8 (0.3) 68.8 (0.2) (5) 1,899 845 682 80.7 (0.4) 68.2 (0.8) all features 192,830 851 690 81.1 69.0 correspondence level of the word pairs.
</nextsent>
<nextsent>(melamed,1997) <papid> P97-1063 </papid>has proposed the competitive linking algorithm for linking the word pairs and method which calculates the optimized correspondence level of the word pairs by hill climbing.these methods could archive high accuracy be cause of the assumption of the sentence alignments for parallel corpora, but they have the problem with narrow applicable domains because there are not too many parallel corpora with sentence alignments at present.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4407">
<title id=" C08-1028.xml">efficiently parsing with the product free lambek calculus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large number of cg formalisms have been introduced including, among others, the lambekcalculus (lambek, 1958) and combinatory categorial grammar (ccg) (steedman, 2000).
</prevsent>
<prevsent>ofthese, ccg has received the most zealous computational attention.
</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
impressive results have been achieved culminating in the state-of-the-art parser of clark and curran (2004) <papid> P04-1014 </papid>which has been used as the parser for the pascal rich textual entailment challenge entry of bos and markert (2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>the appeal of ccg can be attributed to the existence of efficient parsing algorithms for it and the fact that it recognizes mildly context-sensitive language class (joshi et al, 1989), language class more powerful than the context free languages (cfls) that has been argued to be necessary for natural language syntax.
</nextsent>
<nextsent>the lambek calculus provide san ideal contrast between ccg and cfgs by being cg formalism like ccg but by recognizing the cfls like cfgs (pentus, 1997).
</nextsent>
<nextsent>the primary goal of this paper is to provide an algorithm for parsing with the lambek calculus and to sketch its correctness.
</nextsent>
<nextsent>furthermore, atime bound of o(n5) will be shown for this algorithm when restricted to product-free categories of bounded order (see section 2 for definition).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4408">
<title id=" C08-1028.xml">efficiently parsing with the product free lambek calculus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a large number of cg formalisms have been introduced including, among others, the lambekcalculus (lambek, 1958) and combinatory categorial grammar (ccg) (steedman, 2000).
</prevsent>
<prevsent>ofthese, ccg has received the most zealous computational attention.
</prevsent>
</prevsection>
<citsent citstr=" H05-1079 ">
impressive results have been achieved culminating in the state-of-the-art parser of clark and curran (2004) <papid> P04-1014 </papid>which has been used as the parser for the pascal rich textual entailment challenge entry of bos and markert (2005).<papid> H05-1079 </papid></citsent>
<aftsection>
<nextsent>the appeal of ccg can be attributed to the existence of efficient parsing algorithms for it and the fact that it recognizes mildly context-sensitive language class (joshi et al, 1989), language class more powerful than the context free languages (cfls) that has been argued to be necessary for natural language syntax.
</nextsent>
<nextsent>the lambek calculus provide san ideal contrast between ccg and cfgs by being cg formalism like ccg but by recognizing the cfls like cfgs (pentus, 1997).
</nextsent>
<nextsent>the primary goal of this paper is to provide an algorithm for parsing with the lambek calculus and to sketch its correctness.
</nextsent>
<nextsent>furthermore, atime bound of o(n5) will be shown for this algorithm when restricted to product-free categories of bounded order (see section 2 for definition).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4409">
<title id=" C08-1028.xml">efficiently parsing with the product free lambek calculus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>+ 1) for example, o((np\s)/np) = 1 and o((s/ np)\(s/np )) = 2.
</prevsent>
<prevsent>two other papers have provided algorithms similar to the one presented here.
</prevsent>
</prevsection>
<citsent citstr=" W05-1503 ">
carpenter and morrill (2005) <papid> W05-1503 </papid>provided graph representation and dynamic programming algorithm for parsing in the lambek calculus withproduct.</citsent>
<aftsection>
<nextsent>however, due to there use of the lam bek calculus with product and to their choice of correctness conditions, they did not obtain poly nomial time algorithm for any significant fragment of the calculus.
</nextsent>
<nextsent>aarts (1994) provided an algorithm for l2 which is not correct for l. ours is polynomial timefor lk, for any constant k, and is correct for l, albeit in exponential running time.
</nextsent>
<nextsent>a number of authors have provided polynomial time algorithms for parsing with ccg which gives some insight into how good our bound of o(n5) is. in particular, vijay-shanker and weir (1994) provided chart parsing algorithm for ccg with time bound of o(n6).
</nextsent>
<nextsent>this section presents chart parsing algorithm similar to cyk where entries in the chart are arcs annotated with graphs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4410">
<title id=" C08-1007.xml">enhancing multilingual latent semantic analysis with term alignment information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the terms themselves to their underlying semantics, the approach became known as latent semantic analysis (lsa).
</prevsent>
<prevsent>soon after this application of svd was widely publicized, it was suggested by berry et al (1994) that, with parallel corpus, the approach could be extended to pairs of languages to allow cross-language information retrieval (ir).
</prevsent>
</prevsection>
<citsent citstr=" P07-1110 ">
it has since been confirmed that lsa can be applied not just to pairs of languages, but also simultaneously to groups of languages, again given the existence of multi-parallel corpus (chew and abdelali 2007).<papid> P07-1110 </papid></citsent>
<aftsection>
<nextsent>in this paper, we return to the basics of lsa by examining its relationship with svd, and, in turn, the mathematical relationship of svd to the eigenvalue decomposition (evd).
</nextsent>
<nextsent>these details are discussed in section 2.
</nextsent>
<nextsent>it has previously been suggested (for example, in hendrickson 2007) that ir results could be improved by filling in information beyond that available directly in the term-by-document matrix, and replacing svd with the more general evd.
</nextsent>
<nextsent>to our knowledge, however, these suggestions have not been publicized outside the mathematics community, nor have they been empirically tested in ir applications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4412">
<title id=" C04-1021.xml">modern natural language interfaces to databases composing statistical parsing with semantic tract ability </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>as noun, but in the context of the atis database it is verb.1 on the other hand, manually creating and labeling massive corpus of questions for each database is prohibitively expensive.
</prevsent>
<prevsent>we consider two methods of resolving this quandary and assess their performance individually and in concert on the atis dataset.
</prevsent>
</prevsection>
<citsent citstr=" H90-1020 ">
first, we use strong semantic model to correct parsing errors.we introduce theoretical framework for discriminating between semantically tractable (st) questions and difficult ones, and we show that st questions are prevalent in the well-studied atis dataset (price, 1990).<papid> H90-1020 </papid></citsent>
<aftsection>
<nextsent>thus, we show that the semantic component of the nli task can be surprisingly easy and can be used to compensate for syntactic parsing errors.
</nextsent>
<nextsent>second, we re-train the parser using relatively small set of 150 questions, where each word is labeled by its part-of-speech tag.to demonstrate how these methods work in practice, we sketch the fully-implemented precise nli, where parser is modular plug in?.
</nextsent>
<nextsent>this modularity enables precise to leverage continuing advances in parsing technology over time by plugging in improved parsers as they become available.the remainder of this paper is organized as follows.
</nextsent>
<nextsent>we describe precise in section 2, sketch our theory in section 3, and report on our experiments in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4413">
<title id=" C04-1021.xml">modern natural language interfaces to databases composing statistical parsing with semantic tract ability </title>
<section> the precise system overview.  </section>
<citcontext>
<prevsection>
<prevsent>first to the pp to chicago?, and then to the np flights from boston to chicago?, where it belongs.
</prevsent>
<prevsent>2.1 parser enhancements.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we used the charniak parser (charniak, 2000) <papid> A00-2018 </papid>for the experiments reported in this paper.</citsent>
<aftsection>
<nextsent>we found that the charniak parser, which was trained on the wsj corpus, yielded numerous syntactic errors.
</nextsent>
<nextsent>our first step was to hand tag set of 150 questions with part of speech (pos) tags, and re-train the parsers pos tagger.
</nextsent>
<nextsent>as result, the probabilities associated with certain tags changed dramatically.
</nextsent>
<nextsent>for example, initially, list?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4415">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new words such as person names, organization names, technical terms, etc. appear frequently.
</prevsent>
<prevsent>in order for machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.
</prevsent>
</prevsection>
<citsent citstr=" W97-0311 ">
much research has been done on using parallel corpora to learn bilingual lexicons (melamed, 1997; <papid> W97-0311 </papid>moore, 2003).<papid> E03-1035 </papid></citsent>
<aftsection>
<nextsent>but parallel corpora are scarce resources, especially for uncommon language pairs.
</nextsent>
<nextsent>comparable corpora refer to texts that are not direct translation but are about the same topic.
</nextsent>
<nextsent>for example, various news agencies report major world events in different languages, and such news documents form readily available source of comparable corpora.
</nextsent>
<nextsent>being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4416">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new words such as person names, organization names, technical terms, etc. appear frequently.
</prevsent>
<prevsent>in order for machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.
</prevsent>
</prevsection>
<citsent citstr=" E03-1035 ">
much research has been done on using parallel corpora to learn bilingual lexicons (melamed, 1997; <papid> W97-0311 </papid>moore, 2003).<papid> E03-1035 </papid></citsent>
<aftsection>
<nextsent>but parallel corpora are scarce resources, especially for uncommon language pairs.
</nextsent>
<nextsent>comparable corpora refer to texts that are not direct translation but are about the same topic.
</nextsent>
<nextsent>for example, various news agencies report major world events in different languages, and such news documents form readily available source of comparable corpora.
</nextsent>
<nextsent>being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4417">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, various news agencies report major world events in different languages, and such news documents form readily available source of comparable corpora.
</prevsent>
<prevsent>being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
previous research efforts on acquiring translations from comparable corpora include (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>when translating word w, two sources of information can be used to determine its transla tion: the word itself and the surrounding words in the neighborhood (i.e., the context) of w. most previous research only considers one of the two sources of information, but not both.
</nextsent>
<nextsent>for example, the work of (al-onaizan and knight, 2002a; al-onaizan and knight, 2002b; knight and graehl, 1998) used the pronunciation of in translation.
</nextsent>
<nextsent>on the other hand, the work of (cao and li, 2002; fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999) <papid> P99-1067 </papid>used the context of to locate its translation in second language.</nextsent>
<nextsent>in this paper, we propose new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4422">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, various news agencies report major world events in different languages, and such news documents form readily available source of comparable corpora.
</prevsent>
<prevsent>being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P95-1050 ">
previous research efforts on acquiring translations from comparable corpora include (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>when translating word w, two sources of information can be used to determine its transla tion: the word itself and the surrounding words in the neighborhood (i.e., the context) of w. most previous research only considers one of the two sources of information, but not both.
</nextsent>
<nextsent>for example, the work of (al-onaizan and knight, 2002a; al-onaizan and knight, 2002b; knight and graehl, 1998) used the pronunciation of in translation.
</nextsent>
<nextsent>on the other hand, the work of (cao and li, 2002; fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999) <papid> P99-1067 </papid>used the context of to locate its translation in second language.</nextsent>
<nextsent>in this paper, we propose new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4427">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, various news agencies report major world events in different languages, and such news documents form readily available source of comparable corpora.
</prevsent>
<prevsent>being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
previous research efforts on acquiring translations from comparable corpora include (fung and yee, 1998; <papid> P98-1069 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>when translating word w, two sources of information can be used to determine its transla tion: the word itself and the surrounding words in the neighborhood (i.e., the context) of w. most previous research only considers one of the two sources of information, but not both.
</nextsent>
<nextsent>for example, the work of (al-onaizan and knight, 2002a; al-onaizan and knight, 2002b; knight and graehl, 1998) used the pronunciation of in translation.
</nextsent>
<nextsent>on the other hand, the work of (cao and li, 2002; fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999) <papid> P99-1067 </papid>used the context of to locate its translation in second language.</nextsent>
<nextsent>in this paper, we propose new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4433">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when translating word w, two sources of information can be used to determine its transla tion: the word itself and the surrounding words in the neighborhood (i.e., the context) of w. most previous research only considers one of the two sources of information, but not both.
</prevsent>
<prevsent>for example, the work of (al-onaizan and knight, 2002a; al-onaizan and knight, 2002b; knight and graehl, 1998) used the pronunciation of in translation.
</prevsent>
</prevsection>
<citsent citstr=" W02-0902 ">
on the other hand, the work of (cao and li, 2002; fung and yee, 1998; <papid> P98-1069 </papid>koehn and knight, 2002; <papid> W02-0902 </papid>rapp, 1995; <papid> P95-1050 </papid>rapp, 1999) <papid> P99-1067 </papid>used the context of to locate its translation in second language.</citsent>
<aftsection>
<nextsent>in this paper, we propose new approach for the task of mining new word translations from comparable corpora, by combining both context and transliteration information.
</nextsent>
<nextsent>since both sources of information are complementary, the accuracy of our combined approach is better than the accuracy of using just context or transliteration information alone.
</nextsent>
<nextsent>we fully implemented our method and tested it on chinese-english comparable corpora.
</nextsent>
<nextsent>we translated chinese words into english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4457">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> experiment.  </section>
<citcontext>
<prevsection>
<prevsent>5.2 preprocessing.
</prevsent>
<prevsent>unlike english, chinese text is composed of chinese characters with no demarcation for words.
</prevsent>
</prevsection>
<citsent citstr=" W04-3236 ">
so we first segmented chinese text with chinese word segmenter that was based on maximum entropy modeling (ng and low, 2004).<papid> W04-3236 </papid></citsent>
<aftsection>
<nextsent>we then divided the chinese corpus from jul to dec 1995 into 12 periods, each containing text from half-month period.
</nextsent>
<nextsent>then we determined the new chinese words in each half-month period p. by new chinese words, we refer to those words that appeared in this period but not from jan to jun 1995 or any other periods that preceded p. among all these new words, we selected those occurring at least 5 times.
</nextsent>
<nextsent>these words made up our test set.
</nextsent>
<nextsent>we call these words chinese source words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4487">
<title id=" C04-1089.xml">mining new word translations from comparable corpora </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>koehn and knight (2002) <papid> W02-0902 </papid>attempted to combine multiple clues, including similar context and spelling.</prevsent>
<prevsent>but their similar spelling clue uses the longest common sub sequence ratio and works only for cognates (words with very similar spelling).</prevsent>
</prevsection>
<citsent citstr=" N04-1036 ">
the work that is most similar to ours is the recent research of (huang et al, 2004).<papid> N04-1036 </papid></citsent>
<aftsection>
<nextsent>they attempted to improve named entity translation by combining phonetic and semantic information.
</nextsent>
<nextsent>their contextual semantic similarity model is different from our language modeling approach to measuring context similarity.
</nextsent>
<nextsent>it also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of speech tagging.
</nextsent>
<nextsent>they combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4488">
<title id=" C02-2018.xml">an xml based document suite </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the ratio of token not classifiable by the pos tagger to token classified may e.g. serve as an indication of the degree of lexical coverage.
</prevsent>
<prevsent>in principle number of approaches is usable for pos tagging (e.g.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
(brill, 1992)).<papid> A92-1021 </papid></citsent>
<aftsection>
<nextsent>we decided to avoid approaches based on (supervised) learning from tagged corpora, since the cost for creating the necessary training data are likely to be prohibitive for our users (especially in specialized sublanguages).
</nextsent>
<nextsent>the approach chosen was to try to make best use of available resources for german and to enhance them with additional functionality.
</nextsent>
<nextsent>the tool chosen is not only used in pos tagging but serves as general morpho-syntactic component for german: morphix.
</nextsent>
<nextsent>the resources employed in xdocs pos tagger are: - the lexicon and the inflectional analysis from the morphosyntactic component morphix - number of heuristics (e.g. for the classification of token not covered in the lexicon) for german the morphology component morphix (finkler and neumann, 1988) has been developed in anumber of projects and is available in different realisations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4489">
<title id=" C04-1001.xml">grammar modularity and its impact on grammar documentation </title>
<section> an xml-based grammar.  </section>
<citcontext>
<prevsection>
<prevsent>5.1 further features of the approach.
</prevsent>
<prevsent>the described documentation method is powerful tool.
</prevsent>
</prevsection>
<citsent citstr=" A92-1039 ">
besides the copying task, it can be exploited in various other ways, both to further the readibil ity of the documentation and to support the task of grammar writing (see also the suggestions by er bach (1992)).<papid> A92-1039 </papid>8snapshots grammar documentation is much easier to read if pictures of c- and f-structures illustrate the analyses of example sentences.</citsent>
<aftsection>
<nextsent>xle supports 8except for the different output formats, all of the features mentioned in this paper have been implemented.the generation of snapshot postscript files, displaying trees and f-structures, which can be included in latex document.
</nextsent>
<nextsent>note, however, that after any grammar modification, such snapshots have to be updated, since the modified grammar may now yield different c- and f-structure analyses.in our approach, snapshots are updated automatically: all example sentences in the source documentation are marked by special xml tag.
</nextsent>
<nextsent>xlesnapshots are triggered by this markup and automatically generated and updated for the entire documentation, by running the xslt stylesheet.
</nextsent>
<nextsent>indices in our approach, the documentation does not follow the grammar structure but assembles grammar code from different modules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4491">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>systems that interact with the user via natural language are in their infancy.
</prevsent>
<prevsent>as these systems mature and become more complex, it would be desirable for system developer if there were automatic methods for creating natural language generation (nlg) components thatcan produce quality output efficiently.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
stochastic methods for nlg may provide such automaticity, but most previous work (knight and hatzivassiloglou, 1995), (<papid> P95-1034 </papid>langkilde and knight, 1998), (<papid> P98-1116 </papid>oh and rudnicky, 2000), (<papid> W00-0306 </papid>uchimoto etal., 2000), (<papid> C00-2126 </papid>bangalore and rambow, 2000) <papid> C00-1007 </papid>concentrate on the specifics of individual stochastic methods, ignoring other issues such as integra bility, portability, and efficiency.</citsent>
<aftsection>
<nextsent>in contrast, this paper investigates how different stochastic nlg components can be made to work together effectively, whether they can easily be ported tonew domains, and whether they can be integrated in real-time dialog system.
</nextsent>
<nextsent>request(departdate) surface generator fergus tts spot dialog manager sentence planner dm impconf(n) soft merge text to speech implicitconfirm(newark) implicitconfirm(dallas) period impconf(d) flying from newark to dallas.
</nextsent>
<nextsent>what date would you like to leave?
</nextsent>
<nextsent>request(dd) figure 1: components of an nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4492">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>systems that interact with the user via natural language are in their infancy.
</prevsent>
<prevsent>as these systems mature and become more complex, it would be desirable for system developer if there were automatic methods for creating natural language generation (nlg) components thatcan produce quality output efficiently.
</prevsent>
</prevsection>
<citsent citstr=" P98-1116 ">
stochastic methods for nlg may provide such automaticity, but most previous work (knight and hatzivassiloglou, 1995), (<papid> P95-1034 </papid>langkilde and knight, 1998), (<papid> P98-1116 </papid>oh and rudnicky, 2000), (<papid> W00-0306 </papid>uchimoto etal., 2000), (<papid> C00-2126 </papid>bangalore and rambow, 2000) <papid> C00-1007 </papid>concentrate on the specifics of individual stochastic methods, ignoring other issues such as integra bility, portability, and efficiency.</citsent>
<aftsection>
<nextsent>in contrast, this paper investigates how different stochastic nlg components can be made to work together effectively, whether they can easily be ported tonew domains, and whether they can be integrated in real-time dialog system.
</nextsent>
<nextsent>request(departdate) surface generator fergus tts spot dialog manager sentence planner dm impconf(n) soft merge text to speech implicitconfirm(newark) implicitconfirm(dallas) period impconf(d) flying from newark to dallas.
</nextsent>
<nextsent>what date would you like to leave?
</nextsent>
<nextsent>request(dd) figure 1: components of an nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4493">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>systems that interact with the user via natural language are in their infancy.
</prevsent>
<prevsent>as these systems mature and become more complex, it would be desirable for system developer if there were automatic methods for creating natural language generation (nlg) components thatcan produce quality output efficiently.
</prevsent>
</prevsection>
<citsent citstr=" W00-0306 ">
stochastic methods for nlg may provide such automaticity, but most previous work (knight and hatzivassiloglou, 1995), (<papid> P95-1034 </papid>langkilde and knight, 1998), (<papid> P98-1116 </papid>oh and rudnicky, 2000), (<papid> W00-0306 </papid>uchimoto etal., 2000), (<papid> C00-2126 </papid>bangalore and rambow, 2000) <papid> C00-1007 </papid>concentrate on the specifics of individual stochastic methods, ignoring other issues such as integra bility, portability, and efficiency.</citsent>
<aftsection>
<nextsent>in contrast, this paper investigates how different stochastic nlg components can be made to work together effectively, whether they can easily be ported tonew domains, and whether they can be integrated in real-time dialog system.
</nextsent>
<nextsent>request(departdate) surface generator fergus tts spot dialog manager sentence planner dm impconf(n) soft merge text to speech implicitconfirm(newark) implicitconfirm(dallas) period impconf(d) flying from newark to dallas.
</nextsent>
<nextsent>what date would you like to leave?
</nextsent>
<nextsent>request(dd) figure 1: components of an nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4494">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>systems that interact with the user via natural language are in their infancy.
</prevsent>
<prevsent>as these systems mature and become more complex, it would be desirable for system developer if there were automatic methods for creating natural language generation (nlg) components thatcan produce quality output efficiently.
</prevsent>
</prevsection>
<citsent citstr=" C00-2126 ">
stochastic methods for nlg may provide such automaticity, but most previous work (knight and hatzivassiloglou, 1995), (<papid> P95-1034 </papid>langkilde and knight, 1998), (<papid> P98-1116 </papid>oh and rudnicky, 2000), (<papid> W00-0306 </papid>uchimoto etal., 2000), (<papid> C00-2126 </papid>bangalore and rambow, 2000) <papid> C00-1007 </papid>concentrate on the specifics of individual stochastic methods, ignoring other issues such as integra bility, portability, and efficiency.</citsent>
<aftsection>
<nextsent>in contrast, this paper investigates how different stochastic nlg components can be made to work together effectively, whether they can easily be ported tonew domains, and whether they can be integrated in real-time dialog system.
</nextsent>
<nextsent>request(departdate) surface generator fergus tts spot dialog manager sentence planner dm impconf(n) soft merge text to speech implicitconfirm(newark) implicitconfirm(dallas) period impconf(d) flying from newark to dallas.
</nextsent>
<nextsent>what date would you like to leave?
</nextsent>
<nextsent>request(dd) figure 1: components of an nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4495">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>systems that interact with the user via natural language are in their infancy.
</prevsent>
<prevsent>as these systems mature and become more complex, it would be desirable for system developer if there were automatic methods for creating natural language generation (nlg) components thatcan produce quality output efficiently.
</prevsent>
</prevsection>
<citsent citstr=" C00-1007 ">
stochastic methods for nlg may provide such automaticity, but most previous work (knight and hatzivassiloglou, 1995), (<papid> P95-1034 </papid>langkilde and knight, 1998), (<papid> P98-1116 </papid>oh and rudnicky, 2000), (<papid> W00-0306 </papid>uchimoto etal., 2000), (<papid> C00-2126 </papid>bangalore and rambow, 2000) <papid> C00-1007 </papid>concentrate on the specifics of individual stochastic methods, ignoring other issues such as integra bility, portability, and efficiency.</citsent>
<aftsection>
<nextsent>in contrast, this paper investigates how different stochastic nlg components can be made to work together effectively, whether they can easily be ported tonew domains, and whether they can be integrated in real-time dialog system.
</nextsent>
<nextsent>request(departdate) surface generator fergus tts spot dialog manager sentence planner dm impconf(n) soft merge text to speech implicitconfirm(newark) implicitconfirm(dallas) period impconf(d) flying from newark to dallas.
</nextsent>
<nextsent>what date would you like to leave?
</nextsent>
<nextsent>request(dd) figure 1: components of an nlg system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4497">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>during realization, the specification chosen in sentence planning is transformed into surface string by linearizing and inflecting words in the sentence (and typically, adding function words).
</prevsent>
<prevsent>figure 1 shows how such components cooperate to generate text corresponding to set of communicative goals.our work addresses both the sentence planning stage and the realization stage.
</prevsent>
</prevsection>
<citsent citstr=" N01-1003 ">
the sentence planning stage is embodied by the spot sentence planner (walker et al, 2001), <papid> N01-1003 </papid>while the surface realization stage is embodied by the fergus surface realizer (bangalore and ram bow, 2000).<papid> C00-1007 </papid></citsent>
<aftsection>
<nextsent>we extend the work of (walker et al., 2001) <papid> N01-1003 </papid>and (bangalore and rambow, 2000) <papid> C00-1007 </papid>in various ways.</nextsent>
<nextsent>we show that apparently eachof spot and fergus can be ported to different domains with little manual effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4506">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>surface ordering of the lexemes remains unspecified in the tree.
</prevsent>
<prevsent>fergus consists of three models: tree chooser, unraveler, and linear precedencechooser.
</prevsent>
</prevsection>
<citsent citstr=" J99-2004 ">
the tree chooser associates super tag (bangalore and joshi, 1999) <papid> J99-2004 </papid>from tree adjoining grammar (tag) with each node inthe underspecified dependency tree.</citsent>
<aftsection>
<nextsent>this partially specifies the output strings surface order;it is constrained by grammatical constraints encoded by the supertags (e.g. subcategorization constraints, voice), but remains free otherwise (e.g. ordering of modifiers).
</nextsent>
<nextsent>the tree chooser uses stochastic tree model (tm) to select asupertag for each node in the tree based on local tree context.
</nextsent>
<nextsent>the unraveler takes there sulting semi-specified tag derivation tree and creates word lattice corresponding to all of the potential surface orderings consistent with this tree.
</nextsent>
<nextsent>finally, the linear precedence (lp) chooser finds the best path through the word lattice according to trigram language model (lm), specifying the output string completely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4508">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 labor-minimizing approaches to.
</prevsent>
<prevsent>training fergus the resources that are needed to train fergus seem quite labor intensive to develop.
</prevsent>
</prevsection>
<citsent citstr=" W01-0520 ">
but(bangalore et al, 2001) <papid> W01-0520 </papid>show that automatically generated version of these resources can be used by fergus to obtain quality output.two kinds of tag grammar are used in (bangalore et al, 2001).<papid> W01-0520 </papid></citsent>
<aftsection>
<nextsent>one kind is manually developed, broad-coverage grammar for english: the xtag grammar (xtag-group, 2001).
</nextsent>
<nextsent>it consists of approximately 1000 tree frames.
</nextsent>
<nextsent>disadvantages of using xtag are the consider able amount of human labor expended in its development and the lack of treebank basedon xtagthe only way to estimate parameters in the tm is to relyon heuristic mapping of xtag tree frames onto pre-existing treebank (bangalore and joshi, 1999).<papid> J99-2004 </papid></nextsent>
<nextsent>another kind of grammar is tag automatically extracted from treebank using the techniques of (chen, 2001) (cf.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4514">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>disadvantages of using xtag are the consider able amount of human labor expended in its development and the lack of treebank basedon xtagthe only way to estimate parameters in the tm is to relyon heuristic mapping of xtag tree frames onto pre-existing treebank (bangalore and joshi, 1999).<papid> J99-2004 </papid></prevsent>
<prevsent>another kind of grammar is tag automatically extracted from treebank using the techniques of (chen, 2001) (cf.</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
(chiang, 2000), (<papid> P00-1058 </papid>xia, 1999)).these techniques extract linguistically motivated tag using heuristics programmed using modicum of human labor.</citsent>
<aftsection>
<nextsent>they nullify the disadvantages of using the xtag grammar, but they introduce potential complications?
</nextsent>
<nextsent>notably, an extracted grammars size is often much larger than that of xtag, typically more than 2000 tree frames, potentially leading to alarger sparse data problem, and also the resulting grammar is not hand-checked.
</nextsent>
<nextsent>two kinds of treebank are used in (bangalore et al, 2001).<papid> W01-0520 </papid></nextsent>
<nextsent>one kind is the penn treebank(marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4518">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>notably, an extracted grammars size is often much larger than that of xtag, typically more than 2000 tree frames, potentially leading to alarger sparse data problem, and also the resulting grammar is not hand-checked.
</prevsent>
<prevsent>two kinds of treebank are used in (bangalore et al, 2001).<papid> W01-0520 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
one kind is the penn treebank(marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>it consists of approximately 1,000,000 words of hand-checked, bracketed text.
</nextsent>
<nextsent>the text consists of wall street journal news articles.
</nextsent>
<nextsent>the other kind of treebank isthe bllip corpus (charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>it consists of approximately 40,000,000 words of text that has been parsed by broad-coverage statistical parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4519">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>it consists of approximately 1,000,000 words of hand-checked, bracketed text.
</prevsent>
<prevsent>the text consists of wall street journal news articles.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the other kind of treebank isthe bllip corpus (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>it consists of approximately 40,000,000 words of text that has been parsed by broad-coverage statistical parser.
</nextsent>
<nextsent>the text consists of wall street journal news and newswire articles.
</nextsent>
<nextsent>the advantage of the former is that it has been hand checked, whereas the latter has the advantage of being easily produced and hence can easily be enlarged.(bangalore et al, 2001) <papid> W01-0520 </papid>experimentally determine how the quality and quantity of the resources used in training fergus affect the output quality of the generator.</nextsent>
<nextsent>they find that while better quality annotated corpus (penn treebank) results in better model accuracy than lower quality corpus (bllip) of the same size, an (easily-obtained) larger lower quality corpus results in model that eclipses smaller, better quality treebank.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4525">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> automation in training surface.  </section>
<citcontext>
<prevsection>
<prevsent>we do the same for the lms training corpus.
</prevsent>
<prevsent>assessing the output quality of generator is complex issue.
</prevsent>
</prevsection>
<citsent citstr=" W00-1401 ">
here, we select as our metric understand ability accuracy, defined in (bangalore et al, 2000) <papid> W00-1401 </papid>as quantifying the differ ptb tm hh tm ptb lm 0.30 0.38 hh lm 0.37 0.41 table 2: average understand ability accuracies using xtag-based fergus for various kinds of training data ptb tm ptb lm 0.39 hh lm 0.33 table 3: average understand ability accuracies using automatically-extracted grammar based fergus for various kinds of training data ence between the generator output, in terms of both dependency tree and surface string, and the desired reference output.</citsent>
<aftsection>
<nextsent>(bangalore et al,2000) <papid> W00-1401 </papid>finds this metric to correlate well with human judgments of understand ability and qual ity.</nextsent>
<nextsent>understand ability accuracy varies between high score of 1.0 and low score which may be less than zero.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4530">
<title id=" C02-1138.xml">towards automatic generation of natural language generation systems </title>
<section> integration of spot with.  </section>
<citcontext>
<prevsection>
<prevsent>only the xtag grammar is used in this experiment.
</prevsent>
<prevsent>as in previous experiments with the xtag grammar, there is either the option of training using hh or ptb derived data for either the tm or lm, giving total of four possibilities.
</prevsent>
</prevsection>
<citsent citstr=" P98-1118 ">
test data is obtained by output strings that are produced by the combination of spot andthe realpro surface realizer (lavoie and rambow, 1998).<papid> P98-1118 </papid></citsent>
<aftsection>
<nextsent>realpro has the advantage of producing high quality surface strings, but at thecost of having to be hand-tuned to particular domain.
</nextsent>
<nextsent>it is this cost we are attempting to minimize by using fergus.
</nextsent>
<nextsent>only those sentence plans produced by spot ranked 3.0 or greater by human judges are used.
</nextsent>
<nextsent>the surface realization of these sentence plans yields test corpus of 2,200 words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4531">
<title id=" C02-2020.xml">looking for candidate translational equivalents in specialized comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an additional reverse-translationfiltering step improves the precision of the top candidate translation up to 74%, with 33% recall.
</prevsent>
<prevsent>one of the issues that have to be addressed in cross-language information retrieval (clir,grefenstette (1998b)) is that of query translation, which relies on some form of bilinguallexicon.
</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
methods have been proposed to acquire lexicon from corpora when such lexicon does not exist or is not complete enough (fung and mckeown, 1997; <papid> W97-0119 </papid>fung and yee, 1998; <papid> P98-1069 </papid>picchi and peters, 1998; rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>the present work addresses this issue in specialized domain: medicine.
</nextsent>
<nextsent>we aim at identifying french-english translation candidates from comparable medical corpora, extending an existing specialized bilingual lexicon.
</nextsent>
<nextsent>these translational equivalents may then be used, e.g., for query expansion and translation.
</nextsent>
<nextsent>we first recall previous work on this topic, then present the corpora and initial bilingual lexicon westart with, and the method we use to build, transfer and compare context vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4532">
<title id=" C02-2020.xml">looking for candidate translational equivalents in specialized comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an additional reverse-translationfiltering step improves the precision of the top candidate translation up to 74%, with 33% recall.
</prevsent>
<prevsent>one of the issues that have to be addressed in cross-language information retrieval (clir,grefenstette (1998b)) is that of query translation, which relies on some form of bilinguallexicon.
</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
methods have been proposed to acquire lexicon from corpora when such lexicon does not exist or is not complete enough (fung and mckeown, 1997; <papid> W97-0119 </papid>fung and yee, 1998; <papid> P98-1069 </papid>picchi and peters, 1998; rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>the present work addresses this issue in specialized domain: medicine.
</nextsent>
<nextsent>we aim at identifying french-english translation candidates from comparable medical corpora, extending an existing specialized bilingual lexicon.
</nextsent>
<nextsent>these translational equivalents may then be used, e.g., for query expansion and translation.
</nextsent>
<nextsent>we first recall previous work on this topic, then present the corpora and initial bilingual lexicon westart with, and the method we use to build, transfer and compare context vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4534">
<title id=" C02-2020.xml">looking for candidate translational equivalents in specialized comparable corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an additional reverse-translationfiltering step improves the precision of the top candidate translation up to 74%, with 33% recall.
</prevsent>
<prevsent>one of the issues that have to be addressed in cross-language information retrieval (clir,grefenstette (1998b)) is that of query translation, which relies on some form of bilinguallexicon.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
methods have been proposed to acquire lexicon from corpora when such lexicon does not exist or is not complete enough (fung and mckeown, 1997; <papid> W97-0119 </papid>fung and yee, 1998; <papid> P98-1069 </papid>picchi and peters, 1998; rapp, 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>the present work addresses this issue in specialized domain: medicine.
</nextsent>
<nextsent>we aim at identifying french-english translation candidates from comparable medical corpora, extending an existing specialized bilingual lexicon.
</nextsent>
<nextsent>these translational equivalents may then be used, e.g., for query expansion and translation.
</nextsent>
<nextsent>we first recall previous work on this topic, then present the corpora and initial bilingual lexicon westart with, and the method we use to build, transfer and compare context vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4553">
<title id=" C02-2024.xml">an indexing scheme for typed feature structures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the quick check algorithm described in (torisawa and tsujii, 1995; malouf et al, 2000) also uses this condition for the efficient checking of unifiability between two tfss.
</prevsent>
<prevsent>given two tfss and stat ically determined paths, the quick check algorithm can efficiently determine whether these two tfss are non-unifiable or there is some uncertainty about their unifiability by checking the path values.
</prevsent>
</prevsection>
<citsent citstr=" P99-1061 ">
it is worth noting that this algorithm is used in many modern unification grammar-based systems, e.g., the lkb system (copestake, 1999) and the page system (kiefer et al, 1999).<papid> P99-1061 </papid></citsent>
<aftsection>
<nextsent>unlike the quick check algorithm, which checks unifiability between two tfss, our istfs checks unifiability between one tfs and tfss.
</nextsent>
<nextsent>theistfs checks unifiability by using dynamically determined paths, not static ally determined paths.
</nextsent>
<nextsent>in our case, using only static ally determined paths might extremely degrades the system performance.
</nextsent>
<nextsent>suppose that any static ally determined paths are not defined in the query tfs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4554">
<title id=" C02-2024.xml">an indexing scheme for typed feature structures </title>
<section> performance evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the dataset consisting of 249,994 tfss was generated by parsing the
</prevsent>
<prevsent> figure 3: the size of dpi,?
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for the size of the dataset 800 bracketed sentences in the wall street journal corpus (the first 800 sentences in wall street journal 00) in the penn treebank (marcus et al, 1993) <papid> J93-2004 </papid>with the xhpsg grammar (tateisi et al, 1998).</citsent>
<aftsection>
<nextsent>the size of the dataset was 151 mb.
</nextsent>
<nextsent>we also generated two sets of query tfss by parsing five randomly selected sentences in the wall street journal corpus (queryseta and querysetb).
</nextsent>
<nextsent>each set had 100 query tfss.
</nextsent>
<nextsent>each element of queryseta was the daughter part of the grammar rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4555">
<title id=" C08-1092.xml">anomalies in the wordnet verb hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the expression sister terms is used to designate pair of synsets which share hypernym.
</prevsent>
<prevsent>this study distinguishes between transitive causation (causing another to do something) and reflexive causation (causing oneself to do something).
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
the term quale (plural: qualia) is borrowed from pustejovsky (1991).<papid> J91-4003 </papid></citsent>
<aftsection>
<nextsent>as applied to verbs within the context of this study, the formal quale means what is physically done, while the telic quale means the purpose or intended result of the action.
</nextsent>
<nextsent>1.2 application of wordnet relations.
</nextsent>
<nextsent>banerjee &amp; pedersen (2003) have employed wordnet relations in an extension to the lesk (1986) algorithm for word sense disambiguation.
</nextsent>
<nextsent>in order to establish the relatedness of two words, the glosses of their wordnet relatives are compared.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4556">
<title id=" C08-1092.xml">anomalies in the wordnet verb hierarchy </title>
<section> anomalies relating to categories.  </section>
<citcontext>
<prevsection>
<prevsent>competition is subsumed by social, and consumption is subsumed by body.
</prevsent>
<prevsent>weather would seem self contained, but change, creation and stative are not semantic fields at all.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
stative belongs to the aktionsart categorisation of verbs distinguishing it from verbs of activity, achievement and accomplishment, which is orthogonal to the categorisation of verbs into semantic fields (vendler, 1967, moens &amp; steedman 1988, <papid> J88-2003 </papid>amaro, 2006).</citsent>
<aftsection>
<nextsent>moreover, verb can belong to more than one aktionsart category, as these apply to verbs in contexts.
</nextsent>
<nextsent>3.3 suggested revision of categories.
</nextsent>
<nextsent>among verbs, the level of arbitrariness and incorrectness of the wordnet categories seems greater than that of the relations.
</nextsent>
<nextsent>whereas the theoretical basis for wordnet relations is consistent and the errors are failures to conform to the specification, in the case of categories, the theoretical basis is inconsistent, being, compromise between more than one system of categorisation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4557">
<title id=" C04-1025.xml">a grammar formalism and parser for linearization based hpsg </title>
<section> linearization-based hpsg.  </section>
<citcontext>
<prevsection>
<prevsent>in other words, precedence constraints cannot look into compacted domain.note that these are two distinct functions of do main compaction: defining domain as covering acontiguous stretch of terminals is in principle independent of defining domain of elements for lp constraints to apply to.
</prevsent>
<prevsent>in linearization-based hpsg, domain compaction encodes both aspects.
</prevsent>
</prevsection>
<citsent citstr=" P95-1024 ">
later work (kathol and pollard, 1995; <papid> P95-1024 </papid>kathol, 1995; yatabe, 1996) introduced the notion of partial compaction, in which only portion of the daughters order domain is compacted; the remaining elements are domain unioned.</citsent>
<aftsection>
<nextsent>1apart from reapes approach, there have been proposals for more complete separation of word order and syntactic structure in hpsg (see, for example, richter and sailer, 2001 and penn, 1999).
</nextsent>
<nextsent>in this paper, we focus on the majority of linearization-based hpsg approaches, which follow reape.
</nextsent>
<nextsent>formally, theory in the hpsg architecture consists of set of constraints on the data structures introduced in the signature; thus, word order domains and the constraints thereon can be straightforwardly expressed.
</nextsent>
<nextsent>on the computational side, however, most systems employ parsers to efficiently process hpsg-based grammars organized around phrase structure backbone.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4558">
<title id=" C04-1025.xml">a grammar formalism and parser for linearization based hpsg </title>
<section> processing linearization-based hpsg.  </section>
<citcontext>
<prevsection>
<prevsent>in sum, no grammar format is currently available that adequately supports the encoding of processing backbone for linearization-based hpsg grammars.
</prevsent>
<prevsent>as result, implementations of linearization based hpsg grammars have taken one of two options.
</prevsent>
</prevsection>
<citsent citstr=" W97-1506 ">
some simply do not use parser, such as the work based on con troll (gotz and meurers, 1997); <papid> W97-1506 </papid>as consequence, the efficiency and termination properties of parsers cannot be taken for granted in such approaches.</citsent>
<aftsection>
<nextsent>the other approaches use minimal parser thatcan only take advantage of small subset of the requisite constraints.
</nextsent>
<nextsent>such parsers are typically limited to the general concept of resource sensitivity ? every element in the input needs to be found exactly once ? and the ability to require certain categories to dominate contiguous segment of the input.
</nextsent>
<nextsent>some of these approaches (johnson, 1985; <papid> P85-1015 </papid>reape, 1991) lack word order constraints altogether.</nextsent>
<nextsent>others(van noord, 1991; ramsay, 1999) have the grammar writer provide combinatory predicate (such as concatenate, shuffle, or head-wrap) for each rule specifying how the string coverage of the mother is determined from the string coverages of the daugh ter.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4559">
<title id=" C04-1025.xml">a grammar formalism and parser for linearization based hpsg </title>
<section> processing linearization-based hpsg.  </section>
<citcontext>
<prevsection>
<prevsent>the other approaches use minimal parser thatcan only take advantage of small subset of the requisite constraints.
</prevsent>
<prevsent>such parsers are typically limited to the general concept of resource sensitivity ? every element in the input needs to be found exactly once ? and the ability to require certain categories to dominate contiguous segment of the input.
</prevsent>
</prevsection>
<citsent citstr=" P85-1015 ">
some of these approaches (johnson, 1985; <papid> P85-1015 </papid>reape, 1991) lack word order constraints altogether.</citsent>
<aftsection>
<nextsent>others(van noord, 1991; ramsay, 1999) have the grammar writer provide combinatory predicate (such as concatenate, shuffle, or head-wrap) for each rule specifying how the string coverage of the mother is determined from the string coverages of the daughter.
</nextsent>
<nextsent>in either case, the task of constructing word order domain and enforcing word order constraints in that domain is left out of the parsing algorithm; as result, constraints on word order domains either cannot be stated or are tested in separate clean-up phase.
</nextsent>
<nextsent>to develop grammar format for linearization based hpsg, we take the syntax of id/lp rules and augment it with means for specifying which daughters form compacted domains.
</nextsent>
<nextsent>a generalized id/lp (gidlp) grammar consists of four parts: root declaration, set of lexical entries, set of grammar rules, and set of global order constraints.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4561">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of such shallow representation asa common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations.we compared an hpsg parser with several cfg parsers in our experiment and found that meaningful differences among the parsers?
</prevsent>
<prevsent>performance can still be observed by such shallow representation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1041 ">
recently, there have been advancement made in the parsing techniques for large-scale lexicalized grammars (clark and curran, 2004; <papid> C04-1041 </papid>ninomiya et al., 2005; <papid> W05-1511 </papid>ninomiya et al, 2007), <papid> W07-2208 </papid>and it have presumably been accelerated by the development of the semi-automatic acquisition techniques oflarge-scale lexicalized grammars from parsed corpora (hockenmaier and steedman, 2007; miyao ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.et al, 2005).
</nextsent>
<nextsent>in many of the studies on lexicalized grammar parsing, the accuracy of the parsing results is evaluated in terms of the accuracy of the semantic representations output by the parsers.since the formalisms for the semantic representation are different across the grammar frameworks,it has been difficult to directly compare the performance of the parsers developed for different grammar frameworks.
</nextsent>
<nextsent>several researchers in the field of lexicalized grammar parsing have recently started to seek common representation of parsing results across different grammar frameworks (clark and curran, 2007; <papid> P07-1032 </papid>miyao et al, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4562">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of such shallow representation asa common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations.we compared an hpsg parser with several cfg parsers in our experiment and found that meaningful differences among the parsers?
</prevsent>
<prevsent>performance can still be observed by such shallow representation.
</prevsent>
</prevsection>
<citsent citstr=" W05-1511 ">
recently, there have been advancement made in the parsing techniques for large-scale lexicalized grammars (clark and curran, 2004; <papid> C04-1041 </papid>ninomiya et al., 2005; <papid> W05-1511 </papid>ninomiya et al, 2007), <papid> W07-2208 </papid>and it have presumably been accelerated by the development of the semi-automatic acquisition techniques oflarge-scale lexicalized grammars from parsed corpora (hockenmaier and steedman, 2007; miyao ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.et al, 2005).
</nextsent>
<nextsent>in many of the studies on lexicalized grammar parsing, the accuracy of the parsing results is evaluated in terms of the accuracy of the semantic representations output by the parsers.since the formalisms for the semantic representation are different across the grammar frameworks,it has been difficult to directly compare the performance of the parsers developed for different grammar frameworks.
</nextsent>
<nextsent>several researchers in the field of lexicalized grammar parsing have recently started to seek common representation of parsing results across different grammar frameworks (clark and curran, 2007; <papid> P07-1032 </papid>miyao et al, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4563">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the use of such shallow representation asa common format has the advantage of reduced noise introduced by the conversion in comparison with the noise produced by the conversion to deeper representations.we compared an hpsg parser with several cfg parsers in our experiment and found that meaningful differences among the parsers?
</prevsent>
<prevsent>performance can still be observed by such shallow representation.
</prevsent>
</prevsection>
<citsent citstr=" W07-2208 ">
recently, there have been advancement made in the parsing techniques for large-scale lexicalized grammars (clark and curran, 2004; <papid> C04-1041 </papid>ninomiya et al., 2005; <papid> W05-1511 </papid>ninomiya et al, 2007), <papid> W07-2208 </papid>and it have presumably been accelerated by the development of the semi-automatic acquisition techniques oflarge-scale lexicalized grammars from parsed corpora (hockenmaier and steedman, 2007; miyao ? 2008.</citsent>
<aftsection>
<nextsent>licensed under the creative commonsattribution-noncommercial-share alike 3.0 un ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
</nextsent>
<nextsent>some rights reserved.et al, 2005).
</nextsent>
<nextsent>in many of the studies on lexicalized grammar parsing, the accuracy of the parsing results is evaluated in terms of the accuracy of the semantic representations output by the parsers.since the formalisms for the semantic representation are different across the grammar frameworks,it has been difficult to directly compare the performance of the parsers developed for different grammar frameworks.
</nextsent>
<nextsent>several researchers in the field of lexicalized grammar parsing have recently started to seek common representation of parsing results across different grammar frameworks (clark and curran, 2007; <papid> P07-1032 </papid>miyao et al, 2007).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4565">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some rights reserved.et al, 2005).
</prevsent>
<prevsent>in many of the studies on lexicalized grammar parsing, the accuracy of the parsing results is evaluated in terms of the accuracy of the semantic representations output by the parsers.since the formalisms for the semantic representation are different across the grammar frameworks,it has been difficult to directly compare the performance of the parsers developed for different grammar frameworks.
</prevsent>
</prevsection>
<citsent citstr=" P07-1032 ">
several researchers in the field of lexicalized grammar parsing have recently started to seek common representation of parsing results across different grammar frameworks (clark and curran, 2007; <papid> P07-1032 </papid>miyao et al, 2007).</citsent>
<aftsection>
<nextsent>for example, clark and curran (2007) <papid> P07-1032 </papid>developed set of mapping rules from the output of combinatorial categorial grammar parser to the grammatical relations (gr) (carroll et al, 1998).</nextsent>
<nextsent>they found that the manual development of such mapping rules is not trivial task; their mapping rules covered only 85% of the grs in gr-annotated corpus; i.e., 15% of the grs in the corpus could not be covered by the mapping from the gold-standard ccg analyses of those sentences.we propose another method for the cross framework performance analysis of the parsers wherein the output of parsers are first converted to cfg tree.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4567">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>grammar for tree conversion for the purpose of the inverted transformation of simplified hpsg trees to ptb-cfg trees, we use statistical approach based on the stochastic synchronous grammars.
</prevsent>
<prevsent>stochastic synchronous grammars are family of probabilistic models that generate pair of trees by recursively applying synchronous productions, starting with pair of initial symbols.
</prevsent>
</prevsection>
<citsent citstr=" P03-2041 ">
see e.g., eisner (2003) <papid> P03-2041 </papid>for more formal definition.</citsent>
<aftsection>
<nextsent>figure 2 shows an example of synchronous cfg, which generates the pairs of strings of the form (abmc, cbma).
</nextsent>
<nextsent>each nonterminal symbol on the yields of the synchronous production is linked to non-terminal symbol on the other rules yield.
</nextsent>
<nextsent>in the figure, the links are represented by subscripts.
</nextsent>
<nextsent>a linked pair of the nonterminal symbols is simultaneously expanded by another synchronous production.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4572">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the hpsg parser is the enju parser (ninomiya et al, 2007), <papid> W07-2208 </papid>which has been developed for parsing with the enju hpsg grammar.</prevsent>
<prevsent>a disambiguation module based on discriminative maximum-entropy model is used in the enju parser.</prevsent>
</prevsection>
<citsent citstr=" N03-1016 ">
we compared the enjuparser with four cfg parsers: stanfords lexicalized parser (klein and manning, 2003), <papid> N03-1016 </papid>collins parser (collins, 1999), charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and charniak and johnsons reranking parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the first three parsers are based on treebank pcfgs derived from ptb.
</nextsent>
<nextsent>the last parser is combination of charniaks parser and reranking module based on maximum-entropy model.
</nextsent>
<nextsent>the enju parser and collins?
</nextsent>
<nextsent>parser require pos-tagged sentences as the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4573">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the hpsg parser is the enju parser (ninomiya et al, 2007), <papid> W07-2208 </papid>which has been developed for parsing with the enju hpsg grammar.</prevsent>
<prevsent>a disambiguation module based on discriminative maximum-entropy model is used in the enju parser.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
we compared the enjuparser with four cfg parsers: stanfords lexicalized parser (klein and manning, 2003), <papid> N03-1016 </papid>collins parser (collins, 1999), charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and charniak and johnsons reranking parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the first three parsers are based on treebank pcfgs derived from ptb.
</nextsent>
<nextsent>the last parser is combination of charniaks parser and reranking module based on maximum-entropy model.
</nextsent>
<nextsent>the enju parser and collins?
</nextsent>
<nextsent>parser require pos-tagged sentences as the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4574">
<title id=" C08-1069.xml">comparative parser performance analysis across grammar frameworks through automatic tree conversion using synchronous grammars </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the hpsg parser is the enju parser (ninomiya et al, 2007), <papid> W07-2208 </papid>which has been developed for parsing with the enju hpsg grammar.</prevsent>
<prevsent>a disambiguation module based on discriminative maximum-entropy model is used in the enju parser.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
we compared the enjuparser with four cfg parsers: stanfords lexicalized parser (klein and manning, 2003), <papid> N03-1016 </papid>collins parser (collins, 1999), charniaks parser (charniak, 2000), <papid> A00-2018 </papid>and charniak and johnsons reranking parser (charniak and johnson, 2005).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>the first three parsers are based on treebank pcfgs derived from ptb.
</nextsent>
<nextsent>the last parser is combination of charniaks parser and reranking module based on maximum-entropy model.
</nextsent>
<nextsent>the enju parser and collins?
</nextsent>
<nextsent>parser require pos-tagged sentences as the input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4575">
<title id=" C02-1099.xml">an english korean transliteration model using pronunciation and contextual rules </title>
<section> an english-korean transliteration.  </section>
<citcontext>
<prevsection>
<prevsent>one possible alignment between english word board?
</prevsent>
<prevsent>and its pronunciation for automatic epu-p alignment, we used the modified version of kangs e-k alignment algorithm (kang et al, 2000; kang et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" J96-4002 ">
it is based on covingtons algorithm (covington, 1996).<papid> J96-4002 </papid></citsent>
<aftsection>
<nextsent>covington views an alignment as way of stepping through two words ? word in one side and word in the other side ? while performing match?
</nextsent>
<nextsent>or skip?
</nextsent>
<nextsent>operation on each step.
</nextsent>
<nextsent>kang added forward bind?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4576">
<title id=" C04-1024.xml">efficient parsing of highly ambiguous context free grammars with bit vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large context-free grammars extracted from treebanks achieve high coverage and accuracy, but they are difficult to parse with because of their massive ambiguity.
</prevsent>
<prevsent>the application of standard chart-parsing techniques often fails due to excessive memory and runtime requirements.treebank grammars are mostly used as probabilistic grammars and users are usually only interested in the best analysis, the viterbi parse.
</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
to speed up viterbi parsing, sophisticated search strategies have been developed which find the most probable analysis without examining the whole set of possible analyses (charniak et al, 1998; <papid> W98-1115 </papid>klein and manning,2003<papid> P03-1054 </papid>a).</citsent>
<aftsection>
<nextsent>these methods reduce the number of generated edges, but increase the amount of time needed for each edge.
</nextsent>
<nextsent>the parser described in this paper follows contrary approach: instead of reducing the number of edges, it minimises the costs of building edges in terms of memory and runtime.the new parser, called bitpar, is based on bit vector implementation (cf.
</nextsent>
<nextsent>(graham et al, 1980)) of the well-known cocke-younger-kasami (cky) algorithm (kasami, 1965; younger, 1967).
</nextsent>
<nextsent>it buildsa compact parse forest?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4577">
<title id=" C04-1024.xml">efficient parsing of highly ambiguous context free grammars with bit vectors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>large context-free grammars extracted from treebanks achieve high coverage and accuracy, but they are difficult to parse with because of their massive ambiguity.
</prevsent>
<prevsent>the application of standard chart-parsing techniques often fails due to excessive memory and runtime requirements.treebank grammars are mostly used as probabilistic grammars and users are usually only interested in the best analysis, the viterbi parse.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
to speed up viterbi parsing, sophisticated search strategies have been developed which find the most probable analysis without examining the whole set of possible analyses (charniak et al, 1998; <papid> W98-1115 </papid>klein and manning,2003<papid> P03-1054 </papid>a).</citsent>
<aftsection>
<nextsent>these methods reduce the number of generated edges, but increase the amount of time needed for each edge.
</nextsent>
<nextsent>the parser described in this paper follows contrary approach: instead of reducing the number of edges, it minimises the costs of building edges in terms of memory and runtime.the new parser, called bitpar, is based on bit vector implementation (cf.
</nextsent>
<nextsent>(graham et al, 1980)) of the well-known cocke-younger-kasami (cky) algorithm (kasami, 1965; younger, 1967).
</nextsent>
<nextsent>it buildsa compact parse forest?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4581">
<title id=" C02-1166.xml">an approach based on multilingual thesauri and model combination for bilingual lexicon extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(fung, 2000) reports, for the chinese-english language pair an accuracy of 76% to find the correct translation in the top 20 candidates, figure we do not believe to be good enough to consider manual revision.
</prevsent>
<prevsent>furthermore, the evaluation is carried out on 40 english words only.
</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
(rapp, 1999) <papid> P99-1067 </papid>reaches 89% on the german-english language pair, when considering the top 10 candidates.</citsent>
<aftsection>
<nextsent>if this figure is rather high, it was obtained on set of 100 german words, which, even though not explicit in rapp paper, seem to be high frequency words, for which accurate and reliable statistics can be obtained.
</nextsent>
<nextsent>we want to show in this paper how previously proposed methods can be extended to and improved for specialized domains.
</nextsent>
<nextsent>in particular we will focus on the use and enrichment of multilingual thesauri, which, even though partially related they may be to the texts under consideration, are nonetheless an available and valuable resource for the task.
</nextsent>
<nextsent>we rely in this work on two main linguistic resources: general bilingual dictionary (available through the elra consortium1) and specialized multilingual thesaurus (the medical subject headings, mesh, provided through the meta thesaurus unified medical language system, umls2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4584">
<title id=" C02-1166.xml">an approach based on multilingual thesauri and model combination for bilingual lexicon extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the target context vectors are then translated using general bilingual dictionary, and compared with the source context vectors.
</prevsent>
<prevsent>our implementation of this strategy relies on the following steps, and follows the one given in (rapp, 1999): - <papid> P99-1067 </papid>for each word w, build context vector by considering all the words occurring in window encompassing several sentences that is run through the corpus.</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
each word in the context vector of is then weighted with measure of its association with w. we chose the log likelihood ratio test, (dunning, 1993), <papid> J93-1003 </papid>to measure this association - the context vectors of the target words are then translated with our general bilingual dictionary, leaving the weights unchanged (when several translations are proposed by the dictionary, we consider all of them with the same weight) - the similarity of each source word s, for each target word t, is computed on the basis of the cosine measure - the similarities are then normalized to yield probabilistic translation lexicon, p(t|s).</citsent>
<aftsection>
<nextsent>to illustrate the above steps, we give here the first 5 words of the context vector of the german word leber (liver), together with their associated score: (transplantation 138, resektion 53, metastase 41, arterie 38, cirrhose 26).
</nextsent>
<nextsent>once this context vector translated, the english top five becomes: (transplant 138, tumour 48, secondary 42, metastatis 41, artery 38).
</nextsent>
<nextsent>one can note that the german term resektion was not found in our bilingual dictionary, and thus not translated.
</nextsent>
<nextsent>however, the translated context vector contains english terms characteristic of the co-occurrence pattern for liver, allowing one to associate the two words leber and liver.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4585">
<title id=" C04-1007.xml">combining hierarchical clustering and machine learning to predict high level discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>edus are linked to each other by rhetorical relations, such as contrast or elaboration, and then form larger text segments (represented by intermediate nodes in the tree), which in turn can be linked to other segments via rhetorical relations, giving rise to even larger segments.
</prevsent>
<prevsent>discourse parsing is concerned with inferring discourse structure automatically and can be viewed as consisting of three co-dependent subtasks: (i)identifying the edus, (ii) determining which discourse segments (edus or larger segments) relate to each other, i.e. finding the correct attachment site for each segment, and (iii) identifying how discourse segments are related to each other, i.e. inferring the rhetorical relation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
while these tasks have been dealt with quite well for small structures (i.e. on clause and sentence level) (soricut and marcu, 2003), <papid> N03-1030 </papid>many of these approaches cannot be applied directly to higher level structures (e.g. on multi-sentence and inter paragraph level) because they rely nearly exclusively on cue phrases, which are much less useful for large structures (marcu, 2000, p. 129).</citsent>
<aftsection>
<nextsent>in this paper, we focus exclusively on inferring high-levelstructure.
</nextsent>
<nextsent>in particular, we investigate ways to automatically determine the correct attachment site for paragraph and multi-paragraph segments.
</nextsent>
<nextsent>finding good attachment site is complex task; even if one requires the final structure to be tree, the number of valid structures grows rapidly with the number of edus in text.
</nextsent>
<nextsent>an exhaustive search is often not feasible, even for relatively small texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4586">
<title id=" C04-1007.xml">combining hierarchical clustering and machine learning to predict high level discourse structure </title>
<section> feature set.  </section>
<citcontext>
<prevsection>
<prevsent>tense we use 6 tense features: the first, last, and majority tense of the left (right) segment.
</prevsent>
<prevsent>tense information was obtained by using regular expressions to extract verbal complexes from the part-of speech tagged text and then determine their tense.
</prevsent>
</prevsection>
<citsent citstr=" P88-1014 ">
tense often serves as cue for discourse structure (lascarides and asher, 1993; webber, 1988<papid> P88-1014 </papid>b).</citsent>
<aftsection>
<nextsent>a shift from simple past to past perfect, for instance, can indicate the start of an embedded segment.
</nextsent>
<nextsent>cue phrases this set comprises 4 features.
</nextsent>
<nextsent>the first three features are reserved for potential cue phrases in the first sentence of the right segment.
</nextsent>
<nextsent>cue phrases are identified by scanning sentence (or the first 100 characters of it, whichever is shorter) for an occurrence of one of the cue phrases listed in knott (1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4596">
<title id=" C04-1007.xml">combining hierarchical clustering and machine learning to predict high level discourse structure </title>
<section> feature set.  </section>
<citcontext>
<prevsection>
<prevsent>the folsum men couldnt adapt, and they died out.
</prevsent>
<prevsent>thats what is supposed to have happened.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
lexical chains this set comprises 28 features.the idea of using lexical chains as indicators of lexical cohesion goes back to morris and hirst (1991).<papid> J91-1002 </papid></citsent>
<aftsection>
<nextsent>a lexical chain is sequence of semantically related words and can indicate the presence and extent of sub topics in text.
</nextsent>
<nextsent>we use our own implementation to compute chains.
</nextsent>
<nextsent>a distinction is made between common noun chains, which are built on the basis of semantic relatedness using wordnet (miller et al, 1990), and proper noun chains, which contain nouns not found in wordnet and are based on co-reference rather than semantic relatedness.
</nextsent>
<nextsent>as first step, nouns are extracted and lemmatised using the morpha analyser (minnen et al, 2001) and then looked up in wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4597">
<title id=" C04-1007.xml">combining hierarchical clustering and machine learning to predict high level discourse structure </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>as described in section 2, the trained model was combined with the clustering method to build trees for the test set.
</prevsent>
<prevsent>these were evaluated against the manually built discourse trees.
</prevsent>
</prevsection>
<citsent citstr=" H91-1060 ">
precision (p) and recall (r) were defined in accordance with the parseval measures (black et al, 1991), <papid> H91-1060 </papid>i.e. precision is random rb to lb me me   lc me   to me   lcto human* 44.37% 36.76% 49.98% 53.52% 58.06% 55.86% 57.12% 55.26% 64.37% 46.71% 40.35% 52.42% 56.23% 60.78% 58.29% 59.70% 57.69% 64.60% 45.05% 37.58% 50.79% 54.27% 59.00% 56.66% 58.00% 56.07% 64.34% table 1: results on rst-dt test set (* on doubly annotated set)defined as the number of correct nodes (i.e. matching brackets) divided by the number of nodes in the automatically built tree and recall as the number of correct nodes divided by the number of nodes in the manually built tree.</citsent>
<aftsection>
<nextsent>precision and recall are combined in the f-score (f), defined as )
</nextsent>
<nextsent> . table 1 shows the results.
</nextsent>
<nextsent>we compared the performance of our model (me) to yaaris (1997) method of building trees based on term overlap(to).
</nextsent>
<nextsent>in addition, three baselines were used: merging segments randomly (results averaged over 100 runs), producing right-branching tree by always merging the last two segments (rb) and producing left-branching tree by always merging the first two segments (lb).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4598">
<title id=" C02-1066.xml">guaranteeing parsing termination of unification grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to ensure decidabilityof the recognition problem, constraint called offline pars ability (olp) was suggested.
</prevsent>
<prevsent>the recognition problem is decidable for olp grammars.
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
there exist several variants of olp in the literature (pereira and warren, 1983; <papid> P83-1021 </papid>johnson, 1988; haas, 1989; <papid> J89-4001 </papid>torenvliet and trautwein, 1995; shieber, 1992; wintner and francez, 1999; kuhn, 1999).</citsent>
<aftsection>
<nextsent>some variants of olp were suggested without recognizing the existence of all other variants.
</nextsent>
<nextsent>in this paper we make comparative analysis of the different olp variants for the first time.
</nextsent>
<nextsent>some researchers (haas, 1989; <papid> J89-4001 </papid>torenvliet and trautwein, 1995) conjecture that some of the olp variants are undecidable (it is undecidable whether grammar satisfies the constraint), although none of them gives any proof of it.</nextsent>
<nextsent>there exist some variants of olp for which decidability holds, but these conditions are too restrictive; there is large class of non-olpgrammars for which parsing termination is guaran teed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4601">
<title id=" C02-1066.xml">guaranteeing parsing termination of unification grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to ensure decidabilityof the recognition problem, constraint called offline pars ability (olp) was suggested.
</prevsent>
<prevsent>the recognition problem is decidable for olp grammars.
</prevsent>
</prevsection>
<citsent citstr=" J89-4001 ">
there exist several variants of olp in the literature (pereira and warren, 1983; <papid> P83-1021 </papid>johnson, 1988; haas, 1989; <papid> J89-4001 </papid>torenvliet and trautwein, 1995; shieber, 1992; wintner and francez, 1999; kuhn, 1999).</citsent>
<aftsection>
<nextsent>some variants of olp were suggested without recognizing the existence of all other variants.
</nextsent>
<nextsent>in this paper we make comparative analysis of the different olp variants for the first time.
</nextsent>
<nextsent>some researchers (haas, 1989; <papid> J89-4001 </papid>torenvliet and trautwein, 1995) conjecture that some of the olp variants are undecidable (it is undecidable whether grammar satisfies the constraint), although none of them gives any proof of it.</nextsent>
<nextsent>there exist some variants of olp for which decidability holds, but these conditions are too restrictive; there is large class of non-olpgrammars for which parsing termination is guaran teed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4619">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>approaches to chinese segmentation fall roughly into two categories: heuristic dictionary-based methods and statistical machine learning methods.in dictionary-based methods, predefined dictionary is used along with hand-generated rules for segmenting input sequence (wu, 1999).
</prevsent>
<prevsent>however these approaches have been limited by the impossibility of creating lexicon that includes all possible chinese words and by the lack of robust statistical inference in the rules.
</prevsent>
</prevsection>
<citsent citstr=" J00-3004 ">
machine learning approaches are more desirable and have been successful in both unsupervised learning (peng and schuurmans, 2001) and supervised learning (teahan et al, 2000).<papid> J00-3004 </papid></citsent>
<aftsection>
<nextsent>many current approaches suffer from either lackof exact inference over sequences or difficulty in incorporating domain knowledge effectively into segmentation.
</nextsent>
<nextsent>domain knowledge is either not used, used in limited way, or used in complicated way spread across different components.
</nextsent>
<nextsent>for example,the n-gram generative language modeling based approach of teahan et al(2000) <papid> J00-3004 </papid>does not use domainknowledge.</nextsent>
<nextsent>gao et al(2003) <papid> P03-1035 </papid>uses class-based language for word segmentation where some word category information can be incorporated.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4621">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>domain knowledge is either not used, used in limited way, or used in complicated way spread across different components.
</prevsent>
<prevsent>for example,the n-gram generative language modeling based approach of teahan et al(2000) <papid> J00-3004 </papid>does not use domainknowledge.</prevsent>
</prevsection>
<citsent citstr=" P03-1035 ">
gao et al(2003) <papid> P03-1035 </papid>uses class-based language for word segmentation where some word category information can be incorporated.</citsent>
<aftsection>
<nextsent>zhang et al (2003) <papid> W03-1709 </papid>use hierarchical hidden markov model to incorporate lexical knowledge.</nextsent>
<nextsent>a recent advance in this area is xue (2003), in which the author uses sliding-window maximum entropy classifier to tag chinese characters into one of four position tags, and then covert these tags into segmentation using rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4622">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example,the n-gram generative language modeling based approach of teahan et al(2000) <papid> J00-3004 </papid>does not use domainknowledge.</prevsent>
<prevsent>gao et al(2003) <papid> P03-1035 </papid>uses class-based language for word segmentation where some word category information can be incorporated.</prevsent>
</prevsection>
<citsent citstr=" W03-1709 ">
zhang et al (2003) <papid> W03-1709 </papid>use hierarchical hidden markov model to incorporate lexical knowledge.</citsent>
<aftsection>
<nextsent>a recent advance in this area is xue (2003), in which the author uses sliding-window maximum entropy classifier to tag chinese characters into one of four position tags, and then covert these tags into segmentation using rules.
</nextsent>
<nextsent>maximum entropy models give tremendous flexibility to incorporate arbitrary features.
</nextsent>
<nextsent>how ever, traditional maximum entropy tagger, as used in xue (2003), labels characters without considering dependencies among the predicted segmentation labels that is inherent in the state transitions of finite state sequence models.
</nextsent>
<nextsent>linear-chain conditional random fields (crfs) (lafferty et al, 2001) are models that address both issues above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4623">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in their most general form, crfs are arbitrary undirected graphical models trained to maximize the conditional probability of the desired outputs given the corresponding inputs.
</prevsent>
<prevsent>in the linear-chainspecial case we use here, they can be roughly understood as discriminatively-trained hidden markov models with next-state transition functions represented by exponential models (as in maximum entropy classifiers), and with great flexibility to viewthe observation sequence in terms of arbitrary, overlapping features, with long-range dependencies, and at multiple levels of granularity.
</prevsent>
</prevsection>
<citsent citstr=" W00-1207 ">
these beneficial properties suggests that crfs are promising approach for chinese word segmentation.new word detection is one of the most important problems in chinese information processing.many machine learning approaches have been proposed (chen and bai, 1998; wu and jiang, 2000; <papid> W00-1207 </papid>nie et al, 1995).</citsent>
<aftsection>
<nextsent>new word detection is normally considered as separate process from segmentation.however, integrating them would benefit both segmentation and new word detection.
</nextsent>
<nextsent>crfs provide aconvenient framework for doing this.
</nextsent>
<nextsent>they can produce not only segmentation, but also confidence in local segmentation decisions, which can be usedto find new, unfamiliar character sequences surrounded by high-confidence segmentations.
</nextsent>
<nextsent>thus, our new word detection is not stand-alone process, but an integral part of segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4624">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>comparing chinese word segmentation accuracy across systems can be difficult because many research papers use different datasets and different ground-rules.
</prevsent>
<prevsent>some published results claim 98% or99% segmentation precision and recall, but these either count only the words that occur in the lexicon, or use unrealistically simple data, lexicons that have extremely small (or artificially non-existant) outof-vocabulary rates, short sentences or many numbers.
</prevsent>
</prevsection>
<citsent citstr=" W03-1719 ">
a recent chinese word segmentation competition (sproat and emerson, 2003) <papid> W03-1719 </papid>has made comparisons easier.</citsent>
<aftsection>
<nextsent>the competition provided four datasets with significantly different segmentation guidelines, and consistent train-test splits.
</nextsent>
<nextsent>the performance of participating system varies significantly across different datasets.
</nextsent>
<nextsent>our system achieves top performance in two of the runs, and state-of-the-art performance on average.
</nextsent>
<nextsent>this indicates that crfs are viable model for robust chinese word segmentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4626">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> conditional random fields.  </section>
<citcontext>
<prevsection>
<prevsent>t=1 ? kfk(yt1, yt,x, t)?
</prevsent>
<prevsent>logzxi ) . traditional maximum entropy learning algorithms, such as gis and iis (della pietra et al, 1995), canbe used to train crfs.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
however, our implementation uses quasi-newton gradient-climber bfgs for optimization, which has been shown to converge much faster (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>the gradient of the likelihood is p?(y|x)/k = ? i,t fk(yt1, y(i)t ,x(i), t) ? ?
</nextsent>
<nextsent>i,y,t p?(y|x(i))fk(yt1, yt,x(i), t) crfs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.
</nextsent>
<nextsent>2.1 regularization in crfs.
</nextsent>
<nextsent>to avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4627">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> conditional random fields.  </section>
<citcontext>
<prevsection>
<prevsent>t=1 ? kfk(yt1, yt,x, t)?
</prevsent>
<prevsent>logzxi ) . traditional maximum entropy learning algorithms, such as gis and iis (della pietra et al, 1995), canbe used to train crfs.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
however, our implementation uses quasi-newton gradient-climber bfgs for optimization, which has been shown to converge much faster (malouf, 2002; <papid> W02-2018 </papid>sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>the gradient of the likelihood is p?(y|x)/k = ? i,t fk(yt1, y(i)t ,x(i), t) ? ?
</nextsent>
<nextsent>i,y,t p?(y|x(i))fk(yt1, yt,x(i), t) crfs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.
</nextsent>
<nextsent>2.1 regularization in crfs.
</nextsent>
<nextsent>to avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4628">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> conditional random fields.  </section>
<citcontext>
<prevsection>
<prevsent>we bin features by frequency in the training set, and let the features in the same bin share the same variance.
</prevsent>
<prevsent>the discounted value is set to be dck/me2 where ck is the count of features, is the bin size set by held out validation, and dae is the ceiling function.
</prevsent>
</prevsection>
<citsent citstr=" N04-1042 ">
see peng and mccallum (2004) <papid> N04-1042 </papid>for more details and further experiments.</citsent>
<aftsection>
<nextsent>2.2 state transition features.
</nextsent>
<nextsent>varying state-transition structures with different markov order can be specified by different crf feature functions, as determined by the number of output labels examined together in feature function.
</nextsent>
<nextsent>we define four different state transition feature functions corresponding to different markov orders.higher-order features capture more long-range dependencies, but also cause more data sparseness problems and require more memory for training.
</nextsent>
<nextsent>the best markov order for particular application can be selected by held-out cross-validation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4629">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> probabilistic new word identification.  </section>
<citcontext>
<prevsection>
<prevsent>given word segmentation proposed by the crf, we can compute confidence in each segment.
</prevsent>
<prevsent>we detect as new words those that are not in the existing word list, yet are either highly confident segments, or low confident segments that are surrounded by high confident words.
</prevsent>
</prevsection>
<citsent citstr=" N04-4028 ">
a confidence threshold of 0.9 is determined by cross-validation.segment confidence is estimated using constrained forward-backward (culotta and mccallum, 2004).<papid> N04-4028 </papid></citsent>
<aftsection>
<nextsent>the standard forward-backward algorithm (rabiner, 1990) calculates zx, the total likelihood of all label sequences given sequence x. constrained forward-backward algorithm calculates x, total likelihood of all paths passing through constrained segment (in our case, sequence of characters starting with start tag followed by few non start tags before the next start tag).
</nextsent>
<nextsent>the confidence in this segment is then ? zx , real number between 0 and 1.in order to increase recall of new words, we consider not only the most likely (viterbi) segmentation, but the segment ations in the top most likely segment ations (an -best list), and detect new words according to the above criteria in all segmentations.many errors can be corrected by new word detection.
</nextsent>
<nextsent>for example, person name ?????
</nextsent>
<nextsent>happens four times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4635">
<title id=" C04-1081.xml">chinese segmentation and new word detection using conditional random fields </title>
<section> experiments and analysis.  </section>
<citcontext>
<prevsection>
<prevsent>the second most typical error is in new, out-of-vocabulary words, especially proper names.
</prevsent>
<prevsent>although our new word detection fixes many of these problems, it is not effective enough to recognize proper names well.
</prevsent>
</prevsection>
<citsent citstr=" W03-1727 ">
one solution to this problem could use named entity extractor to recognize proper names; this was found to be very helpful in wu (2003).<papid> W03-1727 </papid></citsent>
<aftsection>
<nextsent>one of the most attractive advantages of crfs (and maximum entropy models in general) is its the flexibility to easily incorporate arbitrary features,here in the form domain-knowledge-providing lexicons.
</nextsent>
<nextsent>however, obtaining these lexicons is not trivial matter.
</nextsent>
<nextsent>the quality of lexicons can affect the performance of crfs significantly.
</nextsent>
<nextsent>in addition, compared to simple models like n-gram language models (teahan et al, 2000), <papid> J00-3004 </papid>another shortcoming of crf-based segment ers is that it requires significantly longer training time.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4637">
<title id=" C04-1200.xml">determining the sentiment of opinions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the many opinions on opinions are reflected inconsiderable literature (aristotle 1954; perelman 1970; toulmin et al 1979; wallace 1975; toulmin 2003).
</prevsent>
<prevsent>recent computational work either focuses on sentence subjectivity?
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
(wiebe et al 2002; riloff et al 2003), <papid> W03-0404 </papid>concentrates just on explicit statements of evaluation, such as of films (turney 2002; <papid> P02-1053 </papid>pang et al 2002), <papid> W02-1011 </papid>or focuses on just one aspect of opinion, e.g., (hatzivassiloglou and mckeown 1997) <papid> P97-1023 </papid>on adjectives.</citsent>
<aftsection>
<nextsent>we wish to study opinion in general; our work most closely resembles that of (yu and hatzivassiloglou 2003).<papid> W03-1017 </papid></nextsent>
<nextsent>since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4638">
<title id=" C04-1200.xml">determining the sentiment of opinions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the many opinions on opinions are reflected inconsiderable literature (aristotle 1954; perelman 1970; toulmin et al 1979; wallace 1975; toulmin 2003).
</prevsent>
<prevsent>recent computational work either focuses on sentence subjectivity?
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
(wiebe et al 2002; riloff et al 2003), <papid> W03-0404 </papid>concentrates just on explicit statements of evaluation, such as of films (turney 2002; <papid> P02-1053 </papid>pang et al 2002), <papid> W02-1011 </papid>or focuses on just one aspect of opinion, e.g., (hatzivassiloglou and mckeown 1997) <papid> P97-1023 </papid>on adjectives.</citsent>
<aftsection>
<nextsent>we wish to study opinion in general; our work most closely resembles that of (yu and hatzivassiloglou 2003).<papid> W03-1017 </papid></nextsent>
<nextsent>since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4639">
<title id=" C04-1200.xml">determining the sentiment of opinions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the many opinions on opinions are reflected inconsiderable literature (aristotle 1954; perelman 1970; toulmin et al 1979; wallace 1975; toulmin 2003).
</prevsent>
<prevsent>recent computational work either focuses on sentence subjectivity?
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
(wiebe et al 2002; riloff et al 2003), <papid> W03-0404 </papid>concentrates just on explicit statements of evaluation, such as of films (turney 2002; <papid> P02-1053 </papid>pang et al 2002), <papid> W02-1011 </papid>or focuses on just one aspect of opinion, e.g., (hatzivassiloglou and mckeown 1997) <papid> P97-1023 </papid>on adjectives.</citsent>
<aftsection>
<nextsent>we wish to study opinion in general; our work most closely resembles that of (yu and hatzivassiloglou 2003).<papid> W03-1017 </papid></nextsent>
<nextsent>since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4640">
<title id=" C04-1200.xml">determining the sentiment of opinions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the many opinions on opinions are reflected inconsiderable literature (aristotle 1954; perelman 1970; toulmin et al 1979; wallace 1975; toulmin 2003).
</prevsent>
<prevsent>recent computational work either focuses on sentence subjectivity?
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
(wiebe et al 2002; riloff et al 2003), <papid> W03-0404 </papid>concentrates just on explicit statements of evaluation, such as of films (turney 2002; <papid> P02-1053 </papid>pang et al 2002), <papid> W02-1011 </papid>or focuses on just one aspect of opinion, e.g., (hatzivassiloglou and mckeown 1997) <papid> P97-1023 </papid>on adjectives.</citsent>
<aftsection>
<nextsent>we wish to study opinion in general; our work most closely resembles that of (yu and hatzivassiloglou 2003).<papid> W03-1017 </papid></nextsent>
<nextsent>since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4641">
<title id=" C04-1200.xml">determining the sentiment of opinions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent computational work either focuses on sentence subjectivity?
</prevsent>
<prevsent>(wiebe et al 2002; riloff et al 2003), <papid> W03-0404 </papid>concentrates just on explicit statements of evaluation, such as of films (turney 2002; <papid> P02-1053 </papid>pang et al 2002), <papid> W02-1011 </papid>or focuses on just one aspect of opinion, e.g., (hatzivassiloglou and mckeown 1997) <papid> P97-1023 </papid>on adjectives.</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
we wish to study opinion in general; our work most closely resembles that of (yu and hatzivassiloglou 2003).<papid> W03-1017 </papid></citsent>
<aftsection>
<nextsent>since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion.
</nextsent>
<nextsent>for our purposes, we describe an opinion as quadruple [topic, holder, claim, sentiment] in which the holder believes claim about the topic, and in many cases associates sentiment, such as good or bad, with the belief.
</nextsent>
<nextsent>for example, the following opinions contain claims but no sentiments: believe the world is flat?
</nextsent>
<nextsent>the gap is likely to go bankrupt?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4643">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuition.
</prevsent>
<prevsent>it is thus desirable to develop an automated method for bilingual-dictionary adaptation.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
technologies for extracting pairs of translation equivalents from parallel corpora have been established (gale and church 1991; <papid> H91-1026 </papid>dagan, et al 1993; <papid> W93-0301 </papid>fung 1995; <papid> P95-1032 </papid>kitamura and matsumoto 1996; <papid> W96-0107 </papid>melamed 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>they can, naturally, be used to adapt bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary.
</nextsent>
<nextsent>however, their applicability is limited be cause of the limited availability of large parallel corpora.
</nextsent>
<nextsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</nextsent>
<nextsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4644">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuition.
</prevsent>
<prevsent>it is thus desirable to develop an automated method for bilingual-dictionary adaptation.
</prevsent>
</prevsection>
<citsent citstr=" W93-0301 ">
technologies for extracting pairs of translation equivalents from parallel corpora have been established (gale and church 1991; <papid> H91-1026 </papid>dagan, et al 1993; <papid> W93-0301 </papid>fung 1995; <papid> P95-1032 </papid>kitamura and matsumoto 1996; <papid> W96-0107 </papid>melamed 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>they can, naturally, be used to adapt bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary.
</nextsent>
<nextsent>however, their applicability is limited be cause of the limited availability of large parallel corpora.
</nextsent>
<nextsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</nextsent>
<nextsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4645">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuition.
</prevsent>
<prevsent>it is thus desirable to develop an automated method for bilingual-dictionary adaptation.
</prevsent>
</prevsection>
<citsent citstr=" P95-1032 ">
technologies for extracting pairs of translation equivalents from parallel corpora have been established (gale and church 1991; <papid> H91-1026 </papid>dagan, et al 1993; <papid> W93-0301 </papid>fung 1995; <papid> P95-1032 </papid>kitamura and matsumoto 1996; <papid> W96-0107 </papid>melamed 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>they can, naturally, be used to adapt bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary.
</nextsent>
<nextsent>however, their applicability is limited be cause of the limited availability of large parallel corpora.
</nextsent>
<nextsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</nextsent>
<nextsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4646">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuition.
</prevsent>
<prevsent>it is thus desirable to develop an automated method for bilingual-dictionary adaptation.
</prevsent>
</prevsection>
<citsent citstr=" W96-0107 ">
technologies for extracting pairs of translation equivalents from parallel corpora have been established (gale and church 1991; <papid> H91-1026 </papid>dagan, et al 1993; <papid> W93-0301 </papid>fung 1995; <papid> P95-1032 </papid>kitamura and matsumoto 1996; <papid> W96-0107 </papid>melamed 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>they can, naturally, be used to adapt bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary.
</nextsent>
<nextsent>however, their applicability is limited be cause of the limited availability of large parallel corpora.
</nextsent>
<nextsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</nextsent>
<nextsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4647">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intuition.
</prevsent>
<prevsent>it is thus desirable to develop an automated method for bilingual-dictionary adaptation.
</prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
technologies for extracting pairs of translation equivalents from parallel corpora have been established (gale and church 1991; <papid> H91-1026 </papid>dagan, et al 1993; <papid> W93-0301 </papid>fung 1995; <papid> P95-1032 </papid>kitamura and matsumoto 1996; <papid> W96-0107 </papid>melamed 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>they can, naturally, be used to adapt bilingual dictionary to domains, that is, to select corpus-relevant translation equivalents from among those provided by an existing bilingual dictionary.
</nextsent>
<nextsent>however, their applicability is limited be cause of the limited availability of large parallel corpora.
</nextsent>
<nextsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</nextsent>
<nextsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4648">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>methods of bilingual-dictionary adaptation using weakly comparable corpora, i.e., pair of two language corpora of the same domain, are therefore required.
</prevsent>
<prevsent>there are number of previous works related to bilingual-dictionary adaptation using comparable corpora.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
tanaka and iwasakis (1996) <papid> C96-2098 </papid>optimization method for translation-probability matrix mainly aims at adapting bilingual dictionary to domains.</citsent>
<aftsection>
<nextsent>however, it is hampered by huge amount of computation, and was only demonstrated in small-scale experiment.
</nextsent>
<nextsent>several researchers have developed contextual-similarity-based method for extracting pairs of translation equivalents (kaji and aizono 1996; <papid> C96-1006 </papid>fung and mckeown 1997; <papid> W97-0119 </papid>fung and yee 1998; <papid> P98-1069 </papid>rapp 1999).<papid> P99-1067 </papid></nextsent>
<nextsent>it is computationally efficient compared to tanaka and iwasakis method, but the precision of extracted translation equivalents is still not acceptable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4649">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tanaka and iwasakis (1996) <papid> C96-2098 </papid>optimization method for translation-probability matrix mainly aims at adapting bilingual dictionary to domains.</prevsent>
<prevsent>however, it is hampered by huge amount of computation, and was only demonstrated in small-scale experiment.</prevsent>
</prevsection>
<citsent citstr=" C96-1006 ">
several researchers have developed contextual-similarity-based method for extracting pairs of translation equivalents (kaji and aizono 1996; <papid> C96-1006 </papid>fung and mckeown 1997; <papid> W97-0119 </papid>fung and yee 1998; <papid> P98-1069 </papid>rapp 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>it is computationally efficient compared to tanaka and iwasakis method, but the precision of extracted translation equivalents is still not acceptable.
</nextsent>
<nextsent>in the light of these works, the author proposes two methods for bilingual-dictionary adaptation.
</nextsent>
<nextsent>the first one is variant of the contex tual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting cor pus-relevant translation equivalents from among those provided by bilingual dictionary.
</nextsent>
<nextsent>this selecting may be easier than finding new pairs of translation equivalents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4650">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tanaka and iwasakis (1996) <papid> C96-2098 </papid>optimization method for translation-probability matrix mainly aims at adapting bilingual dictionary to domains.</prevsent>
<prevsent>however, it is hampered by huge amount of computation, and was only demonstrated in small-scale experiment.</prevsent>
</prevsection>
<citsent citstr=" W97-0119 ">
several researchers have developed contextual-similarity-based method for extracting pairs of translation equivalents (kaji and aizono 1996; <papid> C96-1006 </papid>fung and mckeown 1997; <papid> W97-0119 </papid>fung and yee 1998; <papid> P98-1069 </papid>rapp 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>it is computationally efficient compared to tanaka and iwasakis method, but the precision of extracted translation equivalents is still not acceptable.
</nextsent>
<nextsent>in the light of these works, the author proposes two methods for bilingual-dictionary adaptation.
</nextsent>
<nextsent>the first one is variant of the contex tual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting cor pus-relevant translation equivalents from among those provided by bilingual dictionary.
</nextsent>
<nextsent>this selecting may be easier than finding new pairs of translation equivalents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4651">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tanaka and iwasakis (1996) <papid> C96-2098 </papid>optimization method for translation-probability matrix mainly aims at adapting bilingual dictionary to domains.</prevsent>
<prevsent>however, it is hampered by huge amount of computation, and was only demonstrated in small-scale experiment.</prevsent>
</prevsection>
<citsent citstr=" P98-1069 ">
several researchers have developed contextual-similarity-based method for extracting pairs of translation equivalents (kaji and aizono 1996; <papid> C96-1006 </papid>fung and mckeown 1997; <papid> W97-0119 </papid>fung and yee 1998; <papid> P98-1069 </papid>rapp 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>it is computationally efficient compared to tanaka and iwasakis method, but the precision of extracted translation equivalents is still not acceptable.
</nextsent>
<nextsent>in the light of these works, the author proposes two methods for bilingual-dictionary adaptation.
</nextsent>
<nextsent>the first one is variant of the contex tual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting cor pus-relevant translation equivalents from among those provided by bilingual dictionary.
</nextsent>
<nextsent>this selecting may be easier than finding new pairs of translation equivalents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4652">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tanaka and iwasakis (1996) <papid> C96-2098 </papid>optimization method for translation-probability matrix mainly aims at adapting bilingual dictionary to domains.</prevsent>
<prevsent>however, it is hampered by huge amount of computation, and was only demonstrated in small-scale experiment.</prevsent>
</prevsection>
<citsent citstr=" P99-1067 ">
several researchers have developed contextual-similarity-based method for extracting pairs of translation equivalents (kaji and aizono 1996; <papid> C96-1006 </papid>fung and mckeown 1997; <papid> W97-0119 </papid>fung and yee 1998; <papid> P98-1069 </papid>rapp 1999).<papid> P99-1067 </papid></citsent>
<aftsection>
<nextsent>it is computationally efficient compared to tanaka and iwasakis method, but the precision of extracted translation equivalents is still not acceptable.
</nextsent>
<nextsent>in the light of these works, the author proposes two methods for bilingual-dictionary adaptation.
</nextsent>
<nextsent>the first one is variant of the contex tual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting cor pus-relevant translation equivalents from among those provided by bilingual dictionary.
</nextsent>
<nextsent>this selecting may be easier than finding new pairs of translation equivalents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4653">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first one is variant of the contex tual-similarity-based method for extracting pairs of translation equivalents; it focuses on selecting cor pus-relevant translation equivalents from among those provided by bilingual dictionary.
</prevsent>
<prevsent>this selecting may be easier than finding new pairs of translation equivalents.
</prevsent>
</prevsection>
<citsent citstr=" C02-1058 ">
the second one is newly devised method using the ratio of associated words that suggest each translation equivalent; it was inspired by research on word-sense disambiguation using bilingual comparable corpora (kaji and morimoto 2002).<papid> C02-1058 </papid></citsent>
<aftsection>
<nextsent>the two methods were evaluated and compared by using the edr (japan electronic dictionary research institute) bilingual dictionary together with wall street journal and nihon keizai shimbun corpora.
</nextsent>
<nextsent>this method is based on the assumption that word in language and its translation equivalent in another language occur in similar contexts, albeit their contexts are represented by words in their respective languages.
</nextsent>
<nextsent>in the case of the present task (i.e., bilingual-dictionary adaptation), bilingual dictionary provides set of candidate translation equivalents for each target word1.
</nextsent>
<nextsent>the contextual similarity of each of the candidate translation equivalents to the target word is thus evaluated with the assistance of the bilingual dictionary, and predetermined number of translation equivalents are selected in descending order of contextual similarity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4655">
<title id=" C04-1105.xml">bilingual dictionary adaptation to domains </title>
<section> method based on contextual similarity.  </section>
<citcontext>
<prevsection>
<prevsent>the essential issues regarding this method are described in the following.
</prevsent>
<prevsent>word associations are extracted by setting threshold for mutual information between words in the same language.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the mutual information of pair of words is defined in terms of their co-occurrence frequency and respective occurrence frequencies (church and hanks 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>a medium-sized window, i.e., window including few-dozen words, is used to count co-occurrence frequencies.
</nextsent>
<nextsent>only word associations consisting of content words are extracted.
</nextsent>
<nextsent>this is because function words neither have do main-dependent translation equivalents nor represent contexts.
</nextsent>
<nextsent>both target word and each of its candidate translation equivalents are characterized by context vectors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4658">
<title id=" C02-1094.xml">integrating linguistic and performance based constraints for assigning phrase breaks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>  (2) by making his plan known   he brought out the objections of everyone.
</prevsent>
<prevsent>  nevertheless, the  -structure provides strong constraint on the location of breaks between i-phrases, since an i-phrase can never interrupt   -phrase.
</prevsent>
</prevsection>
<citsent citstr=" J90-3003 ">
although   -structure has been used by others to assign prosodic structure algorithmically (gee and grosjean, 1983; bachenko and fitzpatrick, 1990), <papid> J90-3003 </papid>there is no generally accepted method for bundling   -phrases into i-phrases.</citsent>
<aftsection>
<nextsent>the main consensus isthat i-phrases have more or less uniform aver age?
</nextsent>
<nextsent>length  (nespor and vogel, 1986, p.194).
</nextsent>
<nextsent>in similar vein, gee and grosjean (1983) observe that utterances tend to be split into two or three i-phrases of roughly equal length.
</nextsent>
<nextsent>gee and grosjean (1983) (and subsequently,bachenko and fitzpatrick (1990)) <papid> J90-3003 </papid>construct phrases by comparing the length of the prosodic constituents on both the left-hand side and the right hand side of the utterances main verb (or the   phrase containing the verb), and grouping the verb with the shorter neighbouring constituent.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4661">
<title id=" C02-1114.xml">a graph model for unsupervised lexical acquisition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 describes how the graph model can be used to recognise when words are polysemous and to obtain groups of words representative of the different senses.
</prevsent>
<prevsent>most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
the underlying claim is that words which are semantically similar occur with similar distributions and in similar contexts (miller and charles, 1991).the main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in particular category, such as vehicles or weapons (riloff and shepherd, 1997) <papid> W97-0313 </papid>roark and charniak, 1998).<papid> P98-2182 </papid></citsent>
<aftsection>
<nextsent>roark and charniak describe generic algorithm?
</nextsent>
<nextsent>for extracting such lists of similar words using the notion of semantic similarity, as follows (roark and charniak, 1998, <papid> P98-2182 </papid>1).</nextsent>
<nextsent>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4665">
<title id=" C02-1114.xml">a graph model for unsupervised lexical acquisition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>section 6 describes how the graph model can be used to recognise when words are polysemous and to obtain groups of words representative of the different senses.
</prevsent>
<prevsent>most work on automatic lexical acquisition has been based at some point on the notion of semantic similarity.
</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
the underlying claim is that words which are semantically similar occur with similar distributions and in similar contexts (miller and charles, 1991).the main results to date in the field of automatic lexical acquisition are concerned with extracting lists of words reckoned to belong together in particular category, such as vehicles or weapons (riloff and shepherd, 1997) <papid> W97-0313 </papid>roark and charniak, 1998).<papid> P98-2182 </papid></citsent>
<aftsection>
<nextsent>roark and charniak describe generic algorithm?
</nextsent>
<nextsent>for extracting such lists of similar words using the notion of semantic similarity, as follows (roark and charniak, 1998, <papid> P98-2182 </papid>1).</nextsent>
<nextsent>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4679">
<title id=" C02-1114.xml">a graph model for unsupervised lexical acquisition </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>approach is used to extract words in particular semantic categories and expression patterns for recognising relationships between these words for the purposes of information extraction.
</prevsent>
<prevsent>the accuracy achieved in this experiment is sometimes as high as 78% and is therefore comparable to the results reported in this paper.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
another way to obtain word-senses directly from corpora is to use clustering algorithms on feature-vectors (lin, 1998; <papid> P98-2127 </papid>schutze, 1998).clustering techniques can also be used to discriminate between different senses of an ambiguous word.</citsent>
<aftsection>
<nextsent>a general problem for such clustering techniques lies in the question of how many clusters one should have, i.e. how many senses are appropriate for particular word in given domain (manning and schutze, 1999, ch 14).
</nextsent>
<nextsent>lins approach to this problem (lin, 1998) <papid> P98-2127 </papid>isto build similarity tree?</nextsent>
<nextsent>(using what is in effect hierarchical clustering method) of words related to target word (in this case the word duty).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4708">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propbank project (kingsbury and palmer, 2002) provides large human-annotated corpus of semantic verb-argument relations.
</prevsent>
<prevsent>specifically, we use the data provided in the conll-2004 shared task of semantic-role labeling (carreras and ma`rquez, 2003) which consists of portion of the propbank corpus, allowing us to compare the performance of our approach with other systems.
</prevsent>
</prevsection>
<citsent citstr=" P02-1031 ">
previous approaches to the srl task have madeuse of full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (gildea and palmer, 2002; <papid> P02-1031 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>pradhan et al, 2003; pradhan et al, 2004; <papid> N04-1030 </papid>surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>in this work, following the conll-2004 shared task definition, we assume thatthe srl system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases.
</nextsent>
<nextsent>specifically, we assume as input resources part-of-speech tagger, shallow parser that can process the input to the level of based chunks and clauses (tjong kim sang and buchholz, 2000; tjong kim sang and dejean, 2001), and named-entity recognizer (tjong kim sang and de meulder, 2003).
</nextsent>
<nextsent>we do not assume full parse as input.
</nextsent>
<nextsent>srl is difficult task, and one cannot expec thigh levels of performance from either purely manual classifiers or purely learned classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4709">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propbank project (kingsbury and palmer, 2002) provides large human-annotated corpus of semantic verb-argument relations.
</prevsent>
<prevsent>specifically, we use the data provided in the conll-2004 shared task of semantic-role labeling (carreras and ma`rquez, 2003) which consists of portion of the propbank corpus, allowing us to compare the performance of our approach with other systems.
</prevsent>
</prevsection>
<citsent citstr=" W03-1006 ">
previous approaches to the srl task have madeuse of full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (gildea and palmer, 2002; <papid> P02-1031 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>pradhan et al, 2003; pradhan et al, 2004; <papid> N04-1030 </papid>surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>in this work, following the conll-2004 shared task definition, we assume thatthe srl system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases.
</nextsent>
<nextsent>specifically, we assume as input resources part-of-speech tagger, shallow parser that can process the input to the level of based chunks and clauses (tjong kim sang and buchholz, 2000; tjong kim sang and dejean, 2001), and named-entity recognizer (tjong kim sang and de meulder, 2003).
</nextsent>
<nextsent>we do not assume full parse as input.
</nextsent>
<nextsent>srl is difficult task, and one cannot expec thigh levels of performance from either purely manual classifiers or purely learned classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4710">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propbank project (kingsbury and palmer, 2002) provides large human-annotated corpus of semantic verb-argument relations.
</prevsent>
<prevsent>specifically, we use the data provided in the conll-2004 shared task of semantic-role labeling (carreras and ma`rquez, 2003) which consists of portion of the propbank corpus, allowing us to compare the performance of our approach with other systems.
</prevsent>
</prevsection>
<citsent citstr=" W03-1008 ">
previous approaches to the srl task have madeuse of full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (gildea and palmer, 2002; <papid> P02-1031 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>pradhan et al, 2003; pradhan et al, 2004; <papid> N04-1030 </papid>surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>in this work, following the conll-2004 shared task definition, we assume thatthe srl system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases.
</nextsent>
<nextsent>specifically, we assume as input resources part-of-speech tagger, shallow parser that can process the input to the level of based chunks and clauses (tjong kim sang and buchholz, 2000; tjong kim sang and dejean, 2001), and named-entity recognizer (tjong kim sang and de meulder, 2003).
</nextsent>
<nextsent>we do not assume full parse as input.
</nextsent>
<nextsent>srl is difficult task, and one cannot expec thigh levels of performance from either purely manual classifiers or purely learned classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4711">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propbank project (kingsbury and palmer, 2002) provides large human-annotated corpus of semantic verb-argument relations.
</prevsent>
<prevsent>specifically, we use the data provided in the conll-2004 shared task of semantic-role labeling (carreras and ma`rquez, 2003) which consists of portion of the propbank corpus, allowing us to compare the performance of our approach with other systems.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
previous approaches to the srl task have madeuse of full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (gildea and palmer, 2002; <papid> P02-1031 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>pradhan et al, 2003; pradhan et al, 2004; <papid> N04-1030 </papid>surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>in this work, following the conll-2004 shared task definition, we assume thatthe srl system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases.
</nextsent>
<nextsent>specifically, we assume as input resources part-of-speech tagger, shallow parser that can process the input to the level of based chunks and clauses (tjong kim sang and buchholz, 2000; tjong kim sang and dejean, 2001), and named-entity recognizer (tjong kim sang and de meulder, 2003).
</nextsent>
<nextsent>we do not assume full parse as input.
</nextsent>
<nextsent>srl is difficult task, and one cannot expec thigh levels of performance from either purely manual classifiers or purely learned classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4712">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the propbank project (kingsbury and palmer, 2002) provides large human-annotated corpus of semantic verb-argument relations.
</prevsent>
<prevsent>specifically, we use the data provided in the conll-2004 shared task of semantic-role labeling (carreras and ma`rquez, 2003) which consists of portion of the propbank corpus, allowing us to compare the performance of our approach with other systems.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
previous approaches to the srl task have madeuse of full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (gildea and palmer, 2002; <papid> P02-1031 </papid>chen and rambow, 2003; <papid> W03-1006 </papid>gildea and hockenmaier, 2003;<papid> W03-1008 </papid>pradhan et al, 2003; pradhan et al, 2004; <papid> N04-1030 </papid>surdeanu et al, 2003).<papid> P03-1002 </papid></citsent>
<aftsection>
<nextsent>in this work, following the conll-2004 shared task definition, we assume thatthe srl system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases.
</nextsent>
<nextsent>specifically, we assume as input resources part-of-speech tagger, shallow parser that can process the input to the level of based chunks and clauses (tjong kim sang and buchholz, 2000; tjong kim sang and dejean, 2001), and named-entity recognizer (tjong kim sang and de meulder, 2003).
</nextsent>
<nextsent>we do not assume full parse as input.
</nextsent>
<nextsent>srl is difficult task, and one cannot expec thigh levels of performance from either purely manual classifiers or purely learned classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4713">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>this section first introduces the learning system we use and then describes how we learn the classifiers in these two phases.
</prevsent>
<prevsent>3.1 snow learning architecture.
</prevsent>
</prevsection>
<citsent citstr=" C02-1151 ">
the learning algorithm used is variation of the winnow update rule incorporated in snow (roth, 1998; roth and yih, 2002), <papid> C02-1151 </papid>multi-class classifier that is specifically tailored for large scale learningtasks.</citsent>
<aftsection>
<nextsent>snow learns sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over common feature space.
</nextsent>
<nextsent>it incorporates several improvements over the basic winnow multiplicative update rule.
</nextsent>
<nextsent>in particular, regularization term is added, which has the effect of trying to separate the data with thick separator (grove and roth, 2001; hang et al,2002).
</nextsent>
<nextsent>in the work presented here we use this regularization with fixed parameter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4714">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> no duplicate argument classes for a0a5,v..  </section>
<citcontext>
<prevsection>
<prevsent>as discussed previously, collection of potential arguments is not necessarily valid semantic labeling since it must satisfy all of the constraints.
</prevsent>
<prevsent>in this context, inference is the process of finding the best (according to equation 1) valid semantic labels that satisfy all of the specified constraints.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
we take similar approach that has been previously used for entity/relation recognition (roth and yih, 2004), <papid> W04-2401 </papid>and model this inference procedure as solving an ilp.</citsent>
<aftsection>
<nextsent>an integer linear program(ilp) is basically the same as linear program.
</nextsent>
<nextsent>the cost function and the (in)equality constraints are all linear in terms of thevariables.
</nextsent>
<nextsent>the only difference in an ilp is the variables can only take inte gers as their values.
</nextsent>
<nextsent>in our inference problem, the variables are in fact binary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4715">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> no duplicate argument classes for a0a5,v..  </section>
<citcontext>
<prevsection>
<prevsent>note that ordinary search methods (e.g., beam search) are not necessarily faster than solving an ilp problem and do not guarantee the optimal solution.
</prevsent>
<prevsent>5 experimental results.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
the system is evaluated on the data provided in the conll-2004 semantic-role labeling shared task which consists of portion of propbank corpus.the training set is extracted from treebank (mar cus et al, 1993) <papid> J93-2004 </papid>section 1518, the development set,used in tuning parameters of the system, from section 20, and the test set from section 21.</citsent>
<aftsection>
<nextsent>we first compare this system with the basic tagger that we have, the cscl shallow parser from (punyakanok and roth, 2001), which is equivalent to using the scoring function from the first phase with only the non-overlapping/embedding constraints.
</nextsent>
<nextsent>in prec.
</nextsent>
<nextsent>rec.
</nextsent>
<nextsent>f?=1 1st-phase, non-overlap 70.54 61.50 65.71 1st-phase, all const.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4717">
<title id=" C04-1197.xml">semantic role labeling via integer linear programming inference </title>
<section> no duplicate argument classes for a0a5,v..  </section>
<citcontext>
<prevsection>
<prevsent>finally, the overall result on the official test set is given in table 3.
</prevsent>
<prevsent>note that the result here is not comparable with the best in this domain (pradhan et al., 2004) <papid> N04-1030 </papid>where the full parse tree is assumed given.</prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
for fair comparison, our system was among the best at conll-04, where the best system (hacioglu et al, 2004) <papid> W04-2416 </papid>achieve 69.49 f1 score.</citsent>
<aftsection>
<nextsent>we show that linguistic information is useful for semantic role labeling, both in extracting features and dist. prec.
</nextsent>
<nextsent>rec.
</nextsent>
<nextsent>f?=1 overall 100.00 70.07 63.07 66.39 a0 26.87 81.13 77.70 79.38 a1 35.73 74.21 63.02 68.16 a2 7.44 54.16 41.04 46.69 a3 1.56 47.06 26.67 34.04 a4 0.52 71.43 60.00 65.22 am-adv 3.20 39.36 36.16 37.69 am-cau 0.51 45.95 34.69 39.53 am-dir 0.52 42.50 34.00 37.78 am-dis 2.22 52.00 67.14 58.61 am-ext 0.15 46.67 50.00 48.28 am-loc 2.38 33.47 34.65 34.05 am-mnr 2.66 45.19 36.86 40.60 am-mod 3.51 92.49 94.96 93.70 am-neg 1.32 85.92 96.06 90.71 am-pnc 0.89 32.79 23.53 27.40 am-tmp 7.78 59.77 56.89 58.30 r-a0 1.66 81.33 76.73 78.96 r-a1 0.73 58.82 57.14 57.97 r-a2 0.09 100.00 22.22 36.36 r-am-tmp 0.15 54.55 42.86 48.00 table 3: results on the test set.
</nextsent>
<nextsent>deriving hard constraints on the output.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4718">
<title id=" C04-1049.xml">talking robots with lego mind storms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>often-cited examples of such interactive robots that have capability of communicating in natural language are the human oid robotrobovie (kanda et al, 2002) and robotic museum tour guides like rhino (burgard et al, 1999) (deutsches museum bonn), its successor minerva touring the smithsonian in washington (thrun etal., 2000), and robox at the swiss national exhibition expo02 (siegwart and et al 2003).
</prevsent>
<prevsent>how ever, dialogue systems used in robotics appear to be mostly restricted to relatively simple finite-state,query/response interaction.
</prevsent>
</prevsection>
<citsent citstr=" E03-1042 ">
the only robots involving dialogue systems that are state-of-the-art in computational linguistics (and that we are aware of) are those presented by lemon et al (2001), sidner et al (2003) and bos et al (2003), <papid> E03-1042 </papid>who equippeda mobile robot with an information state based dialogue system.there are two obvious reasons for this gap between research on dialogue systems in robotics on the one hand, and computational linguistics on the other hand.</citsent>
<aftsection>
<nextsent>one is that the sheer cost involved in buying or building robot makes traditional robotics research available to only handful of research sites.
</nextsent>
<nextsent>another is that building talking robot combines the challenges presented by robotics and natural language processing, which are further exacerbated by the interactions of the two sides.in this paper, we address at least the first problem by demonstrating how to build talking robots from affordable, commercial off-the-shelf (cots) components.
</nextsent>
<nextsent>we present an approach, tested in aseminar taught at the saarland university in winter 2002/2003, in which we combine the lego mind storms system with cots software for speech recognition/synthesis and dialogue modeling.the lego mindstorms1 system extends the traditional lego bricks with central control unit (thercx), as well as motors and various kinds of sensors.
</nextsent>
<nextsent>it provides severely limited computational platform from traditional robotics point of view, but comes at price of few hundred, rather than tens of thousands of euros per kit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4719">
<title id=" C04-1049.xml">talking robots with lego mind storms </title>
<section> some robots.  </section>
<citcontext>
<prevsection>
<prevsent>it updates its internal data structures, as well as the gnu chess representations, computes move for itself, and sends this move as another message to the rcx.while the dialogue system as it stands already offers some degree of flexibility with regard to move phrasings, there is still plenty of open room for improvements.
</prevsent>
<prevsent>one is to use even more context information, in order to understand commands like take it with the rook?.
</prevsent>
</prevsection>
<citsent citstr=" W04-2305 ">
another is to incorporate recent work on improving recognition results in the chess domain by certain plausibility inferences (gabsdil, 2004).<papid> W04-2305 </papid></citsent>
<aftsection>
<nextsent>3.2 playing shell game.
</nextsent>
<nextsent>figure 6 introduces luigi legonelli.
</nextsent>
<nextsent>the robot represents charismatic italian shell-game player, and engages human player in style: luigi speaks german with heavy italian accent, lets the human player win the first round, and then tries to pull several tricks either to cheat or to keep the player interested in the game.
</nextsent>
<nextsent>figure 6: robot playing shell game.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4720">
<title id=" C04-1049.xml">talking robots with lego mind storms </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>robots, being embodied agents, present host of new challenges beyond the challenges we face in computational linguistics.
</prevsent>
<prevsent>the interpretation of language needs to be grounded in way that isboth based in perception, and on conceptual structures to allow for generalization over experiences.
</prevsent>
</prevsection>
<citsent citstr=" W03-0609 ">
naturally, this problem extends to the acquisition of language, where approaches such as (nicolescu and mataric?, 2001; carbonetto and freitos, 2003; oates, 2003) <papid> W03-0609 </papid>have focused on basing understanding entirely in sensory data.another interesting issue concerns the interpretation of deictic references.</citsent>
<aftsection>
<nextsent>research in multi-modal 8see also http://www.rescuesystem.org/robocuprescue/interfaces has addressed the issue of deictic reference, notably in systems that allow for pen-input (see (oviatt, 2001)).
</nextsent>
<nextsent>embodied agents raise the complexity of the issues by offering broader rangeof sensory input that needs to be combined (cross modally) in order to establish possible referents.
</nextsent>
<nextsent>acknowledgments.
</nextsent>
<nextsent>the authors would like tothank lego and clt sprachtechnologie for providing free components from which to build our robot systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4721">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>tree-based approaches to alignment model translation as sequence of probabilistic operations transforming the syntactic parse tree of sentence in one language into that of the other.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the trees may be learned directly from parallel corpora (wu, 1997), <papid> J97-3002 </papid>or provided by aparser trained on hand-annotated treebanks (ya mada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>in this paper, we compare these approaches on chinese-englishand french-english datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.
</nextsent>
<nextsent>statistical approaches to machine translation, pioneered by brown et al (1990), <papid> J90-2002 </papid>estimate parameters for probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.</nextsent>
<nextsent>in recent years, number of syntactically motivated approaches to statistical machine translation have been proposed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4722">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>tree-based approaches to alignment model translation as sequence of probabilistic operations transforming the syntactic parse tree of sentence in one language into that of the other.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
the trees may be learned directly from parallel corpora (wu, 1997), <papid> J97-3002 </papid>or provided by aparser trained on hand-annotated treebanks (ya mada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>in this paper, we compare these approaches on chinese-englishand french-english datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.
</nextsent>
<nextsent>statistical approaches to machine translation, pioneered by brown et al (1990), <papid> J90-2002 </papid>estimate parameters for probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.</nextsent>
<nextsent>in recent years, number of syntactically motivated approaches to statistical machine translation have been proposed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4723">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the trees may be learned directly from parallel corpora (wu, 1997), <papid> J97-3002 </papid>or provided by aparser trained on hand-annotated treebanks (ya mada and knight, 2001).<papid> P01-1067 </papid></prevsent>
<prevsent>in this paper, we compare these approaches on chinese-englishand french-english datasets, and find that automatically derived trees result in better agreement with human-annotated word-level alignments for unseen test data.</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
statistical approaches to machine translation, pioneered by brown et al (1990), <papid> J90-2002 </papid>estimate parameters for probabilistic model of word-to-word correspondences and word re-orderings directly from large corpora of parallel bilingual text.</citsent>
<aftsection>
<nextsent>in recent years, number of syntactically motivated approaches to statistical machine translation have been proposed.
</nextsent>
<nextsent>these approaches assign parallel tree structure to the two sides of each sentence pair, and model the translation process with reordering operations defined on the tree structure.
</nextsent>
<nextsent>the tree-basedapproach allows us to represent the fact that syntactic constituents tend to move as unit, as well as systematic differences in word order in the grammars of the two languages.
</nextsent>
<nextsent>furthermore, the tree structure allows us to make probabilistic independence assumptions that result in polynomial time algorithms for estimating translation model from parallel training data, and for finding the highest probability translation given new sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4737">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>rather, they both use expectation maximization to find an alignment model by iterativelyimproving the likelihood assigned to unaligned parallel sentences.
</prevsent>
<prevsent>our evaluation is in terms of agreement with word-level alignments created by bilingual human annotators.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
we describe each of the models used in more detail in the next two sections, including the clone operation of gildea (2003).<papid> P03-1011 </papid></citsent>
<aftsection>
<nextsent>the reader who is familiar with these models may proceed directly to our experiments in section 4, and further discussion in section 5.
</nextsent>
<nextsent>the inversion transduction grammar of wu (1997)<papid> J97-3002 </papid>can be thought as a generative process which simultaneously produces strings in both languages through series of synchronous context-free grammar productions.</nextsent>
<nextsent>the grammar is restricted to binary rules, which can have the symbols in the right hand side appear in the same order in both languages, represented with square brackets: ? [y z] or the symbols may appear in reverse order in the two languages, indicated by angle brackets: ? z?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4741">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> the inversion transduction grammar.  </section>
<citcontext>
<prevsection>
<prevsent>individual lexical translations between english words and french words take place at the leaves of the tree, generated by grammar rules with single right hand side symbol in each language: ? e/f given bilingual sentence pair, synchronous parse can be built using two-dimensional extension of chart parsing, where chart items are indexed by their nonterminal and beginning and ending positions l, in the source language string, and beginning and ending positions i, in the target language string.
</prevsent>
<prevsent>for expectation maximization training, we compute inside probabilities ?(y, l, m, i, j) from the bottom up as outlined below: for all l,m, such that 1 ?       ns do for all i, j, such that 1         nt do for all rules ? z ? do ?(x, l, n, i, k)+= ([y z]|x)?(y, l,m, i, j)?(z,m, n, j, k) ?(x, l, n, i, k)+= (y z?|x)?(y,m, n, i, j)?(z, l,m, j, k) end for end for end for similar recur sion is used to compute outside probabilities for each chart item, and the inside and outside probabilities are combined to derive expected counts for occurrence of each grammar rule,including the rules corresponding to individual lexical translations.
</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
in our experiments we use grammar with start symbol s, single pre terminal c, and two nonterminals and used to ensure that only one parse can generate any given word-level alignment (ignoring insertions and deletions) (wu, 1997; <papid> J97-3002 </papid>zens and ney, 2003).<papid> P03-1019 </papid></citsent>
<aftsection>
<nextsent>the individual lexical translations produced by the grammar may include null word on either side, in order to represent insertions and deletions.
</nextsent>
<nextsent>the model of yamada and knight (2001) <papid> P01-1067 </papid>can be thought of as generative process taking tree in one language as input and producing string inthe other through sequence of probabilistic operations.</nextsent>
<nextsent>if we follow the process of an english sentences transformation into french, the english sentence is first given syntactic tree representation by statistical parser (collins, 1999).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4750">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our hand-aligned data consisted of 48 sentence pairs also with less than 25 words in either language, for total of 788 english words and 580 chinese words.
</prevsent>
<prevsent>a separate development set of 49 sentence pairs was used to control overfitting.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
these sets were the data used by hwa et al (2002).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>the hand aligned test data consisted of 745 individual aligned word pairs.
</nextsent>
<nextsent>words could be aligned oneto-many in either direction.
</nextsent>
<nextsent>this limits the performance achievable by our models; the ibm models allow one-to-many alignments in one direction only, while the tree-based models allow only one-to-one alignment unless the cloning operation is used.
</nextsent>
<nextsent>our french-english experiments were based on data from the canadian hansa rds made available by ulrich german.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4751">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our french-english experiments were based on data from the canadian hansa rds made available by ulrich german.
</prevsent>
<prevsent>we used as training data 20,000sentence pairs of no more than 25 words in either language.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by och and ney (2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>a separate development set of 37 sentences was used to control overfitting.
</nextsent>
<nextsent>we used of vocabulary of words occurring at least 10 times in the entire hansard corpus, resulting in 19,304 english words and 22,906 french words.our test set is that used in the alignment evaluation organized by mihalcea and pederson (2003), <papid> W03-0301 </papid>though we retained sentence-initial capitalization, used closed vocabulary, and restricted ourselves to smaller training corpus.</nextsent>
<nextsent>we parsed the english side of the data with the collins parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4753">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>our test data consisted of 447 sentence pairs of no more than 30 words, hand aligned by och and ney (2000).<papid> P00-1056 </papid></prevsent>
<prevsent>a separate development set of 37 sentences was used to control overfitting.</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
we used of vocabulary of words occurring at least 10 times in the entire hansard corpus, resulting in 19,304 english words and 22,906 french words.our test set is that used in the alignment evaluation organized by mihalcea and pederson (2003), <papid> W03-0301 </papid>though we retained sentence-initial capitalization, used closed vocabulary, and restricted ourselves to smaller training corpus.</citsent>
<aftsection>
<nextsent>we parsed the english side of the data with the collins parser.
</nextsent>
<nextsent>as an artifact of the parsers probability model, it outputs sentence-final punctuation attached at the lowest level of the tree.
</nextsent>
<nextsent>we raised sentence-final punctuation to be daughter of the trees root before training our parse-based model.
</nextsent>
<nextsent>as our chinese-english test data did not include sentence-final punctuation, we also removed it from our french-english test set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4755">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this affects the precision/recall tradeoff; better results with respect to human alignments may be possible by adjusting an overall insertion probability in order to optimize aer.
</prevsent>
<prevsent>table 1 provides comparison of results using the tree-based models with the word-level ibm models.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
ibm models 1 and 4 refer to brown et al (1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>we used the giza++ package, including the hmm model of och and ney (2000).<papid> P00-1056 </papid></nextsent>
<nextsent>we ran model 1 for three iterations, then the hmm model for three iterations, and finally model 4 for two iterations, training each model until aer began to increase on ourheld-out cross validation data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4765">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>while both corpora consist of newswire text, typical wsj sentence pierre vinken, 61 years old, will join the board as non executive director nov. 29.
</prevsent>
<prevsent>contrasts dramatically with in the past when education on opposing communists and on resisting russia was stressed, retaking the mainland and unifying china became slogan for the authoritarian system, which made the unification under the martial law tool for oppressing the taiwan people.
</prevsent>
</prevsection>
<citsent citstr=" C02-1145 ">
a typical sentence from our corpus.while we did not have human-annotated gold standard parses for our training data, we did have human annotated parses for the chinese side of our test data, which was taken from the penn chinese treebank (xue et al, 2002).<papid> C02-1145 </papid></citsent>
<aftsection>
<nextsent>we trained secondtree-to-string model in the opposite direction, using chinese trees and english strings.
</nextsent>
<nextsent>the chinese training data was parsed with the bikel (2002) parser, and used the chinese treebank parses for our test data.
</nextsent>
<nextsent>results are shown in table 3.
</nextsent>
<nextsent>because the itg is symmetric, generative model, the itg results in table 3 are identical to those in table 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4766">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>while the experiment does not show significant improvement, it is possible that better parses for the training data might be equally important.
</prevsent>
<prevsent>even when the automatic parser output is correct,the tree structure of the two languages may not correspond.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
dorr (1994) <papid> J94-4004 </papid>categorizes sources of syntactic divergence between languages, and fox (2002)<papid> W02-1039 </papid>analyzed parallel french-english corpus, quantifying how often parse dependencies cross when projecting an english tree onto french string.</citsent>
<aftsection>
<nextsent>evenin this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of verb, or the correspondence ofnot?
</nextsent>
<nextsent>to ne pas?.
</nextsent>
<nextsent>galley et al (2004) <papid> N04-1035 </papid>extract translation rules from large parsed parallel corpus that extend in scope to tree fragments beyond singlenode; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment.</nextsent>
<nextsent>the syntactically supervised model has been found to outperform the ibm word-level alignment models of brown et al (1993) <papid> J93-2003 </papid>for translation by yamada and knight (2002).<papid> P02-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4767">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>while the experiment does not show significant improvement, it is possible that better parses for the training data might be equally important.
</prevsent>
<prevsent>even when the automatic parser output is correct,the tree structure of the two languages may not correspond.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
dorr (1994) <papid> J94-4004 </papid>categorizes sources of syntactic divergence between languages, and fox (2002)<papid> W02-1039 </papid>analyzed parallel french-english corpus, quantifying how often parse dependencies cross when projecting an english tree onto french string.</citsent>
<aftsection>
<nextsent>evenin this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of verb, or the correspondence ofnot?
</nextsent>
<nextsent>to ne pas?.
</nextsent>
<nextsent>galley et al (2004) <papid> N04-1035 </papid>extract translation rules from large parsed parallel corpus that extend in scope to tree fragments beyond singlenode; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment.</nextsent>
<nextsent>the syntactically supervised model has been found to outperform the ibm word-level alignment models of brown et al (1993) <papid> J93-2003 </papid>for translation by yamada and knight (2002).<papid> P02-1039 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4768">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>evenin this closely related language pair with generally similar word order, crossed dependencies were caused by such common occurrences as adverb modification of verb, or the correspondence ofnot?
</prevsent>
<prevsent>to ne pas?.
</prevsent>
</prevsection>
<citsent citstr=" N04-1035 ">
galley et al (2004) <papid> N04-1035 </papid>extract translation rules from large parsed parallel corpus that extend in scope to tree fragments beyond singlenode; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment.</citsent>
<aftsection>
<nextsent>the syntactically supervised model has been found to outperform the ibm word-level alignment models of brown et al (1993) <papid> J93-2003 </papid>for translation by yamada and knight (2002).<papid> P02-1039 </papid></nextsent>
<nextsent>an evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to out perform the ibm models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4770">
<title id=" C04-1060.xml">syntax based alignment supervised or unsupervised </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>to ne pas?.
</prevsent>
<prevsent>galley et al (2004) <papid> N04-1035 </papid>extract translation rules from large parsed parallel corpus that extend in scope to tree fragments beyond singlenode; we believe that adding such larger-scale operations to the translation model is likely to significantly improve the performance of syntactically supervised alignment.</prevsent>
</prevsection>
<citsent citstr=" P02-1039 ">
the syntactically supervised model has been found to outperform the ibm word-level alignment models of brown et al (1993) <papid> J93-2003 </papid>for translation by yamada and knight (2002).<papid> P02-1039 </papid></citsent>
<aftsection>
<nextsent>an evaluation for the alignment task, measuring agreement with human judges, also found the syntax-based model to out perform the ibm models.
</nextsent>
<nextsent>however, relatively small corpus was used to train both models (2121japanese-english sentence pairs), and the evaluations were performed on the same data for training,meaning that one or both models might be significantly overfitting.zens and ney (2003) <papid> P03-1019 </papid>provide thorough analysis of alignment constraints from the perspective of decoding algorithms.</nextsent>
<nextsent>they train the models of wu 1 2 3 4 5 6 7 8 9 400 500 600 700 pe rp le xi ty 0.4 0.45 0.5 0.55 iterations aer figure 1: training curve for itg model, showing perplexity on cross-validation data, and alignment error rate on separate hand-aligned dataset.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4779">
<title id=" C08-1090.xml">almost flat functional semantics for speech translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus sentence like want pepperoni pizza?
</prevsent>
<prevsent>might be represented as something like want1(e, x, y), ref(x, pronoun(i)), quant(y, indef), pizza1(y), pepperoni1(z), nn(z, y) approaches based in the linguistic tradition were dominant about 10 to 15 years ago, when they we reused in major systems like germanys verbmo bil (wahlster, 2000) and sris spoken language translator (rayner et al, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
they are still reasonably popular today, as exemplified by major systems like parcs xle (riezler et al, 2002).<papid> P02-1035 </papid>the competing heritage has its roots in engineering approaches to spoken language systems, which historically have been intimately connected with machine learning.</citsent>
<aftsection>
<nextsent>on this view of things, typical semantic representation is flat list offeature-value pairs, with the features representing semantic concepts: here, want pepperoni pizza?
</nextsent>
<nextsent>would be represented as something like [utterance_type=request, food=pizza, type=pepperoni] it is interesting to see how little contact there has been between these two traditions.
</nextsent>
<nextsent>writer son formal semantics usually treat ad hoc feature value representations as not even worthy of seriousdiscussion.
</nextsent>
<nextsent>conversely, proponents of engineer ing/machine learning approaches often assume in practice that all semantic representations will be some version of flat feature-value list; good 713 example of this tendency is youngs widely cited 2002 survey of machine learning approaches to spoken dialogue (young, 2002).trying to be as neutral as possible, it is reason able to argue that both approaches have important things to offer, and that it is worth trying to find some compromise between them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4780">
<title id=" C08-1090.xml">almost flat functional semantics for speech translation </title>
<section> the medslt system.  </section>
<citcontext>
<prevsection>
<prevsent>a target-language grammar, compiled into generation form, turns this into one or more possible surface strings, after whicha set of generation preferences picks one out.
</prevsent>
<prevsent>finally, the selected string is realised in spoken form.
</prevsent>
</prevsection>
<citsent citstr=" W06-3702 ">
there is also some use of corpus-based statistical methods, both to tune the language model (rayneret al, 2006, section 11.5) and to drive robust embedded help system (chatzichrisafis et al, 2006).<papid> W06-3702 </papid>the treatment of syntactic structure is carefully thought-out compromise between linguistic and engineering traditions.</citsent>
<aftsection>
<nextsent>all grammars used are extracted from general linguistically motivated resource grammars, using corpus-based methods driven by small sets of examples (rayner et al,2006, chapter 9).
</nextsent>
<nextsent>this results in simpler and flatter grammar specific to the domain, whose structure is similar to the ad hoc phrasal grammars typical of engineering approaches.
</nextsent>
<nextsent>the treatment of semantics is however less sophisticated, and basically represents minimal approach in the engineering tradition.
</nextsent>
<nextsent>each lexicon item contributes set of zero or more feature-value pairs (in most cases exactly one pair).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4781">
<title id=" C08-1090.xml">almost flat functional semantics for speech translation </title>
<section> almost flat functional semantics.  </section>
<citcontext>
<prevsection>
<prevsent>the value of the tag feature on each constituent where itis defined is the tag for the most closely enclosing [tag, ...]
</prevsent>
<prevsent>in the original grammar; these values are percolated down to the lexical rules, where they unify with the tags on the semantic fragment contributed by the rule.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
the transformation is straightforward to define in its general form,and the transformed grammars can be readily compiled into efficient generators by standard feature grammar generator-compiler algorithms like semantic head-driven generation (shieber et al, 1990).<papid> J90-1004 </papid></citsent>
<aftsection>
<nextsent>for the concrete experiments described later, we used slightly extended version of the open source regulus generator compiler.
</nextsent>
<nextsent>3.2 transfer.
</nextsent>
<nextsent>our basic strategy for defining transfer between aff expressions is to make it as close as possible to transfer on the original bag-of-concepts 717 utterance:[sem=concat(verb, np)] --  verb:[sem=verb, tag=null], np:[sem=np, tag=obj].
</nextsent>
<nextsent>np:[sem=concat(adj, noun), tag=tag] --  spec:[], adj:[sem=adj, tag=tag], noun:[sem=noun, tag=tag].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4782">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this strategy allows one to handle some frequent question types: who is. . . ,where is. . . , what is the capital of.
</prevsent>
<prevsent>etc.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
(fleis chman et al, 2003; <papid> P03-1001 </papid>jijkoun et al, 2003).</citsent>
<aftsection>
<nextsent>a great deal of work has addressed the problem of extracting semantic relations from unstructured text.
</nextsent>
<nextsent>building on this, much recent work in qa has focused on systems that extract answers from large bodies of text using simple lexico-syntacticpatterns.
</nextsent>
<nextsent>these studies indicate two distinct problems associated with using patterns to extract semantic information from text.
</nextsent>
<nextsent>first, the patterns yield only small subset of the information that may be present in text (the recall problem).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4786">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude in section 7.
</prevsent>
<prevsent>there is large body of work on extracting semantic information using lexical patterns.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
hearst (1992) <papid> C92-2082 </papid>explored the use of lexical patterns for extracting hyponym relations, with patterns such as such as.berland and charniak (1999) <papid> P99-1008 </papid>extract part-of?</citsent>
<aftsection>
<nextsent>relations.
</nextsent>
<nextsent>mann (2002) <papid> W02-1111 </papid>describes method for extracting instances from text by means of part-of-speech patterns involving proper nouns.</nextsent>
<nextsent>the use of lexical patterns to identify answers in corpus-based qa received lots of attention after team taking part in one of the earlier qa track sat trec showed that the approach was competitive at that stage (soubbotin and soubbotin, 2002; ravichandran and hovy, 2002).<papid> P02-1006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4787">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude in section 7.
</prevsent>
<prevsent>there is large body of work on extracting semantic information using lexical patterns.
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
hearst (1992) <papid> C92-2082 </papid>explored the use of lexical patterns for extracting hyponym relations, with patterns such as such as.berland and charniak (1999) <papid> P99-1008 </papid>extract part-of?</citsent>
<aftsection>
<nextsent>relations.
</nextsent>
<nextsent>mann (2002) <papid> W02-1111 </papid>describes method for extracting instances from text by means of part-of-speech patterns involving proper nouns.</nextsent>
<nextsent>the use of lexical patterns to identify answers in corpus-based qa received lots of attention after team taking part in one of the earlier qa track sat trec showed that the approach was competitive at that stage (soubbotin and soubbotin, 2002; ravichandran and hovy, 2002).<papid> P02-1006 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4788">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>hearst (1992) <papid> C92-2082 </papid>explored the use of lexical patterns for extracting hyponym relations, with patterns such as such as.berland and charniak (1999) <papid> P99-1008 </papid>extract part-of?</prevsent>
<prevsent>relations.</prevsent>
</prevsection>
<citsent citstr=" W02-1111 ">
mann (2002) <papid> W02-1111 </papid>describes method for extracting instances from text by means of part-of-speech patterns involving proper nouns.</citsent>
<aftsection>
<nextsent>the use of lexical patterns to identify answers in corpus-based qa received lots of attention after team taking part in one of the earlier qa track sat trec showed that the approach was competitive at that stage (soubbotin and soubbotin, 2002; ravichandran and hovy, 2002).<papid> P02-1006 </papid></nextsent>
<nextsent>different aspects of pattern-based methods have been investigated since.e.g., ravichandran et al (2003) <papid> N03-2029 </papid>collect surface patterns automatically in an unsupervised fashion using collection of trivia question and answer pairs as seeds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4789">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>relations.
</prevsent>
<prevsent>mann (2002) <papid> W02-1111 </papid>describes method for extracting instances from text by means of part-of-speech patterns involving proper nouns.</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
the use of lexical patterns to identify answers in corpus-based qa received lots of attention after team taking part in one of the earlier qa track sat trec showed that the approach was competitive at that stage (soubbotin and soubbotin, 2002; ravichandran and hovy, 2002).<papid> P02-1006 </papid></citsent>
<aftsection>
<nextsent>different aspects of pattern-based methods have been investigated since.e.g., ravichandran et al (2003) <papid> N03-2029 </papid>collect surface patterns automatically in an unsupervised fashion using collection of trivia question and answer pairs as seeds.</nextsent>
<nextsent>these patterns are then used to generate and assess answer candidates for statistical qasystem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4790">
<title id=" C04-1188.xml">information extraction for question answering improving recall through syntactic patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>mann (2002) <papid> W02-1111 </papid>describes method for extracting instances from text by means of part-of-speech patterns involving proper nouns.</prevsent>
<prevsent>the use of lexical patterns to identify answers in corpus-based qa received lots of attention after team taking part in one of the earlier qa track sat trec showed that the approach was competitive at that stage (soubbotin and soubbotin, 2002; ravichandran and hovy, 2002).<papid> P02-1006 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-2029 ">
different aspects of pattern-based methods have been investigated since.e.g., ravichandran et al (2003) <papid> N03-2029 </papid>collect surface patterns automatically in an unsupervised fashion using collection of trivia question and answer pairs as seeds.</citsent>
<aftsection>
<nextsent>these patterns are then used to generate and assess answer candidates for statistical qasystem.
</nextsent>
<nextsent>fleischman et al (2003) <papid> P03-1001 </papid>focus on the precision of the information extracted using simple partof-speech patterns.</nextsent>
<nextsent>they describe machine learning method for removing noise in the collected dataand showed that the qa system based on this approach outperforms an earlier state-of-the-art sys tem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4798">
<title id=" C04-1048.xml">generating discourse structures for written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many recent studies in natural language processing have paid attention to rhetorical structure theory (rst) (mann and thompson 1988; hovy 1993; marcu 2000; forbes et al 2003), method of structured description of text.
</prevsent>
<prevsent>although rhetorical structure has been found to be useful in many fields of text processing (rutledge et al 2000; torrance and bouayad-agha 2001), only afew algorithms for implementing discourse analyzers have been proposed so far.
</prevsent>
</prevsection>
<citsent citstr=" C90-2044 ">
most research in this field concentrates on specific discourse phenomena (schiffrin 1987; litman and hirschberg 1990).<papid> C90-2044 </papid></citsent>
<aftsection>
<nextsent>the amount of research available in discourse segmentation is considered small; in discourse parsing it is even smaller.
</nextsent>
<nextsent>the difficulties in developing discourse parser are (i) recognizing discourse relations between text spans and (ii) deriving discourse structures from these relations.
</nextsent>
<nextsent>marcu (2000)s parser is based on cue phrases, and therefore faces problems when cue phrases are not present in the text.
</nextsent>
<nextsent>this system can apply to unrestricted texts, but faces combinatorial explosion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4799">
<title id=" C04-1048.xml">generating discourse structures for written text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they simplify discourse analysis by developing agr ammar that uses cue phrases as anchors to connect discourse trees.
</prevsent>
<prevsent>despite the potential of this approach for discourse analysis, the case of no cue phrase present in the text has not been fully investigated in their research.
</prevsent>
</prevsection>
<citsent citstr=" W04-2322 ">
polanyi et al (2004) <papid> W04-2322 </papid>propose far more complicated discourse system than that of forbes et al (2003) , which uses syntactic, semantic and lexical rules.</citsent>
<aftsection>
<nextsent>polanyi et al have proved that their approach can provide promising results, especially in text summa riza tion.in this paper, different factors were investigated to achieve better discourse parser, including syntactic information, constraints about textual adjacency and textual organization.
</nextsent>
<nextsent>with given text and its syntactic information, the search space in which well-structured discourse trees of text are produced is minimized.
</nextsent>
<nextsent>the rest of this paper is organized as follows.
</nextsent>
<nextsent>the discourse analyzer at the sentence-level is presented in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4800">
<title id=" C04-1048.xml">generating discourse structures for written text </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>this error is the accumulation of errors made by the discourse segmenter and errors in discourse combination, given correct discourse segments.
</prevsent>
<prevsent>with the set of 14 discourse relations,the f-score of discourse relations at the sentence level using 14 relations (53.0%) is higher than the case of using 22 relations (52.2%).
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
the most recent sentence-level discourse parser providing good results is spade, which is reported in (soricut and marcu 2003).<papid> N03-1030 </papid></citsent>
<aftsection>
<nextsent>spade includes two probabilistic models that can be used to identify edus and build sentence-level discourse parse trees.
</nextsent>
<nextsent>the rst corpus was also used in soricut and marcu (s&m;)s experiment, in which 347 articles were used as the training set we use the f-score version in which precision (p) and recall (r) are weighted equally, defined as 2*p*r/(p+r).
</nextsent>
<nextsent>and 38 ones were used as the test set.
</nextsent>
<nextsent>s&m; evaluated their system using slightly different criteria than those used in this research.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4801">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given that the citation summary of any article usually has more than few sentences, the main challenge of this task is to find subset of these sentences that will lead to better and shorter summary.
</prevsent>
<prevsent>689 cluster nodes edges dp 167 323 pbmt 186 516 summ 839 1425 qa 238 202 te 56 44 table 1: clusters and their citation network size 1.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" W06-1613 ">
although there has been work on analyzing citation and collaboration networks (teufel et al,2006; <papid> W06-1613 </papid>newman, 2001) and scientific article summarization (teufel and moens, 2002), <papid> J02-4002 </papid>to the knowledge of the author there is no previous work that study the text of the citation summaries to produce summary.</citsent>
<aftsection>
<nextsent>(bradshaw, 2003; bradshaw, 2002) get benefit from citations to determine the content of articles and introduce reference directed indexing?
</nextsent>
<nextsent>to improve the results of search engine.
</nextsent>
<nextsent>in other work, (nanba et al, 2004b; nanba etal., 2004a) analyze citation sentences and automatically categorize citations into three groups using160 pre-defined phrase-based rules.
</nextsent>
<nextsent>this categorization is then used to build tool for survey generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4802">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given that the citation summary of any article usually has more than few sentences, the main challenge of this task is to find subset of these sentences that will lead to better and shorter summary.
</prevsent>
<prevsent>689 cluster nodes edges dp 167 323 pbmt 186 516 summ 839 1425 qa 238 202 te 56 44 table 1: clusters and their citation network size 1.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
although there has been work on analyzing citation and collaboration networks (teufel et al,2006; <papid> W06-1613 </papid>newman, 2001) and scientific article summarization (teufel and moens, 2002), <papid> J02-4002 </papid>to the knowledge of the author there is no previous work that study the text of the citation summaries to produce summary.</citsent>
<aftsection>
<nextsent>(bradshaw, 2003; bradshaw, 2002) get benefit from citations to determine the content of articles and introduce reference directed indexing?
</nextsent>
<nextsent>to improve the results of search engine.
</nextsent>
<nextsent>in other work, (nanba et al, 2004b; nanba etal., 2004a) analyze citation sentences and automatically categorize citations into three groups using160 pre-defined phrase-based rules.
</nextsent>
<nextsent>this categorization is then used to build tool for survey generation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4803">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>abstracts tend to summarize the documents topics well, however, they dont include much useof metadata.
</prevsent>
<prevsent>(kan et al, 2002) use annotated bibliographies to cover certain aspects of summarization and suggest guidelines that summaries should also include meta data and critical document features as well as the prominent content-based features.
</prevsent>
</prevsection>
<citsent citstr=" N07-1040 ">
siddharthan and teufel describe new task to decide the scientific attribution of an article (sid dhar than and teufel, 2007) <papid> N07-1040 </papid>and show high human agreement as well as an improvement in the performance of argumentative zoning (teufel, 2005).</citsent>
<aftsection>
<nextsent>argumentative zoning is rhetorical classification task, in which sentences are labeled as one of own, other, background, textual, aim, basis, contrast according to their role in the authors argument.these all show the importance of citation summaries and the vast area for new work to analyze them to produce summary forgiven topic.
</nextsent>
<nextsent>the acl anthology is collection of papers fromthe computational linguistics journal, and proceedings from acl conferences and workshops and includes almost 11, 000 papers.
</nextsent>
<nextsent>to produce the acl anthology network (aan), (joseph andra dev, 2007) manually performed some preprocessing tasks including parsing references and building the network meta data, the citation, and the author collaboration networks.the full aan includes all citation and collaboration data within the acl papers, with the citation network consisting of 8, 898 nodes and 38, 765 directed edges.
</nextsent>
<nextsent>2.1 clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4804">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>each column shows the number of fact-sharing pairs among top highly similar pairs.
</prevsent>
<prevsent>similarity: general idf, an aan-specific idf where idf values are calculated only using the documents of aan, and finally dp-specific idf in which we only used all-dp dataset.
</prevsent>
</prevsection>
<citsent citstr=" N06-1061 ">
table 4 also shows the results for an asymmetric similarity measure, generation probability (erkan, 2006) <papid> N06-1061 </papid>aswell as two string edit distances: the longest common substring and the levenshtein distance (lev enshtein, 1966).</citsent>
<aftsection>
<nextsent>in this section we discuss our graph clustering method for article summarization, as well as other baseline methods used for comparisons.
</nextsent>
<nextsent>4.1 network-based clustering.
</nextsent>
<nextsent>the citation summary network of an article is network in which each sentence in the citation summary of is node.
</nextsent>
<nextsent>this network is complete undirected weighted graph where the weight of an edge between two nodes shows the similarity of the two corresponding sentences of those nodes.the similarity that we use, as described in section 3.2, is such that sentences with the same facts have high similarity values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4805">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>once this network is built, lexrank performs random walkto find the most central nodes in the graph andre ports them as summary sentences.
</prevsent>
<prevsent>5.1 evaluation method.
</prevsent>
</prevsection>
<citsent citstr=" N06-1048 ">
fact-based evaluation systems have been used in several past projects (lin and demner-fushman, 2006; marton and radul, 2006), <papid> N06-1048 </papid>especially in the trec question answering track.</citsent>
<aftsection>
<nextsent>(lin and demner-fushman, 2006) use stemmed unigram similarity of responses with nugget descriptions to produce the evaluation results, whereas (marton and radul, 2006) <papid> N06-1048 </papid>uses both human judgments and human descriptions to evaluate response.an ideal summary in our model is one that covers more facts and more important facts.</nextsent>
<nextsent>our definition for the properties of good?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4807">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>from this viewpoint, there are two criteria for our evaluationmetric.
</prevsent>
<prevsent>first, summaries that contain more important facts are preferred over summaries that cover fewer relevant facts.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
second, facts should not be equally weighted in this model, as some of themmay show more important contributions of paper, while others may not.to evaluate our system, we use the pyramid evaluation method (nenkova and passonneau, 2004) <papid> N04-1019 </papid>at sentence level.</citsent>
<aftsection>
<nextsent>each fact in the citation summary of paper is summarization content unit (scu) (nenkova and passonneau, 2004), <papid> N04-1019 </papid>and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary.</nextsent>
<nextsent>the score given by the pyramid method for summary is ratio of the sum of the weights of its facts to the sum of the weights of an optimal summary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4809">
<title id=" C08-1087.xml">scientific paper summarization using citation summary networks </title>
<section> conclusion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>our experiments show how our clustering method outperforms one of the current state-of-art multi-document summarizing algorithms, lexrank, on this particular problem.a future improvement will be to use reordering approach like maximal marginal relevance r i l g l m a ? r n o l x a k r c e r n d c96-1058 1.00 0.27 0.73 0.73 0.73 p97-1003 1.00 0.08 0.40 0.60 0.40 p99-1065 0.94 0.30 0.54 0.82 0.67 p05-1013 1.00 0.15 0.69 0.97 0.67 p05-1012 0.95 0.14 0.57 0.26 0.62 b t n03-1017 0.96 0.26 0.36 0.61 0.64 w03-0301 1.00 0.60 1.00 1.00 1.00 j04-4002 1.00 0.33 0.70 0.48 0.48 n04-1033 1.00 0.38 0.38 0.31 0.85 p05-1033 1.00 0.37 0.77 0.77 0.85 u m a00-1043 1.00 0.66 0.95 0.71 0.95 a00-2024 1.00 0.26 0.86 0.73 0.60 c00-1072 1.00 0.85 0.85 0.93 0.93 w00-0403 1.00 0.55 0.81 0.41 0.70 w03-0510 1.00 0.58 1.00 0.83 0.83 a a00-1023 1.00 0.57 0.86 0.86 0.86 w00-0603 1.00 0.33 0.53 0.53 0.60 p02-1006 1.00 0.49 0.92 0.49 0.87 d03-1017 1.00 0.00 0.53 0.26 0.85 p03-1001 1.00 0.12 0.29 0.59 0.59 e d04-9907 1.00 0.53 0.88 0.65 0.94 h05-1047 1.00 0.83 0.66 0.83 1.00 h05-1079 1.00 0.67 0.78 0.89 0.56 w05-1203 1.00 0.50 0.71 1.00 0.71 p05-1014 1.00 0.44 1.00 0.89 0.78 mean 0.99 0.41 0.71 0.69 0.75 table 7: evaluation results (|s| = 5) (mmr) (carbonell and goldstein, 1998) to re-rank clustered documents within each cluster in orderto reduce the redundancy in final summary.
</prevsent>
<prevsent>another possible approach is to assume the set of sentences in the citation summary as sentences talking about the same event, yet generated in different sources.
</prevsent>
</prevsection>
<citsent citstr=" P99-1071 ">
then one can apply the method inspired by (barzilay et al, 1999) <papid> P99-1071 </papid>to identify common phrases across sentences and use language generation to form more coherent summary.</citsent>
<aftsection>
<nextsent>the ultimate goal, however, is to produce topic sum marizer system in which the query is scientific topic and the output is summary of all previous works in that topic, preferably sorted to preserve chronology and topicality.
</nextsent>
<nextsent>the authors would like to thank bonnie dorr, jimmy lin, saif mohammad, judith l. klavans, ben shneiderman, and aleks aris from umd,bryan gibson, joshua gerrish, pradeep muthukrishnan, arzucan ? ozgur, ahmed hassan, and thuy vu from university of michigan for annotations.
</nextsent>
<nextsent>this paper is based upon work supported by the national science foundation grant iopener: flexible framework to support rapid learning in unfamiliar research domains?, jointly awarded to u. of michigan and u. of maryland as iis 0705832.
</nextsent>
<nextsent>any opinions, findings, and conclusions or recommendations expressed in this paper are 695 those of the authors and do not necessarily reflect the views of the national science foundation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4810">
<title id=" C04-1096.xml">generation of relative referring expressions based on perceptual grouping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
<prevsent>in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world.
</prevsent>
</prevsection>
<citsent citstr=" E91-1028 ">
for that purpose, most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors.</citsent>
<aftsection>
<nextsent>therefore,these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and dis tractors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression means concise and natural linguistic expression enabling hearers to distinguish the target from distractors.for example, consider indicating object to person in the situation shown in figure 1.
</nextsent>
<nextsent>note that person does not share the label information such as and with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means oftheir appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4811">
<title id=" C04-1096.xml">generation of relative referring expressions based on perceptual grouping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
<prevsent>in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world.
</prevsent>
</prevsection>
<citsent citstr=" J95-3003 ">
for that purpose, most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors.</citsent>
<aftsection>
<nextsent>therefore,these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and dis tractors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression means concise and natural linguistic expression enabling hearers to distinguish the target from distractors.for example, consider indicating object to person in the situation shown in figure 1.
</nextsent>
<nextsent>note that person does not share the label information such as and with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means oftheir appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4812">
<title id=" C04-1096.xml">generation of relative referring expressions based on perceptual grouping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
<prevsent>in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world.
</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
for that purpose, most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors.</citsent>
<aftsection>
<nextsent>therefore,these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and dis tractors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression means concise and natural linguistic expression enabling hearers to distinguish the target from distractors.for example, consider indicating object to person in the situation shown in figure 1.
</nextsent>
<nextsent>note that person does not share the label information such as and with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means oftheir appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4813">
<title id=" C04-1096.xml">generation of relative referring expressions based on perceptual grouping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the evaluation using another 23 subjects showed that the proposed method could effectively generate proper referring expressions.
</prevsent>
<prevsent>in the last two decades, many researchers have studied the generation of referring expressions to enable computers to communicate with humans about concrete objects in the world.
</prevsent>
</prevsection>
<citsent citstr=" J03-1003 ">
for that purpose, most past work (appelt, 1985; dale and haddock, 1991; <papid> E91-1028 </papid>dale, 1992; dale and reiter, 1995; heeman and hirst, 1995; <papid> J95-3003 </papid>horacek, 1997; <papid> P97-1027 </papid>krahmer and theune, 2002; van deemter, 2002; krahmer et al, 2003) <papid> J03-1003 </papid>makes use of attributes of an intended object (the target) and binary relations between the target and others (distractors) to distinguish the target from distractors.</citsent>
<aftsection>
<nextsent>therefore,these methods cannot generate proper referring expressions in situations where no significant surface difference exists between the target and dis tractors, and no binary relation is useful to distinguish the target.
</nextsent>
<nextsent>here, proper referring expression means concise and natural linguistic expression enabling hearers to distinguish the target from distractors.for example, consider indicating object to person in the situation shown in figure 1.
</nextsent>
<nextsent>note that person does not share the label information such as and with the speaker.
</nextsent>
<nextsent>because object is not distinguishable from objects or by means oftheir appearance, one would try to use binary relation between object and the table, i.e., ball to the right of the table?.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4816">
<title id=" C02-1156.xml">putting frames in perspective </title>
<section> the framenet commerce frame.  </section>
<citcontext>
<prevsection>
<prevsent>these dynamic representations allow annotated framenet data to parameterize event simulations (narayanan, 1999b) that produce fine-grained, context-sensitive inferences.
</prevsent>
<prevsent>we illustrate our formalism for the commerce frame and show how it can account for some of the wide ranging consequences of perspective-taking.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the framenet project has thus far produced two databases: collection of approximately 80 frames with frame descriptions, chosen to cover broad range of semantic domains; and hand-annotated dataset of about 50,000 sentences from the british national corpus (baker et al, 1998).<papid> P98-1013 </papid></citsent>
<aftsection>
<nextsent>the databases document both syntactic and semantic behavior of ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????
</nextsent>
<nextsent>figure 1: results of query on the framenet commerce frame, showing annotated data for the verb buy.
</nextsent>
<nextsent>wide variety of lexical items (or lemmas) and thus have the potential to allow corpus-based techniques to be applied to semantically oriented tasks.2 the current release of the framenet databases3 defines commerce frame with frame elements including the familiar buyer, seller, payment and goods, along with several other fes needed to cover the data.
</nextsent>
<nextsent>the frame includes 10 verbs relevant to commercial transactions, for total of 575 annotated sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4817">
<title id=" C02-1156.xml">putting frames in perspective </title>
<section> the framenet commerce frame.  </section>
<citcontext>
<prevsection>
<prevsent>the commerce frame, for example, is implicitly associated with complex, dynamic network of interrelated events, actions and participants.
</prevsent>
<prevsent>our proposal is that perspectival effects may be best understood in terms of subtle inferential effects on interpretation licensed by this network.
</prevsent>
</prevsection>
<citsent citstr=" P00-1065 ">
our task, then, is to make this inferential structure2see (gildea and jurafsky, 2000) <papid> P00-1065 </papid>for some promising initial work in applying statistical techniques to the framenet database to automatically label frame elements.</citsent>
<aftsection>
<nextsent>3we refer to data from framenet i; an interim release of framenet ii is expected soon.
</nextsent>
<nextsent>explicit.
</nextsent>
<nextsent>we take the original commerce frameas our starting point and define the interrelationships present among its fes.
</nextsent>
<nextsent>the additional structure we impose on the commerce frame allows us to distinguish perspective-neutral description of commercial transaction from the perspectivizedsituations described by particular verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4818">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>considerable amount of work has been done in recent years on the named entity recognition task,partly due to the message understanding conferences (muc).
</prevsent>
<prevsent>a named entity recognizer (ner) is useful in many nlp applications such as information extraction, question answering, etc. on its own, ner can also provide users who are looking for person or organization names with quick information.
</prevsent>
</prevsection>
<citsent citstr=" M98-1028 ">
in muc-6 and muc-7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (chinchor, 1998; <papid> M98-1028 </papid>sundheim, 1995)machine learning systems in muc-6 and muc7 achieved accuracy comparable to rule-based systems on the named entity task.</citsent>
<aftsection>
<nextsent>statistical ners usually find the sequence of tags that maximizes the probability
</nextsent>
<nextsent> , where  is the sequence of words in sentence, and  is the sequence of named-entity tags assigned to the words in  . attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same docu ment), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of first ner (mikheev et al, 1998; <papid> M98-1021 </papid>borthwick, 1999).</nextsent>
<nextsent>we propose maximizing  </nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4819">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in muc-6 and muc-7, the named entity task is defined as finding the following classes of names: person, organization, location, date, time, money, and percent (chinchor, 1998; <papid> M98-1028 </papid>sundheim, 1995)machine learning systems in muc-6 and muc7 achieved accuracy comparable to rule-based systems on the named entity task.</prevsent>
<prevsent>statistical ners usually find the sequence of tags that maximizes the probability  </prevsent>
</prevsection>
<citsent citstr=" M98-1021 ">
, where  is the sequence of words in sentence, and  is the sequence of named-entity tags assigned to the words in  . attempts have been made to use global information (e.g., the same named entity occurring in different sentences of the same docu ment), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of first ner (mikheev et al, 1998; <papid> M98-1021 </papid>borthwick, 1999).</citsent>
<aftsection>
<nextsent>we propose maximizing
</nextsent>
<nextsent>   , where  is the sequence of named entity tags assigned to the words in the sentence  , and  is the information that can be extracted from the whole document containing  . our system is built on maximum entropy classifier.
</nextsent>
<nextsent>by making use of global context, it has achieved excellent results on both muc-6 and muc-7 official test data.
</nextsent>
<nextsent>we will refer to our system as menergi(maximum entropy named entity recognizer using global information).as far as we know, no other ners have used information from the whole document (global) as well as information within the same sentence (local) inone framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4821">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we will refer to our system as menergi(maximum entropy named entity recognizer using global information).as far as we know, no other ners have used information from the whole document (global) as well as information within the same sentence (local) inone framework.
</prevsent>
<prevsent>the use of global features has improved the performance on muc-6 test data from 90.75% to 93.27% (27% reduction in errors), and the performance on muc-7 test data from 85.22% to 87.24% (14% reduction in errors).
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
these results are achieved by training on the official muc-6 and muc-7 training data, which is much less training data than is used by other machine learning systems that worked on the muc-6 or muc-7 named entity task (bikel et al, 1997; <papid> A97-1029 </papid>bikel et al, 1999; borthwick, 1999).we believe it is natural for authors to use abbreviations in subsequent mentions of named entity (i.e., first president george bush?</citsent>
<aftsection>
<nextsent>then bush?).
</nextsent>
<nextsent>as such, global information from the whole context of document is important to more accurately recognize named entities.
</nextsent>
<nextsent>although we have not doneany experiments on other languages, this way of using global features from whole document should be applicable to other languages.
</nextsent>
<nextsent>recently, statistical ners have achieved results that are comparable to hand-coded systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4829">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> feature description.  </section>
<citcontext>
<prevsection>
<prevsent>this is because in many cases, the use of hyphens can be considered to be optional (e.g., third-quarter or third quarter).
</prevsent>
<prevsent>out-of-vocabulary: we derived lexicon list from wordnet 1.6, and words that are not found in this list have feature out-of-vocabulary set to 1.dictionaries: due to the limited amount of training material, name dictionaries have been found to be useful in the named entity task.
</prevsent>
</prevsection>
<citsent citstr=" E99-1001 ">
the importance of dictionaries in ners has been investigated in the literature (mikheev et al, 1999).<papid> E99-1001 </papid></citsent>
<aftsection>
<nextsent>the sources of our dictionaries are listed in table 2.
</nextsent>
<nextsent>for all lists except locations, the lists are processed into list of tokens (unigrams).
</nextsent>
<nextsent>location list is processed into list of unigrams and bigrams (e.g., new york).
</nextsent>
<nextsent>for locations, tokens are matched against unigrams, and sequences of two consecutive tokens are matched against bigrams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4832">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we try to compare our results with those obtained by identifinder  97 (bikel et al, 1997), <papid> A97-1029 </papid>identifinder  99 (bikel etal., 1999), and mene (borthwick, 1999).</prevsent>
<prevsent>iden tifinder  99 results are considerably better than identifinder  97 s.</prevsent>
</prevsection>
<citsent citstr=" M98-1009 ">
identifinder performance in muc-7 is published in (miller et al, 1998).<papid> M98-1009 </papid></citsent>
<aftsection>
<nextsent>mene has only been tested on muc-7.
</nextsent>
<nextsent>for fair comparison, we have tabulated all results with the size of training data used (table 5 and table 6).
</nextsent>
<nextsent>besides size of training data, the use of dictionaries is another factor that might affect performance.
</nextsent>
<nextsent>bikel et al (1999) did not report using any dictionaries, but mentioned in footnote that they have added list membership features, which have helped marginally in certain domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="B4833">
<title id=" C02-1025.xml">named entity recognition a maximum entropy approach using global information </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>bikel et al (1999) did not report using any dictionaries, but mentioned in footnote that they have added list membership features, which have helped marginally in certain domains.
</prevsent>
<prevsent>borth2muc data can be obtained from the linguistic data con sortium: http://www.ldc.upenn.edu 3training data for identifinder is actually given in words (i.e., 650k &amp; 790k words), rather than tokens systems size of training data f-measure sra  95 hand-coded 96.4% identifinder  99 650,000 words 94.9% menergi 160,000 tokens 93.27% identifinder  99   200,000 words about 93% (from graph) identifinder  97 450,000 words 93% identifinder  97 about 100,000 words 91%-92% table 5: comparison of results for muc-6 systems size of training data f-measure ltg system  98 hybrid hand-coded 93.39% identifinder  98 790,000 words 90.44% mene + proteus hybrid hand-coded 88.80%  98 321,000 tokens menergi 180,000 tokens 87.24% mene+reference- 321,000 tokens 86.56% resolution  99 mene  98 321,000 tokens 84.22% table 6: comparison of results for muc-7 wick (1999) reported using dictionaries of person first names, corporate names and suffixes, college sand universities, dates and times, state abbreviations, and world regions.
</prevsent>
</prevsection>
<citsent citstr=" M95-1018 ">
in muc-6, the best result is achieved by sra (krupka, 1995).<papid> M95-1018 </papid></citsent>
<aftsection>
<nextsent>in (bikel et al, 1997) <papid> A97-1029 </papid>and (bikel et al., 1999), performance was plotted against training data size to show how performance improves with training data size.</nextsent>
<nextsent>we have estimated the performance of identifinder  99 at 200k words of training data from the graphs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>