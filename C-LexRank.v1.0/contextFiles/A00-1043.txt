ziff-davis corpus most previous work (jing 2000; knight and marcu 2002; riezler et al 2003; nguyen et al 2004a; turner and charniak 2005; mcdonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes. 
jing and mckeown (2000) and jing (2000) propose a cut-and-paste strategy as a computational process of automatic abstracting and a sentence reduction strategy to produce concise sentences. 
for example, jing (2000) trained her system on a set of 500 sentences from the benton foundation (http://www.benton.org) and their reduced forms written by humans. 
jing (2000) proposes a novel algorithm for sentence reduction that takes into account different sources of information to decide whether or not to remove a particular component from a sentence to be included in a summary. 
jing (2000) was perhaps the first to tackle the sentence compression problem. 
depending on the chosen task, such systems either generate single-sentence “headlines” for multi-sentence text (witbrock and mittal, 1999), or they provide a sentence condensation module designed for combination with sentence extraction systems (knight and marcu, 2000; jing, 2000). 
sentence reduction the task of the sentence reduction module, described in detail in (jing, 2000), is to remove extraneous phrases from extracted sentences. 
our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of jing (2000). 
the evaluation of sentence reduction (see (jing, 2000) for details) used a corpus of 500 sentences and their reduced forms in human-written abstracts. 
in contrast to jing (2000), the bulk of the research on sentence compression relies exclusively on corpus data for modelling the compression process without recourse to extensive knowledge sources (e.g., wordnet). 
dependency structure, we cannot use complex methods for text reduction as described, e.g., in (jing, 2000). 
many algorithms exploit parallel corpora (jing 2000; knight and marcu 2002; riezler et al 2003; nguyen et al 2004a; turner and charniak 2005; mcdonald 2006) to learn the correspondences between long and short sentences in a supervised manner, typically using a rich feature space induced from parse trees. 
since then, jing (2000), riezler et al (2003) and knight and marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. 
in addition, an automatic evaluation method based on context-free deletion decisions has been proposed by jing (2000). 
(jing, 2000) studied a new method to remove extraneous phrase from sentences by using multiple source of knowledge to decide which phrase in the sentences can be removed. 
examples include text summarisation (jing 2000), subtitle generation from spoken transcripts (vandeghinste and pan 2004) and information retrieval (olivers and dolan 1999). 
to overcome this problem, linguistic parsing and generation systems are used in the sentence condensation approaches of knight and marcu (2000) and jing (2000). 
between these two extremes, there has been a relatively modest amount of work in sentence simplification (chandrasekar, doran, and bangalore 1996; mahesh 1997; carroll et al 1998; grefenstette 1998; jing 2000; knight and marcu 2002) and document compression (daum´e iii and marcu 2002; daum´e iii and marcu 2004; zajic, dorr, and schwartz 2004) in which words, phrases, and sentences are selected in an extraction process. 
sentence simplification systems (chandrasekar et al, 1996; mahesh, 1997; carroll et al, 1998; grefenstette, 1998; jing, 2000; knight and marcu, 2000) are capable of compressing long sentences by deleting unimportant words and phrases. 
