<paper>
<cited id="ZL0">
<title id=" W99-0209.xml">orthographic coreference resolution between proper nouns through the calculation of the relation of replicancia </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these three disciplines have in common the operation of natural language processing techniques (jacobs and rau, 1993), which thus can evolve synergically.
</prevsent>
<prevsent>identification and treatment of noun phrases is one of the fields of interest shared both by information retrieval and information extraction.
</prevsent>
</prevsection>
<citsent citstr=" J93-3001 ">
such interest must be understood within the trend to carry out only partial analysis of texts so as to process them in reasonable time (chinchor et al., 1993).<papid> J93-3001 </papid></citsent>
<aftsection>
<nextsent>the present work proposes new instrument designed for the treatment of proper nouns and other simple noun phrases in texts written in spanish language.
</nextsent>
<nextsent>the tool can be used both in information retrieval and information extraction systems.
</nextsent>
<nextsent>1.1 information retrieval..
</nextsent>
<nextsent>information retrieval systems aim to discriminate between the documents which form the system entry, according to the information eed posed by the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL1">
<title id=" W99-0609.xml">determining the specificity of nouns from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are cir-cumstances, particularly involving domain- specific text, where wordnet does not have sufficient coverage.
</prevsent>
<prevsent>various automatic meth-ods have been proposed to automatically build lexical resources or augment existing resources.
</prevsent>
</prevsection>
<citsent citstr=" W97-0313 ">
(see, e.g., riloff and shepherd (1997), <papid> W97-0313 </papid>roark and charniak (1998), <papid> P98-2182 </papid>cara- hallo (1999), and berland and charniak (1999).)<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe method which can be used to assist in this problem.
</nextsent>
<nextsent>we present here way to determine the relative specificity of nouns; that is, which nouns are more specific (or more general) than others, using only large text cor-pus and no additional sources of semantic knowledge.
</nextsent>
<nextsent>by gathering simple statistics, we are able to decide which of two nouns is more specific to over 80% accuracy for nouns at  basic level  or below (see, e.g., lakoff (1987)), and about 59% accuracy for nouns above basic level.
</nextsent>
<nextsent>it should be noted that specificity by it- self is not enough information from which to construct noun hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL2">
<title id=" W99-0609.xml">determining the specificity of nouns from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are cir-cumstances, particularly involving domain- specific text, where wordnet does not have sufficient coverage.
</prevsent>
<prevsent>various automatic meth-ods have been proposed to automatically build lexical resources or augment existing resources.
</prevsent>
</prevsection>
<citsent citstr=" P98-2182 ">
(see, e.g., riloff and shepherd (1997), <papid> W97-0313 </papid>roark and charniak (1998), <papid> P98-2182 </papid>cara- hallo (1999), and berland and charniak (1999).)<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe method which can be used to assist in this problem.
</nextsent>
<nextsent>we present here way to determine the relative specificity of nouns; that is, which nouns are more specific (or more general) than others, using only large text cor-pus and no additional sources of semantic knowledge.
</nextsent>
<nextsent>by gathering simple statistics, we are able to decide which of two nouns is more specific to over 80% accuracy for nouns at  basic level  or below (see, e.g., lakoff (1987)), and about 59% accuracy for nouns above basic level.
</nextsent>
<nextsent>it should be noted that specificity by it- self is not enough information from which to construct noun hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL3">
<title id=" W99-0609.xml">determining the specificity of nouns from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are cir-cumstances, particularly involving domain- specific text, where wordnet does not have sufficient coverage.
</prevsent>
<prevsent>various automatic meth-ods have been proposed to automatically build lexical resources or augment existing resources.
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
(see, e.g., riloff and shepherd (1997), <papid> W97-0313 </papid>roark and charniak (1998), <papid> P98-2182 </papid>cara- hallo (1999), and berland and charniak (1999).)<papid> P99-1008 </papid></citsent>
<aftsection>
<nextsent>in this paper, we describe method which can be used to assist in this problem.
</nextsent>
<nextsent>we present here way to determine the relative specificity of nouns; that is, which nouns are more specific (or more general) than others, using only large text cor-pus and no additional sources of semantic knowledge.
</nextsent>
<nextsent>by gathering simple statistics, we are able to decide which of two nouns is more specific to over 80% accuracy for nouns at  basic level  or below (see, e.g., lakoff (1987)), and about 59% accuracy for nouns above basic level.
</nextsent>
<nextsent>it should be noted that specificity by it- self is not enough information from which to construct noun hierarchy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL4">
<title id=" W99-0609.xml">determining the specificity of nouns from text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it should be noted that specificity by it- self is not enough information from which to construct noun hierarchy.
</prevsent>
<prevsent>this project is meant provide tool to support other methods.
</prevsent>
</prevsection>
<citsent citstr=" P99-1016 ">
see caraballo (1999) <papid> P99-1016 </papid>for detailed description of method to construct such hierarchy.</citsent>
<aftsection>
<nextsent>to the best of our knowledge, this is the first attempt automatically rank nouns based on specificity.
</nextsent>
<nextsent>hearst (1992) <papid> C92-2082 </papid>found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.</nextsent>
<nextsent>the sparse-ness of these patterns prevents this from be-ing an effective approach to the problem we address here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL5">
<title id=" W99-0609.xml">determining the specificity of nouns from text </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>see caraballo (1999) <papid> P99-1016 </papid>for detailed description of method to construct such hierarchy.</prevsent>
<prevsent>to the best of our knowledge, this is the first attempt automatically rank nouns based on specificity.</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
hearst (1992) <papid> C92-2082 </papid>found individual pairs of hypernyms and hyponyms from text using pattern-matching techniques.</citsent>
<aftsection>
<nextsent>the sparse-ness of these patterns prevents this from be-ing an effective approach to the problem we address here.
</nextsent>
<nextsent>in caraballo (1999), <papid> P99-1016 </papid>we construct hierar-chy of nouns, including hypernym relations.</nextsent>
<nextsent>however, there are several areas where that work could benefit from the research pre-sented here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL11">
<title id=" W99-0617.xml">pos tags and decision trees for language modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>below we give the equation usually used for class-based trigram model, where the function 9 maps each word to its unambiguous class.
</prevsent>
<prevsent>pr(wilg(wd ) pr(g(wdlg(w~-~ )g(w~-2) ) using classes has the potential of reducing the problem of sparseness ofdata by allowing en- eralizations over similar words, as well as re-ducing the size of the language model.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
to determine the word classes, one can use the algorithm of brown et al (1992), <papid> J92-4003 </papid>which finds the classes that give high mutual informa-tion between the classes of adjacent words.</citsent>
<aftsection>
<nextsent>in other words, for each bigram wi-lwi in train-ing corpus, choose the classes such that the classes for adjacent words 9(wi-1) and 9(wi) lose as little information about each other as possible.
</nextsent>
<nextsent>brown et al give greedy algorithm for finding the classes.
</nextsent>
<nextsent>they start with each word in separate class and iteratively com-bine classes that lead to the smallest decrease in mutual information between adjacent words.
</nextsent>
<nextsent>kneser and ney (1993) found that class-based language model results in perplexity improve-ment for the lob corpus from 541 for word- based bigram model to 478 for class-based bi-gram model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL12">
<title id=" W99-0617.xml">pos tags and decision trees for language modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>only by in- terpolating it with word-based model is anim- prove ment seen (jelinek, 1985).
</prevsent>
<prevsent>1.3 our approach.
</prevsent>
</prevsection>
<citsent citstr=" W98-1121 ">
in past work (heeman and allen, 1997; hee- man, 1998), <papid> W98-1121 </papid>we introduced an alternative for-mulation for using pos tags in language model.</citsent>
<aftsection>
<nextsent>here, pos tags are elevated from inter-mediate objects to be part of the output of the speech recognizer.
</nextsent>
<nextsent>furthermore, we do not use the simplifying assumptions of the previous ap-proach.
</nextsent>
<nextsent>rather, we use clustering algorithm to find words and pos tags that behave sim-ilarly.
</nextsent>
<nextsent>the output of the clustering algorithm is used by decision tree algorithm to build 130 set of equivalenc classes of the contexts from which the word and pos probabilities are esti-mated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL14">
<title id=" W99-0617.xml">pos tags and decision trees for language modeling </title>
<section> estimating the probabilities.  </section>
<citcontext>
<prevsection>
<prevsent>3.4 questions ~ibout word identities.
</prevsent>
<prevsent>for handling word identities, one could follow the approach used for handling the pos tags (e.g.
</prevsent>
</prevsection>
<citsent citstr=" H92-1026 ">
(black et al, 1992; <papid> H92-1026 </papid>magerman, 1994)) and view the pos tags and word identities as two separate sources of information.</citsent>
<aftsection>
<nextsent>instead, we view the word identities as further efine- ment of the pos tags.
</nextsent>
<nextsent>we start the clustering algorithm with separate class for each word and each pos tag that it takes on and only al-low it to merge c!asses if the pos tags are the same.
</nextsent>
<nextsent>this results in word classification tree for each pos tag.
</nextsent>
<nextsent>using pos tags in word clustering means that words that take on differ-ent pos tags can ibe better modeled (heeman, 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL15">
<title id=" W99-0617.xml">pos tags and decision trees for language modeling </title>
<section> results on trains corpus.  </section>
<citcontext>
<prevsection>
<prevsent>contrac-tions, such as  thatll  and  gonna , are treated as separate words:  that  and   11  for the first example, and  going  and  ta  for the second.
</prevsent>
<prevsent>all word fragments were changed to the to-ken  fragment .
</prevsent>
</prevsection>
<citsent citstr=" H89-2027 ">
in searching for the best se-quence of pos tags for the transcribed words, we follow the technique proposed by chow and schwartz (1989) <papid> H89-2027 </papid>and only keep small number of alternative paths by pruning the low proba-bility paths after processing each word.</citsent>
<aftsection>
<nextsent>for our speech recognition results, we used ogi large vocabulary speech recognizer (yan et al, 1998; wu et al, 1999), using acoustic models trained from the trains corpus.
</nextsent>
<nextsent>we ran the decoder in single pass using cross- word acoustic modeling and trigram word- based backoff model (katz, 1987) built with the cmu toolkit (rosenfeld, 1995).
</nextsent>
<nextsent>for the first pass, contracted words were treated as single tokens in order to improve acoustic recognition of them.
</nextsent>
<nextsent>the result of the first pass was word graph, which we rescored in second pass us-ing our other trigram language models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL16">
<title id=" W99-0702.xml">experiments in unsupervised entropy based corpus segmentation </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>such situ-ation may occur e.g. if there is (sufficiently large) corpus of an unknown or unidentified language and alphabet.
</prevsent>
<prevsent>1 based on entropy, we search for separa-tors, without knowing priory by which symbols or sequences of symbols they are constituted.
</prevsent>
</prevsection>
<citsent citstr=" W98-1210 ">
over the last decades, entropy has frequently been used to segment corpora \[wolff, 1977, alder, 1988, hutchens and alder, 1998, <papid> W98-1210 </papid>among many others\].</citsent>
<aftsection>
<nextsent>and it is commonly used with compression tech-niques.
</nextsent>
<nextsent>harris \[1955\] proposed an approach for seg-menting words into morphemes that, although it did not use entropy, was based on an intuitively similar concept: every symbol of word is annotated with the count of all possible successor symbols given the .substring that ends with the current symbol, and with the count of all possible predecessor symbols such corpus can be electronically encoded with arbi-trarily defined symbol codes.
</nextsent>
<nextsent>given the tail of the word that starts with the cur-rent symbol.
</nextsent>
<nextsent>maxima in these counts are used to segment the word into morphemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL20">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> what  is center ing?.  </section>
<citcontext>
<prevsection>

<prevsent>centering theory (c~) is theory of discourse structure which models the interaction of cohe-sion and salience in the internal organlsation of text.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
the main assumptions of the theory as presented by (gross et a11995 (gjw), brennan et al 1987) <papid> P87-1022 </papid>rare: 1.</citsent>
<aftsection>
<nextsent>for each utterance in discourse there is. precisely one entity which is the centre of attention or center.
</nextsent>
<nextsent>ances within discourse segment to keep the same entity as the center, and for the most salient entity ~realised in an utter-ance to be interpreted as the center of the following utterance.
</nextsent>
<nextsent>3.
</nextsent>
<nextsent>the center is the entity which is most likely.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL22">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> what  is center ing?.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>ct has proved attractive to nlp researchers because of the elegance and simplicity of the core proposals; it provides framework for analysing text without having to make tough decisions about what partfcular utterance is  about , since all notions are defined in purely structural terms.
</prevsent>
</prevsection>
<citsent citstr=" J94-2006 ">
much research in ct has concentrated on in-terpretation, particularly reference resolution, developing algorithms to resolve anaph0ric ex-pressions based on the assumption that the text is constructed according to rules 1 and tails of the theory which were left unspecified: what counts as an utterance, and how should transitions be handled in complex sentences (kameyama 1998; cf suri and mccoy 1994)?<papid> J94-2006 </papid></citsent>
<aftsection>
<nextsent>how is salience ranking determined (gordon et al  1993; stevenson etal 1994;.
</nextsent>
<nextsent>strube and hahn 1996)?<papid> P96-1036 </papid></nextsent>
<nextsent>what counts as ~r~migstion ~ - does this include bridging references (strube and hahn op cit.)?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL23">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> what  is center ing?.  </section>
<citcontext>
<prevsection>
<prevsent>much research in ct has concentrated on in-terpretation, particularly reference resolution, developing algorithms to resolve anaph0ric ex-pressions based on the assumption that the text is constructed according to rules 1 and tails of the theory which were left unspecified: what counts as an utterance, and how should transitions be handled in complex sentences (kameyama 1998; cf suri and mccoy 1994)?<papid> J94-2006 </papid></prevsent>
<prevsent>how is salience ranking determined (gordon et al  1993; stevenson etal 1994;.</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
strube and hahn 1996)?<papid> P96-1036 </papid></citsent>
<aftsection>
<nextsent>what counts as ~r~migstion ~ - does this include bridging references (strube and hahn op cit.)?
</nextsent>
<nextsent>how do centering tra~l~itions relate to discourse segment boundaries (walker 1998, passoneau 1998)?
</nextsent>
<nextsent>in fact will leave many of these issues aside for the purposes of this paper; will not ex-amine the empirical adequacy of ct, for which the reader is referred to papers cited above and ? others collected in walker et al (1998).
</nextsent>
<nextsent>i will take different approach, which is to examine how the dements of ct can be applied to the planning of texts, with the rules and constraints interpreted as instructions to generator rather than guide for interpretation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL24">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> what  is center ing?.  </section>
<citcontext>
<prevsection>
<prevsent>continue is preferred over retain which is preferred over smooth shift.
</prevsent>
<prevsent>which is preferred over rough shift . . .
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
@ o o @ o o @ o o o @ @ @ @ figure 2: centering ucanonical ~ formulation of ct as outlined by walker et al  (1998, chapter 1) and the schematic ~consensus ~ generation architecture described by reiter and dale (reiter 1994; <papid> W94-0319 </papid>re-iter and dale 1997).</citsent>
<aftsection>
<nextsent>this consists ~pipelins  of distinct asks: text planning- deciding the content of message, and organising the component propositions into text tree; sentence planning - aggregating proposi-tions into clausal units and choosing lex-ical items corresponding to concepts in the knowledge base, including referring expres-sions (re); linguistic re_8!isation which takes care of surface details uch as agreement, orthog- raphy etc.   previous researchers have implemp.nted pronominalisafion decisions using ct, and so have located centering as part of regeneration (e.g., dale 1992, passoneau 1998), while mittal et al (1998) <papid> J98-3004 </papid>have ~centering module  which forms part of sentence planning and seeks to realise the center as subject in successive sentences.</nextsent>
<nextsent>in what follows will try to separate out the tasks which make up centering theory and argue that the way to implement ct is not as discrete module but as series of constraints on the various levels of the generation process from text planning constraints and rules to re generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL25">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> what  is center ing?.  </section>
<citcontext>
<prevsection>
<prevsent>which is preferred over rough shift . . .
</prevsent>
<prevsent>@ o o @ o o @ o o o @ @ @ @ figure 2: centering ucanonical ~ formulation of ct as outlined by walker et al  (1998, chapter 1) and the schematic ~consensus ~ generation architecture described by reiter and dale (reiter 1994; <papid> W94-0319 </papid>re-iter and dale 1997).</prevsent>
</prevsection>
<citsent citstr=" J98-3004 ">
this consists ~pipelins  of distinct asks: text planning- deciding the content of message, and organising the component propositions into text tree; sentence planning - aggregating proposi-tions into clausal units and choosing lex-ical items corresponding to concepts in the knowledge base, including referring expres-sions (re); linguistic re_8!isation which takes care of surface details uch as agreement, orthog- raphy etc.   previous researchers have implemp.nted pronominalisafion decisions using ct, and so have located centering as part of regeneration (e.g., dale 1992, passoneau 1998), while mittal et al (1998) <papid> J98-3004 </papid>have ~centering module  which forms part of sentence planning and seeks to realise the center as subject in successive sentences.</citsent>
<aftsection>
<nextsent>in what follows will try to separate out the tasks which make up centering theory and argue that the way to implement ct is not as discrete module but as series of constraints on the various levels of the generation process from text planning constraints and rules to regeneration.
</nextsent>
<nextsent>i shall also briefly note points of comparison with systems discussed by cahill and reape (1998) in survey of applied nlg systems, and conclude with some remarks on the applicability of my proposals to the  reference architecture nvisaged by cahill et al .
</nextsent>
<nextsent>(1999), rags (1999).
</nextsent>
<nextsent>the rn~in clahns of ct are formalised in terms of c5, the  backward-looking center ~, c/ , list of ~ orward-looking centers for each ut-terance un, and cp or ~preferred center z, the most salient candidate for subsequent utter* ances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL27">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> transition rules.  </section>
<citcontext>
<prevsection>
<prevsent> that is, the l~n. guages tudied (english and italian) may be suf-ficiently flexible that there is usually some way to realise 6 as subj (or first-mention) but on the other hand the same 6 can only be main-tained for a- finite n,,mher of utterances.
</prevsent>
<prevsent>i suggmtthat these results hould be treated with some caution since it is not dear that the authors have the same assumptions about the claims of ct or that what they are testing di-rectly reflects formulations of ct in the more theoretical literature.
</prevsent>
</prevsection>
<citsent citstr=" P93-1010 ">
for instance passoneau (1998) refers to two variants of ct:  version  based on brennan et al (1987) <papid> P87-1022 </papid>and ~version  taken from kameyama et al  (1993).<papid> P93-1010 </papid></citsent>
<aftsection>
<nextsent>pas- soneau does n~t address the issue of direct vs indirect realisation and it appears from the ex-amples given that she only takes account of en-tities realised by full np or (possibly null) pronoun.
</nextsent>
<nextsent>the analysis according to version results in count of 52% null transitions, i.e. no cb, which gives the impression that ct is in fact rather poor measure of coherence, it is probable that higher measure might have been obtained if passoneau had allowed entities to be added to the u/ by inference, as dis-cussed in (brennan et al  op cit.).
</nextsent>
<nextsent>it is of course impossible to verify this without access to the original texts, but it is instructive to consider the-following example from strube and hahn (1996): (<papid> P96-1036 </papid>1) a. ein reserve-batteriepack vereorgt den 316lt ca.</nextsent>
<nextsent>2 minuten mit strom.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL45">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> arch tec ture.  </section>
<citcontext>
<prevsection>
<prevsent>the result is as follows: ui : cb a, cp = u2:cb=a, cp=b u3 : cb b, cp = u4 : cb = b, cp = in terms of the conventional transitions this works out as u~/u2: ret^in u2/u3: smooth shift us/u~: continua this is consistent with strube and hahn (1996) <papid> P96-1036 </papid>observation that  ii~rain transition ideally predicts smooth ssw in the follow-ing utterance .</prevsent>
<prevsent>brenuan et al (1987) make very similar claim: computational system for 9e-em- tion would try to plan retention as signal of an impending shift, so that after retention, shift would be pre-ferred rather than continuation.</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
grosz et al (1995) <papid> J95-2003 </papid>give the following example of the ~am - shift pattern: (5) a. john has had trouble arranging his va- cation.</citsent>
<aftsection>
<nextsent>b. he (cb; john) cannot find anyone to take over his responsibilities.
</nextsent>
<nextsent>c. he (c/~, john) called up mike yester-day to work out plan.
</nextsent>
<nextsent>conti nub d. mike has annoyed him (cb; john) lot recently, lt~rain e. he (cb; mike) called john at 5 am on friday last week.
</nextsent>
<nextsent>smrr under the approach outlined here, which as-sumes that the cb is independently designated, ? the system does not needto plan particular transition types or even to know about them; the desired effects come about as result of io- cal decisions by the sentence planner using in-formation from the text pl-nner.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL50">
<title id=" W99-0109.xml">cb or not cb centering theory applied to nlg </title>
<section> arch tec ture.  </section>
<citcontext>
<prevsection>
<prevsent>this is an area for future research.
</prevsent>
<prevsent>it is intended that the procedures described in.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
this  paper will be implemented in iconoclast, an authoring tool which enables domain experts to create knowledge base through sequence of interactive choice-- and generates hiexarchi- tally structured text according to various tylis- tic constraints (see power and scott 1998).<papid> P98-2173 </papid></citsent>
<aftsection>
<nextsent>acknowledgements this work was carried out as par of the gnome project (generating nominal expres- sions) which is collaboration between itri in the university of brighton and the h~tc in the ~nlversities of edinburgh and durham, funded by the epsrc under grant reference gr/l51126.
</nextsent>
<nextsent>i would like to thank itrx and gnome colleagues for helpful feedback, particularly christy 1)o- ran, renate henschel, richard power and kees van deemter, as well as two anonymous referees.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL51">
<title id=" W99-0208.xml">coreference resolution in dialogues in english and portuguese </title>
<section> the annotation of coreference cases.  </section>
<citcontext>
<prevsection>
<prevsent>these categories include discourse topic for the dialogue; segment opic for every stretch of dialogue in which the topic is considered to he the same, according to specific procedures; sub segment topic, if further division within segment is needed for the appropriate modelling of topicality; and both global and local thematic elements, which are salient discourse entities related to the topics abovementioned.
</prevsent>
<prevsent>as antecedents may also be discourse chunks of varying length, these same categories were used to classify such antecedents predicates of given topical role thought be the dominant entity within the discourse chunk.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
the aim of this attribute is to use the often mentioned relationship between topicality and coreference (see grosz and sidner 1986) <papid> J86-3001 </papid>for operational purposes.</citsent>
<aftsection>
<nextsent>this classification does not claim to be the actual key for the modelling of topicality in dialogues from psycho linguistic point of view.
</nextsent>
<nextsent>it does claim, however, to be useful tool for the resolution of particularly hard cases of coreference, in which the antecedent is not the nearest syntactically appropriate candidate, as will be shown in section 3.
</nextsent>
<nextsent>the topical roles are assigned on the basis of frequency, distribution and order of appearance.
</nextsent>
<nextsent>this information is used in conjunction with an adaptation for dialogues of hoey method (hoey 1991) to establish patterns of lexis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL52">
<title id=" W99-0208.xml">coreference resolution in dialogues in english and portuguese </title>
<section> the annotation of coreference cases.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical analysis showed thus that the classification model was adequate to represent the anaphora world.
</prevsent>
<prevsent>moreover, it became clear that the attribute named as processing strategy yielded the highest information gain, acting as link between the type of anaphor and the other two attributes which classify the antecedent.
</prevsent>
</prevsection>
<citsent citstr=" W97-1312 ">
therefore, the type of anaphor in itself, which could be mapped from pos tags or, in some cases, skeleton parsing (see mitkov 1997), <papid> W97-1312 </papid>only became truly useful information for the resolution of the anaphoric reference when associated to the definition of processing strategy.</citsent>
<aftsection>
<nextsent>this made of course psycho linguistic sense, as it is not difficult to infer from corpus data that the same anaphor (such as it or that) may appear in contexts that lead to distinct processing demands for their resolution.
</nextsent>
<nextsent>the al theory is made up of series of entries for each type of anaphor.
</nextsent>
<nextsent>entries contain 56 instructions organised in an algorithm-like form to check the applicability of all possible processing strategies, relying on information taken from the training set.
</nextsent>
<nextsent>the initial information considered is the probability of occurrence for each processing strategy and the two other attributes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL53">
<title id=" W99-0907.xml">detecting sub topic correspondence through bipartite term clustering </title>
<section> the model: term subset coupling by.  </section>
<citcontext>
<prevsection>
<prevsent>46 for obtaining subset coupling, we apply clustering methods.
</prevsent>
<prevsent>quite few previous works investigated the idea of identifying semantic substances with term clusters.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
term clustering methods are typically based on the statistics of term co-occurrence within word window, or within syntactic constructs (e.g. pereira et al , 1993).<papid> P93-1024 </papid></citsent>
<aftsection>
<nextsent>the notion pairwise clustering refers to clustering established, as in the present study, on previous assessment of term similarity values process often based by itself on term co-occurrence data (e.g. lin, 1999).
</nextsent>
<nextsent>a standard pairwise clustering problem can be represented by weighted graph, where each node stands for data point and each edge is weighted according to the degree of similarity of the nodes it connects.
</nextsent>
<nextsent>a (hard) clustering procedure produces partition of the graph nodes to disjoint connected components forming cluster configuration.
</nextsent>
<nextsent>our setting is special in that it considers only similarity values referring to term pairs from two distinct text fragments, such as  attendant -  manager  in the example above, but not   attendant -  minister .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL55">
<title id=" W98-1307.xml">learning finite state models for language understanding </title>
<section> sub sequential transduction.  </section>
<citcontext>
<prevsection>
<prevsent>i i i \[\] \[\] \] \[\] \[\] \[\] \[\] \[\] \[\] \[\] \[\] \[\] \[\] | 70 two ssts are equivalent if they perform the same input-output mapping.
</prevsent>
<prevsent>among equivalent ssts there always exists one that is canonical.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
this transducer always adopts an  onward  form, in which the output sub strings are assigned to the edges in such way that they are as  close  to the initial state as they can be (see oncina et al, 1993 \[15\], reutenauer, 1990 \[22\]; for recent re elaboration these concepts see mohri, 1997 \[<papid> J97-2003 </papid>13\]).</citsent>
<aftsection>
<nextsent>on the other hand, any finite (training) set of input-output pairs of strings can be properly represented asa tree sub sequential transducer (tst), which can then be easily converted into corresponding onward tree 8ubsequential transducer (otst).
</nextsent>
<nextsent>fig.1 (left and center) illustrates these concepts (and construction), which are the basis of the so-called onward snb sequential transducer inference algorithm (ostia), by oncina \[14, 15\].
</nextsent>
<nextsent>given an input-output training sample t, the osti algorithm works by merging states in the otst(t) as follows \[15\]: all pairs of states of otst(t) are orderly considered level by level, starting at the root, and, for each of these pairs, the states are tentatively merged.
</nextsent>
<nextsent>if this results in non-deterministic state, then an attempt is made to restore determinism by reeursively pushing-back some output sub strings towards the leaves of the transducer (i.e., partially undoing the onward construction), while performing the necessary additional state merge operations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL56">
<title id=" W99-0505.xml">towards a meaning full comparison of lexical resources </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>the mapping from wordnet hector senses senseval provides  gold standard  against wluch to judge our ability to compare lexlcal resources the  gold standard  is provided through aword overlap analysis (with and without stop list) for flus mapping, achieving at most 36 percent correct mapping (inflated by 9 percent from  empty  assignments) an alternauve component tal analysis of the defimtaons, using syntacuc, collocatmnal, and semantac component and relation identification (through the use ofdefimng patterns integrated seamlessly mto the parsing thclaonary), provides an almost 41 percent correct mapping, with an additaonal 4 percent by recogmzmg semantic omponents not used in the senseval mapping defimtion sets of the senseval words from three pubhshed thclaonanes and dorr lextcal knowledge base were added to wordnet and the hector database to exanune the nature of the mapping process between defimtton sets of more and less sco\[~e the tecbauques described here consutute only an maaal implementation the componenual nalysis approach and suggests that considerable further improvements can be aclueved
</prevsent>
</prevsection>
<citsent citstr=" P92-1054 ">
the difficulty of companng lemcal resources, long s~gnfficant challenge in computauonal hnguistlcs (atlans, 1991), came to the fore in the recent senseval competatton (iolgarnff, 1998), when some systems that relied heavily on the wordnet (miller, et al 1990) sense inventory were faced with the necessity of using another sense inventory (hecto0 hasty solutaon to the problem was the   development of map between the two inventories, but some part~cipants expressed concerns that use of flus map may have degraded their performance toan unknown degree although there were disclaimers about he wordnet-hector map, it nonetheless tands as usable gold standard for efforts to compare lexical resources moreover, we have usable baseline (a word overlap method suggested (lesk, 1986)) against which to compare whether we are able to make improvements the mapping (since flus method has been shown to perform not as well as expected (krovetz, 1992)) <papid> P92-1054 </papid>we first describe the lextcal resources used the study (hector, wordnet, other dicuonanes, and lex~cal knowledge base), first characterizing them in terms ofpolysemy and the types of leracal mformauon each contmns (syntacuc properties and features, emantac components and relauons, and collocauonal properties) we then present results of perfornung the word overlap analysis of the 18 verbs used senseval, analyzing the definitions wordnet and hector we then expand our analysis to include other dictionaries we describe our methods of analysis, particularly the methods of parsing defimtaons and identff)qng semantic relations (semrels) based on defimng patterns, essentially takang first steps implementing the program described by atkms and focusmg on the use of meamng  full mformataon rather than statistical mformauon we identify the results that have been achieved thus far and outline further steps that may add more  meanmg  to the analysis iall analyses described this paper were performed automatically using functlonahty incorporated dimap (dictionary maintenance programs) (available for immediate download at (cl research, 1999a)) this includes automatac extracuon of wordnet reformation for the selected words (mtegrated mdimap) hector defimtlons were up loaded into dimap dicuonanes after use of conversmn program defimtlons for other 30 the lexical resources tlus analysis focuses on the mmn verb senses used in senseval (not ichoms and phrases), specifically the followmg amaze, band, bet, bother, bury, calculate, consume, derive, float, hurdle, invade, promise, sack, sanction, scrap, seize, shake, slight the hector database used in senseval consists of tree of senses, each of which contains defimttons, syntactic properties, example usages, and  clues  (collocational information about he syntactic and semantic enwronment in wluch word appears in the spectfic sense) the wordnet database contmns synonyms (synsets), perhaps defimtton or example usages (gloss), some syntactic mformauon (verb frames), hypernyms, hyponyms, and some other semrels (entails, causes) to extend our analysis in order to look at other issues of lexacal resource comparison, we have included the defirauons or leracal information from the following additional sources ? webster 3 ra new international dictionary (w3) ? oxford advanced l.earners d~ctlonary (oald) ? american hentage dlcuonary (ai-id) ? dorr lexacal knowledge base (dorr) we used only the defimuons from w3, oald, and ahd (which also contmn sample usages and some collocattonal information the form of usage notes, not used at the present tame) dorr database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components, frequently identifying accompanying preposmons (olsen, et al 1998) the following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght average polysemy o 1 2 4 2 3 1 i 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 6 2 10 5 5 4 7 4 4 4 6 3 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 1 2</citsent>
<aftsection>
<nextsent>1 3 4 4 8 1
</nextsent>
<nextsent>3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 0 57 37 120 62 34 22 word overlap analysis we first estab hsh baseline for automatic replication of the lexicographer mappmg from wordnet 1 6 to hector, using s~mple word overlap analysis mular to (lesk, 1986) the lextcographer mapped the 66 wordnet senses (each synset which test occurred) into 102 hector senses total of 86 assignments were made, 9 wordnet senses were gwen no assignments, 40 recewed exactly one, and 17 senses received 2 or 3 asssgnments the wordnet senses contained 348 words (about half of wluch were common words appeanng on our stop list, which contained 165 words, mostly preposmons, pronouns, and conjunctions) the hector senses elected the word overlap analysis contained about 960 words (all hector senses contained 1878 words) we performed strict word overlap analysts (with and wsthout stop hst) between tile definluons in wordnet and the hector senses, that is, we did not attempt to ldenttfy root forms of inflected words we took each word a wordnet sense and determined whether ~t appeared in hector sense, we selected hector sense based on the highest percentage ofwords over all hector senses an 31 empty selection was made ff all the words in the wordnet sense did not appear in any hector sense, only content words were considered when the stop hst was used for example, for bet, wordnet sense 2 (stake (money) on the outcome of an issue) mapped into hector sense 4 ((of person) to risk (a sum of money or property) thts way) in this case, there was an overlap on two words (money, 039 in the hector defimtlon (0 13 of its 15 words) without he stop list when the stop list was invoked, there was an overlap of only one word (money, 0 07 of the hector defimtion) in this case, the lexicographer had made three assignments (hector senses 2, 3, and 4), our scoring method treated flus as only 1 out of 3 correct (not using the relaxed method employed in senseval of treating flus as completely correct) without he stop hst, our selections matched the lexicographer in 28 of 86 cases (32 6%), using the stop list, we were successful in 31 of 86 cases (36 1%) the improvement arising when the stop list was used is deceptive, where 8 cases were due to empty assignments ( that only 23 cases, 26 7%, were due to matching content words) overall, only 41 content words were involved in these 23 successes when the stop list was used, an average of 8 content words to summanze the word overlap analysis (1) despite ncher set of defimtions in hector, 9 of 66 wordnet senses (13 6%) could not be assigned, (2) despite the greater detail in hector senses compared to wordnet senses (2 8 times as many words), only 1 8 content words participated in the assignments, and (3) therefore, the defimng vocabulary between these two definition sets seems to be somewhat divergent although it might appear as if the word overlap analysis does not perform well, this is not the case the analysis provides abroad overview of the defimuon comp anson process between two definmon sets and frames deeper analysis of the differences moreover, it appears that the accuracy of  gold standard  mapping is not crucially important the quality of the mapping may help frame the subsequent analysis more precisely, but it seems ufficient that any reasonable mapping will suffice this will be discussed further after presenting the results of the component lal nalysis of the defimtlons 32 meaning-full analysis of definitions the deeper analysis of the mapping between two defimtion sets relies primarily on two major steps (1) parsing definitions and using defimng patterns to identify semrels present the definitions and (2) relaxing values to these relations by allowing  synonymic  substitution (using wordnet) thus, for example, ffwe identify hypernyms or instruments from parsing adefimtion, we would say that he defimtions are  equal  not just ffthe hypernym or instrument is the same word, but also lf the hypernyms or instruments are members of the same synset this approach is based on the finding (litkowski, 1978) that dictionary induces semantic network where nodes represent  concepts  that may be lexicahzed and verbalized in more than one way this finding implies, in general, the absence of true synonyms, and instead the kind of  concept  embodied in wordnet synsets (with several lexical items and phraseologles) slmdar approach, parsing defimtlons and relaxing semrel values, was followed in (dolan, 1994) <papid> C94-2113 </papid>for clnstenng related senses w~thin single dictionary the ideal toward which this approach strives is complete identification of the meamng components included in defimtion the meaning components can include syntactic features and charactenstlcs (including subcategonzation patterns), semantm components (realized through identification of semrels), selectional restrictions, and couocational specifications the first stage of the analysis parses the definitions (cl research, 1999b, litkowski, to appear) and uses the parse results to extract (via defining patterns) semrels since definitions have many idiosyncrasies (that do not follow ordinary text), an important first step in this stage is preprocessmg the definition text to put it into sentence frame that facilitates the extraction of semrels 2 2note that the stop hst is not applicable to the definition parsing the parser is full-scale sentence parser, where prepositmns and other words on the stop list are necessary for successful parsing moreover, inclusion of the prepositions cmcml to the method, since they are the bearers of much semrel information the extractmn of semrels examines the parse results, e, tree whose mtermedaate nodes represent non-ternunals and whose leaves represent the lextcal atems that compnse the defimuons, where any node may also include annotations such as characterizations of number and tense for all noun or verb defimttons, flus includes identification of the head noun (with recogmtton of empty  heads) or verb, for verbs, we signal whether the defimtaon contmned any select tonal restnctmus (that as, pamcular parenthesazed xpressaons) for the subject and object we then exanune preposattonal phrases in the defimuon and deterrmne whether we have  defining pattern  for the preposauon whach we can use as mdacauve of partacular semrel we also identify adverbs the parse tree and look these up in wordnet adentffy an adjecuve synset from wluch they are derived (if one is gwen) the defimng pattems are actually part of the dictionary used by the parser that is, we do not have to develop specafic routines to look for speclfic patterns defimng pattern ~s regular express aon that arlaculates syntactac pattern to be matched thus, to recograze  manner  semrel, we have the fouowmg entry for   m(dpat((~ rep0 l(det(0)) adj manner(0) st(manner)))) this allows us to recognize   as possibly gwmg rise to  manner  component, where we recogmze   (the tdde, which allows us to specify partacular elements before the   as well), vath noun phrase that cons asts of 0 or 1 determiner, an adjectwe, and the lateral  manner  the  0 ? after the detenmner and the hteral mdacate that these words are not copied into the value for  manner  role, so that the value to the  manner  semrel becomes only the adjectwe that as recogmzed the second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons this process follows the general methodology used senseval (lltkowska, to appear) specifically, the defimtaon comparison, we first exanune xclusaon cntena to rule out specific mappings these criteria include syntacuc proper ues (e g, verb sense that is only transluve cannot map into one that is only mtransrave) and collocataonal proper taes (eg, sense that is used with parucle cannot map into one that uses different particle) at the present tune, these are used only rmmmally 33 we next score each viable sense based on rots semrels we increment the score ff the senses have common hypernym or if sense hypernyms belong ? to the same synset as the other sense hypernyms if parucular sense con~ns large number of synonyms (that as, no different iae on the hypernym) and they overlap consaderably the synsets they evoke, the score can be increased substanually currently, we add 5 points for each match 3 we increment the score based on common semrels in tins amtml tmplementauon, wehave defimng patterns (usually qmte nummal) for recogmzmg instrument, means, location, purpose, source, manner, has-constituents, has-members, is-part-of, locale, and goal 4 we increment the score by 2 points when we have common semrel and then by another 5 points when the value is ~dentacal orm the same synset after all pos sable increments othe scores have been made, we then select he sense(s) w~th the lughest score finally, we compare our selecuon with that of the gold standard to assess our mapping over all senses another way an wluch our methodology follows the senseval process as that at proceeds incrementally thus, ~t ms not necessary to have  final  perfect parse and mapping rouune we can make conunual refinements atany stage of the process and exarmne the overall effect as senseval, we may make changes to deal wath particular phenomenon with the result hat overall performance chnes, but w~th sounder basis for making subsequent amprovements results of component ial analysis the  gold standard  analysis involves mapping 66 wordnet senses with 348 words into 102 hector.</nextsent>
<nextsent>senses with 1878 words using the method described above, we obtained 35 out of 86 correct 3at the present tame, we use wordnet adentffy semreis we envaslon usmg the full semanlac network created by parsing all dlcuonary defimtaons thas would include richer set of semrels than currently included wordnet 4the defimng patterns are developed by hand we have only just begun this effort, so the current set ms somewhat impoverished mappmgs (407%), shght improvement over the 31 correct assignments mg the stop-last word overlap techmque however, as mentioned above, the stop- hst techmque had aclueved 8 of its successes by mat clung null assignments consad ered on tlus basins, ~t seems that the component aal analysis techmque provides ubstantial ~mprovement in addition, our technique  erred  on 4 cases by malang assagnments where none were made by the leracographer we suggest that these cases do con~n some common elements of meaning and may conceivably not be construed as errors the mapping from wordnet hector had relatavely few empty mappings, enses for wtuch it was not pos sable to make an assignment these are the cases where at appears that the chetmnanes do not overlap and thus prowde tentative mdacataon where two dictionaries may have different coverage the cases of multiple assignments mchcate the degree ofamblgmty the mapping the average both darecuons between hector and wordnet were donunated by the mabdaty to obtain good dascnnunatton for the word  semze  thus, tlus method identifies individual words where the &scnnunatwe; ablhty needs to be further efined ? perhaps more importantly, the componentml analysis method exploits consaderably more wordnet - hector ? mformauon than the word overlap methods whereas the stop-hst word overlap mapping was ? based on only 41 content words, the componenual ~ approach (in the selected mappings) had 228 hits in ~ .~ ? developing ats scores, with only small number of ~ .~ ~ defining patterns comparison of dictionaries tel ~3 0 3 we next exanuned the nature of the mterrelalaons between parrs of chctaonanes w~thout use of  gold standard  to assess the process of mapping for t/us purpose, we mapped both &recttons; between the paars {wordnet, hector}, {w3, oald}, and {w3, ahd we exanune dorr lexacal knowledge base for the amphcatlons it may have the mapping process neither wordnet nor hector are properly v~ewed as chcuonanes, ince there was no mtenuon to pubhsh them as such wordnet  glosses  are generally smaller (53 words per sense) compared to hector (184 words per sense), whach contains many words specff3nng selectmnal restnct~ons on the subject and object of the verbs hector was used primarily for large-scale sense tagging project the three formal d~ctmnanes were subject to rigorous pubhslung and style standards the average number of words per sense were 87 (oald), 7 1 (ahd), and 9 9 (w3), w~th an average of 3 4, 62, and 120 senses per word each table shows the average number of senses being mapped, the average number of assignments the target dlctmnary, the average number of senses for which no assagnment could be made, the average number of muluple assignments per word, and the average score of the assignments hat were made wn-hector 37 47 06 17 119 hector-wn 57 64 14 22 113 these points are further emphasized the mapping between w3 and oald, where the disparity between the empty and muluple assagnments indicate that we are mapping between dictionaries qmte disparate this tends to be the case not only for the enure set of words, but also is evident for individual words where there is considerable d~spanty mthe number of senses, wtuch then dominate the overall dlspanty thus, for example, w3 has 41 defimuons for  float , while oald has 10 we tend to be unable to find the specific sense going from w3 to oald, because at is likely that we have many more specific defimtlons that are not present in the other direction, we are hkely to have considerable ambiguity and multiple assignments w3-oald oald-w3 w3 - oald 120 78 60 18 99 34 60 07 32 86 34 between w3 and ahd, there ss less overall das panty between the defimtaon sets, although since w3 is tmabndged, we stall have relatavely lugh number of senses w3 that do not appear to be present ahd finally, it should be noted that the scores for the published ictaonanes tend to be little lower than for wordnet and hector tlus reflects the hkehhood that we have not extracted as much mformataon as we dad parsing and analyzmg the defimtaon sets used senseval w3 - ahd oj ?  o w3-ahd 120 115 40 36 90 ahd-w3 6 2 9 1 1 2 4 1 9 1 we next considered dorr lexacal database we first transformed her theta grids ? to syntactic spectflcataons (transttave or lntransmttve) and identtficataon semreis (e g, where she identified an instr component, we added such semrel to the dimap sense) we were able to identify mappmg from wordnet her senses for two words ( float  and  shake ) for wluch dorr has several entries however, smce she has considerably more semanuc components han we are currently able to recogmze, we dad not pursue this avenue any further at flus time more important than just mappmg between two words, dorr data mda cates the posstbday of further exploitation of richer set of semanuc omponents spectfically, as reported (olsen, et al 1998), descnbmg procedures for automatically acqumng thematic grids for mandann chinese, ~t was noted that  verbs that incorporate hemauc elements their meamng would not allow that element to appear the complement ructure  thus, by usmg dorr thematic grids when verb are parsed defimtaons, it ~s possible to ~dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themauc grid (complement or subcategonzataon pattern) for the defimendum the transmiss~on semantic omponents tothe thematic gnd ~s also reflected overtly many defimtlons for example, shake has one definition,  to bnng to specified condatton by or as ffby repeated qmck jerky movements  we would thus expect that the thema tac grid for this defimtaon should include  goal  and, deed, dorr database has two senses whch reqmre  goal  as part of their thematic grid smularly, for many defimtaons the sample set, we ~dentlfied source defimng pattern based on the word  from,  frequently, the object of the preposmon was the word  source  ttseff, mdacatmg that the subcategonzauon, properties of the defimendum should elude source component discussion wlule the improvement mapping by using the component aal analysis techmque (over the word overlap methods) is modest, we consider these results qmte slgmficant wew of the very small number of defimng patterns we have implemented most of the improvement stems from the word substatuuon pnnclple described earlier (as ewdenced by the preponderance of 5 point scores) this techmque also provides mechamsm for bnngmg back the stop words, wz, the preposmons, wluch are the careers of mformatmn about semrels (the 2 point scores ) the more general conclusion (from the word subsutuuon) is that the success arises from no longer considenng defimtmn ~solation the proper context for word and its defimtions consists not .lust of the words that make up the definition, but also the total semantac network represented bythe dictaonary we have aclueved our results by explomng only small part of that network we have moved only few steps ? to that network beyond the mdawdual words and their definitions we would expect hat further expansmn, first by the addon of further and ~mproved semrel defining patterns, and second, through the identaficataon fmore pnmmve semanuc components, will add considerably toour abflay to map between lexacal resources we also expect ~mprovements from consideration fother techniques, uch as attempts at ontology ahgnment (hovy, 1998) although tile definition analysis provlded here was performed on definmons with?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL57">
<title id=" W99-0505.xml">towards a meaning full comparison of lexical resources </title>
<section> 1  </section>
<citcontext>
<prevsection>
<prevsent>the difficulty of companng lemcal resources, long s~gnfficant challenge in computauonal hnguistlcs (atlans, 1991), came to the fore in the recent senseval competatton (iolgarnff, 1998), when some systems that relied heavily on the wordnet (miller, et al 1990) sense inventory were faced with the necessity of using another sense inventory (hecto0 hasty solutaon to the problem was the   development of map between the two inventories, but some part~cipants expressed concerns that use of flus map may have degraded their performance toan unknown degree although there were disclaimers about he wordnet-hector map, it nonetheless tands as usable gold standard for efforts to compare lexical resources moreover, we have usable baseline (a word overlap method suggested (lesk, 1986)) against which to compare whether we are able to make improvements the mapping (since flus method has been shown to perform not as well as expected (krovetz, 1992)) <papid> P92-1054 </papid>we first describe the lextcal resources used the study (hector, wordnet, other dicuonanes, and lex~cal knowledge base), first characterizing them in terms ofpolysemy and the types of leracal mformauon each contmns (syntacuc properties and features, emantac components and relauons, and collocauonal properties) we then present results of perfornung the word overlap analysis of the 18 verbs used senseval, analyzing the definitions wordnet and hector we then expand our analysis to include other dictionaries we describe our methods of analysis, particularly the methods of parsing defimtaons and identff)qng semantic relations (semrels) based on defimng patterns, essentially takang first steps implementing the program described by atkms and focusmg on the use of meamng  full mformataon rather than statistical mformauon we identify the results that have been achieved thus far and outline further steps that may add more  meanmg  to the analysis iall analyses described this paper were performed automatically using functlonahty incorporated dimap (dictionary maintenance programs) (available for immediate download at (cl research, 1999a)) this includes automatac extracuon of wordnet reformation for the selected words (mtegrated mdimap) hector defimtlons were up loaded into dimap dicuonanes after use of conversmn program defimtlons for other 30 the lexical resources tlus analysis focuses on the mmn verb senses used in senseval (not ichoms and phrases), specifically the followmg amaze, band, bet, bother, bury, calculate, consume, derive, float, hurdle, invade, promise, sack, sanction, scrap, seize, shake, slight the hector database used in senseval consists of tree of senses, each of which contains defimttons, syntactic properties, example usages, and  clues  (collocational information about he syntactic and semantic enwronment in wluch word appears in the spectfic sense) the wordnet database contmns synonyms (synsets), perhaps defimtton or example usages (gloss), some syntactic mformauon (verb frames), hypernyms, hyponyms, and some other semrels (entails, causes) to extend our analysis in order to look at other issues of lexacal resource comparison, we have included the defirauons or leracal information from the following additional sources ? webster 3 ra new international dictionary (w3) ? oxford advanced l.earners d~ctlonary (oald) ? american hentage dlcuonary (ai-id) ? dorr lexacal knowledge base (dorr) we used only the defimuons from w3, oald, and ahd (which also contmn sample usages and some collocattonal information the form of usage notes, not used at the present tame) dorr database contains thematic grids wluch characterize the thematic roles of obligatory and optional semanuc components, frequently identifying accompanying preposmons (olsen, et al 1998) the following table identities the number of senses and average overall polysemy for each of these resources dictionaries were entered by hand word amaze band bet bother bury calculate consume denve float hurdle invade pronuse sack sanction scrap seize shake shght average polysemy o 1 2 4 2 3 1 i 4 4 2 5 5 7 6 9 7 12 6 14 5 5 5 10 9 6 6 8 8 6 5 15 5 16 4 41 14 6 2 10 5 5 4 7 4 4 4 6 3 3 1 3 3 11 6 21 13 8 8 37 17 1 1 6 3 1 2</prevsent>
<prevsent>1 3 4 4 8 1</prevsent>
</prevsection>
<citsent citstr=" C94-2113 ">
3 1 3 2 10 5 1 0 3 1 3 2 2 0 1 1 1 0 7 1 7 12 0 57 37 120 62 34 22 word overlap analysis we first estab hsh baseline for automatic replication of the lexicographer mappmg from wordnet 1 6 to hector, using s~mple word overlap analysis mular to (lesk, 1986) the lextcographer mapped the 66 wordnet senses (each synset which test occurred) into 102 hector senses total of 86 assignments were made, 9 wordnet senses were gwen no assignments, 40 recewed exactly one, and 17 senses received 2 or 3 asssgnments the wordnet senses contained 348 words (about half of wluch were common words appeanng on our stop list, which contained 165 words, mostly preposmons, pronouns, and conjunctions) the hector senses elected the word overlap analysis contained about 960 words (all hector senses contained 1878 words) we performed strict word overlap analysts (with and wsthout stop hst) between tile definluons in wordnet and the hector senses, that is, we did not attempt to ldenttfy root forms of inflected words we took each word a wordnet sense and determined whether ~t appeared in hector sense, we selected hector sense based on the highest percentage ofwords over all hector senses an 31 empty selection was made ff all the words in the wordnet sense did not appear in any hector sense, only content words were considered when the stop hst was used for example, for bet, wordnet sense 2 (stake (money) on the outcome of an issue) mapped into hector sense 4 ((of person) to risk (a sum of money or property) thts way) in this case, there was an overlap on two words (money, 039 in the hector defimtlon (0 13 of its 15 words) without he stop list when the stop list was invoked, there was an overlap of only one word (money, 0 07 of the hector defimtion) in this case, the lexicographer had made three assignments (hector senses 2, 3, and 4), our scoring method treated flus as only 1 out of 3 correct (not using the relaxed method employed in senseval of treating flus as completely correct) without he stop hst, our selections matched the lexicographer in 28 of 86 cases (32 6%), using the stop list, we were successful in 31 of 86 cases (36 1%) the improvement arising when the stop list was used is deceptive, where 8 cases were due to empty assignments ( that only 23 cases, 26 7%, were due to matching content words) overall, only 41 content words were involved in these 23 successes when the stop list was used, an average of 8 content words to summanze the word overlap analysis (1) despite ncher set of defimtions in hector, 9 of 66 wordnet senses (13 6%) could not be assigned, (2) despite the greater detail in hector senses compared to wordnet senses (2 8 times as many words), only 1 8 content words participated in the assignments, and (3) therefore, the defimng vocabulary between these two definition sets seems to be somewhat divergent although it might appear as if the word overlap analysis does not perform well, this is not the case the analysis provides abroad overview of the defimuon comp anson process between two definmon sets and frames deeper analysis of the differences moreover, it appears that the accuracy of  gold standard  mapping is not crucially important the quality of the mapping may help frame the subsequent analysis more precisely, but it seems ufficient that any reasonable mapping will suffice this will be discussed further after presenting the results of the component lal nalysis of the defimtlons 32 meaning-full analysis of definitions the deeper analysis of the mapping between two defimtion sets relies primarily on two major steps (1) parsing definitions and using defimng patterns to identify semrels present the definitions and (2) relaxing values to these relations by allowing  synonymic  substitution (using wordnet) thus, for example, ffwe identify hypernyms or instruments from parsing adefimtion, we would say that he defimtions are  equal  not just ffthe hypernym or instrument is the same word, but also lf the hypernyms or instruments are members of the same synset this approach is based on the finding (litkowski, 1978) that dictionary induces semantic network where nodes represent  concepts  that may be lexicahzed and verbalized in more than one way this finding implies, in general, the absence of true synonyms, and instead the kind of  concept  embodied in wordnet synsets (with several lexical items and phraseologles) slmdar approach, parsing defimtlons and relaxing semrel values, was followed in (dolan, 1994) <papid> C94-2113 </papid>for clnstenng related senses w~thin single dictionary the ideal toward which this approach strives is complete identification of the meamng components included in defimtion the meaning components can include syntactic features and charactenstlcs (including subcategonzation patterns), semantm components (realized through identification of semrels), selectional restrictions, and couocational specifications the first stage of the analysis parses the definitions (cl research, 1999b, litkowski, to appear) and uses the parse results to extract (via defining patterns) semrels since definitions have many idiosyncrasies (that do not follow ordinary text), an important first step in this stage is preprocessmg the definition text to put it into sentence frame that facilitates the extraction of semrels 2 2note that the stop hst is not applicable to the definition parsing the parser is full-scale sentence parser, where prepositmns and other words on the stop list are necessary for successful parsing moreover, inclusion of the prepositions cmcml to the method, since they are the bearers of much semrel information the extractmn of semrels examines the parse results, e, tree whose mtermedaate nodes represent non-ternunals and whose leaves represent the lextcal atems that compnse the defimuons, where any node may also include annotations such as characterizations of number and tense for all noun or verb defimttons, flus includes identification of the head noun (with recogmtton of empty  heads) or verb, for verbs, we signal whether the defimtaon contmned any select tonal restnctmus (that as, pamcular parenthesazed xpressaons) for the subject and object we then exanune preposattonal phrases in the defimuon and deterrmne whether we have  defining pattern  for the preposauon whach we can use as mdacauve of partacular semrel we also identify adverbs the parse tree and look these up in wordnet adentffy an adjecuve synset from wluch they are derived (if one is gwen) the defimng pattems are actually part of the dictionary used by the parser that is, we do not have to develop specafic routines to look for speclfic patterns defimng pattern ~s regular express aon that arlaculates syntactac pattern to be matched thus, to recograze  manner  semrel, we have the fouowmg entry for   m(dpat((~ rep0 l(det(0)) adj manner(0) st(manner)))) this allows us to recognize   as possibly gwmg rise to  manner  component, where we recogmze   (the tdde, which allows us to specify partacular elements before the   as well), vath noun phrase that cons asts of 0 or 1 determiner, an adjectwe, and the lateral  manner  the  0 ? after the detenmner and the hteral mdacate that these words are not copied into the value for  manner  role, so that the value to the  manner  semrel becomes only the adjectwe that as recogmzed the second stage of the analysis uses the populated lexacal database to compare senses and make the selectaons this process follows the general methodology used senseval (lltkowska, to appear) specifically, the defimtaon comparison, we first exanune xclusaon cntena to rule out specific mappings these criteria include syntacuc proper ues (e g, verb sense that is only transluve cannot map into one that is only mtransrave) and collocataonal proper taes (eg, sense that is used with parucle cannot map into one that uses different particle) at the present tune, these are used only rmmmally 33 we next score each viable sense based on rots semrels we increment the score ff the senses have common hypernym or if sense hypernyms belong ? to the same synset as the other sense hypernyms if parucular sense con~ns large number of synonyms (that as, no different iae on the hypernym) and they overlap consaderably the synsets they evoke, the score can be increased substanually currently, we add 5 points for each match 3 we increment the score based on common semrels in tins amtml tmplementauon, wehave defimng patterns (usually qmte nummal) for recogmzmg instrument, means, location, purpose, source, manner, has-constituents, has-members, is-part-of, locale, and goal 4 we increment the score by 2 points when we have common semrel and then by another 5 points when the value is ~dentacal orm the same synset after all pos sable increments othe scores have been made, we then select he sense(s) w~th the lughest score finally, we compare our selecuon with that of the gold standard to assess our mapping over all senses another way an wluch our methodology follows the senseval process as that at proceeds incrementally thus, ~t ms not necessary to have  final  perfect parse and mapping rouune we can make conunual refinements atany stage of the process and exarmne the overall effect as senseval, we may make changes to deal wath particular phenomenon with the result hat overall performance chnes, but w~th sounder basis for making subsequent amprovements results of component ial analysis the  gold standard  analysis involves mapping 66 wordnet senses with 348 words into 102 hector.</citsent>
<aftsection>
<nextsent>senses with 1878 words using the method described above, we obtained 35 out of 86 correct 3at the present tame, we use wordnet adentffy semreis we envaslon usmg the full semanlac network created by parsing all dlcuonary defimtaons thas would include richer set of semrels than currently included wordnet 4the defimng patterns are developed by hand we have only just begun this effort, so the current set ms somewhat impoverished mappmgs (407%), shght improvement over the 31 correct assignments mg the stop-last word overlap techmque however, as mentioned above, the stop- hst techmque had aclueved 8 of its successes by mat clung null assignments consad ered on tlus basins, ~t seems that the component aal analysis techmque provides ubstantial ~mprovement in addition, our technique  erred  on 4 cases by malang assagnments where none were made by the leracographer we suggest that these cases do con~n some common elements of meaning and may conceivably not be construed as errors the mapping from wordnet hector had relatavely few empty mappings, enses for wtuch it was not pos sable to make an assignment these are the cases where at appears that the chetmnanes do not overlap and thus prowde tentative mdacataon where two dictionaries may have different coverage the cases of multiple assignments mchcate the degree ofamblgmty the mapping the average both darecuons between hector and wordnet were donunated by the mabdaty to obtain good dascnnunatton for the word  semze  thus, tlus method identifies individual words where the &scnnunatwe; ablhty needs to be further efined ? perhaps more importantly, the componentml analysis method exploits consaderably more wordnet - hector ? mformauon than the word overlap methods whereas the stop-hst word overlap mapping was ? based on only 41 content words, the componenual ~ approach (in the selected mappings) had 228 hits in ~ .~ ? developing ats scores, with only small number of ~ .~ ~ defining patterns comparison of dictionaries tel ~3 0 3 we next exanuned the nature of the mterrelalaons between parrs of chctaonanes w~thout use of  gold standard  to assess the process of mapping for t/us purpose, we mapped both &recttons; between the paars {wordnet, hector}, {w3, oald}, and {w3, ahd we exanune dorr lexacal knowledge base for the amphcatlons it may have the mapping process neither wordnet nor hector are properly v~ewed as chcuonanes, ince there was no mtenuon to pubhsh them as such wordnet  glosses  are generally smaller (53 words per sense) compared to hector (184 words per sense), whach contains many words specff3nng selectmnal restnct~ons on the subject and object of the verbs hector was used primarily for large-scale sense tagging project the three formal d~ctmnanes were subject to rigorous pubhslung and style standards the average number of words per sense were 87 (oald), 7 1 (ahd), and 9 9 (w3), w~th an average of 3 4, 62, and 120 senses per word each table shows the average number of senses being mapped, the average number of assignments the target dlctmnary, the average number of senses for which no assagnment could be made, the average number of muluple assignments per word, and the average score of the assignments hat were made wn-hector 37 47 06 17 119 hector-wn 57 64 14 22 113 these points are further emphasized the mapping between w3 and oald, where the disparity between the empty and muluple assagnments indicate that we are mapping between dictionaries qmte disparate this tends to be the case not only for the enure set of words, but also is evident for individual words where there is considerable d~spanty mthe number of senses, wtuch then dominate the overall dlspanty thus, for example, w3 has 41 defimuons for  float , while oald has 10 we tend to be unable to find the specific sense going from w3 to oald, because at is likely that we have many more specific defimtlons that are not present in the other direction, we are hkely to have considerable ambiguity and multiple assignments w3-oald oald-w3 w3 - oald 120 78 60 18 99 34 60 07 32 86 34 between w3 and ahd, there ss less overall das panty between the defimtaon sets, although since w3 is tmabndged, we stall have relatavely lugh number of senses w3 that do not appear to be present ahd finally, it should be noted that the scores for the published ictaonanes tend to be little lower than for wordnet and hector tlus reflects the hkehhood that we have not extracted as much mformataon as we dad parsing and analyzmg the defimtaon sets used senseval w3 - ahd oj ?  o w3-ahd 120 115 40 36 90 ahd-w3 6 2 9 1 1 2 4 1 9 1 we next considered dorr lexacal database we first transformed her theta grids ? to syntactic spectflcataons (transttave or lntransmttve) and identtficataon semreis (e g, where she identified an instr component, we added such semrel to the dimap sense) we were able to identify mappmg from wordnet her senses for two words ( float  and  shake ) for wluch dorr has several entries however, smce she has considerably more semanuc components han we are currently able to recogmze, we dad not pursue this avenue any further at flus time more important than just mappmg between two words, dorr data mda cates the posstbday of further exploitation of richer set of semanuc omponents spectfically, as reported (olsen, et al 1998), descnbmg procedures for automatically acqumng thematic grids for mandann chinese, ~t was noted that  verbs that incorporate hemauc elements their meamng would not allow that element to appear the complement ructure  thus, by usmg dorr thematic grids when verb are parsed defimtaons, it ~s possible to ~dentffy where partacular semantac components are lexicahzed and which others are transnutted through to the themauc grid (complement or subcategonzataon pattern) for the defimendum the transmiss~on semantic omponents tothe thematic gnd ~s also reflected overtly many defimtlons for example, shake has one definition,  to bnng to specified condatton by or as ffby repeated qmck jerky movements  we would thus expect that the thema tac grid for this defimtaon should include  goal  and, deed, dorr database has two senses whch reqmre  goal  as part of their thematic grid smularly, for many defimtaons the sample set, we ~dentlfied source defimng pattern based on the word  from,  frequently, the object of the preposmon was the word  source  ttseff, mdacatmg that the subcategonzauon, properties of the defimendum should elude source component discussion wlule the improvement mapping by using the component aal analysis techmque (over the word overlap methods) is modest, we consider these results qmte slgmficant wew of the very small number of defimng patterns we have implemented most of the improvement stems from the word substatuuon pnnclple described earlier (as ewdenced by the preponderance of 5 point scores) this techmque also provides mechamsm for bnngmg back the stop words, wz, the preposmons, wluch are the careers of mformatmn about semrels (the 2 point scores ) the more general conclusion (from the word subsutuuon) is that the success arises from no longer considenng defimtmn ~solation the proper context for word and its defimtions consists not .lust of the words that make up the definition, but also the total semantac network represented bythe dictaonary we have aclueved our results by explomng only small part of that network we have moved only few steps ? to that network beyond the mdawdual words and their definitions we would expect hat further expansmn, first by the addon of further and ~mproved semrel defining patterns, and second, through the identaficataon fmore pnmmve semanuc components, will add considerably toour abflay to map between lexacal resources we also expect ~mprovements from consideration fother techniques, uch as attempts at ontology ahgnment (hovy, 1998) although tile definition analysis provlded here was performed on definmons with?
</nextsent>
<nextsent>a stogie language, the vanous meamng components m m m m 35 correspond to those used in an interhngua the use of the exuncuon method (developed morder to charactenze rbs another language, clunese) can frmtfully be applied here as well two further observauons about lus process can be made the first is that rchance on well- established semantic network such as wordnet ,s not necessary the componenual nalysis method rehes on the local neighborhood fwords the defimuons, not on the completeness of the network indeed, the network ~tsel?
</nextsent>
<nextsent>can be boot strapped based on the parsing results the method can work vath any semanuc network or ontology and may be used to refine or flesh out the network or ontology the second observation is that it is not necessary to have well-estabhshed  gold standard  any mapping vail do all that is necessary is for any mvesugator (lemcographer or not) to create judgmental mappmg the methods employed here can then quanufy ttus mapping based on word overlap analysis and then further examine tt based on the componenaal nalysis the componenual analysis method can then be used to exanune underlying subtle ues and nuances tn the defimuous, wluch lemcographer or analyst can then examine further detail to assess the mapping future work tlus work has marked the first ume that all the necessary mfrastructure has been combmed tn rudimentary form because of its rudimentary status, the opportumues for improvement are quite extensive in addluon, there are many opportumues for using the techmques descnbed here further nlp apphcatlons first, the techmques described here have immediate apphcabtllty as part of lexicographer workstauon when defimuons are parsed and semrels are zdenttfied, the resulung data structures can be apphed against corpus of instances for parucular words (as senseval) for improving word-sense disamblguauon the techmques will also permit comparing an entry vath itself to deternune the mterrelattonshtps among ~ts defimuons and of companng the defimuons of two  synonyms  to deternune the amount of overlap between them on defimtlon by defimuon bas~s although the analys,s here has focused on the parsing of defimuous, the development of defimng patterns clearly extends to generalized text parsing since the defimng patterns have been incorporated mto the same chcttonary used for parsing free text, the patterns can be used threctly to identify the presence of parucular semrels among sentenual consutuents we are working to integrate th~s funcuonahty into our word-sense &sambiguauon; techruques (both the defimng patterns and the semrels) even further, mt seems that mat clung defimng patterns in free text can be used for lextcal acquisition textual matenal that contains these patterns could concewably be flagged as providing defimuonal matenal which can then be compared to emstmg defimuons to assess whether their use ts cous,stent vath these defimuons, and ff not, at least to flag the inconsistency the tecluuques descnbed here can be apphed directly to the fields of ontology development and analysis of ternunologlcal databases for ontoiogles, vath or w~thout defimuons, the methods employed can be used to compare ntries dai erent ontologles based pnmanly on the relattous the ontology, both luerarclucal nd other for ternunologlcal databases, the methods descnbed here can be used to exanune the set of conceptual relauons lmphed by the defimtmus the defimuon parsing wall facd~tate he development of the termmolog~ca network tn the pamcular field covered by the database the componenual nalysts methods result a richer semantic network that can be used other apphcattous thus, for example, ~t possible to extend the leracal chatmng methods described (green, 1997), which are based on the semrels used wordnet the semrels developed with the component tal analysis method would provide additional detad available for apphcauon of lexlcal cohesion methods in particular, addtuonal relattous would penmt some structunng wmthm the individual leracal chams, rather than just consldenng each cham as an amorphous set (green, 1999) finally, we are currently investigating the use of the componenual nalysts techmque for mformauon extracuon the techmque identifies (from defimtlous) lots that can be used as slots or fields template generataon once these slots are identified, we wall be attemptmg to extract slot values from items large catalog databases (mdhons of items) 36 in conclusion, it would seem that, instead of paucity of tnformation allo vang us to compare lexmal resources, by bnngmg the full semantic network of the lexicon, we are overwhelmed with plethora of data acknowledgments would like to thank bonnie dorr, chnstiane fellbaum, steve green, ed hovy, ramesh knshnamurthy, bob krovetz, thomas potter, lucy vanderwende, and an anonymous reviewer for their comments on an earlier draft of this paper lttkowskl, c (to appear) senseval the cl research expenence computers and the humamttes mtller, a, beckwlth, r, fellbaum, c, gross, d, &amp; miller, j (1990) introduction to wordnet an on-hne lexical database lnternatwnal journal of lexicography, 3(4), 235-244 olsen, b, dorr, j, &amp; thomas, c (1998, 28-31 october) enhancmg automatic acqulsmon of thematic structure in large-scale lexacon for mandann chinese tlurd conference of the association for machine translation the americas, amta-98 langhorne, pa
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL58">
<title id=" W98-1415.xml">clause aggregation using linguistic knowledge </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>magic \[dalal et al, 1996, mekeown et al, 1997\] automatically generates multimedia briefings to describe the post-operative status of patient after undergoing coronary artery bypass graft (cabg) surgery.
</prevsent>
<prevsent>it uses the existing computerized information infrastructure in the operating rooms at columbia presbyterian medical center.
</prevsent>
</prevsection>
<citsent citstr=" A94-1002 ">
plandoc\[kukich et al, 1994, mckeown et al, 1994\] <papid> A94-1002 </papid>generates english summaries based on somewhat cryptic traces of the interaction between planning engineers and leis-plan tm.</citsent>
<aftsection>
<nextsent>it documents the timing, placement and cost of new facilities for routes in telephone networks.
</nextsent>
<nextsent>in section 2, we present corpus analysis to identify the complexity of the target output in magic.
</nextsent>
<nextsent>section 3 describes the semantic representation used in casper.
</nextsent>
<nextsent>details of hypo tactic operators are presented in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL59">
<title id=" W98-1415.xml">clause aggregation using linguistic knowledge </title>
<section> hypo tactic operators.  </section>
<citcontext>
<prevsection>
<prevsent>even though the lexicon is accessed twice in our system, casper prunes the search space drastically by delaying expensive detailed lexical decisions after it knows?
</prevsent>
<prevsent>about how many concepts are involved in the desired sentence.
</prevsent>
</prevsection>
<citsent citstr=" J97-2001 ">
efficiency issues in generation were also addressed in \[mcdonald et al, 1987, elhadad et al, 1997\].<papid> J97-2001 </papid></citsent>
<aftsection>
<nextsent>we will use an imaginary human resource report system for technical support eam as an example to illustrate our para tactic algorithm.
</nextsent>
<nextsent>the example shown in figure 4 has the following slots: pred, arc1, arc2, mod-beneficiary,.
</nextsent>
<nextsent>mod-time.
</nextsent>
<nextsent>we currently have two approaches to combine propositions using coordinate constructions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL60">
<title id=" W98-1415.xml">clause aggregation using linguistic knowledge </title>
<section> para tactic operators.  </section>
<citcontext>
<prevsection>
<prevsent>  1 the arg1 in second proposition  john  is deleted.
</prevsent>
<prevsent>due to limited space, we only describe the algorithm used in casper to produce sentences with coordinations.
</prevsent>
</prevsection>
<citsent citstr=" P98-2199 ">
for more detailed discussion with relevant linguistic motivations, please see \[shaw, 1998\].<papid> P98-2199 </papid></citsent>
<aftsection>
<nextsent>we have divided the algorithm into four stages~ where the first three stages take place in the sentence planner and the last stage takes place in the lexical chooser: stage 1: group propositions and order them according to their similarities while satisfying pragmatic and contextual constraints.
</nextsent>
<nextsent>stage 2: determine recurring elements in the ordered propositions that will be combined.
</nextsent>
<nextsent>stage 3: create sentence boundary when the combined clause reaches pre-determined thresholds.
</nextsent>
<nextsent>stage 4: determine which recurring elements are redundant and should be deleted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL61">
<title id=" W98-1415.xml">clause aggregation using linguistic knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>logical derivations were used to combine clauses and remove easily infer able clauses in \[mann and moore, 1980\].
</prevsent>
<prevsent>in such systems, ag-gregation decisions are made without lexical information.
</prevsent>
</prevsection>
<citsent citstr=" P95-1053 ">
newer systems, such as \[shaw, 1995, <papid> P95-1053 </papid>wanner and hovy, 1996, <papid> W96-0401 </papid>huang and fiedler, 1997\]: use sentence planner to make decisions at clause level between the strategic and tactical component.</citsent>
<aftsection>
<nextsent>with the exception of \[scott and desouza, 1990\] and \[robin, 1995\], most research in aggrega-tion did not transform clauses into modifiers, such as adjectives, pp, or relative clauses, in sys-tematic manner.
</nextsent>
<nextsent>\[scott.
</nextsent>
<nextsent>and desouza, 1990\] proposed heuristics for carrying out clause combining based on rst and specifically identified which rhetorical relations are appropriate for  embedding  : 145 which corresponds to our hypo tactic operators.
</nextsent>
<nextsent>we will incorporate rhetorical aggregation the future.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL63">
<title id=" W98-1415.xml">clause aggregation using linguistic knowledge </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>logical derivations were used to combine clauses and remove easily infer able clauses in \[mann and moore, 1980\].
</prevsent>
<prevsent>in such systems, ag-gregation decisions are made without lexical information.
</prevsent>
</prevsection>
<citsent citstr=" W96-0401 ">
newer systems, such as \[shaw, 1995, <papid> P95-1053 </papid>wanner and hovy, 1996, <papid> W96-0401 </papid>huang and fiedler, 1997\]: use sentence planner to make decisions at clause level between the strategic and tactical component.</citsent>
<aftsection>
<nextsent>with the exception of \[scott and desouza, 1990\] and \[robin, 1995\], most research in aggrega-tion did not transform clauses into modifiers, such as adjectives, pp, or relative clauses, in sys-tematic manner.
</nextsent>
<nextsent>\[scott.
</nextsent>
<nextsent>and desouza, 1990\] proposed heuristics for carrying out clause combining based on rst and specifically identified which rhetorical relations are appropriate for  embedding  : 145 which corresponds to our hypo tactic operators.
</nextsent>
<nextsent>we will incorporate rhetorical aggregation the future.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL66">
<title id=" W99-0411.xml">automated essay scoring for nonnative english speakers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the paper, we will refer to the e-rater system tm as e-rater.
</prevsent>
<prevsent>confounded by non-standard english syntactic structures or stylistic discourse structures which one might expect to be problem for system designed to evaluate native speaker writing.
</prevsent>
</prevsection>
<citsent citstr=" W98-0303 ">
research and development in automated essay scoring has begun to flourish in the past five years or so, bringing about whole new field of interest to the nlp community (burstein, et al(1998<papid> W98-0303 </papid>a, 1998b and 1998c), foltz, et al(1998), larkey (1998), page and peterson (1995)).</citsent>
<aftsection>
<nextsent>research at educational testing service (ets) has led to the recent development of e-rater, an operational automated essay scoring system.
</nextsent>
<nextsent>e-rater is based on features in holistic scoring guides for human reader scoring.
</nextsent>
<nextsent>scoring guides have 6-point score scale.
</nextsent>
<nextsent>six are assigned to the  best  essays, and    to the least well-written.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL68">
<title id=" W99-0411.xml">automated essay scoring for nonnative english speakers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the scoring guide criteria assume standard written english.
</prevsent>
<prevsent>non-standard english may show up in the writing of native english speakers of non-standard ialects.
</prevsent>
</prevsection>
<citsent citstr=" J93-2001 ">
for general nlp research purposes, it is useful to have computer-based corpora that represent language variation (biber (1993)).<papid> J93-2001 </papid></citsent>
<aftsection>
<nextsent>such corpora llow us to explore issues with regard to how the system will handle responses that might be written in non-standard english.
</nextsent>
<nextsent>current research at ets for the graduate record examination (gre) (burstein, et al 1999) is making use of essay corpora that represent subgroups where variations in standard written english might be found, such as in the writing of african americans, latinos and asians (breland, et al(1995) and bridgeman and mchale (1996)).
</nextsent>
<nextsent>in addition, ets is accumulating essay corpora of nonnative speakers that can be used for research.
</nextsent>
<nextsent>this paper focuses on preliminary data that show e-rater performance on test of written english (twe) essay responses written by nonnative english speakers whose native language is chinese, arabic, or spanish.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL71">
<title id=" W99-0205.xml">resolution of indirect anaphora in japanese sentences using examples </title>
<section> experiment  and  iscuss ion.  </section>
<citcontext>
<prevsection>
<prevsent>we regarded errors made for one of the following three reasons as right answers: 35 table 6: results non-verbal noun verbal noun total recall precis in recall precision recall \] precision experiment made when the system does not use any semantic information 85%(56/66) 67%(56/83) 40%(14/35) 44%(14/32) 69%(70/101) 61%(70/115) 42%(15/36) (35/70) 1 46% (35/76) 50%(20/40) 47%(15/32) experiment using  no  and verb case frame 91%(60/66) 86%(60/70) 66%(23/35) 79%(23/29) 82%(83/101) 84%(83/99) 83%(24/29) (44/70) 168% 56%(20/36) (44/65) estimation for the hypothetical use of noun case frame dictionary 91%(60/66) 88%(60/68) 69%(24/35) 89%(24/27) 83%(84/101) 88%(84/95) 79%(30/38) 186%(30/35)63%(20/32)177%(20/26) (50/70) 82% 71% (50/61) the upper row and the lower row of this table show rates on training sentences and test sentences respectively.
</prevsent>
<prevsent>the training sentences are used to set the values given in the rules (section 3.2) by hand.
</prevsent>
</prevsection>
<citsent citstr=" J94-2003 ">
training sentences {example sentences (walker et al, 1994) (<papid> J94-2003 </papid>43 sentences), folk tale kobutori jiisan (nakao, 1985) (93 sentences), an essay in tenseijingo (26 sentences), an editorial (26 sentences)} test sentences {a folk tale tsuru no ongaeshi (nakao, 1985) (91 sentences), two essays in tenseijingo (50 sentences), an editorial (30 sentences)} precision is the fraction of the noun phrases which were judged to have the indirect anaphora an- tecedents.</citsent>
<aftsection>
<nextsent>recall is the fraction of the noun phrases which have the antecedents of indirect anaphora.
</nextsent>
<nextsent>we use precision and recall to evaluate because the system judges that noun which is not an antecedent of indirect anaphora is an antecedent of indirect anaphora, and we check these errors thoroughly.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>proper examples do not exist in examples of  x. no  or in the verb case frame dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL72">
<title id=" W98-1410.xml">macro planning with a cognitive architecture for the adaptive explanation of proofs </title>
<section> the dialog planner.  </section>
<citcontext>
<prevsection>
<prevsent>an analyzer :to be designed receives the user interactions and passes them on to the dialog planner.
</prevsent>
<prevsent>92 i i
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
in the community of nlg, there is broad consensus that the generation of natural anguage should be done in three major steps \[reiter, 1994\].<papid> W94-0319 </papid></citsent>
<aftsection>
<nextsent>first macro planner (text planner) determines what to say, i.e. content and order of the information to be conveyed.
</nextsent>
<nextsent>then micro planner (sentence planner) determines how to say it, i.e. it plans the scope and the internal structure of the sentences.
</nextsent>
<nextsent>finally, realizer (surface generator) produces the surface text.
</nextsent>
<nextsent>in this classification, the dialog planner is macro planner for managing dialogs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL73">
<title id=" W98-1410.xml">macro planning with a cognitive architecture for the adaptive explanation of proofs </title>
<section> the dialog planner.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 mathematics communicating acts.
</prevsent>
<prevsent>mathematics communicating acts (mcas) are the primitive actions planned by the dialog planner.
</prevsent>
</prevsection>
<citsent citstr=" C94-1053 ">
they are derived from proverb proof communicative acts \[huang, 1994\].<papid> C94-1053 </papid></citsent>
<aftsection>
<nextsent>mcas are viewed as ? speech acts that are independent of the modality to be chosen.
</nextsent>
<nextsent>each mcaat least can be realized as portion of text.
</nextsent>
<nextsent>moreover some mcas manifest hem selves in the graphical arrangement of the text (see below for examples).
</nextsent>
<nextsent>in p. rez we distinguish between two types of mcas: ? mcas of the first type, called derivational mcas, convey step of the derivation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL74">
<title id=" W99-0804.xml">intra net learning tools for nlp </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the data to be displayed and manipulated in this applet is specified in applet parameters, it is possible to use it to illustrate different analyses at different parts of an educational hypertext.
</prevsent>
<prevsent>there is one structured string parameter which encodes the tree, and one further parameter for each node, which encodes the content of the respective avms.
</prevsent>
</prevsection>
<citsent citstr=" W98-0211 ">
the thistle tree-editing suite (calder, 1998) <papid> W98-0211 </papid>is well-developed interactive tool for working with linguistic representations such as trees and avms is more sophisticated alternative.</citsent>
<aftsection>
<nextsent>however, the tree-drawing program described is only part of more sophisticated mechanism which links the linguistic information displays to on-line parsing, 3 on-line parsing.
</nextsent>
<nextsent>having chosen to use java for the development of graphical displays of linguistic data, we have to consider what is the most appropriate ngine for the analysis or generation behind them.
</nextsent>
<nextsent>one possibility would be to re-write the code for those algorithms in java, but this ignores the possibility of re-using existing programs written in prolog or lisp, which are documented in various textbooks.
</nextsent>
<nextsent>these implementations are more established than existing java-based parsers, which have not so far featured in published learning materials.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL75">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this article reports the results of prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of george orwell nmeteen eighty-four the goal of the study is to determine the degree to which translatmn equivalents for different meamngs of polysemous word in enghsh are lexlcahzed differently across variety of languages, and to detelmme whether this information can be used to structure or create set of sense distinctions useful in natural language processing apphcatmns coherence index is computed that measures the tendency for different senses o1 the same english word to be lexlcahzed ifferently, and flora this data clustering algorithm is used to create sense hierat chles
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
it ~s well known that the most nagging issue for word sense disamblguanon (wsd) is the definmon of just what word sense is at its base, the problem is philosophical and linguistic one that is far from being resolved however, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural anguage processing tasks such as summarization, document retrieval, and machine translataon several criteria have been suggested and exploited to automatically determine the sense of word context (see ide and v6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, hearst, 1991, yarowsky, 1993), <papid> H93-1052 </papid>words co-occurring global context (e g, gale et al 1993, yarowsky, 1992 <papid> C92-2070 </papid>schutze, 1992, 1993), etc no clear criteria have emerged, however, and the problem continues to loom large for wsd work the notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as basis for some recent work on wsd foi example, brown et al(1991)and gale et al (1992a), gale et al (1993) used the parallel, aligned hansard corpus of canadian parhamentary debates foi wsd, and dagan et al(1991) <papid> P91-1017 </papid>and dagan and ital (1994) <papid> J94-4003 </papid>used monohngual corpora of hebrew and german and bilingual dictionary these studies relyon the assumption that the mapping between words and word senses vanes significantly among languages for example, the word duty in english t~anslates into french as devoir ~ts obhgatlon sense, and tmpot ~ts tax sense by determining the translation 52 ,..--.,.~ - eqmvalent ot duty in parallel french text, the correct sense of the enghsh word is identified these studies exploit th~s lnformatmn order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate w texts in related work, dywk (1998) used patterns of translational relatmns in an enghsh- norwegian paralle ! corpus (enpc, oslo umverslty) to define semantic propemes uch as synonymy, ambtgmty, vagueness, and semantic helds and suggested derivation ot- semantic representations for signs (eg , lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns recently, resnlk and yarowsky (1997) suggested that fol the purposes ot wsd, the different senses of wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally in particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to p~ovtde an answer, at least part, to the problem of determining different senses of word mtumvely, one assumes that ff another language lexlcahzes word two or more ways, there must be conceptual monvatmn if we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of word however, th~s suggestmn raises several questions fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the french tntdryt and the enghsh interest), especmlly languages that are relatively closely related assuming this problem can be overcome, should differences found closely related languages be given lesser (or greater) weight than those found more distantly related languages 9 more generally, which languages hould be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes  ~ mixture of the two 9 how many languages, and of which types, would be  enough  to provide adequate lnfotmanon tot this purpose~ there ts also the questmn ot the crlterm that would be used to estab hsh that sense distinction is  lexlcahzed cross-hngu~stmally  how consistent must the d~stlnctlon be 9 does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot different lexlcahzatlon exists a certain percentage of cases 9 another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky (1997) suggest eutowordnet (vossen, 1998) as possible somce of mformatmn, but, given that euro wordnet pttmatdy lexmon and not corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries an alternative would be to gather the reformation from parallel, ahgned corpma unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate ach instance ot word use context rather than as part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in dictionary however, at present very few parallel ahgned corpora exist the vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often english ideally, serious 53 evaluation of resnik and yarowsky proposal would include parallel texts languages from several different language families, and, to maximally ensure that the word question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwell nmeteen eighty-four (erjavec and ide, 1998), plato repubhc (erjavec, et al 1998), the multext journal .o/ the commt.~ston corpus (ide and v6roms, 1994), and the bible (resnlk, et al press) it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also, ~t is not clear how the lexlcahzatlon of sense distractions across languages affected by genre, domain, style, etc thls paper attempts to provide some prehmlnary answers to the questions outhned above, in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exam manon of small sample of parallel data can, as first step, provide the basis and dlrectmn for more extensive studies 1 methodology.</citsent>
<aftsection>
<nextsent>i have conducted small study using parallel, aligned versmns ot george orwell nineteen etghtv-fo,lr (euavec and ide, 1998)m five languages enghsh, slovene, estonian, roman lan, and czech the study therefole involves languages from four language families the o~well parallel corpus also includes vers|ons o) ntneteen-e~gho four hungarian, bulgarmn, latwan, llthuaman, se~bmn, and russmn (germanic, slavic, fmno-ugrec, and romance), two languages from the same family (czech and slovene), as well as one non-indo-european language (estoman) nmeteen eighty-four is text of about 100,000 words, translated irectly from the original english each of the other languages the parallel versions of the text are sentence-aligned to the english and tagged for part of speech although nineteen eighty-four is work of fiction, orwell prose is not highly stylized and, as such, it provides reasonable sample ot modern, ordinary language that ~s not tied to given topic or sub-domain (such as newspapers, technical reports, etc ) furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (prlest-dorman, etal, 1997) nine ambiguous english words were considered hard, head, country, hne, promise, shght, seize, scrap, float the first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used the senseval dlsamblguatlon exercise (kllgamff and palmer, forthcoming) in all cases, the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration f~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (including morphological variants) of each of the nine words were extracted from the enghsh text, together w~th the parallel sentences which they occur the texts ot the four comparison languages (czech, estonian, roman tan, slovene) as walks and stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions wordnet, (version 1 6) \[miller et al, 1990, fellbaum, 1998\]) the sense categonzatmn was performed by the author and two student assistants, results from the three were compared and final, mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to l lngmst and natl,ve speaker of the comparison language the hngmsts were asked to provide the lexlcal item each parallel sentence that corresponds to the ambiguous enghsh word if inflected, they were asked to provide both the inflected form and the root form in addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given table 1 for over 85% of the enghsh word occurrences (corresponding to types 1 and 2 table 1), specific lexlcal item or items could be identified as the rans lat ion equ iva lent for the corresponding enghsh word for comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, coherence index ( cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which g~ven se,ls,z ~s translated with the same word ~ note that the the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used the study note that the ci ~s similar to semanuc entropy (melamed, 1997) <papid> W97-0207 </papid>however, melamed computes cis do not determine whether or not sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently the translated text however, tt can be assumed that the cis provide measure of the tendency to lex~cahze different wordnet senses differently, which can turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word, the ci is computed for each pair of senses, as follows q  cl (sqs , ) =  =1 rnr where @ ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences ol- sense sqand sense s~ the enghsh corpus, respectively, including occurrences that have no idenufiable translation, ~ ~ ts the number of times that senses and are translated by the same lex~cal item language t, e , x=y ~tjan ~( ), r~oan~( ) the ci ts value between 0 and 1, computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then cl(s, s~) = 1, if they are translated with different word every occurrence, cl(s, ~) = 0 in general, the ci for pans of different senses provides an index of thmr relatedness, e , the greater the value of cl(s, sj), the more frequently occurrences of-sense and sense are translated with the same lextcal item when = j, we entropy tol wold types, lather than word senses 55 obtain measure of the coherence of ~lven sense type meaning 1 slngle lexlcal item is used to translate the en@izsh equivalent (possibly 2 the english word is translated by phrase of two or more words or compound,.</nextsent>
<nextsent>meaning as the slngle english word 3 the en@izsh word is not lexzcalized in the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL76">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this article reports the results of prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of george orwell nmeteen eighty-four the goal of the study is to determine the degree to which translatmn equivalents for different meamngs of polysemous word in enghsh are lexlcahzed differently across variety of languages, and to detelmme whether this information can be used to structure or create set of sense distinctions useful in natural language processing apphcatmns coherence index is computed that measures the tendency for different senses o1 the same english word to be lexlcahzed ifferently, and flora this data clustering algorithm is used to create sense hierat chles
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
it ~s well known that the most nagging issue for word sense disamblguanon (wsd) is the definmon of just what word sense is at its base, the problem is philosophical and linguistic one that is far from being resolved however, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural anguage processing tasks such as summarization, document retrieval, and machine translataon several criteria have been suggested and exploited to automatically determine the sense of word context (see ide and v6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, hearst, 1991, yarowsky, 1993), <papid> H93-1052 </papid>words co-occurring global context (e g, gale et al 1993, yarowsky, 1992 <papid> C92-2070 </papid>schutze, 1992, 1993), etc no clear criteria have emerged, however, and the problem continues to loom large for wsd work the notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as basis for some recent work on wsd foi example, brown et al(1991)and gale et al (1992a), gale et al (1993) used the parallel, aligned hansard corpus of canadian parhamentary debates foi wsd, and dagan et al(1991) <papid> P91-1017 </papid>and dagan and ital (1994) <papid> J94-4003 </papid>used monohngual corpora of hebrew and german and bilingual dictionary these studies relyon the assumption that the mapping between words and word senses vanes significantly among languages for example, the word duty in english t~anslates into french as devoir ~ts obhgatlon sense, and tmpot ~ts tax sense by determining the translation 52 ,..--.,.~ - eqmvalent ot duty in parallel french text, the correct sense of the enghsh word is identified these studies exploit th~s lnformatmn order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate w texts in related work, dywk (1998) used patterns of translational relatmns in an enghsh- norwegian paralle ! corpus (enpc, oslo umverslty) to define semantic propemes uch as synonymy, ambtgmty, vagueness, and semantic helds and suggested derivation ot- semantic representations for signs (eg , lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns recently, resnlk and yarowsky (1997) suggested that fol the purposes ot wsd, the different senses of wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally in particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to p~ovtde an answer, at least part, to the problem of determining different senses of word mtumvely, one assumes that ff another language lexlcahzes word two or more ways, there must be conceptual monvatmn if we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of word however, th~s suggestmn raises several questions fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the french tntdryt and the enghsh interest), especmlly languages that are relatively closely related assuming this problem can be overcome, should differences found closely related languages be given lesser (or greater) weight than those found more distantly related languages 9 more generally, which languages hould be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes  ~ mixture of the two 9 how many languages, and of which types, would be  enough  to provide adequate lnfotmanon tot this purpose~ there ts also the questmn ot the crlterm that would be used to estab hsh that sense distinction is  lexlcahzed cross-hngu~stmally  how consistent must the d~stlnctlon be 9 does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot different lexlcahzatlon exists a certain percentage of cases 9 another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky (1997) suggest eutowordnet (vossen, 1998) as possible somce of mformatmn, but, given that euro wordnet pttmatdy lexmon and not corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries an alternative would be to gather the reformation from parallel, ahgned corpma unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate ach instance ot word use context rather than as part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in dictionary however, at present very few parallel ahgned corpora exist the vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often english ideally, serious 53 evaluation of resnik and yarowsky proposal would include parallel texts languages from several different language families, and, to maximally ensure that the word question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwell nmeteen eighty-four (erjavec and ide, 1998), plato repubhc (erjavec, et al 1998), the multext journal .o/ the commt.~ston corpus (ide and v6roms, 1994), and the bible (resnlk, et al press) it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also, ~t is not clear how the lexlcahzatlon of sense distractions across languages affected by genre, domain, style, etc thls paper attempts to provide some prehmlnary answers to the questions outhned above, in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exam manon of small sample of parallel data can, as first step, provide the basis and dlrectmn for more extensive studies 1 methodology.</citsent>
<aftsection>
<nextsent>i have conducted small study using parallel, aligned versmns ot george orwell nineteen etghtv-fo,lr (euavec and ide, 1998)m five languages enghsh, slovene, estonian, roman lan, and czech the study therefole involves languages from four language families the o~well parallel corpus also includes vers|ons o) ntneteen-e~gho four hungarian, bulgarmn, latwan, llthuaman, se~bmn, and russmn (germanic, slavic, fmno-ugrec, and romance), two languages from the same family (czech and slovene), as well as one non-indo-european language (estoman) nmeteen eighty-four is text of about 100,000 words, translated irectly from the original english each of the other languages the parallel versions of the text are sentence-aligned to the english and tagged for part of speech although nineteen eighty-four is work of fiction, orwell prose is not highly stylized and, as such, it provides reasonable sample ot modern, ordinary language that ~s not tied to given topic or sub-domain (such as newspapers, technical reports, etc ) furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (prlest-dorman, etal, 1997) nine ambiguous english words were considered hard, head, country, hne, promise, shght, seize, scrap, float the first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used the senseval dlsamblguatlon exercise (kllgamff and palmer, forthcoming) in all cases, the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration f~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (including morphological variants) of each of the nine words were extracted from the enghsh text, together w~th the parallel sentences which they occur the texts ot the four comparison languages (czech, estonian, roman tan, slovene) as walks and stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions wordnet, (version 1 6) \[miller et al, 1990, fellbaum, 1998\]) the sense categonzatmn was performed by the author and two student assistants, results from the three were compared and final, mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to l lngmst and natl,ve speaker of the comparison language the hngmsts were asked to provide the lexlcal item each parallel sentence that corresponds to the ambiguous enghsh word if inflected, they were asked to provide both the inflected form and the root form in addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given table 1 for over 85% of the enghsh word occurrences (corresponding to types 1 and 2 table 1), specific lexlcal item or items could be identified as the rans lat ion equ iva lent for the corresponding enghsh word for comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, coherence index ( cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which g~ven se,ls,z ~s translated with the same word ~ note that the the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used the study note that the ci ~s similar to semanuc entropy (melamed, 1997) <papid> W97-0207 </papid>however, melamed computes cis do not determine whether or not sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently the translated text however, tt can be assumed that the cis provide measure of the tendency to lex~cahze different wordnet senses differently, which can turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word, the ci is computed for each pair of senses, as follows q  cl (sqs , ) =  =1 rnr where @ ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences ol- sense sqand sense s~ the enghsh corpus, respectively, including occurrences that have no idenufiable translation, ~ ~ ts the number of times that senses and are translated by the same lex~cal item language t, e , x=y ~tjan ~( ), r~oan~( ) the ci ts value between 0 and 1, computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then cl(s, s~) = 1, if they are translated with different word every occurrence, cl(s, ~) = 0 in general, the ci for pans of different senses provides an index of thmr relatedness, e , the greater the value of cl(s, sj), the more frequently occurrences of-sense and sense are translated with the same lextcal item when = j, we entropy tol wold types, lather than word senses 55 obtain measure of the coherence of ~lven sense type meaning 1 slngle lexlcal item is used to translate the en@izsh equivalent (possibly 2 the english word is translated by phrase of two or more words or compound,.</nextsent>
<nextsent>meaning as the slngle english word 3 the en@izsh word is not lexzcalized in the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL77">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this article reports the results of prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of george orwell nmeteen eighty-four the goal of the study is to determine the degree to which translatmn equivalents for different meamngs of polysemous word in enghsh are lexlcahzed differently across variety of languages, and to detelmme whether this information can be used to structure or create set of sense distinctions useful in natural language processing apphcatmns coherence index is computed that measures the tendency for different senses o1 the same english word to be lexlcahzed ifferently, and flora this data clustering algorithm is used to create sense hierat chles
</prevsent>
</prevsection>
<citsent citstr=" P91-1017 ">
it ~s well known that the most nagging issue for word sense disamblguanon (wsd) is the definmon of just what word sense is at its base, the problem is philosophical and linguistic one that is far from being resolved however, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural anguage processing tasks such as summarization, document retrieval, and machine translataon several criteria have been suggested and exploited to automatically determine the sense of word context (see ide and v6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, hearst, 1991, yarowsky, 1993), <papid> H93-1052 </papid>words co-occurring global context (e g, gale et al 1993, yarowsky, 1992 <papid> C92-2070 </papid>schutze, 1992, 1993), etc no clear criteria have emerged, however, and the problem continues to loom large for wsd work the notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as basis for some recent work on wsd foi example, brown et al(1991)and gale et al (1992a), gale et al (1993) used the parallel, aligned hansard corpus of canadian parhamentary debates foi wsd, and dagan et al(1991) <papid> P91-1017 </papid>and dagan and ital (1994) <papid> J94-4003 </papid>used monohngual corpora of hebrew and german and bilingual dictionary these studies relyon the assumption that the mapping between words and word senses vanes significantly among languages for example, the word duty in english t~anslates into french as devoir ~ts obhgatlon sense, and tmpot ~ts tax sense by determining the translation 52 ,..--.,.~ - eqmvalent ot duty in parallel french text, the correct sense of the enghsh word is identified these studies exploit th~s lnformatmn order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate w texts in related work, dywk (1998) used patterns of translational relatmns in an enghsh- norwegian paralle ! corpus (enpc, oslo umverslty) to define semantic propemes uch as synonymy, ambtgmty, vagueness, and semantic helds and suggested derivation ot- semantic representations for signs (eg , lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns recently, resnlk and yarowsky (1997) suggested that fol the purposes ot wsd, the different senses of wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally in particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to p~ovtde an answer, at least part, to the problem of determining different senses of word mtumvely, one assumes that ff another language lexlcahzes word two or more ways, there must be conceptual monvatmn if we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of word however, th~s suggestmn raises several questions fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the french tntdryt and the enghsh interest), especmlly languages that are relatively closely related assuming this problem can be overcome, should differences found closely related languages be given lesser (or greater) weight than those found more distantly related languages 9 more generally, which languages hould be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes  ~ mixture of the two 9 how many languages, and of which types, would be  enough  to provide adequate lnfotmanon tot this purpose~ there ts also the questmn ot the crlterm that would be used to estab hsh that sense distinction is  lexlcahzed cross-hngu~stmally  how consistent must the d~stlnctlon be 9 does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot different lexlcahzatlon exists a certain percentage of cases 9 another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky (1997) suggest eutowordnet (vossen, 1998) as possible somce of mformatmn, but, given that euro wordnet pttmatdy lexmon and not corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries an alternative would be to gather the reformation from parallel, ahgned corpma unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate ach instance ot word use context rather than as part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in dictionary however, at present very few parallel ahgned corpora exist the vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often english ideally, serious 53 evaluation of resnik and yarowsky proposal would include parallel texts languages from several different language families, and, to maximally ensure that the word question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwell nmeteen eighty-four (erjavec and ide, 1998), plato repubhc (erjavec, et al 1998), the multext journal .o/ the commt.~ston corpus (ide and v6roms, 1994), and the bible (resnlk, et al press) it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also, ~t is not clear how the lexlcahzatlon of sense distractions across languages affected by genre, domain, style, etc thls paper attempts to provide some prehmlnary answers to the questions outhned above, in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exam manon of small sample of parallel data can, as first step, provide the basis and dlrectmn for more extensive studies 1 methodology.</citsent>
<aftsection>
<nextsent>i have conducted small study using parallel, aligned versmns ot george orwell nineteen etghtv-fo,lr (euavec and ide, 1998)m five languages enghsh, slovene, estonian, roman lan, and czech the study therefole involves languages from four language families the o~well parallel corpus also includes vers|ons o) ntneteen-e~gho four hungarian, bulgarmn, latwan, llthuaman, se~bmn, and russmn (germanic, slavic, fmno-ugrec, and romance), two languages from the same family (czech and slovene), as well as one non-indo-european language (estoman) nmeteen eighty-four is text of about 100,000 words, translated irectly from the original english each of the other languages the parallel versions of the text are sentence-aligned to the english and tagged for part of speech although nineteen eighty-four is work of fiction, orwell prose is not highly stylized and, as such, it provides reasonable sample ot modern, ordinary language that ~s not tied to given topic or sub-domain (such as newspapers, technical reports, etc ) furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (prlest-dorman, etal, 1997) nine ambiguous english words were considered hard, head, country, hne, promise, shght, seize, scrap, float the first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used the senseval dlsamblguatlon exercise (kllgamff and palmer, forthcoming) in all cases, the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration f~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (including morphological variants) of each of the nine words were extracted from the enghsh text, together w~th the parallel sentences which they occur the texts ot the four comparison languages (czech, estonian, roman tan, slovene) as walks and stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions wordnet, (version 1 6) \[miller et al, 1990, fellbaum, 1998\]) the sense categonzatmn was performed by the author and two student assistants, results from the three were compared and final, mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to l lngmst and natl,ve speaker of the comparison language the hngmsts were asked to provide the lexlcal item each parallel sentence that corresponds to the ambiguous enghsh word if inflected, they were asked to provide both the inflected form and the root form in addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given table 1 for over 85% of the enghsh word occurrences (corresponding to types 1 and 2 table 1), specific lexlcal item or items could be identified as the rans lat ion equ iva lent for the corresponding enghsh word for comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, coherence index ( cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which g~ven se,ls,z ~s translated with the same word ~ note that the the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used the study note that the ci ~s similar to semanuc entropy (melamed, 1997) <papid> W97-0207 </papid>however, melamed computes cis do not determine whether or not sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently the translated text however, tt can be assumed that the cis provide measure of the tendency to lex~cahze different wordnet senses differently, which can turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word, the ci is computed for each pair of senses, as follows q  cl (sqs , ) =  =1 rnr where @ ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences ol- sense sqand sense s~ the enghsh corpus, respectively, including occurrences that have no idenufiable translation, ~ ~ ts the number of times that senses and are translated by the same lex~cal item language t, e , x=y ~tjan ~( ), r~oan~( ) the ci ts value between 0 and 1, computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then cl(s, s~) = 1, if they are translated with different word every occurrence, cl(s, ~) = 0 in general, the ci for pans of different senses provides an index of thmr relatedness, e , the greater the value of cl(s, sj), the more frequently occurrences of-sense and sense are translated with the same lextcal item when = j, we entropy tol wold types, lather than word senses 55 obtain measure of the coherence of ~lven sense type meaning 1 slngle lexlcal item is used to translate the en@izsh equivalent (possibly 2 the english word is translated by phrase of two or more words or compound,.</nextsent>
<nextsent>meaning as the slngle english word 3 the en@izsh word is not lexzcalized in the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL78">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>this article reports the results of prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of george orwell nmeteen eighty-four the goal of the study is to determine the degree to which translatmn equivalents for different meamngs of polysemous word in enghsh are lexlcahzed differently across variety of languages, and to detelmme whether this information can be used to structure or create set of sense distinctions useful in natural language processing apphcatmns coherence index is computed that measures the tendency for different senses o1 the same english word to be lexlcahzed ifferently, and flora this data clustering algorithm is used to create sense hierat chles
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
it ~s well known that the most nagging issue for word sense disamblguanon (wsd) is the definmon of just what word sense is at its base, the problem is philosophical and linguistic one that is far from being resolved however, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural anguage processing tasks such as summarization, document retrieval, and machine translataon several criteria have been suggested and exploited to automatically determine the sense of word context (see ide and v6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, hearst, 1991, yarowsky, 1993), <papid> H93-1052 </papid>words co-occurring global context (e g, gale et al 1993, yarowsky, 1992 <papid> C92-2070 </papid>schutze, 1992, 1993), etc no clear criteria have emerged, however, and the problem continues to loom large for wsd work the notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as basis for some recent work on wsd foi example, brown et al(1991)and gale et al (1992a), gale et al (1993) used the parallel, aligned hansard corpus of canadian parhamentary debates foi wsd, and dagan et al(1991) <papid> P91-1017 </papid>and dagan and ital (1994) <papid> J94-4003 </papid>used monohngual corpora of hebrew and german and bilingual dictionary these studies relyon the assumption that the mapping between words and word senses vanes significantly among languages for example, the word duty in english t~anslates into french as devoir ~ts obhgatlon sense, and tmpot ~ts tax sense by determining the translation 52 ,..--.,.~ - eqmvalent ot duty in parallel french text, the correct sense of the enghsh word is identified these studies exploit th~s lnformatmn order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate w texts in related work, dywk (1998) used patterns of translational relatmns in an enghsh- norwegian paralle ! corpus (enpc, oslo umverslty) to define semantic propemes uch as synonymy, ambtgmty, vagueness, and semantic helds and suggested derivation ot- semantic representations for signs (eg , lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns recently, resnlk and yarowsky (1997) suggested that fol the purposes ot wsd, the different senses of wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally in particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to p~ovtde an answer, at least part, to the problem of determining different senses of word mtumvely, one assumes that ff another language lexlcahzes word two or more ways, there must be conceptual monvatmn if we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of word however, th~s suggestmn raises several questions fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the french tntdryt and the enghsh interest), especmlly languages that are relatively closely related assuming this problem can be overcome, should differences found closely related languages be given lesser (or greater) weight than those found more distantly related languages 9 more generally, which languages hould be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes  ~ mixture of the two 9 how many languages, and of which types, would be  enough  to provide adequate lnfotmanon tot this purpose~ there ts also the questmn ot the crlterm that would be used to estab hsh that sense distinction is  lexlcahzed cross-hngu~stmally  how consistent must the d~stlnctlon be 9 does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot different lexlcahzatlon exists a certain percentage of cases 9 another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky (1997) suggest eutowordnet (vossen, 1998) as possible somce of mformatmn, but, given that euro wordnet pttmatdy lexmon and not corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries an alternative would be to gather the reformation from parallel, ahgned corpma unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate ach instance ot word use context rather than as part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in dictionary however, at present very few parallel ahgned corpora exist the vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often english ideally, serious 53 evaluation of resnik and yarowsky proposal would include parallel texts languages from several different language families, and, to maximally ensure that the word question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwell nmeteen eighty-four (erjavec and ide, 1998), plato repubhc (erjavec, et al 1998), the multext journal .o/ the commt.~ston corpus (ide and v6roms, 1994), and the bible (resnlk, et al press) it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also, ~t is not clear how the lexlcahzatlon of sense distractions across languages affected by genre, domain, style, etc thls paper attempts to provide some prehmlnary answers to the questions outhned above, in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exam manon of small sample of parallel data can, as first step, provide the basis and dlrectmn for more extensive studies 1 methodology.</citsent>
<aftsection>
<nextsent>i have conducted small study using parallel, aligned versmns ot george orwell nineteen etghtv-fo,lr (euavec and ide, 1998)m five languages enghsh, slovene, estonian, roman lan, and czech the study therefole involves languages from four language families the o~well parallel corpus also includes vers|ons o) ntneteen-e~gho four hungarian, bulgarmn, latwan, llthuaman, se~bmn, and russmn (germanic, slavic, fmno-ugrec, and romance), two languages from the same family (czech and slovene), as well as one non-indo-european language (estoman) nmeteen eighty-four is text of about 100,000 words, translated irectly from the original english each of the other languages the parallel versions of the text are sentence-aligned to the english and tagged for part of speech although nineteen eighty-four is work of fiction, orwell prose is not highly stylized and, as such, it provides reasonable sample ot modern, ordinary language that ~s not tied to given topic or sub-domain (such as newspapers, technical reports, etc ) furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (prlest-dorman, etal, 1997) nine ambiguous english words were considered hard, head, country, hne, promise, shght, seize, scrap, float the first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used the senseval dlsamblguatlon exercise (kllgamff and palmer, forthcoming) in all cases, the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration f~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (including morphological variants) of each of the nine words were extracted from the enghsh text, together w~th the parallel sentences which they occur the texts ot the four comparison languages (czech, estonian, roman tan, slovene) as walks and stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions wordnet, (version 1 6) \[miller et al, 1990, fellbaum, 1998\]) the sense categonzatmn was performed by the author and two student assistants, results from the three were compared and final, mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to l lngmst and natl,ve speaker of the comparison language the hngmsts were asked to provide the lexlcal item each parallel sentence that corresponds to the ambiguous enghsh word if inflected, they were asked to provide both the inflected form and the root form in addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given table 1 for over 85% of the enghsh word occurrences (corresponding to types 1 and 2 table 1), specific lexlcal item or items could be identified as the rans lat ion equ iva lent for the corresponding enghsh word for comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, coherence index ( cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which g~ven se,ls,z ~s translated with the same word ~ note that the the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used the study note that the ci ~s similar to semanuc entropy (melamed, 1997) <papid> W97-0207 </papid>however, melamed computes cis do not determine whether or not sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently the translated text however, tt can be assumed that the cis provide measure of the tendency to lex~cahze different wordnet senses differently, which can turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word, the ci is computed for each pair of senses, as follows q  cl (sqs , ) =  =1 rnr where @ ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences ol- sense sqand sense s~ the enghsh corpus, respectively, including occurrences that have no idenufiable translation, ~ ~ ts the number of times that senses and are translated by the same lex~cal item language t, e , x=y ~tjan ~( ), r~oan~( ) the ci ts value between 0 and 1, computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then cl(s, s~) = 1, if they are translated with different word every occurrence, cl(s, ~) = 0 in general, the ci for pans of different senses provides an index of thmr relatedness, e , the greater the value of cl(s, sj), the more frequently occurrences of-sense and sense are translated with the same lextcal item when = j, we entropy tol wold types, lather than word senses 55 obtain measure of the coherence of ~lven sense type meaning 1 slngle lexlcal item is used to translate the en@izsh equivalent (possibly 2 the english word is translated by phrase of two or more words or compound,.</nextsent>
<nextsent>meaning as the slngle english word 3 the en@izsh word is not lexzcalized in the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL79">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this article reports the results of prehmlnary analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of george orwell nmeteen eighty-four the goal of the study is to determine the degree to which translatmn equivalents for different meamngs of polysemous word in enghsh are lexlcahzed differently across variety of languages, and to detelmme whether this information can be used to structure or create set of sense distinctions useful in natural language processing apphcatmns coherence index is computed that measures the tendency for different senses o1 the same english word to be lexlcahzed ifferently, and flora this data clustering algorithm is used to create sense hierat chles
</prevsent>
<prevsent>it ~s well known that the most nagging issue for word sense disamblguanon (wsd) is the definmon of just what word sense is at its base, the problem is philosophical and linguistic one that is far from being resolved however, work in automated language processing has led to effotts to flnd practical means to dlstmgmsh word senses, at least to the degree that they are useful for natural anguage processing tasks such as summarization, document retrieval, and machine translataon several criteria have been suggested and exploited to automatically determine the sense of word context (see ide and v6roms, 1998), including syntactic behavior, semantic and pragmatic knowledge, and especially in more recent empirical studies, word co-occurrence within syntactic relations (e g, hearst, 1991, yarowsky, 1993), <papid> H93-1052 </papid>words co-occurring global context (e g, gale et al 1993, yarowsky, 1992 <papid> C92-2070 </papid>schutze, 1992, 1993), etc no clear criteria have emerged, however, and the problem continues to loom large for wsd work the notion that cross-hngual comparison can be useful fol sense dlsamblguauon has served as basis for some recent work on wsd foi example, brown et al(1991)and gale et al (1992a), gale et al (1993) used the parallel, aligned hansard corpus of canadian parhamentary debates foi wsd, and dagan et al(1991) <papid> P91-1017 </papid>and dagan and ital (1994) <papid> J94-4003 </papid>used monohngual corpora of hebrew and german and bilingual dictionary these studies relyon the assumption that the mapping between words and word senses vanes significantly among languages for example, the word duty in english t~anslates into french as devoir ~ts obhgatlon sense, and tmpot ~ts tax sense by determining the translation 52 ,..--.,.~ - eqmvalent ot duty in parallel french text, the correct sense of the enghsh word is identified these studies exploit th~s lnformatmn order to gather co-occurrence data for the different senses, which ts then used to dtsamb~guate w texts in related work, dywk (1998) used patterns of translational relatmns in an enghsh- norwegian paralle ! corpus (enpc, oslo umverslty) to define semantic propemes uch as synonymy, ambtgmty, vagueness, and semantic helds and suggested derivation ot- semantic representations for signs (eg , lexemes), captunng semantm relatmnshlps such as hyponymy etc, fiom such translatmnal relatmns recently, resnlk and yarowsky (1997) suggested that fol the purposes ot wsd, the different senses of wo~d could be detelmlned by considering only sense d~stmctmns that are lextcahzed cross-hngmstlcally in particular, they propose that some set of target languages be ~dent~fied, and that the sense d~stmctmns to be considered for language processing appllcatmns and evaluatmn be restricted to those that are reahzed lexlcally in some minimum subset of those languages this idea would seem to p~ovtde an answer, at least part, to the problem of determining different senses of word mtumvely, one assumes that ff another language lexlcahzes word two or more ways, there must be conceptual monvatmn if we look at enough languages, we would be likely to fred the s~gmficant lexlcal differences that dehmtt different senses of word however, th~s suggestmn raises several questions fo~ instance, ~t ~s well known that many amb~gumes are preserved across languages (for example, the french tntdryt and the enghsh interest), especmlly languages that are relatively closely related assuming this problem can be overcome, should differences found closely related languages be given lesser (or greater) weight than those found more distantly related languages 9 more generally, which languages hould be considered for this exermse 9 all languages 9 closely related languages9 languages from different language famlhes  ~ mixture of the two 9 how many languages, and of which types, would be  enough  to provide adequate lnfotmanon tot this purpose~ there ts also the questmn ot the crlterm that would be used to estab hsh that sense distinction is  lexlcahzed cross-hngu~stmally  how consistent must the d~stlnctlon be 9 does it mean that two concepts are expressed by mutually non-lntetchangeable lexmal items in some slgmficant number ot other languages, or need tt only be the case that the option ot different lexlcahzatlon exists a certain percentage of cases 9 another conslderatmn ts where the cross-hngual mformatlon to answer these questmns would come from using bdmgual dictionaries would be extremely tedmus and error-prone, g~ven the substantial d~vergence among d~ctlonanes in terms of the kinds and degree of sense dlstmctmns they make resmk and yalowsky (1997) suggest eutowordnet (vossen, 1998) as possible somce of mformatmn, but, given that euro wordnet pttmatdy lexmon and not corpus, ~t is subject to many of the same objections as for bl-hngual dictionaries an alternative would be to gather the reformation from parallel, ahgned corpma unlike bilingual and muttt-hngual dictionaries, translatmn eqmvalents xn parallel texts a~e determined by experienced translatols, who evaluate ach instance ot word use context rather than as part of the meta-hngmst~c actlvlty of classifying senses for mclusmn in dictionary however, at present very few parallel ahgned corpora exist the vast majority ot these are bl-texts, mvolwng only two languages, one of which is very often english ideally, serious 53 evaluation of resnik and yarowsky proposal would include parallel texts languages from several different language families, and, to maximally ensure that the word question is used in the exact same sense across languages, ~t would be preferable that the same text were used over all languages in the study the only currently avadable parallel corpora for more than two languages are olwell nmeteen eighty-four (erjavec and ide, 1998), plato repubhc (erjavec, et al 1998), the multext journal .o/ the commt.~ston corpus (ide and v6roms, 1994), and the bible (resnlk, et al press) it is likely that these corpora do not provide enough appropriate data to reliably determine sense distinctions also, ~t is not clear how the lexlcahzatlon of sense distractions across languages affected by genre, domain, style, etc thls paper attempts to provide some prehmlnary answers to the questions outhned above, in order to eventually determine the degree to which the use of parallel data ts vmble to determine sense distinctions, and, ff so, the ways in which th~s reformation might be used given the lack of lalge parallel texts across multiple languages, the study is necessarily hmlted, however, close exam manon of small sample of parallel data can, as first step, provide the basis and dlrectmn for more extensive studies 1 methodology.</prevsent>
</prevsection>
<citsent citstr=" W97-0207 ">
i have conducted small study using parallel, aligned versmns ot george orwell nineteen etghtv-fo,lr (euavec and ide, 1998)m five languages enghsh, slovene, estonian, roman lan, and czech the study therefole involves languages from four language families the o~well parallel corpus also includes vers|ons o) ntneteen-e~gho four hungarian, bulgarmn, latwan, llthuaman, se~bmn, and russmn (germanic, slavic, fmno-ugrec, and romance), two languages from the same family (czech and slovene), as well as one non-indo-european language (estoman) nmeteen eighty-four is text of about 100,000 words, translated irectly from the original english each of the other languages the parallel versions of the text are sentence-aligned to the english and tagged for part of speech although nineteen eighty-four is work of fiction, orwell prose is not highly stylized and, as such, it provides reasonable sample ot modern, ordinary language that ~s not tied to given topic or sub-domain (such as newspapers, technical reports, etc ) furthermore, the translations of the text seem to be relatively faithful to the original for instance, over 95% ot the sentence alignments in the full pmallel corpus of seven languages are one-to-one (prlest-dorman, etal, 1997) nine ambiguous english words were considered hard, head, country, hne, promise, shght, seize, scrap, float the first four were chosen because they have been used in other dlsamb~guatlon studies, the latter five were chosen from among the words used the senseval dlsamblguatlon exercise (kllgamff and palmer, forthcoming) in all cases, the study was necessarily hmlted to words that occurred frequently enough in the orwell text to warrant consideration f~ve hundred forty-two sentences conta|nmg an occurrence or occurrences (including morphological variants) of each of the nine words were extracted from the enghsh text, together w~th the parallel sentences which they occur the texts ot the four comparison languages (czech, estonian, roman tan, slovene) as walks and stevenson (1998) have pointed out, pa~t-of-speech tagging accomplishes good portion of the work ot semantic dlsamb~guatmn, therefore occmrences of wolds that appemed in the data in more than 54 one part of speech were grouped separately 2 the enghsh occurrences were then grouped usmg the sense distinctions wordnet, (version 1 6) \[miller et al, 1990, fellbaum, 1998\]) the sense categonzatmn was performed by the author and two student assistants, results from the three were compared and final, mutually agreeable set of sense assignments was estabhshed for each of the four comparison languages, the corpus of sense-grouped parallel sentences were sent to l lngmst and natl,ve speaker of the comparison language the hngmsts were asked to provide the lexlcal item each parallel sentence that corresponds to the ambiguous enghsh word if inflected, they were asked to provide both the inflected form and the root form in addttmn, the lmgmsts were asked to indicate the type of translatmn, according to the dtstmctmns given table 1 for over 85% of the enghsh word occurrences (corresponding to types 1 and 2 table 1), specific lexlcal item or items could be identified as the rans lat ion equ iva lent for the corresponding enghsh word for comparison purposes, each translanon equivalent was represented by ~ts lemma (or the lemma of the toot form in the case of derivatives) and associated w~th the wordnet sense to which it corresponds in order to determine the degree to which the assigned sense dlstlncttons correspond to translation eqmvalents, coherence index ( cl) was computed that measures how often each pmr of senses is translated usmg the same word as well as the consistency with which g~ven se,ls,z ~s translated with the same word ~ note that the the adjective and adverb senses of hard are consadeied together because the distinction is not consistent across the translations used the study note that the ci ~s similar to semanuc entropy (melamed, 1997) <papid> W97-0207 </papid>however, melamed computes cis do not determine whether or not sense dtstmctton can be lextcahzed in the target language, but only the degree to whmh they are lexicahzed differently the translated text however, tt can be assumed that the cis provide measure of the tendency to lex~cahze different wordnet senses differently, which can turn be seen as an mdtcatmn of the degree to which the distraction ts vahd for each ambiguous word, the ci is computed for each pair of senses, as follows q  cl (sqs , ) =  =1 rnr where @ ~s the number of comparison languages under consideration, nl~q and m,, are the nt~mber of occurrences ol- sense sqand sense s~ the enghsh corpus, respectively, including occurrences that have no idenufiable translation, ~ ~ ts the number of times that senses and are translated by the same lex~cal item language t, e , x=y ~tjan ~( ), r~oan~( ) the ci ts value between 0 and 1, computed by examining clusters of occurrences translated by the same word in the othel languages if sense and sense ) are consistently translated w~th the same wo~d in each comparison language, then cl(s, s~) = 1, if they are translated with different word every occurrence, cl(s, ~) = 0 in general, the ci for pans of different senses provides an index of thmr relatedness, e , the greater the value of cl(s, sj), the more frequently occurrences of-sense and sense are translated with the same lextcal item when = j, we entropy tol wold types, lather than word senses 55 obtain measure of the coherence of ~lven sense type meaning 1 slngle lexlcal item is used to translate the en@izsh equivalent (possibly 2 the english word is translated by phrase of two or more words or compound,.</citsent>
<aftsection>
<nextsent>meaning as the slngle english word 3 the en@izsh word is not lexzcalized in the translation.
</nextsent>
<nextsent>4 pronoun is substituted for the english word in the translation an english phrase contalnmng the ambiguous word is translated by single language which has broader or more specific meanlng, or by phrase in whl corresponding to the english word is not expliclt l~ lexlcallzed table 1 translation types and their trequencles % izen whl%h 6% 6% 6% of p same word # description hard 1 1 difficult 2 head i 1 table 2 1 2 _meta~horlcally hard _\] 3 not yielding to pressure , 1 4 very strong or ~lgorous, ar 3 earnestly, intently (adv) _ ~art of the body . . .
</nextsent>
<nextsent>3 intellect 4 _r~le_!r, ch,%ef 7 front, front part woldnet senses ot hard and head cis were also computed for each language individually as well as for different language groupings romaman, czech, and estonian (three different language families) czech and slovene (same family), romaman, czech, slovene (indo-european, and estonian (non- indo-european) to better visualize the relationship between senses, hierarchical clustering algorithm was applied to the ci data to generate trees reflecting sense proximity 4 finally, in order to determine the degree to which the linguistic relauon between languages may affect coherence, correlation was run among cis for all pairs of the four target languages fol example, table 2 gives the senses of hard and head that occurred in the data the ci data .s  sobs  hard and head are given in tables 3 and 4 ~uous cis measuring the aff, mty of sense with itself--that is, the tendency for all occurrences of that sense to be translated wlth the same word--show that all of the s,x senses of ha,d have greatel internal consistency tfian athmty with other senses, with senses 1 1 ( dlff|cult  - ci = 56) and 13 (, not soft,, - i = 63) registenng the h,ghest internal consistency 6 the same holds true for three of the four senses of head, while the ci for senses 1 3 ( intellect ) and 1 1 ( part of the body ) is higher than the ci for 1 3/1 3 wordnet sense
</nextsent>
<nextsent>2 3 1 4 1 3 1 1 1 2 21 23 1 4 13 0 50 13 ool 0 o0 0 25 o0 0 04 0 50 0 17 0 56 0 19 0 00 0 00 0 00 0 00 0 00 0 25 0 21 table 3 cis for hard i 12 0,,63 0 00 0 50 2 results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL80">
<title id=" W99-0508.xml">parallel translations as sense discriminators </title>
<section> 1  </section>
<citcontext>
<prevsection>
<prevsent>although the data sample is small, it gives some insight into ways which larger sample might contribute to sense discrimination 4 developed by andleas stolcke.
</prevsent>
<prevsent>results tor all words the study are avadable at http//www cs vassar edu/~~de/wsd/cross-hng html 6 senses 2 3 and 1 4 have cis ot 1 because ach ot.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
these senses exists a single occurrence the corpus, and have there tote been dlscarded horn consideration ot cis to~ individual senses we a~e currently mvesugatmg the use oi the kappa staust~c (carletta, 1996) <papid> J96-2004 </papid>to normahze these sparse data 56 wordnet sense 1 1 1 3 1 4 1 7 1 1 0 69 1 3 0 53 0 45 1 4 0 12 0 07, 0 50 1 7 0 40 0 001 0 00 1 00 table 4 cis for head figure 2 shows the sense clusters for hard generated from the ci data 7 the senses fall into two mare clusters, w~th the two most internally consistent senses (1 1 and 1 3) at the deepest level of each ot the respecuve groups the two adverbml forms 8 are placed in separate groups, lef lectmg thmr semantic proximity to the different adjecuval meanings of hard the clusters for head (figure 2) stmdarly show two dlstmct groupings, each anchored in the two senses with the h~ghest internal consistency and the lowest mutual ci ( part of the body  (1 1) and  ruler, chief  (1 4)) the h~erarchtes apparent the cluster graphs make intuitive sense structured hke dictmnary enmes, the clusters for hard and head might appeal as f~gure 1 this ts not dissimilar to actual dlctlonary entries for hard and head, for example, the enmes for hard in four differently constructed dlctmnanes ( colhns enghsh (ced), longman (ldoce), oxjotd advanced learner (oald), and cobuild) all hst the   d~fficult  and  not soft  senses first and second, whmh, since most dictionaries hst the most common ol frequently used senses hrst, reflects the gross dlwslon apparent the clusters beyond this, ~t ~s difficult to assess the 7 foi the purposes ot the cluster analys~s, cis of 00.</citsent>
<aftsection>
<nextsent>resulting from single occurrrence were normahzed to 5 8 because ~oot o, ms were used the analysis, no.
</nextsent>
<nextsent>dzstlncuon uanslauon eqmvalents was made tor part ot speech correspondence between the senses in the dictionary entries and the clusters the remamlng wordnet senses are scattered at various places within the entries or, some cases, split across various senses the h~erarchlcal relatmns apparent the clusters are not reflected the d~cttonary enmes, smce the senses are for the most part presented in flat, hnear hsts however, it is interesting to note that the first five senses of hard in the cobuild d~cuonary, which is the only d~cttonary in the group constructed on the bas~s of colpus examples 9 and presents senses ruder of frequency, correspond to hve of the six wordnet senses in thls study wordnet  metaphorically hard  is spread over multiple senses in the cob uild, as it.is in the other d~ctlonarles hard head 1 l f lcu t 2 lgorous ly ii 1 not soft strong 2 earnestly metaphor lca ly hard 1 part of the body zntel lect 2 front, front part ii ruler, chlef flgme 1 clusteis tol hard and head suuctured as dlcuonary entt ~es the results tor dlftment language groupings show that the tendency to lextcahze senses differently is not aftected by language d~stance (table 5) in fact, the mean ci fol estonian, the only non-indo-european language the study, ~s lower than that for any other group, mdmatmg that wordnet sense dtstmctmns are slightly less hkely to be lexlcahzed ifferently estonian 9 edmons ot the ldoce (1987 vexsmn) and oald.
</nextsent>
<nextsent>(1985 version) dictlonalles consulted this study ple-date dmons ol those same d~ctlonanes based on colpus evidence 57 correlations of cis for each language pair (table 5) also show no relationship between the degree to which sense d~stmcuons are lexlcahzed differently and language distance this is contrary to results obtained by resmk and yarowsky (subm,tted), who, using memc slmdar to the one used in this study, found that that non-indo-european languages tended to lexlcallze english sense d~stmctlons more than indo-european languages, especially at finer- grained levels however, their translation data was generated by native speakers presented with isolated sentences in english, who were asked to provide the translation forgiven word in the sentence it is not clear how this data compares to translations generated by trained translators working with full context lanquaqe qroup averaqe ci all 0 27 ro/es/sl 0 28 sl/cs 0 28 ro/sl/cs 0 27 es 0 26 table 5 average ci values lanqs hard country llne head ave es/cs 0 86 0 72 0 68 0 69 0 74 ro/sl 0 73 0 78 0 68 1 00 0 80 ro/cs 0 83 0 66 0 67 0 72 0 72 sl/cs 0 88 0 51 0 72 0 71 0 71 ro/es 0 97 0 26 0 70 0 98 0 73 es/sl 0 73 0 59 0 90 0 99 0 80 table 6 ci correlauon tor the tour target languages -i . . .
</nextsent>
<nextsent>i m~nlmum ls tance = 0 249399 m~nlmum d~stance = 0 434856 mln lmum ls tance = 0 555158 mln lmum ls tance = 0 602972 m~nlmum ls tance = 0 761327 . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL81">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this work we hypothesize that discourse structure (segmentation) is indeed vital in the decision of whether or not to generate pronoun.
</prevsent>
<prevsent>however, we argue that single definition of disburse segment is not sufficient to explain the patterns of pronoun use found, and seek more general notion.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
here, to distingni~h o~ notion from other notions of disc muse segmentation found in the literature (e.g., reidmum (1985) or grosz &amp; sidner (1986)), <papid> J86-3001 </papid>we use the term disco mae thread to capture the structuring notion to whidz we refer.</citsent>
<aftsection>
<nextsent>we propose that discourse generally contains multiple threads which run through the discourse and can serve to structure the dis-course.
</nextsent>
<nextsent>in general, single thread is evident at par-ticular point in the discourse, but this thread may be re-placed by another thread and then picked up again at an- other point in the discourse (cf.
</nextsent>
<nextsent>ros6 et al (1995)).
</nextsent>
<nextsent>the  threading device  used to structure will be different for different kinds ofdisc0urses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL84">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> previous work  on  pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>however, as pointed out in example 1, it does not seem to explain the patterns found in the texts we analyzed.
</prevsent>
<prevsent>the centering model (orosz et al, 1995) itself makes predictions about pronoun generation only in specific instance - that where rule 1 is appficeble.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
centering rule 1 states that if any element of the previous utter-ance forward looking center list is realized in the cur-rent utterance as pronoun, then the backward looking center must be realized as pronoun as well (grosz et al, 1995.<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>p.214).
</nextsent>
<nextsent>notice that the mr. curtis at the beginning of the second sentence in example is an apparent vio-lation of this rule.
</nextsent>
<nextsent>but, more generally, we must have theory that is able to handle all cases of pronoun use.
</nextsent>
<nextsent>a pronoun interpretation algorithm based on centering which relied on centering transition preferences was de-veloped in brennan et al (1987)~ <papid> P87-1022 </papid>using transition pref-erences in pronoun generation rule would cover more cases of pronoun use than is covered by rule 1, but the application of such transition preferences also proved un-helpful in explaining pronoun patterns in our corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL85">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> previous work  on  pronoun generation.  </section>
<citcontext>
<prevsection>
<prevsent>notice that the mr. curtis at the beginning of the second sentence in example is an apparent vio-lation of this rule.
</prevsent>
<prevsent>but, more generally, we must have theory that is able to handle all cases of pronoun use.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
a pronoun interpretation algorithm based on centering which relied on centering transition preferences was de-veloped in brennan et al (1987)~ <papid> P87-1022 </papid>using transition pref-erences in pronoun generation rule would cover more cases of pronoun use than is covered by rule 1, but the application of such transition preferences also proved un-helpful in explaining pronoun patterns in our corpus.</citsent>
<aftsection>
<nextsent>reichman (1985) and grosz &amp; sidner (1986) <papid> J86-3001 </papid>indi-cate that discourse segmentation has an effect on the lin-guistic realization of referring expressions.</nextsent>
<nextsent>while this is intuitively appealing, it is unclear how to apply this to the generation problem (in part because it is unclear how to define discourse segments a generation sys- tem).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL88">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> long- and short-distance anaphor/c.  </section>
<citcontext>
<prevsection>
<prevsent>we needed to find structuring device that was part of the input a sentence generation system and that was recognizable on the surface (so that we could evaluate our algorithm on naturally occurring text).
</prevsent>
<prevsent>after invest/- 65 gating some work on narrative structure (genette, 1980; prince.
</prevsent>
</prevsection>
<citsent citstr=" J94-2004 ">
1982; vogt, 1990),  we determined that changes in the deictic center of the story (nakhimoysky, 1988: wiebe, 1994) <papid> J94-2004 </papid>not only must be part of the input a sen-tence generator (e.g., for appropriate nse generation), but were also both well marked in the text and seemed to have an influence on anaphoric expression choice.</citsent>
<aftsection>
<nextsent>a shift in the deictic center can be signaled by shift of topic, shift of time scale, shift in spatial scale, or shift in perspective (nakhimovsky, 1988; <papid> J88-2004 </papid>wiebe, 1994).<papid> J94-2004 </papid></nextsent>
<nextsent>since shift in time scale is often indi-cated by linguistic means and the time being referred to must be part of the input a sentence generator, we con-centrated on this point.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL89">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> long- and short-distance anaphor/c.  </section>
<citcontext>
<prevsection>
<prevsent>after invest/- 65 gating some work on narrative structure (genette, 1980; prince.
</prevsent>
<prevsent>1982; vogt, 1990),  we determined that changes in the deictic center of the story (nakhimoysky, 1988: wiebe, 1994) <papid> J94-2004 </papid>not only must be part of the input a sen-tence generator (e.g., for appropriate nse generation), but were also both well marked in the text and seemed to have an influence on anaphoric expression choice.</prevsent>
</prevsection>
<citsent citstr=" J88-2004 ">
a shift in the deictic center can be signaled by shift of topic, shift of time scale, shift in spatial scale, or shift in perspective (nakhimovsky, 1988; <papid> J88-2004 </papid>wiebe, 1994).<papid> J94-2004 </papid></citsent>
<aftsection>
<nextsent>since shift in time scale is often indi-cated by linguistic means and the time being referred to must be part of the input a sentence generator, we con-centrated on this point.
</nextsent>
<nextsent>we also acknowledge that the changes in time seem quite important in the news stories that we analyzed.
</nextsent>
<nextsent>other genres of text might depend on other kinds of structuring devices.
</nextsent>
<nextsent>changes in time scale or time, as we redefined the cat-egory, may require world knowledge reasoning to rec- oguize but are often indicated by either cue words and phrases (e.g.,  n/he years ago  ,   year ,  for months ;  several months ago ), change in .grammatical time of the verb (e.g., past tense versus present ense), or changes in aspect (e.g., atomic versus extended events versus tates as defined by moens &amp; steedman (1988)).<papid> J88-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL91">
<title id=" W99-0108.xml">generating anaphoric expressions pronoun or definite description </title>
<section> long- and short-distance anaphor/c.  </section>
<citcontext>
<prevsection>
<prevsent>we also acknowledge that the changes in time seem quite important in the news stories that we analyzed.
</prevsent>
<prevsent>other genres of text might depend on other kinds of structuring devices.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
changes in time scale or time, as we redefined the cat-egory, may require world knowledge reasoning to rec- oguize but are often indicated by either cue words and phrases (e.g.,  n/he years ago  ,   year ,  for months ;  several months ago ), change in .grammatical time of the verb (e.g., past tense versus present ense), or changes in aspect (e.g., atomic versus extended events versus tates as defined by moens &amp; steedman (1988)).<papid> J88-2003 </papid></citsent>
<aftsection>
<nextsent>in considering how time change might affect anaphoric expression choice, we consider the choice for the first mention of discourse ntity in sentence where that entity has recently been referred to in the discourse.
</nextsent>
<nextsent>our hypothesis that: changes in time reliably signal changes of the thread in newspaper ticles; definite de-scriptions hould appear when the current reference to discourse ntity is in different thread from the last ref-erence to that entity and pronouns should occur when the previous mention is in the same thread 3.
</nextsent>
<nextsent>in order to evaluate this hypothesis, we mapped out the time being referenced in our texts on clause-by- clause basis.
</nextsent>
<nextsent>for each clause in the texts we indicated the time which was referred to.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL94">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J94-4002 ">
most current anaphora resolution systems im-plement pipeline architecture with three mod-ules clappin and leass,  1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>a collect module determines list of poten-tial antecedents (lpa) for each anaphor (pro- noun, definite noun, proper name, etc.) that have the potential to resolve it,
</nextsent>
<nextsent>patible with the anaphor f~m the lpa.
</nextsent>
<nextsent>likely antecedent onthe basis of an ordering policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL95">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W97-1303 ">
most current anaphora resolution systems im-plement pipeline architecture with three mod-ules clappin and leass,  1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>a collect module determines list of poten-tial antecedents (lpa) for each anaphor (pro- noun, definite noun, proper name, etc.) that have the potential to resolve it,
</nextsent>
<nextsent>patible with the anaphor f~m the lpa.
</nextsent>
<nextsent>likely antecedent onthe basis of an ordering policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL98">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W97-1307 ">
most current anaphora resolution systems im-plement pipeline architecture with three mod-ules clappin and leass,  1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></citsent>
<aftsection>
<nextsent>1.
</nextsent>
<nextsent>a collect module determines list of poten-tial antecedents (lpa) for each anaphor (pro- noun, definite noun, proper name, etc.) that have the potential to resolve it,
</nextsent>
<nextsent>patible with the anaphor f~m the lpa.
</nextsent>
<nextsent>likely antecedent onthe basis of an ordering policy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL106">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>patible with the anaphor f~m the lpa.
</prevsent>
<prevsent>likely antecedent onthe basis of an ordering policy.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
in most cases,, the collect module determines an lpa by enumerating all antecedents in win-dow of text that pleced__es the anaphor under scrutiny (hobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997; <papid> W97-1307 </papid>ge et al, 1998).<papid> W98-1119 </papid></citsent>
<aftsection>
<nextsent>this window can be as small as two or three sen-tences or as large as the entire preceding text.
</nextsent>
<nextsent>the filter module usually imposes emantic con-straints by requiring that the anaphor and poten-tial antecedents have the same number and gender, that selectional restrictions are obeyed, etc. the preference module imposes preferences on po-tential antecedents on the basis of their grammati-cal roles, parallelism, frequency, proximity, etc. in some cases, anaphora esolution systems implement these modules explicitly (i-iobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></nextsent>
<nextsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reason-ing techniques (mitkov, 1997).<papid> W97-1303 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL117">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>the filter module usually imposes emantic con-straints by requiring that the anaphor and poten-tial antecedents have the same number and gender, that selectional restrictions are obeyed, etc. the preference module imposes preferences on po-tential antecedents on the basis of their grammati-cal roles, parallelism, frequency, proximity, etc. in some cases, anaphora esolution systems implement these modules explicitly (i-iobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reason-ing techniques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" J81-4001 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in or-der to determine the lpa of an anaphor seems odd, given that several studies have claimed that there is strong relation between discourse structure and reference (sidner, 1981; <papid> J81-4001 </papid>gmsz and sidner, 1986; grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk et al, 1992; azzam et al, 1998; <papid> P98-1011 </papid>hitzeman and .oesio, 1998).<papid> P98-1090 </papid></citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that he use of referents in naturally occurring texts imposes con- stmints on the interpretation discourse; and, on the other, that the structure of discourse constrains the has to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by the fact that both groups eem prima facie to be righl em- pkical experiments studies that employ linear tech-niques for determining the lpas of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and i.eass, 1994; ge et al, 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of dis-course one has the potential of determining correct co-referential links for more than 80% of the refer-ential expressions (fox, 1987; cristea et al, 1998) <papid> P98-1044 </papid>although to date, no discourse-based anaphora res-olution system has been implemented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL118">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>the filter module usually imposes emantic con-straints by requiring that the anaphor and poten-tial antecedents have the same number and gender, that selectional restrictions are obeyed, etc. the preference module imposes preferences on po-tential antecedents on the basis of their grammati-cal roles, parallelism, frequency, proximity, etc. in some cases, anaphora esolution systems implement these modules explicitly (i-iobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reason-ing techniques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in or-der to determine the lpa of an anaphor seems odd, given that several studies have claimed that there is strong relation between discourse structure and reference (sidner, 1981; <papid> J81-4001 </papid>gmsz and sidner, 1986; grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk et al, 1992; azzam et al, 1998; <papid> P98-1011 </papid>hitzeman and .oesio, 1998).<papid> P98-1090 </papid></citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that he use of referents in naturally occurring texts imposes con- stmints on the interpretation discourse; and, on the other, that the structure of discourse constrains the has to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by the fact that both groups eem prima facie to be righl em- pkical experiments studies that employ linear tech-niques for determining the lpas of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and i.eass, 1994; ge et al, 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of dis-course one has the potential of determining correct co-referential links for more than 80% of the refer-ential expressions (fox, 1987; cristea et al, 1998) <papid> P98-1044 </papid>although to date, no discourse-based anaphora res-olution system has been implemented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL119">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>the filter module usually imposes emantic con-straints by requiring that the anaphor and poten-tial antecedents have the same number and gender, that selectional restrictions are obeyed, etc. the preference module imposes preferences on po-tential antecedents on the basis of their grammati-cal roles, parallelism, frequency, proximity, etc. in some cases, anaphora esolution systems implement these modules explicitly (i-iobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reason-ing techniques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1011 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in or-der to determine the lpa of an anaphor seems odd, given that several studies have claimed that there is strong relation between discourse structure and reference (sidner, 1981; <papid> J81-4001 </papid>gmsz and sidner, 1986; grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk et al, 1992; azzam et al, 1998; <papid> P98-1011 </papid>hitzeman and .oesio, 1998).<papid> P98-1090 </papid></citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that he use of referents in naturally occurring texts imposes con- stmints on the interpretation discourse; and, on the other, that the structure of discourse constrains the has to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by the fact that both groups eem prima facie to be righl em- pkical experiments studies that employ linear tech-niques for determining the lpas of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and i.eass, 1994; ge et al, 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of dis-course one has the potential of determining correct co-referential links for more than 80% of the refer-ential expressions (fox, 1987; cristea et al, 1998) <papid> P98-1044 </papid>although to date, no discourse-based anaphora res-olution system has been implemented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL120">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>the filter module usually imposes emantic con-straints by requiring that the anaphor and poten-tial antecedents have the same number and gender, that selectional restrictions are obeyed, etc. the preference module imposes preferences on po-tential antecedents on the basis of their grammati-cal roles, parallelism, frequency, proximity, etc. in some cases, anaphora esolution systems implement these modules explicitly (i-iobbs, 1978; lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1997; <papid> W97-1303 </papid>kameyama, 1997).<papid> W97-1307 </papid></prevsent>
<prevsent>in other cases, these modules are integrated by means of statistical (ge et al, 1998) <papid> W98-1119 </papid>or uncertainty reason-ing techniques (mitkov, 1997).<papid> W97-1303 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1090 ">
the fact that current anaphora resolution systems rely exclusively on the linear nature of texts in or-der to determine the lpa of an anaphor seems odd, given that several studies have claimed that there is strong relation between discourse structure and reference (sidner, 1981; <papid> J81-4001 </papid>gmsz and sidner, 1986; grosz et al, 1995; <papid> J95-2003 </papid>fox, 1987; vonk et al, 1992; azzam et al, 1998; <papid> P98-1011 </papid>hitzeman and .oesio, 1998).<papid> P98-1090 </papid></citsent>
<aftsection>
<nextsent>these studies claim, on the one hand, that he use of referents in naturally occurring texts imposes con- stmints on the interpretation discourse; and, on the other, that the structure of discourse constrains the has to which anaphors can be resolved.
</nextsent>
<nextsent>the oddness of the situation can be explained by the fact that both groups eem prima facie to be righl em- pkical experiments studies that employ linear tech-niques for determining the lpas of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and i.eass, 1994; ge et al, 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of dis-course one has the potential of determining correct co-referential links for more than 80% of the refer-ential expressions (fox, 1987; cristea et al, 1998) <papid> P98-1044 </papid>although to date, no discourse-based anaphora res-olution system has been implemented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL123">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>these studies claim, on the one hand, that he use of referents in naturally occurring texts imposes con- stmints on the interpretation discourse; and, on the other, that the structure of discourse constrains the has to which anaphors can be resolved.
</prevsent>
<prevsent>the oddness of the situation can be explained by the fact that both groups eem prima facie to be righl em- pkical experiments studies that employ linear tech-niques for determining the lpas of anaphom report recall and precision anaphora resolution results in the range of 80% ~in and i.eass, 1994; ge et al, 1998).<papid> W98-1119 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-1044 ">
empirical experiments that investigated the relation between discourse structure and reference also claim that by exploiting the structure of dis-course one has the potential of determining correct co-referential links for more than 80% of the refer-ential expressions (fox, 1987; cristea et al, 1998) <papid> P98-1044 </papid>although to date, no discourse-based anaphora res-olution system has been implemented.</citsent>
<aftsection>
<nextsent>since no di- 46 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 rect comparison of these two classes of approaches has been made, ? it is difficult to determine which group is right, and what method is the best.
</nextsent>
<nextsent>in this paper, we attempt to fill this gap by em-pirically comparing the potential of linear- and hi-erarchical models of discourse to correctly establish co-referential links in texts, and hence, their poten- tiai to correctly resolve anaphors.
</nextsent>
<nextsent>since it is likely that both linear- and discourse-based anaphora res-olution systems can implement similar filter and preference strategies, we focus here only on the strategies that can be used to collect lists of po-tential antecedents.
</nextsent>
<nextsent>specifically, we focus on de-termining whether discourse theories can help an anaphora esolution system determine lpas that are  better  than the lpas that can be computed from linear interpretation texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL127">
<title id=" W99-0106.xml">discourse structure and coreference an empirical study </title>
<section> a prefei~nce module detennm  esthe most  </section>
<citcontext>
<prevsection>
<prevsent>the texts were also manually annotated with discourse structures built in the style of mann and thompson (1988).
</prevsent>
<prevsent>each analysis yielded an average of 52 elementary discourse units.
</prevsent>
</prevsection>
<citsent citstr=" W99-0307 ">
details of the discourse annotation process are given in (marcu et al, 1999).<papid> W99-0307 </papid></citsent>
<aftsection>
<nextsent>3-~ comparing potential to establish co-referential links 3~,.1 method the annotations for co-reference relations and rhetorical structure trees for the thirty texts were fused, yielding representations tha ~flect not only the discourse structure, but also the c~reference 49 equivalence lasses pecific to each text.
</nextsent>
<nextsent>based on this information, we evaluated the potential of each of the two classes of models discussed in section 2 (linear-k and discourse-vt-k) to correctly estab- ? lish co-referential links as follows: for each model, each k, and each marked referential expression a, we determined whether or not the corresponding lpa (defined over elementary units) contained referee from the same equivalence lass.
</nextsent>
<nextsent>for exam-ple, for the linear-2 model and referential expres-sion the smaller company in unit 9, we estimated whether co-referential link could be established between the smaller company and another referen-tial expression units 7, 8, or 9.
</nextsent>
<nextsent>for the discourse- vt-2 model and the same referential expression, we estimated whether co-referential link could be es-tablished between the smaller company and another referential expression units 1, 8, or 9.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL128">
<title id=" W99-0110.xml">comprehension of coreferentiai expressions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the results that we have obtained in those comparisons are consistent with other theoretical and methodological approaches.
</prevsent>
<prevsent>research on psychological heuristics for interpreting ambiguous pronouns has provided support for subject-assignment strategy, where pronoun is preferentially interpreted as coreferential with the subject of the preceding clause (crawley, stevenson, &amp; kleinman, 1990; frederick.sen, 1981); in our framework, the subject of sentence has the most structural prominence.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
research on the success of algorithms for pronoun resolution (lappin &amp; leass, 1994) <papid> J94-4002 </papid>shows that syntactic factors (such as being subject, being direct object, and not being contained within another noun phrase) contribute to the likelihood that an expression is the antecedent of subsequent pronoun.</citsent>
<aftsection>
<nextsent>coreferenee we have developed model of coreference called discourse prominence theory or dpt (gordon &amp; hendrick, 1998).
</nextsent>
<nextsent>i adapts the formalism provided by kamp and reyle (1993) discourse representation theory (drt).
</nextsent>
<nextsent>in this approach, construction rules (crs) map linguistic expressions onto universes f discourse.
</nextsent>
<nextsent>in dpt the construction rules and representation drt are modified so that they can account for the basic facts of coreference described above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL129">
<title id=" W99-0110.xml">comprehension of coreferentiai expressions </title>
<section> dpt: an account of basic.  </section>
<citcontext>
<prevsection>
<prevsent>in this approach, construction rules (crs) map linguistic expressions onto universes f discourse.
</prevsent>
<prevsent>in dpt the construction rules and representation drt are modified so that they can account for the basic facts of coreference described above.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the principle modifications are: (1) the construction rules for proper names and definite descriptions introduce ntities into the discourse model as they do in kamp and reyle (1993), but the construction rule for pronouns interprets pronouns as referring directly to entities in the discourse model which is not how the rule works in kamp and reyle (1993); (2) discourse ntities in the model are ranked in terms of prominence (an idea that derives from the set of forward-looking centers in centering theory, grosz, et al 1995) <papid> J95-2003 </papid>which influences the way in which coreference is established.</citsent>
<aftsection>
<nextsent>discourse prominence theory includes three construction rules for the major types of reference and coreference, those for proper names, pronouns, and equivalence as shown below.
</nextsent>
<nextsent>we explain these rules by showing how they account for the differences in the ease of establishing coreference in sequences of different types of referring expressions.
</nextsent>
<nextsent>then we discuss how this process is influenced by syntactic prominence.
</nextsent>
<nextsent>cr.pn (construction rule for proper names) triggering condition: \[__...
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL131">
<title id=" W98-1408.xml">introducing maximal variation in text planning for small domains </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>thus, the text planner is not designed to generate p lan that will eventually transfer the information to the user ? optimally, but instead to generate as many plans as possible, which nevertheless transfer the information in an acceptabie way.
</prevsent>
<prevsent>70 !1 i i  !1 ! :i ! i|  concept unit  dep=place unknown arr-place unkno vn / bmb conveyance unknown \[.direction unknownj  go .from (dep-place) to (arr-place) with the (conveyance) towards (direction)   concept unit data done next ? bmb (i   data)  (i   done)  dep-place known arr-place known dep-time (i    bmb dep-time) arr-time (~    bmb arr-time) conveyance known direction known platform   (~  1  1  bmb platform).
</prevsent>
</prevsection>
<citsent citstr=" P84-1018 ">
figure 3: one of the grammar alternatives for unit the text planner is implemented as functional unification grammar (kay 1984) <papid> P84-1018 </papid>in fuf (elhadad 1993).</citsent>
<aftsection>
<nextsent>the grammar is feature description that consists of number of alter-natives, most of which represent an utterance with ts constraints on application, its semantic structure and its effect on the user knowledge.
</nextsent>
<nextsent>the process of text pl .anning is step-wise uni-fication of the input with the grammar.
</nextsent>
<nextsent>the control mechanism of fuf traverses all concepts in the input structure (i.e. sub-fd that contain the attribute concept), unifying them with suitable alternatives of the grammar.
</nextsent>
<nextsent>during this process, the input structure is enriched with new concepts, semantic structures and updates of the user knowledge state.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL132">
<title id=" W99-0802.xml">a modern computational linguistics course using dutch </title>
<section> f in te  state  methods   </section>
<citcontext>
<prevsection>
<prevsent>3 tile best solutions for this task resulted in 5% error rate (i.e. percentage of words in which wrongly placed hyphenation point occurs).
</prevsent>
<prevsent>exercise h verbal inflection second exercise concentrated on finite state transducers.
</prevsent>
</prevsection>
<citsent citstr=" P95-1003 ">
regular expressions had to be con- athe hyphenation task itself was defined as finite state transducer: macro(hyph, rep lace( \ [ \ ] : - , sy l , sy l ) ) the operator replace (target, left context, rightcontext) implements  left most  ( and  longest match ) replacement (karttunen, 1995).<papid> P95-1003 </papid></citsent>
<aftsection>
<nextsent>this ensures that in the cases where consonant could be either final in coda or initial in tile next onset, it is in fact added to the onset.
</nextsent>
<nextsent>underly ing surface gloss a. werk+en werken work\[mf\] b. bak+en bakken bak@nf\] c. raaksen raken hit\[inf\] d. verwen+en verwennenpamper\[inf\] e. teken+en tekenen draw\[lnf\] f. aanpik+en aanpikken catch up\[inf\] g. zanik+en zaniken wine\[inf\] h. leev+en leven liv@nf\] i. leev leef live\[paes, 1, sa.\] j. leev+t leeft live(s)\[paes, 2/3, sg.\] k. doe+en doen do\[ine\] ga+t gaat go(es)\[pres, 2/3, so.\] m. zit+t zit sit(s)\[pres, 2/3, s(~.\] n. werk+te werkte worked\[past, sa\] o. hoor+te hoorde heard\[past, sg\] p. blaf+te blafte barked\[past, sg\] q. leev+te leefde lived\[past, sg\] figure 3: dutch verbal inflection structed for computing the surface form of ab-stract verbal stem forms and combinations of stem and verbal inflection suffix (see figure 3).
</nextsent>
<nextsent>several spelling rules need to be captured.
</nextsent>
<nextsent>ex-amples (b) and (c) show that single consonants following short vowel are doubled when followed by the  +en  suffix, while long vowels (normally represented by two identical characters) are writ-ten as single character when followed by single consonant and   +en  examples (d-g) illustrate that the rule which requires doubling of conso-nant after short vowel is not applied if the pre-ceding vowel is schwa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL133">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the #-tbl system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including constraint grammar dis- ambiguators well as more traditional  brill-taggers .
</prevsent>
<prevsent>results from number of experiments and benchmarks are presented which show that the system is both flex-   ible and efficient.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
since eric brill first introduced the method of trans- formation-based learning (tbl) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging \[brill, 1995\], <papid> J95-4004 </papid>pp- attachment disambiguation \[brill and resnik, 1994\], <papid> C94-2195 </papid>text chunking \[ramshaw and marcus, 1995\], <papid> W95-0107 </papid>spell-ing correction \[mangu and brill, 1997\], dialogue act tagging \[samuel et al, 1998\] <papid> P98-2188 </papid>and ellipsis resolution \[hardt, 1998\].</citsent>
<aftsection>
<nextsent>thus, tbl has proved very useful, in many different ways, and is likely to continue to do so in the future.
</nextsent>
<nextsent>moreover, since brill generously made his own tbl implementation publicly available, many researchers in need of all off-the-shelf re trainable part-of-speech tag-ger have found what they were looking for.
</nextsent>
<nextsent>however, although very useful, brill original implementation is somewhat opaque, templates are not compositional, ithroughout this paper, when referring to brill tbl implementation, it is always his contextual-rule-learner - implemented in - that have in mind.
</nextsent>
<nextsent> it is available from http://www, cs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL134">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the #-tbl system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including constraint grammar dis- ambiguators well as more traditional  brill-taggers .
</prevsent>
<prevsent>results from number of experiments and benchmarks are presented which show that the system is both flex-   ible and efficient.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
since eric brill first introduced the method of trans- formation-based learning (tbl) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging \[brill, 1995\], <papid> J95-4004 </papid>pp- attachment disambiguation \[brill and resnik, 1994\], <papid> C94-2195 </papid>text chunking \[ramshaw and marcus, 1995\], <papid> W95-0107 </papid>spell-ing correction \[mangu and brill, 1997\], dialogue act tagging \[samuel et al, 1998\] <papid> P98-2188 </papid>and ellipsis resolution \[hardt, 1998\].</citsent>
<aftsection>
<nextsent>thus, tbl has proved very useful, in many different ways, and is likely to continue to do so in the future.
</nextsent>
<nextsent>moreover, since brill generously made his own tbl implementation publicly available, many researchers in need of all off-the-shelf re trainable part-of-speech tag-ger have found what they were looking for.
</nextsent>
<nextsent>however, although very useful, brill original implementation is somewhat opaque, templates are not compositional, ithroughout this paper, when referring to brill tbl implementation, it is always his contextual-rule-learner - implemented in - that have in mind.
</nextsent>
<nextsent> it is available from http://www, cs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL135">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the #-tbl system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including constraint grammar dis- ambiguators well as more traditional  brill-taggers .
</prevsent>
<prevsent>results from number of experiments and benchmarks are presented which show that the system is both flex-   ible and efficient.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
since eric brill first introduced the method of trans- formation-based learning (tbl) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging \[brill, 1995\], <papid> J95-4004 </papid>pp- attachment disambiguation \[brill and resnik, 1994\], <papid> C94-2195 </papid>text chunking \[ramshaw and marcus, 1995\], <papid> W95-0107 </papid>spell-ing correction \[mangu and brill, 1997\], dialogue act tagging \[samuel et al, 1998\] <papid> P98-2188 </papid>and ellipsis resolution \[hardt, 1998\].</citsent>
<aftsection>
<nextsent>thus, tbl has proved very useful, in many different ways, and is likely to continue to do so in the future.
</nextsent>
<nextsent>moreover, since brill generously made his own tbl implementation publicly available, many researchers in need of all off-the-shelf re trainable part-of-speech tag-ger have found what they were looking for.
</nextsent>
<nextsent>however, although very useful, brill original implementation is somewhat opaque, templates are not compositional, ithroughout this paper, when referring to brill tbl implementation, it is always his contextual-rule-learner - implemented in - that have in mind.
</nextsent>
<nextsent> it is available from http://www, cs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL137">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the #-tbl system recognizes four kinds of rules, that can be used to implement various kinds of disambiguators, including constraint grammar dis- ambiguators well as more traditional  brill-taggers .
</prevsent>
<prevsent>results from number of experiments and benchmarks are presented which show that the system is both flex-   ible and efficient.
</prevsent>
</prevsection>
<citsent citstr=" P98-2188 ">
since eric brill first introduced the method of trans- formation-based learning (tbl) it has been used to learn rules for many natural anguage processing tasks, such as part-of-speech tagging \[brill, 1995\], <papid> J95-4004 </papid>pp- attachment disambiguation \[brill and resnik, 1994\], <papid> C94-2195 </papid>text chunking \[ramshaw and marcus, 1995\], <papid> W95-0107 </papid>spell-ing correction \[mangu and brill, 1997\], dialogue act tagging \[samuel et al, 1998\] <papid> P98-2188 </papid>and ellipsis resolution \[hardt, 1998\].</citsent>
<aftsection>
<nextsent>thus, tbl has proved very useful, in many different ways, and is likely to continue to do so in the future.
</nextsent>
<nextsent>moreover, since brill generously made his own tbl implementation publicly available, many researchers in need of all off-the-shelf re trainable part-of-speech tag-ger have found what they were looking for.
</nextsent>
<nextsent>however, although very useful, brill original implementation is somewhat opaque, templates are not compositional, ithroughout this paper, when referring to brill tbl implementation, it is always his contextual-rule-learner - implemented in - that have in mind.
</nextsent>
<nextsent> it is available from http://www, cs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL140">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>98.1% 1.04 table 5: result of constraint grammar induction ta word is deemed to be accurately tagged if the correct tag is an element in the set of tags that the word has been assigned.
</prevsent>
<prevsent>these results are promising.
</prevsent>
</prevsection>
<citsent citstr=" P98-2128 ">
but before it would be fair to compare with other methods for inducing con-straint grammars from annotated corpora, e.g. the methods described ill \[samuelsson etal., 1996\] or in \[lindberg and eineborg, 1998\], <papid> P98-2128 </papid>it remains to determine the optimal set of templates and the optimal settings of the accuracy threshold.</citsent>
<aftsection>
<nextsent>very likely, the learning pro-cess (applied to the learning of reduction rules) can also be optimized for speed.
</nextsent>
<nextsent>in short, lot more has to be done, but at least this section has shown how easily an experiment like this can be set up in the #-tbl envi-ronment.
</nextsent>
<nextsent>summary and conclusions the #-tbl system is not just re-implementation of original tbl in another programming language.
</nextsent>
<nextsent>rather it should be seen as all attempt use tile rea-soning and database capabilities of prolog to do tbl ill more high-level way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL141">
<title id=" W99-0705.xml">the utbl system logic programming tools for transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>interact ive - prolog is all interactive language and this is something that the #-tbl system inherits.
</prevsent>
<prevsent>smal - thanks to the choice of implementation lan-guage, the system code base can be kept quite small.
</prevsent>
</prevsection>
<citsent citstr=" E99-1047 ">
indeed,  light  version of the #-tbl system., con-sisting of just one page of prolog code, has been im-plemented \[lager, 1999\].<papid> E99-1047 </papid></citsent>
<aftsection>
<nextsent>in short, the #-tbl system is powerful environ-ment in which to experiment with transformation-based learning.
</nextsent>
<nextsent>acknowledgements thanks tolars borin, mats dahllsf and natalia zi- novjeva at uppsala university for comments and sug-gestions.
</nextsent>
<nextsent>joakim nivre in gsteborg  also provided me with valuable insights and advice.
</nextsent>
<nextsent>41 i i i i i i i i i i
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL142">
<title id=" W99-0628.xml">ppattachment a committee machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if this were true for all of the cases, determining pp assignment would require highly complex computation.
</prevsent>
<prevsent>in some other cases, the information de-termining the pp attachment seems to be local.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
some works \[woods et al 1972\], \[boguraev, 1979\], \[marcus et al 1993\] <papid> J93-2004 </papid>sug-gested several strategies that based their 231 decision-making on the relationships ex-isting between predicates and arguments- what \[katz and fodor, 1963\] called selec-tional restrictions.</citsent>
<aftsection>
<nextsent>cases belonging to this group seem to be easier to handle computa-tionally than the former ones.
</nextsent>
<nextsent>regarding these different cases we can speak of two kinds of disambiguation mechanisms.
</nextsent>
<nextsent>one that can be called low level mechanism which uses mainly information regarding selectional re-strictions between predicates and arguments.
</nextsent>
<nextsent>this mechanism uses local context in order to solve syntactic disambiguation: that which is constituted by the predicate and its arguments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL143">
<title id=" W99-0628.xml">ppattachment a committee machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>if the low level mechanism does not solve the ambiguity, the high level mechanism, which would be acti-vated later, should be able to do it.
</prevsent>
<prevsent>there are empirical data that seem to support the fact that human beings use these two mechanisms both for word sense disambiguation and syntactic disam-biguation.
</prevsent>
</prevsection>
<citsent citstr=" P98-2201 ">
for review see \[sopena et al 1998\].<papid> P98-2201 </papid></citsent>
<aftsection>
<nextsent>1.1 local isambiguat ion.
</nextsent>
<nextsent>the low level disambiguation for the pp is one task that has been somewhat successfully treated using statistical methods.
</nextsent>
<nextsent>not all of the methods use the selectional restrictions mechanism since they don make use of semantic lasses.
</nextsent>
<nextsent>we will use the term local disambiguation to encompass the methods based on selectional restrictions as well as those based on lexical association.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL145">
<title id=" W99-0628.xml">ppattachment a committee machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2.a) to eat the strawberry with pleasure entity object substance food green goods edible fruit adequate level of abstraction of strawberry in (2.a) (2.b) to take strawberry from the box.
</prevsent>
<prevsent>entity object substance - - - food green goods edible fruit adequate level of abstraction of strawberry in (2.b) figure 1.
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
most of the statistical methods that have used classes do not carry out prior disambiguation the words \[brill, resnick 1994\], \[ratnaparkhi et. al 1994\] <papid> H94-1048 </papid>and others, nor do they determine the adequate level of abstraction.</citsent>
<aftsection>
<nextsent>some that do make the determination have poor level of efficiency.
</nextsent>
<nextsent>table 1 shows the accuracy of the results reported in previous work.
</nextsent>
<nextsent>the worst results were obtained when only classes were used.
</nextsent>
<nextsent>stet tina and nagao used the ratnaparkhi dataset but they eliminated 3,224 4-tuples (15~) from the training set containing contra-dicting examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL149">
<title id=" W99-0628.xml">ppattachment a committee machine approach </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>classes of given sense and classes of differ-ent senses of different words can have complex interactions and the preceding methods cannot take such interactions into account.
</prevsent>
<prevsent>neural networks (nns) are appropriates in dealing with this complexity.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
a very impor- 232 author best hindle and rooth (1993) 80.0 % resnik and hearst (1993) 83.9 % wn resnik and hearst (1993) 75.0 % ratnaparkhi et al (1994) <papid> H94-1048 </papid>81.6 % brill and resnik (1994) <papid> C94-2195 </papid>81.8 collins and brooks (1995) 84.5 stet tina and nagao (1997) 88.0 sopena et al (1998) <papid> P98-2201 </papid>86.2 li and abe (1998) 82.4 table 1: test and accuracy results % % % % % classes use of ratnaparkhi set no no wn no no mic yes wn no no yes wn yes wn no wn no reported in previous works.</citsent>
<aftsection>
<nextsent>tant characteristic of nns is their capacity to deal with multidimensional inputs.
</nextsent>
<nextsent>they need much fewer parameters to achieve the same re-sult than traditional numerical methods.
</nextsent>
<nextsent>re-cently \[barton, 1993\] has shown that feedfor- ward networks with one layer of sigmoidal non- linear ities achieve an integrated squared error of order o(?)
</nextsent>
<nextsent>for input spaces of dimension d, where is the number of units of the network.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL157">
<title id=" W99-0620.xml">learning discourse relations with active data selection </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>however, to ac-quire merely few hundred texts annotated for discourse information is often impossible due to the enormity of the haman labor required.
</prevsent>
<prevsent>this paper presents novel method for reduc-ing the amount of data for training decision tree classifier, while not compromising the accu-racy.
</prevsent>
</prevsection>
<citsent citstr=" P97-1013 ">
while there has been some work explor-ing the use of machine leaning techniques for discourse and dialogue (marcu, 1997; <papid> P97-1013 </papid>samuel et al., 1998), <papid> P98-2188 </papid>to our knowledge, no computational research on discourse or dialogue so far has ad-dressed the problem of reducing or minimizing the amount of data for training learning algo- rithm.</citsent>
<aftsection>
<nextsent>* the work reported here was conducted while the first author was with advanced research lab., hitachi ltd, 2520 hatoyama saitama 350-0395 japan..
</nextsent>
<nextsent>a particular method proposed here is built on the committee-based sampling, initially pro-posed for probabilistic lassifiers by dagan and engelson (1995), where an example is selected from the corpus according to its utility in im-proving statistics.
</nextsent>
<nextsent>we extend the method for decision tree classifiers using statistical tech-nique called bootstrapping (cohen, 1995).
</nextsent>
<nextsent>with an additional extension, which we call error .feedback, it is found that the method achieves an increased accuracy as well as significant reduction of training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL158">
<title id=" W99-0620.xml">learning discourse relations with active data selection </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>however, to ac-quire merely few hundred texts annotated for discourse information is often impossible due to the enormity of the haman labor required.
</prevsent>
<prevsent>this paper presents novel method for reduc-ing the amount of data for training decision tree classifier, while not compromising the accu-racy.
</prevsent>
</prevsection>
<citsent citstr=" P98-2188 ">
while there has been some work explor-ing the use of machine leaning techniques for discourse and dialogue (marcu, 1997; <papid> P97-1013 </papid>samuel et al., 1998), <papid> P98-2188 </papid>to our knowledge, no computational research on discourse or dialogue so far has ad-dressed the problem of reducing or minimizing the amount of data for training learning algo- rithm.</citsent>
<aftsection>
<nextsent>* the work reported here was conducted while the first author was with advanced research lab., hitachi ltd, 2520 hatoyama saitama 350-0395 japan..
</nextsent>
<nextsent>a particular method proposed here is built on the committee-based sampling, initially pro-posed for probabilistic lassifiers by dagan and engelson (1995), where an example is selected from the corpus according to its utility in im-proving statistics.
</nextsent>
<nextsent>we extend the method for decision tree classifiers using statistical tech-nique called bootstrapping (cohen, 1995).
</nextsent>
<nextsent>with an additional extension, which we call error .feedback, it is found that the method achieves an increased accuracy as well as significant reduction of training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL159">
<title id=" W99-0620.xml">learning discourse relations with active data selection </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>a se-lection function is given in probabilistic terms, 160 based on v(e).
</prevsent>
<prevsent>g pselect(e) = log v(e) here is called the entropy gain and is used to determine the number of times an example is selected; grea~ter would increase the number of examples elected for tagging.
</prevsent>
</prevsection>
<citsent citstr=" P96-1042 ">
engelson and dagan (1996) <papid> P96-1042 </papid>investigated several plausible ap-proaches to the selection function but were un-able to find significant differences among them.</citsent>
<aftsection>
<nextsent>at the beginning of the section, we mentioned some properties of  useful  examples.
</nextsent>
<nextsent>a useful example is one which contributes to reducing variance in parameter values and also affects classification.
</nextsent>
<nextsent>by randomly generating multiple models and measuring disagreement among them, one would be able to tell whether an ex-ample is useful in the sense above; if there were large disagreement, then one would know that the example is relevant to classification and also is associated with parameters with large vari-ance and thus with insufficient statistics.
</nextsent>
<nextsent>in the following section, we investigate how we might extend cbs for use in decision tree classifiers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL166">
<title id=" W99-0409.xml">exploiting the student model to emphasize language pedagogy in natural language processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the problem of resolving multiple interpretations is compounded in an intelligent language tutoring system (ilts) because the grammar must not only admit grammatical structures, but must also be able to navigate over ungrammatical structures and record the errors that the student has made.
</prevsent>
<prevsent>as consequence, grammar for an ilts will not only assign structures to grammatical sentence, but may also find analyses which interpret the sentence as ungrammatical, set of analyses that traditionally constrained grammar would not find.
</prevsent>
</prevsection>
<citsent citstr=" P98-2196 ">
the usual method of limiting the number of parses that an ilts grammar assigns is to examine the effects of relaxing those constraints that represent likely sources of error by students and introduce new constraints into the grammar rules to block unlikely parses (schneider &amp; mccoy 1998).<papid> P98-2196 </papid></citsent>
<aftsection>
<nextsent>such techniques, however, overlook individual learner differences as key factor in language teaching pedagogy.
</nextsent>
<nextsent>the system introduced in this paper differs from the traditional approach by permitting the grammar to freely generate as many parses as it can and using separate pedagogic principles to select the appropriate interpretation and response.
</nextsent>
<nextsent>the system tightly integrates the student model into the process of selecting the appropriate interpretation and generating response tailored to the student level of expertise.
</nextsent>
<nextsent>the student model keeps record of students  performance history which provides information essential to the analysis of multiple parses, multiple errors, and the level of interaction with the student.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL167">
<title id=" W98-1425.xml">a flexible shallow approach to text generation </title>
<section> in-depth and shallow generation.  </section>
<citcontext>
<prevsection>
<prevsent>their inclusion may require the intermediate presentation layer?
</prevsent>
<prevsent>to bemodif ied.
</prevsent>
</prevsection>
<citsent citstr=" P83-1022 ">
2 .3 potent ia shortcomings f sha low generation methods prominent example for an early shallow generation system is ana \[kukich, 1983\], <papid> P83-1022 </papid>which reports about stockmarket developments.</citsent>
<aftsection>
<nextsent>while the kind of texts it produces can still be considered valuable today, anais implemented as widely unstructured rule-based system, which does not seem to be easily extendable and portable.
</nextsent>
<nextsent>since then, various shallow methods including canned text parts and some template-based techniques have been utilized, e.g. in cogent help \[white and caldwell, 1997\], <papid> A97-1038 </papid>in the system described in \[cawsey et al , 1995\], and in idas \[reiter et al , 1995\].</nextsent>
<nextsent>they feature simplicity where the intended application does not require fine-grained istinctions, such as the following techniques used in idas: ? canned text with embedded kb references ( carefully slide \[x\] out along its guide ), ? case frames with textual slot fillers, ( gently  in (manner :  gently ) ) . although these techniques seem to be able to provide the necessary distinctions for many practical applications in much simpler way than in-depth surface realization components can do, serious limitation lies in their inflexibility.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL168">
<title id=" W98-1425.xml">a flexible shallow approach to text generation </title>
<section> in-depth and shallow generation.  </section>
<citcontext>
<prevsection>
<prevsent>2 .3 potent ia shortcomings f sha low generation methods prominent example for an early shallow generation system is ana \[kukich, 1983\], <papid> P83-1022 </papid>which reports about stockmarket developments.</prevsent>
<prevsent>while the kind of texts it produces can still be considered valuable today, anais implemented as widely unstructured rule-based system, which does not seem to be easily extendable and portable.</prevsent>
</prevsection>
<citsent citstr=" A97-1038 ">
since then, various shallow methods including canned text parts and some template-based techniques have been utilized, e.g. in cogent help \[white and caldwell, 1997\], <papid> A97-1038 </papid>in the system described in \[cawsey et al , 1995\], and in idas \[reiter et al , 1995\].</citsent>
<aftsection>
<nextsent>they feature simplicity where the intended application does not require fine-grained istinctions, such as the following techniques used in idas: ? canned text with embedded kb references ( carefully slide \[x\] out along its guide ), ? case frames with textual slot fillers, ( gently  in (manner :  gently ) ) . although these techniques seem to be able to provide the necessary distinctions for many practical applications in much simpler way than in-depth surface realization components can do, serious limitation lies in their inflexibility.
</nextsent>
<nextsent>the first example above requires the realization of ix\] to agree in number with the canned part; as this is not explicitly treated, the system seems to implicitly  know  that only singular descriptions will be inserted.
</nextsent>
<nextsent>moreover, canned texts as case role fillers may bear contextual influence, too, such as pronominals, or word order phenomena.
</nextsent>
<nextsent>thus, the flexibility of shallow generation techniques should be increased significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL169">
<title id=" W98-1425.xml">a flexible shallow approach to text generation </title>
<section> shallow generation in temsis.  </section>
<citcontext>
<prevsection>
<prevsent>it retrieves the relevant data from the temsis database.
</prevsent>
<prevsent>it combines fixed text blocks with the results of the realizer in language-neutral way.
</prevsent>
</prevsection>
<citsent citstr=" W96-0411 ">
ir expressions are consumed by the text realizer, which is version of the production system tg/2 described in \[busemann, 1996<papid> W96-0411 </papid>\].<papid> W96-0411 </papid></citsent>
<aftsection>
<nextsent>3.1 the temsis app i ca ion.
</nextsent>
<nextsent>with temsis transnational environmental management support and information system was created as part of transnational cooperation between the communities in the french-german urban agglomeration, moselle est and stadt verband saarbrijcken.
</nextsent>
<nextsent>networked information kiosks are being installed in number of communities to provide public and expert environmental infor-mation.
</nextsent>
<nextsent>the timely availability of relevant environmental information will improve the planning and reactive capabilities of the administration considerably.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL174">
<title id=" W98-1427.xml">generation as a solution to its own problem </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>lfs takes statistical data from labour force surveys and from this produces report on employ-ment statistics over the given period (iordanskaja et al 1992).
</prevsent>
<prevsent>modelexplainer takes data from graphical object oriented ata models and from this gen-erates textual description of the model (lavoie et al 1996).
</prevsent>
</prevsection>
<citsent citstr=" W96-0406 ">
postgraphe takes tabular data (of the sort found in typical spreadsheet) and generates report integrating both graphics and text (fasciano and lapalme, 1996).<papid> W96-0406 </papid></citsent>
<aftsection>
<nextsent>plandoc takes the data from simulation log file and from this produces report of the explored simulation options (mckeown et al 1994).
</nextsent>
<nextsent>alethgen takes data from customer database and produces customised letter (in french) (coch, 1996).<papid> C96-1043 </papid></nextsent>
<nextsent>a system developed by johanna moore and her colleagues at the university of pittsburg takes the data from sage, graphics presentation system (roth et al 1994), and produces an accom-panying natural language caption (mittal et al in press).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL175">
<title id=" W98-1427.xml">generation as a solution to its own problem </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>postgraphe takes tabular data (of the sort found in typical spreadsheet) and generates report integrating both graphics and text (fasciano and lapalme, 1996).<papid> W96-0406 </papid></prevsent>
<prevsent>plandoc takes the data from simulation log file and from this produces report of the explored simulation options (mckeown et al 1994).</prevsent>
</prevsection>
<citsent citstr=" C96-1043 ">
alethgen takes data from customer database and produces customised letter (in french) (coch, 1996).<papid> C96-1043 </papid></citsent>
<aftsection>
<nextsent>a system developed by johanna moore and her colleagues at the university of pittsburg takes the data from sage, graphics presentation system (roth et al 1994), and produces an accom-panying natural language caption (mittal et al in press).
</nextsent>
<nextsent>however, it doesn make the only sense.
</nextsent>
<nextsent>some applications require the user to interact rather more closely with the semantic model that drives the generation process, and there is small, but growing, number of systems that are able to provide this kind of interaction.
</nextsent>
<nextsent>they achieve this through common solution: interfaces that allow the user to engage in symbolic authoring of the generated text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL176">
<title id=" W98-1427.xml">generation as a solution to its own problem </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>exclass is an intelligent support ool for personnel officers writing (bilingual english and french) job descriptions.
</prevsent>
<prevsent>the user builds the job description by composing and editing concep-tual representations; these representations are trees of concepts from structured conceptual dictionary.
</prevsent>
</prevsection>
<citsent citstr=" A94-1001 ">
concepts are presented to the user through diagrammatic trees with natural an- guage labels (caldwell and korelsky, 1994).<papid> A94-1001 </papid></citsent>
<aftsection>
<nextsent>drafter-i s an authoring tool to support echnical authors and software developers in writing (bilingual english and french) software manuals.
</nextsent>
<nextsent>the user directly builds the domain model (semantic knowledge base) describing the procedures for using selected software application.
</nextsent>
<nextsent>as it is being constructed, the model is presented to the user through diagrams and fragments of text (paris et al 1995).
</nextsent>
<nextsent>g ist is an authoring tool to support forms designers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL177">
<title id=" W99-0503.xml">supervised learning of lexical semantic verb classes using frequency distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>vve zeport number of computatmnal ex-periments supervised learning whose goal is to automatmally classify set of verbs into lexmal semanuc classes, based on frequency dlstnbutmn approxlmatmns of grammatical features extracted from very large annotated corpus dlstnbuuons of five syntactic features that approximate tranmuvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance we conclude that corpus data is usable repository of verb class mformatmn, and that corpus- driven extraction of grammaucal features is promising methodology for automatm lexmal acqum,uon
</prevsent>
</prevsection>
<citsent citstr=" J93-2002 ">
recent years have witnessed shift in grammar de-velopment methodology, from crafting large gram-mars, to annotation of corpora correspondingly, there has been change from developing rule-based parsers to developing statmucal methods for reduc-ing grammatmal knowledge from annotated corpus data the shift has mostly occurred because build- mg w~de-coverage rammars is ume-consummg, er-ror prone, and difficult the same can be said for crafting the rich lexlcal representatmns that are central component of hngmstlc knowledge, andre- search automauc lexmal acquisition has sought to address this ((doff and jones, 1996, dorr, 1997), among others) yet there have been few attempts to learn fine-grained lexical classifications from the sta- tlsucal analysis of dlstnbutmnal data, analogously to the induction of syntacuc knowledge (though see, g , (brent, 1993, <papid> J93-2002 </papid>klavans and chodorow, 1992, <papid> C92-4177 </papid>resmk, 1992)) in this paper, we propose such a~ approach for the automauc classfficauon of ~erbs into lexlcal semantic lasses we can express the issues raised by this app loach as follows 1 whmh hngulstlc dlstmcuons among \[exlcsl.</citsent>
<aftsection>
<nextsent>classes can we expect to find a corpus ~ butions that approximate he relevant hngmstlc properttes?
</nextsent>
<nextsent>3 which frequency dlstnbuuons work best to dis-.
</nextsent>
<nextsent>tinguish the verb classes ~ in exploring these quesuons, we focus on verb clas- slficauon for several reasons verbs are very impor-tant sources of knowledge in many language ngi- neering tasks, and the relationships among verbs ap-pear to play major role the orgamzatmn and use of this knowledge knowledge about verb classes is crucml for lex,cal acqmsltton support of language generation and machine translatmn (dolt, 1997) and document cl~sfficatmn (klavans and kan, 1998), <papid> P98-1112 </papid>yet manual classfficauon of large numbers of verbs is difficult and resource intensive task (levm, 1993 miller et al, 1990, dang et l , 1998) to address these issues, we suggest hat one can tram an automatic lassffier for verbs on the basts of staustmal approxlmauons to verb dla theses we use dlatheses--alternatmns the expression of the ar-guments of the verb--following levm and dorr, for two reasons fnst, verb dla theses are syntacuc ues 1 we are aware that dlstnbutmnal pproach rests on.</nextsent>
<nextsent>one strong assumptmn regarding the nature of the repre- sentatmns under study semantic notmns and syntacuc notmns are correlated, at least part this assurapuon is under debate (bnscoe and copestake, 1995, levm, 1993, dorr and jones, 1996, <papid> C96-1055 </papid>dorr, 1997), but we adopt ~t here without further dlscussmn 15   ? to semantic lasses, hence they can be more easily captured by corpus-based techniques second, using verb d~atheses reduces no,se there ~s certain con-sensus (bnscoe and copestake, 1995, pustejovsky, 1995, palmer, 1999) that verb dmtheses are regular sense extensmns hence focussing on thin type of classfficatmn allows one to abstract from the prob-lem of word sense dmamb,guatmn d treat remdual d~fferences word senses as no~se the classffica- tmn task we present an m-depth case study, which we apply machine learning techmques to automaucally classify set of verbs based on d~stnbutmns of gram- maucal indicators of dmtheses, extracted from very large corpus we look at three very mterest- mg classes of verbs unergauves, unaccusauves, and object-drop verbs (levm, 1993) these are interest- mg classes because they all parucapate the trans~- uvlty alternatmn, and they are minimal parrs - that as, small number of well-defined mtmctmns d~ffer- entmte their trans,tlve/mtranmuve behavmr thus, we expect the differences their dmtnbuttons to be small, entailing fine-grained lscr,mmauon task that prowdes challenging testbed for automatic classfficatmn the specffic theoretical questmn we mvesugate ~s whether the factors underlying the verb class dm- tmctmns are reflected the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs the corpus in doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh e behav- tor of the classes of verbs w~th respect the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the sta- ustmal dmtnbutmns are available, can be used suc-cessfully by an automatic lassifier in m~ttal work (stevenson and merlo, 1999), ~<papid> E99-1007 </papid>e found that hngmstlcally motivated features that d~s- tmgmsh the verb classes can be extracted from an annotated, and one case parsed, corpus these features are sufficient to almost halve the error rate compared to chance (45% reductmn) auto- mauc verb classtficauon, suggesting that d~stnbu- uonal data prowdes knowledge useful to the class~- ficauon of verbs the focus of our original stud~ was tho demonstration prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm fea-tures, and an analysm of the relauve contrtbutmn of the various features to learmng th~s paper turns to the nnportant next steps of rephcatmg our find- rags using other training methods and learning al-gorithms, and analyzing the performance on each of tbe three classes of verbs this more detailed anal- ys~s of accuracy within each class turn leads to the development of new dlstrtbutmnal feature m- tended to improve dlscnmmabthty among t~o of the classes the addltmn of the ne~ feature successfully reduces the error rate of out mltml results classl- ficatmn by 19%, for 56% overall reductmn error rate compared to chance</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL178">
<title id=" W99-0503.xml">supervised learning of lexical semantic verb classes using frequency distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>

<prevsent>vve zeport number of computatmnal ex-periments supervised learning whose goal is to automatmally classify set of verbs into lexmal semanuc classes, based on frequency dlstnbutmn approxlmatmns of grammatical features extracted from very large annotated corpus dlstnbuuons of five syntactic features that approximate tranmuvlty alternatmns and thematic role assignments are sufficient to reduce error rate by 56% over chance we conclude that corpus data is usable repository of verb class mformatmn, and that corpus- driven extraction of grammaucal features is promising methodology for automatm lexmal acqum,uon
</prevsent>
</prevsection>
<citsent citstr=" C92-4177 ">
recent years have witnessed shift in grammar de-velopment methodology, from crafting large gram-mars, to annotation of corpora correspondingly, there has been change from developing rule-based parsers to developing statmucal methods for reduc-ing grammatmal knowledge from annotated corpus data the shift has mostly occurred because build- mg w~de-coverage rammars is ume-consummg, er-ror prone, and difficult the same can be said for crafting the rich lexlcal representatmns that are central component of hngmstlc knowledge, andre- search automauc lexmal acquisition has sought to address this ((doff and jones, 1996, dorr, 1997), among others) yet there have been few attempts to learn fine-grained lexical classifications from the sta- tlsucal analysis of dlstnbutmnal data, analogously to the induction of syntacuc knowledge (though see, g , (brent, 1993, <papid> J93-2002 </papid>klavans and chodorow, 1992, <papid> C92-4177 </papid>resmk, 1992)) in this paper, we propose such a~ approach for the automauc classfficauon of ~erbs into lexlcal semantic lasses we can express the issues raised by this app loach as follows 1 whmh hngulstlc dlstmcuons among \[exlcsl.</citsent>
<aftsection>
<nextsent>classes can we expect to find a corpus ~ butions that approximate he relevant hngmstlc properttes?
</nextsent>
<nextsent>3 which frequency dlstnbuuons work best to dis-.
</nextsent>
<nextsent>tinguish the verb classes ~ in exploring these quesuons, we focus on verb clas- slficauon for several reasons verbs are very impor-tant sources of knowledge in many language ngi- neering tasks, and the relationships among verbs ap-pear to play major role the orgamzatmn and use of this knowledge knowledge about verb classes is crucml for lex,cal acqmsltton support of language generation and machine translatmn (dolt, 1997) and document cl~sfficatmn (klavans and kan, 1998), <papid> P98-1112 </papid>yet manual classfficauon of large numbers of verbs is difficult and resource intensive task (levm, 1993 miller et al, 1990, dang et l , 1998) to address these issues, we suggest hat one can tram an automatic lassffier for verbs on the basts of staustmal approxlmauons to verb dla theses we use dlatheses--alternatmns the expression of the ar-guments of the verb--following levm and dorr, for two reasons fnst, verb dla theses are syntacuc ues 1 we are aware that dlstnbutmnal pproach rests on.</nextsent>
<nextsent>one strong assumptmn regarding the nature of the repre- sentatmns under study semantic notmns and syntacuc notmns are correlated, at least part this assurapuon is under debate (bnscoe and copestake, 1995, levm, 1993, dorr and jones, 1996, <papid> C96-1055 </papid>dorr, 1997), but we adopt ~t here without further dlscussmn 15   ? to semantic lasses, hence they can be more easily captured by corpus-based techniques second, using verb d~atheses reduces no,se there ~s certain con-sensus (bnscoe and copestake, 1995, pustejovsky, 1995, palmer, 1999) that verb dmtheses are regular sense extensmns hence focussing on thin type of classfficatmn allows one to abstract from the prob-lem of word sense dmamb,guatmn d treat remdual d~fferences word senses as no~se the classffica- tmn task we present an m-depth case study, which we apply machine learning techmques to automaucally classify set of verbs based on d~stnbutmns of gram- maucal indicators of dmtheses, extracted from very large corpus we look at three very mterest- mg classes of verbs unergauves, unaccusauves, and object-drop verbs (levm, 1993) these are interest- mg classes because they all parucapate the trans~- uvlty alternatmn, and they are minimal parrs - that as, small number of well-defined mtmctmns d~ffer- entmte their trans,tlve/mtranmuve behavmr thus, we expect the differences their dmtnbuttons to be small, entailing fine-grained lscr,mmauon task that prowdes challenging testbed for automatic classfficatmn the specffic theoretical questmn we mvesugate ~s whether the factors underlying the verb class dm- tmctmns are reflected the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs the corpus in doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh e behav- tor of the classes of verbs w~th respect the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the sta- ustmal dmtnbutmns are available, can be used suc-cessfully by an automatic lassifier in m~ttal work (stevenson and merlo, 1999), ~<papid> E99-1007 </papid>e found that hngmstlcally motivated features that d~s- tmgmsh the verb classes can be extracted from an annotated, and one case parsed, corpus these features are sufficient to almost halve the error rate compared to chance (45% reductmn) auto- mauc verb classtficauon, suggesting that d~stnbu- uonal data prowdes knowledge useful to the class~- ficauon of verbs the focus of our original stud~ was tho demonstration prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm fea-tures, and an analysm of the relauve contrtbutmn of the various features to learmng th~s paper turns to the nnportant next steps of rephcatmg our find- rags using other training methods and learning al-gorithms, and analyzing the performance on each of tbe three classes of verbs this more detailed anal- ys~s of accuracy within each class turn leads to the development of new dlstrtbutmnal feature m- tended to improve dlscnmmabthty among t~o of the classes the addltmn of the ne~ feature successfully reduces the error rate of out mltml results classl- ficatmn by 19%, for 56% overall reductmn error rate compared to chance</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL179">
<title id=" W99-0503.xml">supervised learning of lexical semantic verb classes using frequency distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>classes can we expect to find a corpus ~ butions that approximate he relevant hngmstlc properttes?
</prevsent>
<prevsent>3 which frequency dlstnbuuons work best to dis-.
</prevsent>
</prevsection>
<citsent citstr=" P98-1112 ">
tinguish the verb classes ~ in exploring these quesuons, we focus on verb clas- slficauon for several reasons verbs are very impor-tant sources of knowledge in many language ngi- neering tasks, and the relationships among verbs ap-pear to play major role the orgamzatmn and use of this knowledge knowledge about verb classes is crucml for lex,cal acqmsltton support of language generation and machine translatmn (dolt, 1997) and document cl~sfficatmn (klavans and kan, 1998), <papid> P98-1112 </papid>yet manual classfficauon of large numbers of verbs is difficult and resource intensive task (levm, 1993 miller et al, 1990, dang et l , 1998) to address these issues, we suggest hat one can tram an automatic lassffier for verbs on the basts of staustmal approxlmauons to verb dla theses we use dlatheses--alternatmns the expression of the ar-guments of the verb--following levm and dorr, for two reasons fnst, verb dla theses are syntacuc ues 1 we are aware that dlstnbutmnal pproach rests on.</citsent>
<aftsection>
<nextsent>one strong assumptmn regarding the nature of the repre- sentatmns under study semantic notmns and syntacuc notmns are correlated, at least part this assurapuon is under debate (bnscoe and copestake, 1995, levm, 1993, dorr and jones, 1996, <papid> C96-1055 </papid>dorr, 1997), but we adopt ~t here without further dlscussmn 15   ? to semantic lasses, hence they can be more easily captured by corpus-based techniques second, using verb d~atheses reduces no,se there ~s certain con-sensus (bnscoe and copestake, 1995, pustejovsky, 1995, palmer, 1999) that verb dmtheses are regular sense extensmns hence focussing on thin type of classfficatmn allows one to abstract from the prob-lem of word sense dmamb,guatmn d treat remdual d~fferences word senses as no~se the classffica- tmn task we present an m-depth case study, which we apply machine learning techmques to automaucally classify set of verbs based on d~stnbutmns of gram- maucal indicators of dmtheses, extracted from very large corpus we look at three very mterest- mg classes of verbs unergauves, unaccusauves, and object-drop verbs (levm, 1993) these are interest- mg classes because they all parucapate the trans~- uvlty alternatmn, and they are minimal parrs - that as, small number of well-defined mtmctmns d~ffer- entmte their trans,tlve/mtranmuve behavmr thus, we expect the differences their dmtnbuttons to be small, entailing fine-grained lscr,mmauon task that prowdes challenging testbed for automatic classfficatmn the specffic theoretical questmn we mvesugate ~s whether the factors underlying the verb class dm- tmctmns are reflected the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs the corpus in doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh e behav- tor of the classes of verbs w~th respect the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the sta- ustmal dmtnbutmns are available, can be used suc-cessfully by an automatic lassifier in m~ttal work (stevenson and merlo, 1999), ~<papid> E99-1007 </papid>e found that hngmstlcally motivated features that d~s- tmgmsh the verb classes can be extracted from an annotated, and one case parsed, corpus these features are sufficient to almost halve the error rate compared to chance (45% reductmn) auto- mauc verb classtficauon, suggesting that d~stnbu- uonal data prowdes knowledge useful to the class~- ficauon of verbs the focus of our original stud~ was tho demonstration prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm fea-tures, and an analysm of the relauve contrtbutmn of the various features to learmng th~s paper turns to the nnportant next steps of rephcatmg our find- rags using other training methods and learning al-gorithms, and analyzing the performance on each of tbe three classes of verbs this more detailed anal- ys~s of accuracy within each class turn leads to the development of new dlstrtbutmnal feature m- tended to improve dlscnmmabthty among t~o of the classes the addltmn of the ne~ feature successfully reduces the error rate of out mltml results classl- ficatmn by 19%, for 56% overall reductmn error rate compared to chance</nextsent>
<nextsent>in this sectmn, we present mouvatmn for the mttml features that we mvesugated terms of their role learmng the verb classes we first present the hngmstlcally den~ed features then turn to e~tdence from experimental psychohngutstlcs to e\tend the set of potentially relevant features 2.1 features of the velb classes.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL180">
<title id=" W99-0503.xml">supervised learning of lexical semantic verb classes using frequency distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 which frequency dlstnbuuons work best to dis-.
</prevsent>
<prevsent>tinguish the verb classes ~ in exploring these quesuons, we focus on verb clas- slficauon for several reasons verbs are very impor-tant sources of knowledge in many language ngi- neering tasks, and the relationships among verbs ap-pear to play major role the orgamzatmn and use of this knowledge knowledge about verb classes is crucml for lex,cal acqmsltton support of language generation and machine translatmn (dolt, 1997) and document cl~sfficatmn (klavans and kan, 1998), <papid> P98-1112 </papid>yet manual classfficauon of large numbers of verbs is difficult and resource intensive task (levm, 1993 miller et al, 1990, dang et l , 1998) to address these issues, we suggest hat one can tram an automatic lassffier for verbs on the basts of staustmal approxlmauons to verb dla theses we use dlatheses--alternatmns the expression of the ar-guments of the verb--following levm and dorr, for two reasons fnst, verb dla theses are syntacuc ues 1 we are aware that dlstnbutmnal pproach rests on.</prevsent>
</prevsection>
<citsent citstr=" C96-1055 ">
one strong assumptmn regarding the nature of the repre- sentatmns under study semantic notmns and syntacuc notmns are correlated, at least part this assurapuon is under debate (bnscoe and copestake, 1995, levm, 1993, dorr and jones, 1996, <papid> C96-1055 </papid>dorr, 1997), but we adopt ~t here without further dlscussmn 15   ? to semantic lasses, hence they can be more easily captured by corpus-based techniques second, using verb d~atheses reduces no,se there ~s certain con-sensus (bnscoe and copestake, 1995, pustejovsky, 1995, palmer, 1999) that verb dmtheses are regular sense extensmns hence focussing on thin type of classfficatmn allows one to abstract from the prob-lem of word sense dmamb,guatmn d treat remdual d~fferences word senses as no~se the classffica- tmn task we present an m-depth case study, which we apply machine learning techmques to automaucally classify set of verbs based on d~stnbutmns of gram- maucal indicators of dmtheses, extracted from very large corpus we look at three very mterest- mg classes of verbs unergauves, unaccusauves, and object-drop verbs (levm, 1993) these are interest- mg classes because they all parucapate the trans~- uvlty alternatmn, and they are minimal parrs - that as, small number of well-defined mtmctmns d~ffer- entmte their trans,tlve/mtranmuve behavmr thus, we expect the differences their dmtnbuttons to be small, entailing fine-grained lscr,mmauon task that prowdes challenging testbed for automatic classfficatmn the specffic theoretical questmn we mvesugate ~s whether the factors underlying the verb class dm- tmctmns are reflected the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs the corpus in doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh e behav- tor of the classes of verbs w~th respect the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the sta- ustmal dmtnbutmns are available, can be used suc-cessfully by an automatic lassifier in m~ttal work (stevenson and merlo, 1999), ~<papid> E99-1007 </papid>e found that hngmstlcally motivated features that d~s- tmgmsh the verb classes can be extracted from an annotated, and one case parsed, corpus these features are sufficient to almost halve the error rate compared to chance (45% reductmn) auto- mauc verb classtficauon, suggesting that d~stnbu- uonal data prowdes knowledge useful to the class~- ficauon of verbs the focus of our original stud~ was tho demonstration prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm fea-tures, and an analysm of the relauve contrtbutmn of the various features to learmng th~s paper turns to the nnportant next steps of rephcatmg our find- rags using other training methods and learning al-gorithms, and analyzing the performance on each of tbe three classes of verbs this more detailed anal- ys~s of accuracy within each class turn leads to the development of new dlstrtbutmnal feature m- tended to improve dlscnmmabthty among t~o of the classes the addltmn of the ne~ feature successfully reduces the error rate of out mltml results classl- ficatmn by 19%, for 56% overall reductmn error rate compared to chance</citsent>
<aftsection>
<nextsent>in this sectmn, we present mouvatmn for the mttml features that we mvesugated terms of their role learmng the verb classes we first present the hngmstlcally den~ed features then turn to e~tdence from experimental psychohngutstlcs to e\tend the set of potentially relevant features 2.1 features of the velb classes.
</nextsent>
<nextsent>the three verb classes under mvesugatmn - unerga- uves, unaccusauves, and object-drop -differ the properties of their translttve/mtranslhve a\[terna- uons, which are exemphfied below unergauve (la) the horse raced past the barn (lb) the jockey raced the horse past the barn wnaccusatave (2a) the butter melted the pan (2b) the cook melted the butter the pan object-drop (3a) the boy washed the hall (3b) the boy washed the sentences (1) use an unergatwe velb.
</nextsent>
<nextsent>,accd unergatlves are mttansluve actmn verbs whose tran- sttlve form is the causattve counterpart of the m- transluve form thus, the subject of the intr ansi tive (la) becomes the object of the translh~e (lb) (brousseau and rltter 1991, hale and ke~set 1993 levm and rappaport ho~,av, 1995) the sentences (2) use an unaccusauve verb, melted lnac- cusatlves are in transitive change of state ~et bs (2a) hke unergauves, the translu~e counterpart for the.,e verbs ts also causative (2b) the sentence~ (3) use an object-dtop verb washed, the~e  , elt:,~ haxe non-causau~e tran ~ltl~,e/intransltl~,,e al\[elnltton in ~ hlch the object is sm~pl~ opt tonal both unergauves and unaccusatl~es \[la~e causattve trans~u~e form, but differ the semanuc roles that they assign to the paructpants the e~ent described in an mtranstuve unetgauve, the  ,ubject ts an 4.gent ithe doer of the e~ent), and an intran- sitive unaccusauve, the subject ts theme (~ome- thing affected by the e~ent) the role assignments the corresponding semanuc arguments of the ttan- s~u~e forms--i , the dnect objects--a~e the ~ame 16 with the addition of causal agent (the causer of the event) as subject in both cases object-drop verbs simply assign agent to the subject and theme to the optional object we expect the differing semantic role assignments of the verb classes to be reflected their syntac-tic behavior, and consequently in the distributional data we collect from corpus the three classes can be characterized by their occurrence in two alter-nations the transittve/mtrans~tive alt rnation and the causative alternation unergatives are distin-guished from the other classes being rare in the transitive form (see (stevenson and merlo, 1997) for an explanation of this fact) both unergatives and unaccusatives are dlstmgmshed from object-drop being causative in their transitive form, and sun- darly we expect this to be reflected in amount of detectable causative use furthermore, since the caus&tlve; is transitive use, and the transitive use of unergatlves expected to be rare, causativity should primarily distinguish unaccusatlves from object- drops in conclusion, we expect he defining features of the verb classes--the intransitive/transitive and causative ~lternatlons--to lead to distributional dif-ferences the observed usages of the verbs in these alternations 2 2 psychollngmst~cally relevant features the verbs under study not only differ in their thematic properties, they also differ in their pro- cessmg properties because these verbs can occur both in trans~tive and an in transitive form, they have been particularly studied in the context of the mare verb/reduced relative (mv/i:tr) ambiguity il-lustrated below (bever, 1970) the horse raced past the barn fell the verb ~aced can be interpreted as either past tense main verb, or as past participle w~thm re-duced relative clause (l , the horse \[that was\] raced past the barn) because fell is the main verb, the le- duced relative lnterpretatmn of raced is required for coherent analysis of the complete sentence but the main verb interpretation of raced is so strongly preferred that people experience great difficulty at the verb fell, unable to integrate it with the inter-pretation that has been developed to that point however, the reduced relative interpretation is not difficult for all verbs, as in the follo~mg example the boy washed in the tub was angry the difference in ease of interpreting the lesolu- tions of this ambiguity has been shown to be sen-sitive to both frequency differentials (macdonald 1994, true swell, 1996) and to verb class d~stmctmns (stevenson and merlo, 1997, flhp et al, 1999) consider the features that d~stmguish e t~o res-olutions of the m\ , /rr ambiguity mv the horse raced past the barn quickly rr the horse raced past the barn fell in the main verb resolution, the ambiguous ~erb raced is used in its in transitive form, while in there- duced relative, it is used in its transitive, causative form these features correspond irectly to the defining alternations of the three verb classes un-der study (intransitive/transitive, causative) ~,ddl- tion ally, we see that other related features to these usages erve to distinguish the two resolutions of the ambiguity the mare verb form is active and mare verb part-of-speech (labeled as vbd by automatic pos taggers), by contrast, the reduced relative foim is passive and past partic~ple (tagged as \ bn) since these features (active/passive and vbd/vbn) are related to the intransitive/transitive alteination, we expect them to also exhibit d~stributloaal differ-ences among the verb classes specifically, ~e expect the unergatives to yield higher proportion of act~ and  vbd usage, since, as noted above, the transitive use of unergatwes rare
</nextsent>
<nextsent>features we assume that currently available large cotpola are reasonable approximation to language (pul- lum, 1996) using combined corpus of 65-mllhon words, we measured the relative frequenc) distribu-tions of the four linguistic features (vbd/~ bn ac- tive/passive, intransitive/transitive, causative/non- causative) over sample of verbs from the three lex- tcal semantic lasses 3 1 mater ia s ~e chose set of 20 verbs from each class based pll- maidy on the classfficatlon of verbs (le~ 1993) (see appendl~ ~) the uneigatlves ale maanei oi motion verbs the unaccusatl~es ale ~erbs of~haage of state the object-drop verbs are unspecified ob-ject alternation verbs the ~e~bs ~ere sele~led flora lenin classes based on their absolute fiequenc} ful ther more, they do not generally sho~ ma~l~ de- paitures from the intended verb sense the cotpu~ (though note that there are only 19 unaccu~atlxes because ,zpped, ~hlch ~as initially counted the unaccusatives, was then excluded from the aaal~- sis as it occurred mostly in different usage the corpus, as velb plus paltlcle ) most of the vetb~ can occur the transitive and in the passive each ~erb presents the ~ame folm the simple pa~t and the past palticlple in order to smlphf~ the ~ouat- 17 mg procedure, we made the assumptron that counts on this single verb form would approximate he dis-tribution of the features across all forms of the verb most counts were performed on the tagged versron of the brown corpus and on the portion of the wall street journal distmbuted by the acl/dci (years 1987, 1988, 1989), combined corpus excess of 65 mdhon words, with the exceptmn of causatrv- lty which was counted only for the 1988 year of the wsj, corpus of 29 million words 3 2 method we counted the occurrences of each verb token in transrtlve or mt~ansltr~e use (intr), an active or passive use (act), rn past pamcrple or smaple past use (vbd), and in causative or non-causative use (caus) more precrsely, features were counted as follows intr verb occurrence was counted as transrtlve if rmmediately followed by nominal group, else rt was counted as mtransitrve act mare verbs (tagged vbd) were counted as actrve, participles (tagged bn) counted as actrve ff the closest preceding auxiliary was have, as passive ff the closest preceding auxiliary was be vbd occurrences tagged vbd were simple past, vbn were past participle (each of the above three counts was normalized over all occurrences of the verb, yielding single relative frequency measure for each verb for that fea-ture ) caus the causative feature was approximated by the followmg steps frrst, for each verb, all cooc- currmg subjects and objects were extracted from parsed corpus (colhns, 1997) then the propor- tmn of overlap between the two multrsets of nouns was calculated, meant to capture the causative al-ternation, ~here the subject of the mtransrtrve can occur as the object of the trans~trve vve define overlap as the largest multi set of elements belong- mg to both the subjects and the object multi sets, eg {a,a,a,b}(3 {a} = {a,a,a} the proportron is the ratio between the o~erlap and the sum of the subject and object multrsets (for example, for the rumple sets above, the ratio would be 3/5 or 60 ) all ra~ and normahzed corpus data ale a~adable from the authors, and more detarl concerning data collect ron can be found (stevenson and merto, 1999) <papid> E99-1007 </papid>the frequency drstnbutrons of the verb alternatmn features yield vector for each verb that represents the relative frequency values for the verb on each drmensron, the set of 59 vectors constrtute the data for our machine learmng experiments template \[verb, vbd, act, intr, cads, class\] example \[opened, 79, 91, 31, 16, unacc\] our goal was to determine whether automatm clas- sfficatlon techniques could determine the class of verb from the distributional proper tms represented this vector in related work (stevenson and merlo, 1999) ~<papid> E99-1007 </papid>e describe initial unsupervised and supervised lealnmg experiments on this data, and discuss the contllbu- tlon of the four different features (the frequenc.~ dis- tributions) to accurac~ verb classfficatlon in thzs paper, we extend the work in several ~ays fu~t, ~e report further analysis of rephcauons of our mmal supervised learning results next, we demonstrate srmdar performance using different training methods and learning algorithms, mdmatmg that the perfor-mance rs independent of the particular learning ap-proach furthermore, these addrtronal e~penments allow us to evaluate the performance separately on each of the three verb classes finally, based on tins evaluation, we suggest new feature to better drs- tmgmsh the thematic proper tms of the classes, and present experimental results howing that its use rm- proves our original accuracy rate 4.1 in tml exper iments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL181">
<title id=" W99-0503.xml">supervised learning of lexical semantic verb classes using frequency distributions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 which frequency dlstnbuuons work best to dis-.
</prevsent>
<prevsent>tinguish the verb classes ~ in exploring these quesuons, we focus on verb clas- slficauon for several reasons verbs are very impor-tant sources of knowledge in many language ngi- neering tasks, and the relationships among verbs ap-pear to play major role the orgamzatmn and use of this knowledge knowledge about verb classes is crucml for lex,cal acqmsltton support of language generation and machine translatmn (dolt, 1997) and document cl~sfficatmn (klavans and kan, 1998), <papid> P98-1112 </papid>yet manual classfficauon of large numbers of verbs is difficult and resource intensive task (levm, 1993 miller et al, 1990, dang et l , 1998) to address these issues, we suggest hat one can tram an automatic lassffier for verbs on the basts of staustmal approxlmauons to verb dla theses we use dlatheses--alternatmns the expression of the ar-guments of the verb--following levm and dorr, for two reasons fnst, verb dla theses are syntacuc ues 1 we are aware that dlstnbutmnal pproach rests on.</prevsent>
</prevsection>
<citsent citstr=" E99-1007 ">
one strong assumptmn regarding the nature of the repre- sentatmns under study semantic notmns and syntacuc notmns are correlated, at least part this assurapuon is under debate (bnscoe and copestake, 1995, levm, 1993, dorr and jones, 1996, <papid> C96-1055 </papid>dorr, 1997), but we adopt ~t here without further dlscussmn 15   ? to semantic lasses, hence they can be more easily captured by corpus-based techniques second, using verb d~atheses reduces no,se there ~s certain con-sensus (bnscoe and copestake, 1995, pustejovsky, 1995, palmer, 1999) that verb dmtheses are regular sense extensmns hence focussing on thin type of classfficatmn allows one to abstract from the prob-lem of word sense dmamb,guatmn d treat remdual d~fferences word senses as no~se the classffica- tmn task we present an m-depth case study, which we apply machine learning techmques to automaucally classify set of verbs based on d~stnbutmns of gram- maucal indicators of dmtheses, extracted from very large corpus we look at three very mterest- mg classes of verbs unergauves, unaccusauves, and object-drop verbs (levm, 1993) these are interest- mg classes because they all parucapate the trans~- uvlty alternatmn, and they are minimal parrs - that as, small number of well-defined mtmctmns d~ffer- entmte their trans,tlve/mtranmuve behavmr thus, we expect the differences their dmtnbuttons to be small, entailing fine-grained lscr,mmauon task that prowdes challenging testbed for automatic classfficatmn the specffic theoretical questmn we mvesugate ~s whether the factors underlying the verb class dm- tmctmns are reflected the statmttcal dmtnbutmns of lex~cal features related to dmtheses presented by the md,v~dual verbs the corpus in doing th~s, we address the questmns above by determining what are the lexmal features that could d~stmgmsh e behav- tor of the classes of verbs w~th respect the relevant dmtheses, ~hmh of those features can be gleaned from the corpus, and which of those, once the sta- ustmal dmtnbutmns are available, can be used suc-cessfully by an automatic lassifier in m~ttal work (stevenson and merlo, 1999), ~<papid> E99-1007 </papid>e found that hngmstlcally motivated features that d~s- tmgmsh the verb classes can be extracted from an annotated, and one case parsed, corpus these features are sufficient to almost halve the error rate compared to chance (45% reductmn) auto- mauc verb classtficauon, suggesting that d~stnbu- uonal data prowdes knowledge useful to the class~- ficauon of verbs the focus of our original stud~ was tho demonstration prmctple of l~a.nmg verb classes from frequency d~stnbutmns ofsyntactm fea-tures, and an analysm of the relauve contrtbutmn of the various features to learmng th~s paper turns to the nnportant next steps of rephcatmg our find- rags using other training methods and learning al-gorithms, and analyzing the performance on each of tbe three classes of verbs this more detailed anal- ys~s of accuracy within each class turn leads to the development of new dlstrtbutmnal feature m- tended to improve dlscnmmabthty among t~o of the classes the addltmn of the ne~ feature successfully reduces the error rate of out mltml results classl- ficatmn by 19%, for 56% overall reductmn error rate compared to chance</citsent>
<aftsection>
<nextsent>in this sectmn, we present mouvatmn for the mttml features that we mvesugated terms of their role learmng the verb classes we first present the hngmstlcally den~ed features then turn to e~tdence from experimental psychohngutstlcs to e\tend the set of potentially relevant features 2.1 features of the velb classes.
</nextsent>
<nextsent>the three verb classes under mvesugatmn - unerga- uves, unaccusauves, and object-drop -differ the properties of their translttve/mtranslhve a\[terna- uons, which are exemphfied below unergauve (la) the horse raced past the barn (lb) the jockey raced the horse past the barn wnaccusatave (2a) the butter melted the pan (2b) the cook melted the butter the pan object-drop (3a) the boy washed the hall (3b) the boy washed the sentences (1) use an unergatwe velb.
</nextsent>
<nextsent>,accd unergatlves are mttansluve actmn verbs whose tran- sttlve form is the causattve counterpart of the m- transluve form thus, the subject of the intr ansi tive (la) becomes the object of the translh~e (lb) (brousseau and rltter 1991, hale and ke~set 1993 levm and rappaport ho~,av, 1995) the sentences (2) use an unaccusauve verb, melted lnac- cusatlves are in transitive change of state ~et bs (2a) hke unergauves, the translu~e counterpart for the.,e verbs ts also causative (2b) the sentence~ (3) use an object-dtop verb washed, the~e  , elt:,~ haxe non-causau~e tran ~ltl~,e/intransltl~,,e al\[elnltton in ~ hlch the object is sm~pl~ opt tonal both unergauves and unaccusatl~es \[la~e causattve trans~u~e form, but differ the semanuc roles that they assign to the paructpants the e~ent described in an mtranstuve unetgauve, the  ,ubject ts an 4.gent ithe doer of the e~ent), and an intran- sitive unaccusauve, the subject ts theme (~ome- thing affected by the e~ent) the role assignments the corresponding semanuc arguments of the ttan- s~u~e forms--i , the dnect objects--a~e the ~ame 16 with the addition of causal agent (the causer of the event) as subject in both cases object-drop verbs simply assign agent to the subject and theme to the optional object we expect the differing semantic role assignments of the verb classes to be reflected their syntac-tic behavior, and consequently in the distributional data we collect from corpus the three classes can be characterized by their occurrence in two alter-nations the transittve/mtrans~tive alt rnation and the causative alternation unergatives are distin-guished from the other classes being rare in the transitive form (see (stevenson and merlo, 1997) for an explanation of this fact) both unergatives and unaccusatives are dlstmgmshed from object-drop being causative in their transitive form, and sun- darly we expect this to be reflected in amount of detectable causative use furthermore, since the caus&tlve; is transitive use, and the transitive use of unergatlves expected to be rare, causativity should primarily distinguish unaccusatlves from object- drops in conclusion, we expect he defining features of the verb classes--the intransitive/transitive and causative ~lternatlons--to lead to distributional dif-ferences the observed usages of the verbs in these alternations 2 2 psychollngmst~cally relevant features the verbs under study not only differ in their thematic properties, they also differ in their pro- cessmg properties because these verbs can occur both in trans~tive and an in transitive form, they have been particularly studied in the context of the mare verb/reduced relative (mv/i:tr) ambiguity il-lustrated below (bever, 1970) the horse raced past the barn fell the verb ~aced can be interpreted as either past tense main verb, or as past participle w~thm re-duced relative clause (l , the horse \[that was\] raced past the barn) because fell is the main verb, the le- duced relative lnterpretatmn of raced is required for coherent analysis of the complete sentence but the main verb interpretation of raced is so strongly preferred that people experience great difficulty at the verb fell, unable to integrate it with the inter-pretation that has been developed to that point however, the reduced relative interpretation is not difficult for all verbs, as in the follo~mg example the boy washed in the tub was angry the difference in ease of interpreting the lesolu- tions of this ambiguity has been shown to be sen-sitive to both frequency differentials (macdonald 1994, true swell, 1996) and to verb class d~stmctmns (stevenson and merlo, 1997, flhp et al, 1999) consider the features that d~stmguish e t~o res-olutions of the m\ , /rr ambiguity mv the horse raced past the barn quickly rr the horse raced past the barn fell in the main verb resolution, the ambiguous ~erb raced is used in its in transitive form, while in there- duced relative, it is used in its transitive, causative form these features correspond irectly to the defining alternations of the three verb classes un-der study (intransitive/transitive, causative) ~,ddl- tion ally, we see that other related features to these usages erve to distinguish the two resolutions of the ambiguity the mare verb form is active and mare verb part-of-speech (labeled as vbd by automatic pos taggers), by contrast, the reduced relative foim is passive and past partic~ple (tagged as \ bn) since these features (active/passive and vbd/vbn) are related to the intransitive/transitive alteination, we expect them to also exhibit d~stributloaal differ-ences among the verb classes specifically, ~e expect the unergatives to yield higher proportion of act~ and  vbd usage, since, as noted above, the transitive use of unergatwes rare
</nextsent>
<nextsent>features we assume that currently available large cotpola are reasonable approximation to language (pul- lum, 1996) using combined corpus of 65-mllhon words, we measured the relative frequenc) distribu-tions of the four linguistic features (vbd/~ bn ac- tive/passive, intransitive/transitive, causative/non- causative) over sample of verbs from the three lex- tcal semantic lasses 3 1 mater ia s ~e chose set of 20 verbs from each class based pll- maidy on the classfficatlon of verbs (le~ 1993) (see appendl~ ~) the uneigatlves ale maanei oi motion verbs the unaccusatl~es ale ~erbs of~haage of state the object-drop verbs are unspecified ob-ject alternation verbs the ~e~bs ~ere sele~led flora lenin classes based on their absolute fiequenc} ful ther more, they do not generally sho~ ma~l~ de- paitures from the intended verb sense the cotpu~ (though note that there are only 19 unaccu~atlxes because ,zpped, ~hlch ~as initially counted the unaccusatives, was then excluded from the aaal~- sis as it occurred mostly in different usage the corpus, as velb plus paltlcle ) most of the vetb~ can occur the transitive and in the passive each ~erb presents the ~ame folm the simple pa~t and the past palticlple in order to smlphf~ the ~ouat- 17 mg procedure, we made the assumptron that counts on this single verb form would approximate he dis-tribution of the features across all forms of the verb most counts were performed on the tagged versron of the brown corpus and on the portion of the wall street journal distmbuted by the acl/dci (years 1987, 1988, 1989), combined corpus excess of 65 mdhon words, with the exceptmn of causatrv- lty which was counted only for the 1988 year of the wsj, corpus of 29 million words 3 2 method we counted the occurrences of each verb token in transrtlve or mt~ansltr~e use (intr), an active or passive use (act), rn past pamcrple or smaple past use (vbd), and in causative or non-causative use (caus) more precrsely, features were counted as follows intr verb occurrence was counted as transrtlve if rmmediately followed by nominal group, else rt was counted as mtransitrve act mare verbs (tagged vbd) were counted as actrve, participles (tagged bn) counted as actrve ff the closest preceding auxiliary was have, as passive ff the closest preceding auxiliary was be vbd occurrences tagged vbd were simple past, vbn were past participle (each of the above three counts was normalized over all occurrences of the verb, yielding single relative frequency measure for each verb for that fea-ture ) caus the causative feature was approximated by the followmg steps frrst, for each verb, all cooc- currmg subjects and objects were extracted from parsed corpus (colhns, 1997) then the propor- tmn of overlap between the two multrsets of nouns was calculated, meant to capture the causative al-ternation, ~here the subject of the mtransrtrve can occur as the object of the trans~trve vve define overlap as the largest multi set of elements belong- mg to both the subjects and the object multi sets, eg {a,a,a,b}(3 {a} = {a,a,a} the proportron is the ratio between the o~erlap and the sum of the subject and object multrsets (for example, for the rumple sets above, the ratio would be 3/5 or 60 ) all ra~ and normahzed corpus data ale a~adable from the authors, and more detarl concerning data collect ron can be found (stevenson and merto, 1999) <papid> E99-1007 </papid>the frequency drstnbutrons of the verb alternatmn features yield vector for each verb that represents the relative frequency values for the verb on each drmensron, the set of 59 vectors constrtute the data for our machine learmng experiments template \[verb, vbd, act, intr, cads, class\] example \[opened, 79, 91, 31, 16, unacc\] our goal was to determine whether automatm clas- sfficatlon techniques could determine the class of verb from the distributional proper tms represented this vector in related work (stevenson and merlo, 1999) ~<papid> E99-1007 </papid>e describe initial unsupervised and supervised lealnmg experiments on this data, and discuss the contllbu- tlon of the four different features (the frequenc.~ dis- tributions) to accurac~ verb classfficatlon in thzs paper, we extend the work in several ~ays fu~t, ~e report further analysis of rephcauons of our mmal supervised learning results next, we demonstrate srmdar performance using different training methods and learning algorithms, mdmatmg that the perfor-mance rs independent of the particular learning ap-proach furthermore, these addrtronal e~penments allow us to evaluate the performance separately on each of the three verb classes finally, based on tins evaluation, we suggest new feature to better drs- tmgmsh the thematic proper tms of the classes, and present experimental results howing that its use rm- proves our original accuracy rate 4.1 in tml exper iments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL184">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>shallow parsing informa-tion such as nps and other syntactic sequences have been found useful in many large-scale lan-guage processing applications including infor-mation extraction and text summarization.
</prevsent>
<prevsent>a lot of the work on shallow parsing over the past years has concentrated on manual construction of rules.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
the observation that shallow syntactic information can be extracted using local infor-mation - by examining the pattern itself, its nearby context and the local part-of-speech in-formation - has motivated the use of learning methods to recognize these patterns (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>* research supported by nsf grants iis-9801638 and sbr-9873450.
</nextsent>
<nextsent>t research supported by nsf grant ccr-9502540.
</nextsent>
<nextsent>this paper presents general learning ap-proach for identifying syntactic patterns, based on the snow learning architecture (roth, 1998; roth, 1999).
</nextsent>
<nextsent>the snow learning architecture is sparse network of linear ftmctions over pre-defined or incrementally learned feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL187">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>shallow parsing informa-tion such as nps and other syntactic sequences have been found useful in many large-scale lan-guage processing applications including infor-mation extraction and text summarization.
</prevsent>
<prevsent>a lot of the work on shallow parsing over the past years has concentrated on manual construction of rules.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
the observation that shallow syntactic information can be extracted using local infor-mation - by examining the pattern itself, its nearby context and the local part-of-speech in-formation - has motivated the use of learning methods to recognize these patterns (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>* research supported by nsf grants iis-9801638 and sbr-9873450.
</nextsent>
<nextsent>t research supported by nsf grant ccr-9502540.
</nextsent>
<nextsent>this paper presents general learning ap-proach for identifying syntactic patterns, based on the snow learning architecture (roth, 1998; roth, 1999).
</nextsent>
<nextsent>the snow learning architecture is sparse network of linear ftmctions over pre-defined or incrementally learned feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL189">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>shallow parsing informa-tion such as nps and other syntactic sequences have been found useful in many large-scale lan-guage processing applications including infor-mation extraction and text summarization.
</prevsent>
<prevsent>a lot of the work on shallow parsing over the past years has concentrated on manual construction of rules.
</prevsent>
</prevsection>
<citsent citstr=" P98-1010 ">
the observation that shallow syntactic information can be extracted using local infor-mation - by examining the pattern itself, its nearby context and the local part-of-speech in-formation - has motivated the use of learning methods to recognize these patterns (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>* research supported by nsf grants iis-9801638 and sbr-9873450.
</nextsent>
<nextsent>t research supported by nsf grant ccr-9502540.
</nextsent>
<nextsent>this paper presents general learning ap-proach for identifying syntactic patterns, based on the snow learning architecture (roth, 1998; roth, 1999).
</nextsent>
<nextsent>the snow learning architecture is sparse network of linear ftmctions over pre-defined or incrementally learned feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL190">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>shallow parsing informa-tion such as nps and other syntactic sequences have been found useful in many large-scale lan-guage processing applications including infor-mation extraction and text summarization.
</prevsent>
<prevsent>a lot of the work on shallow parsing over the past years has concentrated on manual construction of rules.
</prevsent>
</prevsection>
<citsent citstr=" P98-1034 ">
the observation that shallow syntactic information can be extracted using local infor-mation - by examining the pattern itself, its nearby context and the local part-of-speech in-formation - has motivated the use of learning methods to recognize these patterns (church, 1988; <papid> A88-1019 </papid>ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998; <papid> P98-1010 </papid>cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>* research supported by nsf grants iis-9801638 and sbr-9873450.
</nextsent>
<nextsent>t research supported by nsf grant ccr-9502540.
</nextsent>
<nextsent>this paper presents general learning ap-proach for identifying syntactic patterns, based on the snow learning architecture (roth, 1998; roth, 1999).
</nextsent>
<nextsent>the snow learning architecture is sparse network of linear ftmctions over pre-defined or incrementally learned feature space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL191">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> i t roduct ion   </section>
<citcontext>
<prevsection>
<prevsent>the snow learning architecture is sparse network of linear ftmctions over pre-defined or incrementally learned feature space.
</prevsent>
<prevsent>snow is specifically tailored for learning in do-mains in which the potential number of infor-mation sources (features) taking part in deci-sions is very large - of which nlp is princi-pal example.
</prevsent>
</prevsection>
<citsent citstr=" P98-2186 ">
preliminary versions of it have al-ready been used successfully on several tasks in natural language processing (roth, 1998; gold-ing and roth, 1999; roth and zelenko, 1998).<papid> P98-2186 </papid></citsent>
<aftsection>
<nextsent>in particular, snow sparse architecture sup-ports well chaining and combining predictors to produce coherent inference.
</nextsent>
<nextsent>this property of the architecture is the base for the learning ap-proach studied here in the context of shallow parsing.
</nextsent>
<nextsent>shallow parsing tasks often involve the iden-tification of syntactic phrases or of words that participate in syntactic relationship.
</nextsent>
<nextsent>com-putationally, each decision of this sort involves multiple predictions that interact in some way.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL213">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> snow.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 data.
</prevsent>
<prevsent>in order to be able to compare our results with the results obtained by other researchers, we worked with the same datasets already used by (ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al., 1998) <papid> P98-1010 </papid>for np and sv detection.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
these datasets were based on the wall street journal cor-pus in the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>for np, the training and test corpus was pre-pared from sections 15 to 18 and section 20, respectively; the sv corpus was prepared from sections 1 to 9 for training and section 0 for testing.
</nextsent>
<nextsent>instead of using the np bracketing in-formation present in the tagged treebank data, ramshaw and marcus modified the data so as to include bracketing information related only to the non-recursive, base nps present in each sentence while the subject verb phrases were taken as is. the datasets include pos tag information generated by ramshaw and mar-cus using brill transformational part-of-speech tagger (brill, 1995).<papid> J95-4004 </papid></nextsent>
<nextsent>the sizes of the training and test data are summarized in table 1 and table 2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL214">
<title id=" W99-0621.xml">a learning approach to shallow parsing </title>
<section> snow.  </section>
<citcontext>
<prevsection>
<prevsent>these datasets were based on the wall street journal cor-pus in the penn treebank (marcus et al, 1993).<papid> J93-2004 </papid></prevsent>
<prevsent>for np, the training and test corpus was pre-pared from sections 15 to 18 and section 20, respectively; the sv corpus was prepared from sections 1 to 9 for training and section 0 for testing.</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
instead of using the np bracketing in-formation present in the tagged treebank data, ramshaw and marcus modified the data so as to include bracketing information related only to the non-recursive, base nps present in each sentence while the subject verb phrases were taken as is. the datasets include pos tag information generated by ramshaw and mar-cus using brill transformational part-of-speech tagger (brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>the sizes of the training and test data are summarized in table 1 and table 2.
</nextsent>
<nextsent>4.2 parameters.
</nextsent>
<nextsent>the open/close system has two adjustable pa-rameters, r\[ and v\], the threshold for the open and close bracket predictors, respectively.
</nextsent>
<nextsent>for all experiments, the system is first trained on 90% of the training data and then tested on the remaining 10%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL224">
<title id=" W98-1241.xml">reconciliation of unsupervised clustering segmentation and cohesion </title>
<section> segmentation and grammar.  </section>
<citcontext>
<prevsection>
<prevsent>classes were formed by technique which tunas out to be clustering using hamming distance of 2 (or 3 in some experiments), in which classes can be merged (union) and the eornmon coset determined (intersection).
</prevsent>
<prevsent>the size and coverage of the individual left and fight cosets and their union and intersection gave eight measures of the strength of class, and in all eases identified the vowels as the strongest class for the original dictionary corpus, and for most other corpora tried, with right context appearing more useful than left, eoset size being more accurate than coset coverage, union size being more reliable than intersection size.
</prevsent>
</prevsection>
<citsent citstr=" W97-1011 ">
note that powers (1997<papid> W97-1011 </papid>a) generalizes the approach and considers multitude of different clustering metrics and methods, introducing pair of goodness measures which allow more principled approach to closing and evaluating clusters (rather than closing at specific cluster, you close when the goodness measure aches its first local maximum).</citsent>
<aftsection>
<nextsent>in the powers (1992) experiments, classes were added as new units and the process was repeated.
</nextsent>
<nextsent>the fuzzy variable size candidate units for the next level meant hat hyperelasses of context-free rules were learned.
</nextsent>
<nextsent>however the grammar led to high levels of ambiguity using non-deterministic parsing, and the presented hierarchy is based arbitrarily on simple greedy approach, but (for this reason) performance as recognizer/parser was not evaluated.
</nextsent>
<nextsent>though in this work phonologically, morphologically and grammatically meaningful classes and structure were formed, up to phrase/clause level, no interpretation the structures or classes was offered, and no attempt was made to discover or propose cohesive constraints or semantic relationships.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL245">
<title id=" W99-0307.xml">experiments in constructing a corpus of discourse trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>empirical studies of discourse structure have pri-marily focused on identifying discourse segment boundaries and their linguistic correlates.
</prevsent>
<prevsent>very little attention has been paid so far to the high- level, rhetorical relations that hold between dis-course segments.
</prevsent>
</prevsection>
<citsent citstr=" P87-1023 ">
in some cases, the role of these relations was considered to fall outside the scope of study (flammia and zue, 1995); in other cases, judgements were made with respect a taxon-omy of very few intention-based relations (usually dominance and satisfaction-precedence) (grosz and hirschberg, 1992; nakatani et al, 1995; hirschberg and litman, 1987; <papid> P87-1023 </papid>passonneau and litman, 1997; <papid> J97-1005 </papid>carletta et al, 1997).<papid> J97-1002 </papid></citsent>
<aftsection>
<nextsent>and in the only case in which rich taxonomy of 29 relations was used (moser and moore, 1997), the corpus was small and spe-cific to very restricted genre: written interactions between student and tutor on the subject of fault location and repair in electronic ircuitry.
</nextsent>
<nextsent>inspite of many influential proposals in the lin-guistic of discourse structures and relations (ballard et al, 1971; grimes, 1975; halliday and hasan, 1976; martin, 1992; mann and thompson, 1988; sanders et al, 1992; sanders et al, 1993; asher, 1993; lascarides and asher, 1993; knott, 1995; hovy and maier, 1993), number of empirical questions remain to be answered.
</nextsent>
<nextsent>can human judges construct rich discourse structures in manner that ensures inter-judge agreement that is statistically significant?
</nextsent>
<nextsent>how can one measure the agreement?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL246">
<title id=" W99-0307.xml">experiments in constructing a corpus of discourse trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>empirical studies of discourse structure have pri-marily focused on identifying discourse segment boundaries and their linguistic correlates.
</prevsent>
<prevsent>very little attention has been paid so far to the high- level, rhetorical relations that hold between dis-course segments.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
in some cases, the role of these relations was considered to fall outside the scope of study (flammia and zue, 1995); in other cases, judgements were made with respect a taxon-omy of very few intention-based relations (usually dominance and satisfaction-precedence) (grosz and hirschberg, 1992; nakatani et al, 1995; hirschberg and litman, 1987; <papid> P87-1023 </papid>passonneau and litman, 1997; <papid> J97-1005 </papid>carletta et al, 1997).<papid> J97-1002 </papid></citsent>
<aftsection>
<nextsent>and in the only case in which rich taxonomy of 29 relations was used (moser and moore, 1997), the corpus was small and spe-cific to very restricted genre: written interactions between student and tutor on the subject of fault location and repair in electronic ircuitry.
</nextsent>
<nextsent>inspite of many influential proposals in the lin-guistic of discourse structures and relations (ballard et al, 1971; grimes, 1975; halliday and hasan, 1976; martin, 1992; mann and thompson, 1988; sanders et al, 1992; sanders et al, 1993; asher, 1993; lascarides and asher, 1993; knott, 1995; hovy and maier, 1993), number of empirical questions remain to be answered.
</nextsent>
<nextsent>can human judges construct rich discourse structures in manner that ensures inter-judge agreement that is statistically significant?
</nextsent>
<nextsent>how can one measure the agreement?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL247">
<title id=" W99-0307.xml">experiments in constructing a corpus of discourse trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>empirical studies of discourse structure have pri-marily focused on identifying discourse segment boundaries and their linguistic correlates.
</prevsent>
<prevsent>very little attention has been paid so far to the high- level, rhetorical relations that hold between dis-course segments.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
in some cases, the role of these relations was considered to fall outside the scope of study (flammia and zue, 1995); in other cases, judgements were made with respect a taxon-omy of very few intention-based relations (usually dominance and satisfaction-precedence) (grosz and hirschberg, 1992; nakatani et al, 1995; hirschberg and litman, 1987; <papid> P87-1023 </papid>passonneau and litman, 1997; <papid> J97-1005 </papid>carletta et al, 1997).<papid> J97-1002 </papid></citsent>
<aftsection>
<nextsent>and in the only case in which rich taxonomy of 29 relations was used (moser and moore, 1997), the corpus was small and spe-cific to very restricted genre: written interactions between student and tutor on the subject of fault location and repair in electronic ircuitry.
</nextsent>
<nextsent>inspite of many influential proposals in the lin-guistic of discourse structures and relations (ballard et al, 1971; grimes, 1975; halliday and hasan, 1976; martin, 1992; mann and thompson, 1988; sanders et al, 1992; sanders et al, 1993; asher, 1993; lascarides and asher, 1993; knott, 1995; hovy and maier, 1993), number of empirical questions remain to be answered.
</nextsent>
<nextsent>can human judges construct rich discourse structures in manner that ensures inter-judge agreement that is statistically significant?
</nextsent>
<nextsent>how can one measure the agreement?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL248">
<title id=" W99-0307.xml">experiments in constructing a corpus of discourse trees </title>
<section> the experiment.  </section>
<citcontext>
<prevsection>
<prevsent>the table also shows the percentage ofcases in which the annotators used the label other-relat1 on.
</prevsent>
<prevsent>problems with the method.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
it has been argued that the reliability of coding schema can be as-sessed only on the basis of judgments made by naive coders (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>although we agree with this, we believe that more experiments of the kind reported here will have to be carried out before we can produce tagging manual that is usable by naive coders.
</nextsent>
<nextsent>in our experiment, it is not clear how much of the agreement came from the manual and how much from the common understanding that we reached uring the training session.
</nextsent>
<nextsent>for our annota-tion task, we felt that it was more important to arrive at common understanding instead of tightly con-trolling how this understanding was reached.
</nextsent>
<nextsent>this position was taken by other computational linguists as well (carletta et al, 1997, <papid> J97-1002 </papid>p. 25).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL253">
<title id=" W99-0307.xml">experiments in constructing a corpus of discourse trees </title>
<section> tagging style.  </section>
<citcontext>
<prevsection>
<prevsent>overall, if our method for computing the kappa statistic is not skewed towards higher values, our experiment suggests that even simple, intuitive def-initions of rhetorical relations, textual saliency, and discourse structure can lead to reliable annotation schemata.
</prevsent>
<prevsent>however, the results do not exclude that better definitions of edu and parenthetical units and rhetorical relations can lead to significant improve-ments in the reliability scores.
</prevsent>
</prevsection>
<citsent citstr=" P97-1012 ">
the vast majority of the computational pproaches to discourse parsing relyon models that implic-itly or explicitly assume that parsing is incremen-tal (polanyi, 1988; lascarides and asher, 1993; gardent, 1997; schilder, 1997; vanden berg, 1996; cristea and webber, 1997).<papid> P97-1012 </papid></citsent>
<aftsection>
<nextsent>that is, as edus are processed, they are immediately added to one par-tial discourse structure that subsumes all previous text.
</nextsent>
<nextsent>however, the logs of our experiment show that, quite often, annotators axe unable to decide where to attach newly created edu.
</nextsent>
<nextsent>the annotation style varies significantly among annotators; but neverthe-less, even the most aggressive annotator still needs to postpone 9.2% of the time the decision of where to attach anewly created edu (see table 4).
</nextsent>
<nextsent>note that this percentage does not reflect undo steps, which may also correlate with attachment decisions that are eventually proven to be incorrect.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL254">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(9) s: it is 80,000 won.
</prevsent>
<prevsent>(10) u: (pointing to the model 100) d like to buy thb and tke prev/ous se/ect/on.
</prevsent>
</prevsection>
<citsent citstr=" C94-2133 ">
figure 1: motivational example previous rescaw, on multi-modal dialogue system was focused on finding the relationship between pointing gesture and deictic expression (bolt (1980), neal et al (1988), salisbury et al (1990.), shimazu et al (1994), <papid> C94-2133 </papid>shimazu and takmhima (1996))<papid> C96-2156 </papid>and on mapping predefined symbol to simple s means multi-modal dialogue system and means user.</citsent>
<aftsection>
<nextsent>our goal is developing multi-modal dialogue system (kim and son (1997)).
</nextsent>
<nextsent>of which domain is home shopping and in which user purchases furniture using korean utc~ ,mccs with pointing gestures on touch screw.
</nextsent>
<nextsent>21 command (johnston et al (1997)).<papid> P97-1036 </papid></nextsent>
<nextsent>none of them, however, suggest methods of resolving deictic expressions with which pointing gestures are omitted: e.g..</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL255">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(9) s: it is 80,000 won.
</prevsent>
<prevsent>(10) u: (pointing to the model 100) d like to buy thb and tke prev/ous se/ect/on.
</prevsent>
</prevsection>
<citsent citstr=" C96-2156 ">
figure 1: motivational example previous rescaw, on multi-modal dialogue system was focused on finding the relationship between pointing gesture and deictic expression (bolt (1980), neal et al (1988), salisbury et al (1990.), shimazu et al (1994), <papid> C94-2133 </papid>shimazu and takmhima (1996))<papid> C96-2156 </papid>and on mapping predefined symbol to simple s means multi-modal dialogue system and means user.</citsent>
<aftsection>
<nextsent>our goal is developing multi-modal dialogue system (kim and son (1997)).
</nextsent>
<nextsent>of which domain is home shopping and in which user purchases furniture using korean utc~ ,mccs with pointing gestures on touch screw.
</nextsent>
<nextsent>21 command (johnston et al (1997)).<papid> P97-1036 </papid></nextsent>
<nextsent>none of them, however, suggest methods of resolving deictic expressions with which pointing gestures are omitted: e.g..</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL256">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our goal is developing multi-modal dialogue system (kim and son (1997)).
</prevsent>
<prevsent>of which domain is home shopping and in which user purchases furniture using korean utc~ ,mccs with pointing gestures on touch screw.
</prevsent>
</prevsection>
<citsent citstr=" P97-1036 ">
21 command (johnston et al (1997)).<papid> P97-1036 </papid></citsent>
<aftsection>
<nextsent>none of them, however, suggest methods of resolving deictic expressions with which pointing gestures are omitted: e.g..
</nextsent>
<nextsent>the red item in utterance (8).
</nextsent>
<nextsent>these approaches do not consider resolving an anaphoric expression that refers an object mentioned in previous utterances or displayed on previous creens.
</nextsent>
<nextsent>it, however, is important also for multi-modal dialogue system to resolve all of these anaphora so that the system should correctly catch his/her intention.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL257">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to resolve deictic expression like this in utterance (4) which c~rs with pointing gesture and the red item in utterance (8) which is uttered with no pointing gestures, the system counts the  number of pointing estures and the number of anaphoric noun phrases included in user utterance, and compares them.
</prevsent>
<prevsent>then, the system aps the noun phrases to pointed items.
</prevsent>
</prevsection>
<citsent citstr=" P83-1007 ">
to resolve referring expression, one of the we l known methods is centering theory developed by grosz, jo~hi, and weinstein (grosz et al (1983)).<papid> P83-1007 </papid></citsent>
<aftsection>
<nextsent>the centering algorithm was further developed by brennan, friedman and pollard for pronoun resolution (brennan et al.
</nextsent>
<nextsent>(1987)) and was improved by walker (walker (1998)).
</nextsent>
<nextsent>however,  those centering algorithms are not applicable to resolve anaphora in multi-medal dialogue because the algurithm excludes the gestures and facial ? expression of dialogue partner, which are important clues to mgierstand his/her uttexances.
</nextsent>
<nextsent>and.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL258">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>- {8} how much nt ff~  *~ steoo (~ ) we have s.,~se mooel$ i ter* , ttm feuee?
</prevsent>
<prevsent>..   wf fch t~ - ime cache memory figure 2: walker cache model ? ~ m~ra~ sk~ ~maml lo lwmbll~ ~ ~ r ,e~ m~ jg~m~ id~ ~a~ ~iw um~q mb t~.~ ~.
</prevsent>
</prevsection>
<citsent citstr=" P89-1031 ">
/ leh lno - im~ iffo.fg t~ ~k~| {2} ~111, so me ~ml o) mwem,~u? dual cache figure 3: dual cache model oual memory (1986), brennan et al (1987), walker (1989)).<papid> P89-1031 </papid></citsent>
<aftsection>
<nextsent>walker replaced the stack model with cache model because some objects were often referred in other discourse segments (walker (1998)).
</nextsent>
<nextsent>the fundamental idea of the cache model is that the function of the cache when processing discourse is analogous to that of cache when executing program on computer.
</nextsent>
<nextsent>in the cache model, the centers of an utterance are stored in the cache fill the cache is full.
</nextsent>
<nextsent>when it is full, the least ~ently accessed centet~ in the cache are replaced to main memory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL259">
<title id=" W99-0103.xml">anaphora resolution using extended centering algorithm in a multimodal dialogue system </title>
<section> anaphora resolution algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>(5) u: (poindng to model/00) d like to buy this and the pr~ioas red item.
</prevsent>
<prevsent>figure 6: motivational example 2 in this paper, the ranking of the items in cf also follows figure 5.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
if the items have the same priority, the algorithm ranks them by the oblique ness of grammatical relation of the subcategorized functions of the main verb: that is, first the subject, object, and objects2, followed by other subcategorized fen~ons, and finally, adjuncts (grosz and sidner (1986), <papid> J86-3001 </papid>brennan et ul.</citsent>
<aftsection>
<nextsent>(1987)).
</nextsent>
<nextsent>the centering algorithm is based on constraints and rules as well as cbs and c~.
</nextsent>
<nextsent>in this paper, we propose xtended constraints and rules as shown in figure 7 because the structure of the cache model and the priority of the utterances are changed according to figure 4 and 5.
</nextsent>
<nextsent>25 ? - . . ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL260">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>combination of classi- tiers have also been applied to pos tagging.
</prevsent>
<prevsent>for instance, van halteren (1996) combined num-ber of similar tuggers by way of straightfor-ward majority vote.
</prevsent>
</prevsection>
<citsent citstr=" P98-1029 ">
more recently, two parallel works (van halteren et al, 1998; brill and wu, 1998) <papid> P98-1029 </papid>combined, with remarkable success, the output of set of four tuggers based on different principles and feature modelling.</citsent>
<aftsection>
<nextsent>finally, in the work by msxquez et al (1998) the combination of taggers is used in bootstrapping algorithm to train part of speech tagger from limited amount of training material.
</nextsent>
<nextsent>the aim of the present work is to improve an existing pos tagger based on decision trees (mkrquez and rodriguez, 1997), by using en-sembles of classifiers.
</nextsent>
<nextsent>this tagger treats sepa-rately the different ypes (classes) of ambiguity by considering different decision tree for each class.
</nextsent>
<nextsent>this fact allows selective construction of ensembles of decision trees focusing on the most relevant ambiguity classes, which greatly vary in size and difficulty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL261">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> tree-based  taggers.  </section>
<citcontext>
<prevsection>
<prevsent>for more details, we refer the reader to (mgrquez and rodrfguez, 1997).
</prevsent>
<prevsent>2.2 stt: statistical tree-based.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
tagger the aim of statistical or probabilistic tagging (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992) <papid> A92-1018 </papid>is to as- sign the most likely sequence of tags given the observed sequence of words.</citsent>
<aftsection>
<nextsent>for doing so, two kinds of information are used: the lexical prob-abilities, i.e, the probability of particular tag conditional on the particular word, and the con-textual probabilities, which describe the proba-bility of particular tag conditional on the sur-rounding tags.
</nextsent>
<nextsent>contextual (or transition) probabilities are usually reduced to the conditioning of the pre-ceding tag (bigrams), or pair of tags (tri- grams), however, the general formulation allows broader definition of context.
</nextsent>
<nextsent>in this way, the set of acquired statistical decision trees can be seen as compact representation a rich con-textual model, which can be straightforwardly incorporated inside statistical tagger.
</nextsent>
<nextsent>the point here is that the context is not restricted to the n-1 preceding tags as in the n-gram formu-lation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL262">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> tree-based  taggers.  </section>
<citcontext>
<prevsection>
<prevsent>for more details, we refer the reader to (mgrquez and rodrfguez, 1997).
</prevsent>
<prevsent>2.2 stt: statistical tree-based.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
tagger the aim of statistical or probabilistic tagging (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992) <papid> A92-1018 </papid>is to as- sign the most likely sequence of tags given the observed sequence of words.</citsent>
<aftsection>
<nextsent>for doing so, two kinds of information are used: the lexical prob-abilities, i.e, the probability of particular tag conditional on the particular word, and the con-textual probabilities, which describe the proba-bility of particular tag conditional on the sur-rounding tags.
</nextsent>
<nextsent>contextual (or transition) probabilities are usually reduced to the conditioning of the pre-ceding tag (bigrams), or pair of tags (tri- grams), however, the general formulation allows broader definition of context.
</nextsent>
<nextsent>in this way, the set of acquired statistical decision trees can be seen as compact representation a rich con-textual model, which can be straightforwardly incorporated inside statistical tagger.
</nextsent>
<nextsent>the point here is that the context is not restricted to the n-1 preceding tags as in the n-gram formu-lation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL263">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> tree-based  taggers.  </section>
<citcontext>
<prevsection>
<prevsent>55 training test w w/s aw t/w t/aw t/dw 40,977 998,354 24.36 339,916(34.05%) 1.48 2.40 - - - - 7,167 175,412 24.47 59,440 (33.89%) 1.45 2.40 3.49 3,941 (2.25%) total 48,144 1,173,766 24.38 399,356 (34.02%) 1.47 2.40 - - - - table 1: information about the wsj training and test corpora.
</prevsent>
<prevsent>s: number of sentences; w: number of words; w/s: average number of words per sentence; aw: number and percentage of ambiguous words; t/w: average number of tags per word; t/aw: average number of tags per ambiguous t:nown word; t/dw: average number of tags per ambiguous word (including unknown words); and u: number and percentage of unknown words classes 8 11 14 18 36 57 111 239 table 2: number of ambiguity classes that cover the x% of the ambiguous words of the training corpus the prefix (or the suffix) of any word in the lexicon?
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the last group of features are inspired in those applied by brill (1995) <papid> J95-4004 </papid>when addressing unknown words.</citsent>
<aftsection>
<nextsent>the learning algorithm 3 acquired, in about thirty minutes, base of 191 trees (the other ambiguity classes had not enough examples) which required about 0,68 mb of storage.
</nextsent>
<nextsent>the results of the taggers working with this tree-base is presented in table 3.
</nextsent>
<nextsent>mft stands for baseline most-frequent-tag tagger.
</nextsent>
<nextsent>rtt, stt, and stt + stand for the basic versions of the taggers presented in section 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL268">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> nn-vb-vbp.  </section>
<citcontext>
<prevsection>
<prevsent>the price to pay for the enriched models is substantial overhead in storage require-ment and speed decreasing, which in the worst case is divided by 5.
</prevsent>
<prevsent>in order to compare our results to others, we list in table 6 the results reported by sev-eral state-of-the-art po5 taggers, tested on the wsj corpus with the open vocabulary as-sumption.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
in that table, tbl stands for brill transformation-based error-driven tag- get (brill, 1995), <papid> J95-4004 </papid>me stands for tagger based on the maimum entropy modelling (ratna- parkhi, 1996), <papid> W96-0213 </papid>spatter stands for statisti-cal parser based on decision trees (magerman, 1996), igtree stands for the memory-based tagger by daelemans et al (1996), <papid> W96-0102 </papid>and, finally, tcomb stands for tagger that works by com-bination of statistical trigram-based tagger, 59 tagger tbl me spatter igtree tcomb stt+(cpd+ens) train test 950 kw 150 kw.</citsent>
<aftsection>
<nextsent>963 kw 193 kw.
</nextsent>
<nextsent>~975 kw 47 kw 2,000 kw 200 kw 1,i00 kw 265 kw overall known unknown 96.6% - - 82.2% 96.5% - - 86.2% 96.5% - - - - 96.4% 96.7% 90.6% 97.2% - - - - ambig 998 kw 175 kw 97.2% 97.5% 84.5% 92.8%.
</nextsent>
<nextsent>table 6: comparison of different uggers on the wsj corpus tbl and me (brill and wu, 1998).
</nextsent>
<nextsent>comparing to all the individual tuggers we observe that our approach reports the highest accuracy, and that it is comparable to that of ycomb obtained by the combination of three tuggers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL269">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> nn-vb-vbp.  </section>
<citcontext>
<prevsection>
<prevsent>the price to pay for the enriched models is substantial overhead in storage require-ment and speed decreasing, which in the worst case is divided by 5.
</prevsent>
<prevsent>in order to compare our results to others, we list in table 6 the results reported by sev-eral state-of-the-art po5 taggers, tested on the wsj corpus with the open vocabulary as-sumption.
</prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
in that table, tbl stands for brill transformation-based error-driven tag- get (brill, 1995), <papid> J95-4004 </papid>me stands for tagger based on the maimum entropy modelling (ratna- parkhi, 1996), <papid> W96-0213 </papid>spatter stands for statisti-cal parser based on decision trees (magerman, 1996), igtree stands for the memory-based tagger by daelemans et al (1996), <papid> W96-0102 </papid>and, finally, tcomb stands for tagger that works by com-bination of statistical trigram-based tagger, 59 tagger tbl me spatter igtree tcomb stt+(cpd+ens) train test 950 kw 150 kw.</citsent>
<aftsection>
<nextsent>963 kw 193 kw.
</nextsent>
<nextsent>~975 kw 47 kw 2,000 kw 200 kw 1,i00 kw 265 kw overall known unknown 96.6% - - 82.2% 96.5% - - 86.2% 96.5% - - - - 96.4% 96.7% 90.6% 97.2% - - - - ambig 998 kw 175 kw 97.2% 97.5% 84.5% 92.8%.
</nextsent>
<nextsent>table 6: comparison of different uggers on the wsj corpus tbl and me (brill and wu, 1998).
</nextsent>
<nextsent>comparing to all the individual tuggers we observe that our approach reports the highest accuracy, and that it is comparable to that of ycomb obtained by the combination of three tuggers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL270">
<title id=" W99-0608.xml">improving pos tagging using machine learning techniques </title>
<section> nn-vb-vbp.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we have applied several ml tech-niques for constructing ensembles of classifiers to address the most representative and/or diffi-cult cases of ambiguity within decision-tree- based english pos tagger.
</prevsent>
<prevsent>as result, the over- all accuracy has been significantly improved.
</prevsent>
</prevsection>
<citsent citstr=" P97-1032 ">
comparing to other approaches, we see that our tagger performs better on the wsj corpus and under the open vocabulary assumption, than number of state-of- the-art pos tuggers, and similar to another approach based on the com-bination of several tuggers s. 8however, it has to be said that the pure statistical or machine-learning based approaches to pos tagging sti l significantly underperform some sophisticated man-ually constructed systems, such as the english shallow parser based on constraint grammars developed at the helsinki university (samuelsson and voutilainen, 1997).<papid> P97-1032 </papid></citsent>
<aftsection>
<nextsent>the cost of this improvement has been quan- tiffed in terms of storage requirement and speed of the resulting enriched tuggers.
</nextsent>
<nextsent>of course, there exists clear tradeoff between accuracy and efficiency which should be resolved on the basis of the user needs.
</nextsent>
<nextsent>although all proposed techniques are fully automatic, it has to be said that the construction of appropriate nsembles requires significant human and computational effort.
</nextsent>
<nextsent>there are several features that should be fur-ther studied with respect the used methods for constructing the ensembles of decision trees, the way they are combined and included in the tuggers, etc. however, we are now more inter-ested on experimenting with the inclusion of our tagger as component in an ensemble of pre-existing tuggers, in the style of (brill and wu, 1998; van halteren et al, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL271">
<title id=" W99-0603.xml">mapping multilingual hierarchies using relaxation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, very active field in- side nl during the last years has been the fast development of generictanguage resources.
</prevsent>
<prevsent>several attempts have been performed to pro-duce multilingual ontologies.
</prevsent>
</prevsection>
<citsent citstr=" C94-1052 ">
in (ageno et al, 1994), <papid> C94-1052 </papid>spanish/english bilingual dictionary is used to (semi)automatically ink spanish and english taxonomies extracted from dgile (a1- var, 1987) and ldoce (procter, 1987).</citsent>
<aftsection>
<nextsent>sim-ilarly, simple automatic approach for link-ing spanish taxonomies extracted from dgile to wordnet (miller et al, 1991) synsets is proposed in (rigau et al, 1995).
</nextsent>
<nextsent>the work reported in (knight and luk, 1994) focuses on the construction of sensus, large knowl-edge base for supporting the pan gloss machine translation system.
</nextsent>
<nextsent>in (okumura and hovy, 1994) (<papid> H94-1025 </papid>semi)automatic methods for associating japanese lexicon to an english ontology us-ing bilingual dictionary are described.</nextsent>
<nextsent>sev-eral experiments aligning edr and wordnet on-tologies are described in (utiyama and hasida, 1997).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL272">
<title id=" W99-0603.xml">mapping multilingual hierarchies using relaxation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sim-ilarly, simple automatic approach for link-ing spanish taxonomies extracted from dgile to wordnet (miller et al, 1991) synsets is proposed in (rigau et al, 1995).
</prevsent>
<prevsent>the work reported in (knight and luk, 1994) focuses on the construction of sensus, large knowl-edge base for supporting the pan gloss machine translation system.
</prevsent>
</prevsection>
<citsent citstr=" H94-1025 ">
in (okumura and hovy, 1994) (<papid> H94-1025 </papid>semi)automatic methods for associating japanese lexicon to an english ontology us-ing bilingual dictionary are described.</citsent>
<aftsection>
<nextsent>sev-eral experiments aligning edr and wordnet on-tologies are described in (utiyama and hasida, 1997).
</nextsent>
<nextsent>several exical resources and techniques are combined in (atserias et al, 1997) to map spanish words from bilingual dictionary to wordnet, and in (farreres et al, 1998) <papid> W98-0709 </papid>the use of the taxonomic structure derived from monolingual madis proposed as an aid to this mapping process.</nextsent>
<nextsent>this paper presents novel approach for merging already existing hierarchies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL273">
<title id=" W99-0603.xml">mapping multilingual hierarchies using relaxation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (okumura and hovy, 1994) (<papid> H94-1025 </papid>semi)automatic methods for associating japanese lexicon to an english ontology us-ing bilingual dictionary are described.</prevsent>
<prevsent>sev-eral experiments aligning edr and wordnet on-tologies are described in (utiyama and hasida, 1997).</prevsent>
</prevsection>
<citsent citstr=" W98-0709 ">
several exical resources and techniques are combined in (atserias et al, 1997) to map spanish words from bilingual dictionary to wordnet, and in (farreres et al, 1998) <papid> W98-0709 </papid>the use of the taxonomic structure derived from monolingual madis proposed as an aid to this mapping process.</citsent>
<aftsection>
<nextsent>this paper presents novel approach for merging already existing hierarchies.
</nextsent>
<nextsent>the method has been applied to attach substan-tial fragments of the spanish taxonomy derived from dgile (rigau et al, 1998) <papid> P98-2181 </papid>to the english wordnet using bilingual dictionary for con-necting both hierarchies.</nextsent>
<nextsent>this paper is organized as follows: in section 2 we describe the used technique (the relaxation labeling algorithm) and its application to hier-archy mapping.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL274">
<title id=" W99-0603.xml">mapping multilingual hierarchies using relaxation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several exical resources and techniques are combined in (atserias et al, 1997) to map spanish words from bilingual dictionary to wordnet, and in (farreres et al, 1998) <papid> W98-0709 </papid>the use of the taxonomic structure derived from monolingual madis proposed as an aid to this mapping process.</prevsent>
<prevsent>this paper presents novel approach for merging already existing hierarchies.</prevsent>
</prevsection>
<citsent citstr=" P98-2181 ">
the method has been applied to attach substan-tial fragments of the spanish taxonomy derived from dgile (rigau et al, 1998) <papid> P98-2181 </papid>to the english wordnet using bilingual dictionary for con-necting both hierarchies.</citsent>
<aftsection>
<nextsent>this paper is organized as follows: in section 2 we describe the used technique (the relaxation labeling algorithm) and its application to hier-archy mapping.
</nextsent>
<nextsent>in section 3 we describe the constraints used in the relaxation process, and finally, after presenting some experiments and preliminary results, we offer some conclusions and outline further lines of research.
</nextsent>
<nextsent>labeling to nlp relaxation labeling (rl) is generic name for family of iterative algorithms which perform function optimization, based on local informa-tion.
</nextsent>
<nextsent>see (torras, 1989) for summary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL276">
<title id=" W99-0603.xml">mapping multilingual hierarchies using relaxation labeling </title>
<section> experiments  and results.  </section>
<citcontext>
<prevsection>
<prevsent>14 4.1 spanish taxonomies.
</prevsent>
<prevsent>we tested the relaxation labeling algorithm with the described constraints on set of disambiguated spanish taxonomies automat-ically acquired from monolingual dictionar-ies.
</prevsent>
</prevsection>
<citsent citstr=" P97-1007 ">
these taxonomies were automatically as-signed to wordnet semantic file (rigau et al., 1997; <papid> P97-1007 </papid>rigau et al, 1998).<papid> P98-2181 </papid></citsent>
<aftsection>
<nextsent>we tested the performance of the method on two dif-ferent kinds of taxonomies: those assigned to well defined and concrete semantic files (noun.
</nextsent>
<nextsent>animal, noun.
</nextsent>
<nextsent>food), and those assigned to more abstract and less structured ones (noun.
</nextsent>
<nextsent>cogn t ion and noun.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL279">
<title id=" W98-1426.xml">the practical value of ngrams is in generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P98-1116 ">
langkilde and knight (1998) <papid> P98-1116 </papid>introduced nitrogen, sys-tem that implements new style of generation in which corpus-based ngram statistics are used in place of deep, ex-tensive symbolic knowledge to provide very large-scale gener-ation (lexicons and knowledge bases on the order of 200,000 entities), and simultaneously simplify the input and improve robustness for sentence generation.</citsent>
<aftsection>
<nextsent>nitrogen generation oc-curs in two stages, as shown in figure 1.
</nextsent>
<nextsent>first the input is mapped to word lattice, compact representation of mul-tiple generation possibilities.
</nextsent>
<nextsent>then, statistical extractor selects the most fluent path through the lattice.
</nextsent>
<nextsent>the word lattice encodes alternative english expressions for the input when the symbolic knowledge is unavailable (whether from the input, or from the knowledge bases) for making realization decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL280">
<title id=" W98-1426.xml">the practical value of ngrams is in generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the word lattice encodes alternative english expressions for the input when the symbolic knowledge is unavailable (whether from the input, or from the knowledge bases) for making realization decisions.
</prevsent>
<prevsent>the nitrogen statistical ex-tractor ranks these alternative using bigram (adjacent word pairs) and unigram (single word) statistics collected from two years of the wall street journal.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
the extraction algorithm is presented in (knight and hatzivassiloglou, 1995).<papid> P95-1034 </papid></citsent>
<aftsection>
<nextsent>meaning symbolic generator ? \[  --- lexicon ~-- gralnmar $ word lattice of possible renderings statistical extractor \[ 4 english string i--- corpus figure combining symbolic and statisti-cal knowledge in natural language gener-ator (knight and hatzivassil0glou, 1995).
</nextsent>
<nextsent>in essence, nitrogen uses ngram statistics to robustly make wide variety of decisions, from tense to word choice?
</nextsent>
<nextsent>to syntactic subcategorization, that traditionally are handled either with defaults (e.g., assume present ense, use the alphabetically-first synonyms, use nominal arguments), explicit input specification, or by using deep, detailed knowledge bases.
</nextsent>
<nextsent>however, in scaling up generator system, these methods become unsatisfactory.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL281">
<title id=" W99-0619.xml">word informative ness and automatic pitch accent modeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our experiments show that ic is always the dominant predictor when both ic and tf*idf are presented.
</prevsent>
<prevsent>information based approaches were applied in some natural anguages applications be-fore.
</prevsent>
</prevsection>
<citsent citstr=" H93-1054 ">
in (resnik, 1993; <papid> H93-1054 </papid>resnik, 1995), ic was used to measure semantic similarity be-tween words and it is shown to be more effective than traditional measurements of semantic distance within the wordnet hi- erarchy.</citsent>
<aftsection>
<nextsent>a similar log-based information- like measurement was also employed in (lea- cock and chodorow, 1998) to measure se-mantic similarity.
</nextsent>
<nextsent>tf*idf scores are mainly used in keyword-based information retrieval tasks.
</nextsent>
<nextsent>for example, tf*idf has been used in (salton, :1989; salton, 1991) to index the words ini document and is also imple-mented in smart (buckley, 1985) which is general;-purpose information retrieval package, providing basic tools and libraries to facilitate information retrieval tasks.
</nextsent>
<nextsent>some early work on pitch accent predic-tion in speech synthesis only uses the dis-tinction between content words and function words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL282">
<title id=" W99-0619.xml">word informative ness and automatic pitch accent modeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>more advanced pitch ac-cent models make use of other information, such as part-of-speech, given/new distinc-tions and contrast information (hirschberg, 1993).
</prevsent>
<prevsent>semantic information is also em-ployed in predicting accent patterns for com-plex nominal phrases (sproat, 1994).
</prevsent>
</prevsection>
<citsent citstr=" P98-2165 ">
other comprehensive pitch accent models have been suggested in (pan and mckeown, 1998) <papid> P98-2165 </papid>in the framework of concept-to-speech gen-eration where the output of natural an- guage generation system is used to predict pitch accent.</citsent>
<aftsection>
<nextsent>since ic is not perfect measurement of in- formative ness, it can cause problems in ac-cent prediction.
</nextsent>
<nextsent>moreover, even if perfect measurement of informative ness is available, more features may be needed in order to build satisfactory pitch accent model.
</nextsent>
<nextsent>in this section, we discuss each of these issues.
</nextsent>
<nextsent>ic does not directly measure the informa- tive ness of word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL283">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to choose hypernym for each alter-native sense, we employ novel technique which uses x 2 test to measure the homo-geneity of sets of concepts in the hierarchy.
</prevsent>
<prevsent>knowledge of the constraints verb places on the semantic types of its arguments (var- iously called selectional restrictions, selec-tional preferences, elec tional constraints) is of use in many areas of natural anguage pro-cessing, particularly structural disambigua-tion.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
recent treatments of selectional re- str ictions have been probabilistic in nature (resnik, 1993), (li and abe, 1998), (<papid> J98-2002 </papid>ribas, 1995), (<papid> E95-1016 </papid>mccarthy, 1997), <papid> W97-0808 </papid>and estimation of the relevant probabilities has required corpus-based counts of the number of times word senses, or concepts, appear in the dif-ferent argument positions of verbs.</citsent>
<aftsection>
<nextsent>a dif-ficulty arises due to the absence of large volume of sense disambiguated data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more than one sense.
</nextsent>
<nextsent>the tech-niques in resnik (1993), li and abe (1998) <papid> J98-2002 </papid>and ribas (1995) <papid> E95-1016 </papid>simply distribute the count equally among the alternative senses of noun.</nextsent>
<nextsent>abney and light (1998) have at-tempted to obtain selectional preferences us-ing the expectation maximization algorithm by encoding wordnet as hidden markov model and using modified form of the forward-backward algorithm to estimate the parameters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL284">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to choose hypernym for each alter-native sense, we employ novel technique which uses x 2 test to measure the homo-geneity of sets of concepts in the hierarchy.
</prevsent>
<prevsent>knowledge of the constraints verb places on the semantic types of its arguments (var- iously called selectional restrictions, selec-tional preferences, elec tional constraints) is of use in many areas of natural anguage pro-cessing, particularly structural disambigua-tion.
</prevsent>
</prevsection>
<citsent citstr=" E95-1016 ">
recent treatments of selectional re- str ictions have been probabilistic in nature (resnik, 1993), (li and abe, 1998), (<papid> J98-2002 </papid>ribas, 1995), (<papid> E95-1016 </papid>mccarthy, 1997), <papid> W97-0808 </papid>and estimation of the relevant probabilities has required corpus-based counts of the number of times word senses, or concepts, appear in the dif-ferent argument positions of verbs.</citsent>
<aftsection>
<nextsent>a dif-ficulty arises due to the absence of large volume of sense disambiguated data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more than one sense.
</nextsent>
<nextsent>the tech-niques in resnik (1993), li and abe (1998) <papid> J98-2002 </papid>and ribas (1995) <papid> E95-1016 </papid>simply distribute the count equally among the alternative senses of noun.</nextsent>
<nextsent>abney and light (1998) have at-tempted to obtain selectional preferences us-ing the expectation maximization algorithm by encoding wordnet as hidden markov model and using modified form of the forward-backward algorithm to estimate the parameters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL285">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to choose hypernym for each alter-native sense, we employ novel technique which uses x 2 test to measure the homo-geneity of sets of concepts in the hierarchy.
</prevsent>
<prevsent>knowledge of the constraints verb places on the semantic types of its arguments (var- iously called selectional restrictions, selec-tional preferences, elec tional constraints) is of use in many areas of natural anguage pro-cessing, particularly structural disambigua-tion.
</prevsent>
</prevsection>
<citsent citstr=" W97-0808 ">
recent treatments of selectional re- str ictions have been probabilistic in nature (resnik, 1993), (li and abe, 1998), (<papid> J98-2002 </papid>ribas, 1995), (<papid> E95-1016 </papid>mccarthy, 1997), <papid> W97-0808 </papid>and estimation of the relevant probabilities has required corpus-based counts of the number of times word senses, or concepts, appear in the dif-ferent argument positions of verbs.</citsent>
<aftsection>
<nextsent>a dif-ficulty arises due to the absence of large volume of sense disambiguated data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more than one sense.
</nextsent>
<nextsent>the tech-niques in resnik (1993), li and abe (1998) <papid> J98-2002 </papid>and ribas (1995) <papid> E95-1016 </papid>simply distribute the count equally among the alternative senses of noun.</nextsent>
<nextsent>abney and light (1998) have at-tempted to obtain selectional preferences us-ing the expectation maximization algorithm by encoding wordnet as hidden markov model and using modified form of the forward-backward algorithm to estimate the parameters.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL289">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> the input data and.  </section>
<citcontext>
<prevsection>
<prevsent>the set con-sisting of the children of  meat , however, is homogeneous with respect the object po-sition of eat, and so  meat  is not too high level of representation.
</prevsent>
<prevsent>the measure of ho-mogeneity we use is detailed in section 5.
</prevsent>
</prevsection>
<citsent citstr=" A97-1052 ">
semantic hierarchy the input data used to estimate frequencies and probabilities over the semantic hierarchy has been obtained from the shallow parser described in briscoe and carroll (1997).<papid> A97-1052 </papid></citsent>
<aftsection>
<nextsent>the data consists of multi set of  co-occurrence triples , each triple consisting of noun lemma, verb lemma, and argument position.
</nextsent>
<nextsent>we refer to the data as follows: let the uni-verse of verbs, argument positions and nouns that can appear in the input data be denoted = {vl,.
</nextsent>
<nextsent>,vkv }, 1z---- {r , . . .
</nextsent>
<nextsent>, rkn} and af = {n , . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL291">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> the measure of association.  </section>
<citcontext>
<prevsection>
<prevsent>note that, for 5the number of nodes in the graph with more than one parent is only around one percent of the total.
</prevsent>
<prevsent>6note that resnik (1993) uses rather non-standard terminology by refering to this second set as the synsets of c. 7this work restricts itself to verbs, but can be ex-tended to other kinds of predicates that take nouns as arguments, uch as adjectives.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
c c, p(c\]v,r) is just the probability of the disjunction of the concepts in c; that is, = zp(clv, r) cec in order to see how p(clv ,r) relates to the input data, note that given concept c, verb and argument position r, noun can be generated according to the distribution p(n\[c, v, r), where p(nlc, v, r) = 1 nesyn(c) now we have model for the input data: p(n, v, r) = p(v,r)p(niv ,r) = p(v,r) p(clv, rlp(ntc, v,r) cecn(n) note that for ? cn(n), p(nlc, v, r) = o. the association orm (and similar mea-sures such as the mutual information score) have been criticised (dunning, 1993) <papid> J93-1003 </papid>because these scores can be greatly over-estimated when frequency counts are low.</citsent>
<aftsection>
<nextsent>this prob-lem is overcome to some extent in the scheme presented below since, generally speaking, we only calculate the association orms for concepts that have accumulated significant count.
</nextsent>
<nextsent>the association norm can be estimated using maximum likelihood estimates of the probabilities as follows.
</nextsent>
<nextsent>?(c ,v , ) _ p(c v , ) (clr)
</nextsent>
<nextsent>let freq(c, v,r), for particular c, and r, be the number of (n, v, r) triples in the data in which is being used to denote c, and let freq(v, r) be the number of times verb appears with something in position in the data; then the relevant maximum likelihood estimates, for e c, e 12, e 7~, are as 260 follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL292">
<title id=" W99-0631.xml">an iterative approach to estimating frequencies over a semantic hierarchy </title>
<section> estimating frequencies.  </section>
<citcontext>
<prevsection>
<prevsent>freq(~, v, r) freq(v, r) ~eee freq(g, v, r) freq(v, r) if(fir) = evevfreq(c  r) ~,ev freq(v, r) _ ever ~,ev freq(v, r) since we do not have sense disambiguated data, we cannot obtain freq(c, v, r) by sim-ply counting senses.
</prevsent>
<prevsent>the standard approach is to estimate freq(c, v, r) by distributing the count for each noun in syn(c) evenly among all senses of the noun as follows: freq(n, v, r) freq(c, v, r) = ~ cn(n)l nesyn(c) where freq(n, v, r) is the number times the triple (n,v,r) appears in the data, and \[ cn(n)\] is the cardinality of an(n).
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
although this approach can give inaccu-rate estimates, the counts given to the incor-rect senses will disperse randomly through- out the hierarchy as noise, and by accu-mulating counts up the hierarchy we will tend to gather counts from the correct senses of related words (yarowsky, 1992; <papid> C92-2070 </papid>resnik, 1993).</citsent>
<aftsection>
<nextsent>to see why, consider two instances of possible triples in the data, drink wine and drink water.
</nextsent>
<nextsent>(this example is adapted from resnik (1993).)
</nextsent>
<nextsent>the word water is member of seven synsets in wordnet 1.6, and wine is member of two synsets.
</nextsent>
<nextsent>thus each sense of water will be incremented by 0.14 counts, and each sense of wine will be incremented by 0.5 counts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL296">
<title id=" W99-0112.xml">reference hashed </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>a so-called hashing list is employed to store discourse referents according to their grammatical features.
</prevsent>
<prevsent>the ac-count proposed combines insights from several the- odes of discourse comprehension.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
segmented dis-course representation theory (asher, 1993) is en-riched by the ranking system developed in center-ing theory (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>in addition, tree logic is used to represent under specification within the discourse structure (schilder, 1998).<papid> P98-2194 </papid></nextsent>
<nextsent>discourse referents are represented quite differently by current diso~urse theories.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL297">
<title id=" W99-0112.xml">reference hashed </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>the ac-count proposed combines insights from several the- odes of discourse comprehension.
</prevsent>
<prevsent>segmented dis-course representation theory (asher, 1993) is en-riched by the ranking system developed in center-ing theory (grosz et al, 1995).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2194 ">
in addition, tree logic is used to represent under specification within the discourse structure (schilder, 1998).<papid> P98-2194 </papid></citsent>
<aftsection>
<nextsent>discourse referents are represented quite differently by current diso~urse theories.
</nextsent>
<nextsent>discourse represen-tation theory (drt), for example, mploys arather unstructured data structure for the domain of dis-course referents: set (kamp and reyle, 1993).
</nextsent>
<nextsent>a drt-implementation byasherand wada (1988), however, employs amore complex data type: tree representation.
</nextsent>
<nextsent>i further work by asher (i 993) ref-erents are grouped together into segments depend-ing on ihe discourse structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL299">
<title id=" W99-0112.xml">reference hashed </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>i further work by asher (i 993) ref-erents are grouped together into segments depend-ing on ihe discourse structure.
</prevsent>
<prevsent>his segmented drt (sdrt) uses tree-like representation for the dis-course suuctui~ centering theory (cd proposes a//st structure for the entities one preferably refers to in subse-quent sentences.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
in order to cover coreference over discourse segments the centering model was ex-tended by stack mechanism (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>recently, these data structures have been criticized by walker (1998), because they seem to be too restrictive.
</nextsent>
<nextsent>she proposes acache storage for the refer enis in the focus of attention.
</nextsent>
<nextsent>propose instead novel data structure for the representation of discourse referents.
</nextsent>
<nextsent>a/rushing list tsimilarly, rhetorical structure theory (rst) makes use of tree representation (mann et al,1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL304">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we classify problems associated with technical term translation as given below: (1) technical terms are often compound word~ which can be progressively created simply by combining multiple existing morphemes ( base words ), and therefore it is not en-tirely satisfactory to exhaustively enumer-ate newly emerging terms in dictionaries, (2) asian languages often represent loan words based on their special phonograms (primar- ily for technical terms and proper nouns), which creates new base words progressively (in the case of japanese, the phonogram is called katakana).
</prevsent>
<prevsent>to counter problem (1), we use the compound word translation method we proposed (fujii and ishikawa, 1999), which selects appropri-ate translations based on the probability of oc-currence of each combination of base words in the target language.
</prevsent>
</prevsection>
<citsent citstr=" P98-1036 ">
for problem (2), we use  transliteration  (chen et al, 1998; <papid> P98-1036 </papid>knight and graehl, 1998; <papid> J98-4003 </papid>wan and verspoor, 1998).<papid> P98-2220 </papid></citsent>
<aftsection>
<nextsent>chen et al (1998) <papid> P98-1036 </papid>and wan and verspoor (1998) <papid> P98-2220 </papid>proposed english-chinese transliteration meth-ods relying on the property of the chinese phonetic system, which cannot be directly ap-plied to transliteration between english and japanese.</nextsent>
<nextsent>knight and graehl (1998) <papid> J98-4003 </papid>proposed japanese-english transliteration method based on the mapping probability between english and japanese katakana sounds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL305">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we classify problems associated with technical term translation as given below: (1) technical terms are often compound word~ which can be progressively created simply by combining multiple existing morphemes ( base words ), and therefore it is not en-tirely satisfactory to exhaustively enumer-ate newly emerging terms in dictionaries, (2) asian languages often represent loan words based on their special phonograms (primar- ily for technical terms and proper nouns), which creates new base words progressively (in the case of japanese, the phonogram is called katakana).
</prevsent>
<prevsent>to counter problem (1), we use the compound word translation method we proposed (fujii and ishikawa, 1999), which selects appropri-ate translations based on the probability of oc-currence of each combination of base words in the target language.
</prevsent>
</prevsection>
<citsent citstr=" J98-4003 ">
for problem (2), we use  transliteration  (chen et al, 1998; <papid> P98-1036 </papid>knight and graehl, 1998; <papid> J98-4003 </papid>wan and verspoor, 1998).<papid> P98-2220 </papid></citsent>
<aftsection>
<nextsent>chen et al (1998) <papid> P98-1036 </papid>and wan and verspoor (1998) <papid> P98-2220 </papid>proposed english-chinese transliteration meth-ods relying on the property of the chinese phonetic system, which cannot be directly ap-plied to transliteration between english and japanese.</nextsent>
<nextsent>knight and graehl (1998) <papid> J98-4003 </papid>proposed japanese-english transliteration method based on the mapping probability between english and japanese katakana sounds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL306">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we classify problems associated with technical term translation as given below: (1) technical terms are often compound word~ which can be progressively created simply by combining multiple existing morphemes ( base words ), and therefore it is not en-tirely satisfactory to exhaustively enumer-ate newly emerging terms in dictionaries, (2) asian languages often represent loan words based on their special phonograms (primar- ily for technical terms and proper nouns), which creates new base words progressively (in the case of japanese, the phonogram is called katakana).
</prevsent>
<prevsent>to counter problem (1), we use the compound word translation method we proposed (fujii and ishikawa, 1999), which selects appropri-ate translations based on the probability of oc-currence of each combination of base words in the target language.
</prevsent>
</prevsection>
<citsent citstr=" P98-2220 ">
for problem (2), we use  transliteration  (chen et al, 1998; <papid> P98-1036 </papid>knight and graehl, 1998; <papid> J98-4003 </papid>wan and verspoor, 1998).<papid> P98-2220 </papid></citsent>
<aftsection>
<nextsent>chen et al (1998) <papid> P98-1036 </papid>and wan and verspoor (1998) <papid> P98-2220 </papid>proposed english-chinese transliteration meth-ods relying on the property of the chinese phonetic system, which cannot be directly ap-plied to transliteration between english and japanese.</nextsent>
<nextsent>knight and graehl (1998) <papid> J98-4003 </papid>proposed japanese-english transliteration method based on the mapping probability between english and japanese katakana sounds.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL310">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> trans la ion  module.  </section>
<citcontext>
<prevsection>
<prevsent>31 here, si and ti denote i-th base words in source and target languages, respectively.
</prevsent>
<prevsent>our task, i.e., to select which maximizes p(tis), is transformed into equation (1) through use of the bayesian theorem.
</prevsent>
</prevsection>
<citsent citstr=" J93-1001 ">
arg n~x p(tis ) = arg n~x p(sit ) ? p(t) (1) p(sit ) and p(t) are approximated asin equa-tion (2), which has commonly been used in the recent statistical nlp research (church and mercer, 1993).<papid> J93-1001 </papid></citsent>
<aftsection>
<nextsent>n p(sit) ~ i~p(silti) i----1 n-1 p(t)  ~ 1y~ p(ti+llti) i= (2) we produced our own dictionary, because conventional dictionaries are comprised primar-ily of general words and verbose definitions aimed at human readers.
</nextsent>
<nextsent>we extracted 59,533 english/japanese translations consisting of two base words from the edr technical terminol-ogy dictionary, which contains about 120,000 translations related to the information process-ing field (japan electronic dictionary research institute, 1995), and segment japanese ntries into two parts 3.
</nextsent>
<nextsent>for this purpose, simple heuris-tic rules based mainly on japanese character types (i.e., kanji, katakana, hiragana, alpha-bets and other characters like numerals) were used.
</nextsent>
<nextsent>given the set of compound words where japanese ntries are segmented, we correspond english-japanese base words on word-by-word basis, maintaining the word order between en-glish and japanese, to produce japanese- english/english-japanese base word dictionary.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL311">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>i)recision ratio to j-.l ? i-j 0.204 -- cwt + tran slit 0.212 1.04 compound wor(1 translation and translitera-tion, with an existing monolingual retrieval method.
</prevsent>
<prevsent>our experimental results showed that compound word translation and transliteration methods individually improve on the baseline performance, and when used together the im-provement is even greater.
</prevsent>
</prevsection>
<citsent citstr=" P95-1032 ">
future work will in- elude the application of automatic word align-ment methods (fung, 1995; <papid> P95-1032 </papid>smadja et al, 1996) <papid> J96-1001 </papid>to enhance the dictionary.</citsent>
<aftsection>
<nextsent>acknowledgments the authors would like to thank noriko kando (national center for science information sys-tems, japan) for her support with the nacsis collection.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL312">
<title id=" W99-0605.xml">cross language information retrieval for technical documents </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>i)recision ratio to j-.l ? i-j 0.204 -- cwt + tran slit 0.212 1.04 compound wor(1 translation and translitera-tion, with an existing monolingual retrieval method.
</prevsent>
<prevsent>our experimental results showed that compound word translation and transliteration methods individually improve on the baseline performance, and when used together the im-provement is even greater.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
future work will in- elude the application of automatic word align-ment methods (fung, 1995; <papid> P95-1032 </papid>smadja et al, 1996) <papid> J96-1001 </papid>to enhance the dictionary.</citsent>
<aftsection>
<nextsent>acknowledgments the authors would like to thank noriko kando (national center for science information sys-tems, japan) for her support with the nacsis collection.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL313">
<title id=" W99-0632.xml">using subcategorization to resolve verb class ambiguity </title>
<section> 173 738,854  </section>
<citcontext>
<prevsection>
<prevsent>consider for example verbs participating in one alternation only: of these, 90.4% have one semantic class, 8.6% have two classes, 0.7% have three classes and 0.3% have four classes.
</prevsent>
<prevsent>in contrast, of the verbs licensing six different alternations, 14% have one class, 17% have two classes, 12.4% have three classes, 53.6% have four classes, 2% have six classes and 1% has seven classes.
</prevsent>
</prevsection>
<citsent citstr=" P98-1046 ">
palmer (1999) and dang et al (1998) <papid> P98-1046 </papid>argue that the use of syntactic frames and verb classes can sim-plify the definition of different verb senses.</citsent>
<aftsection>
<nextsent>beyond this, we claim that information about he argument structure of polysemous verb can often help dis- ambiguating it.
</nextsent>
<nextsent>consider for instance the verb serve which is member of four classes: give, fit, masquerade and fulfilling.
</nextsent>
<nextsent>each of these classes can in turn license four distinct syntactic frames.
</nextsent>
<nextsent>as shown in the examples below, in (4a) serve appears ditran- sit ively and belongs to the semantic lass of give verbs, in (4b) it occurs transit ively and is mem-ber of the class of fit verbs, in (4c) it takes the predicative complement as minister of the interior and is member of masquerade verbs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL314">
<title id=" W99-0632.xml">using subcategorization to resolve verb class ambiguity </title>
<section> 173 738,854  </section>
<citcontext>
<prevsection>
<prevsent>f (classi) (14) p(frame) ,~ (frame) {framei) it is easy to obtain f(verb) from the lemmatized bnc.
</prevsent>
<prevsent>for the experiments reported here, syntactic frames for the dative and benefactive alternations were automatically extracted from the bnc using gsearch (keller et al, 1999), tool which facilitates search of arbitrary pos-tagged corpora for shallow syntactic patterns based on user-specified context- free grammar and syntactic query.
</prevsent>
</prevsection>
<citsent citstr=" P99-1051 ">
the acquisition and filtering process is detailed in lapata (1999).<papid> P99-1051 </papid></citsent>
<aftsection>
<nextsent>we relyon gsearch to provide moderately accu-rate information about verb frames in the same way that hindle and rooth (1993) <papid> J93-1005 </papid>relied on fid ditch to provide moderately accurate information about syn-tactic structure, and ratnaparkhi (1998) <papid> P98-2177 </papid>relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that pro-vided by fidditch.</nextsent>
<nextsent>we estimated f(verb,frame) as the number of times verb co-occurred with par-ticular frame in the corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL315">
<title id=" W99-0632.xml">using subcategorization to resolve verb class ambiguity </title>
<section> 173 738,854  </section>
<citcontext>
<prevsection>
<prevsent>for the experiments reported here, syntactic frames for the dative and benefactive alternations were automatically extracted from the bnc using gsearch (keller et al, 1999), tool which facilitates search of arbitrary pos-tagged corpora for shallow syntactic patterns based on user-specified context- free grammar and syntactic query.
</prevsent>
<prevsent>the acquisition and filtering process is detailed in lapata (1999).<papid> P99-1051 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
we relyon gsearch to provide moderately accu-rate information about verb frames in the same way that hindle and rooth (1993) <papid> J93-1005 </papid>relied on fid ditch to provide moderately accurate information about syn-tactic structure, and ratnaparkhi (1998) <papid> P98-2177 </papid>relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that pro-vided by fidditch.</citsent>
<aftsection>
<nextsent>we estimated f(verb,frame) as the number of times verb co-occurred with par-ticular frame in the corpus.
</nextsent>
<nextsent>we cannot read off p(frame\[class) from the cor-pus, because it is not annotated with verb classes.
</nextsent>
<nextsent>nevertheless we can use the information listed in levin with respect the syntactic frames exhib-ited by the verbs of given class.
</nextsent>
<nextsent>for each class 268 class frames manner np-v-np-pp#om, np-v-np, np-v-ppat, np-v-np-pred accompany np-v-np, np-v-np-ppt,, throw np-v-np-np, np-v-np-pptoc, np-v-np-pp#om-ppto, np-v-np, np-v-np-ppto, np-v-np-ppar, performance np-v, np-v-np, np-v-np-np, np-v-np-ppto, np-v-np-pp?~r, np-v-np i e np-v-np-ppto, np-v-np-np contribute np-v-np-ppto table 2: sample of verb classes and their syntactic frames we recorded the syntactic frames it licenses (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL317">
<title id=" W99-0632.xml">using subcategorization to resolve verb class ambiguity </title>
<section> 173 738,854  </section>
<citcontext>
<prevsection>
<prevsent>for the experiments reported here, syntactic frames for the dative and benefactive alternations were automatically extracted from the bnc using gsearch (keller et al, 1999), tool which facilitates search of arbitrary pos-tagged corpora for shallow syntactic patterns based on user-specified context- free grammar and syntactic query.
</prevsent>
<prevsent>the acquisition and filtering process is detailed in lapata (1999).<papid> P99-1051 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
we relyon gsearch to provide moderately accu-rate information about verb frames in the same way that hindle and rooth (1993) <papid> J93-1005 </papid>relied on fid ditch to provide moderately accurate information about syn-tactic structure, and ratnaparkhi (1998) <papid> P98-2177 </papid>relied on simple heuristics defined over part-of-speech tags to deliver information early as useful as that pro-vided by fidditch.</citsent>
<aftsection>
<nextsent>we estimated f(verb,frame) as the number of times verb co-occurred with par-ticular frame in the corpus.
</nextsent>
<nextsent>we cannot read off p(frame\[class) from the cor-pus, because it is not annotated with verb classes.
</nextsent>
<nextsent>nevertheless we can use the information listed in levin with respect the syntactic frames exhib-ited by the verbs of given class.
</nextsent>
<nextsent>for each class 268 class frames manner np-v-np-pp#om, np-v-np, np-v-ppat, np-v-np-pred accompany np-v-np, np-v-np-ppt,, throw np-v-np-np, np-v-np-pptoc, np-v-np-pp#om-ppto, np-v-np, np-v-np-ppto, np-v-np-ppar, performance np-v, np-v-np, np-v-np-np, np-v-np-ppto, np-v-np-pp?~r, np-v-np i e np-v-np-ppto, np-v-np-np contribute np-v-np-ppto table 2: sample of verb classes and their syntactic frames we recorded the syntactic frames it licenses (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL318">
<title id=" W99-0632.xml">using subcategorization to resolve verb class ambiguity </title>
<section> 173 738,854  </section>
<citcontext>
<prevsection>
<prevsent>finally, we wanted to estimate the probability of given frame, p(frame).
</prevsent>
<prevsent>we could have done this by acquiring levin compatible subcategoriza-tion frames from the bnc.
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
techniques for the auto-matic acquisition of subcategofization dictionaries have been developed by manning (1993), <papid> P93-1032 </papid>bfiscoe and carroll (1997) and carroll and rooth (1998).</citsent>
<aftsection>
<nextsent>but the present study was less ambitious, and nar-rowly focused on the frames representing theda- tive and the benefactive alternation.
</nextsent>
<nextsent>in default of the more ambitious tudy, which we plan for the future, the estimation of p(frame) was carried out on types and not on tokens.
</nextsent>
<nextsent>the mapping of levin linguis-tic specifications into surface syntactic information resulted in 79 different frame types.
</nextsent>
<nextsent>by counting the number of times given frame is licensed by several semantic lasses we get distribution of frames, sample of which is shown in figure 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL322">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> mo~a~n.  </section>
<citcontext>
<prevsection>
<prevsent>so.
</prevsent>
<prevsent>we will try to mark set of features which is superset of the necessary features.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
drawing on the feature sets used in connolly et al (1997) and ge et al (1998), <papid> W98-1119 </papid>we believe the following factors might indicate co-referenco: ? syntactic role (e.g. subject, object, prepositional object,...), ? pronominal ization (yea or no), ? distance between ea and ee (an integer), ? definite ness (yes or no), ? semantic role (e.g. indicating location, manner, time,...), ? nesting depth of an p (an integer), ? information status (as defined by strube (1998)) <papid> P98-2204 </papid>of the de, ? gender, number, animacy.</citsent>
<aftsection>
<nextsent>the tool must allow the coder to assign values for these features to each marked expression, but should not de-mand that every expression has value assigned for ev-ery feature.
</nextsent>
<nextsent>since we cannot claim that this set of features ex-haustive, the tool must allow further features to be added by the user.
</nextsent>
<nextsent>since reliability of feature assignment is important, he tool should have the abifity to extract as many features as possible automatically (for example, from parsed corpus).
</nextsent>
<nextsent>in addition since some features must be hand-marked, the tool must have the ability to compare feature marking between two coders for the same texl 1.2 evaluating anaphora generation and.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL323">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> mo~a~n.  </section>
<citcontext>
<prevsection>
<prevsent>so.
</prevsent>
<prevsent>we will try to mark set of features which is superset of the necessary features.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
drawing on the feature sets used in connolly et al (1997) and ge et al (1998), <papid> W98-1119 </papid>we believe the following factors might indicate co-referenco: ? syntactic role (e.g. subject, object, prepositional object,...), ? pronominal ization (yea or no), ? distance between ea and ee (an integer), ? definite ness (yes or no), ? semantic role (e.g. indicating location, manner, time,...), ? nesting depth of an p (an integer), ? information status (as defined by strube (1998)) <papid> P98-2204 </papid>of the de, ? gender, number, animacy.</citsent>
<aftsection>
<nextsent>the tool must allow the coder to assign values for these features to each marked expression, but should not de-mand that every expression has value assigned for ev-ery feature.
</nextsent>
<nextsent>since we cannot claim that this set of features ex-haustive, the tool must allow further features to be added by the user.
</nextsent>
<nextsent>since reliability of feature assignment is important, he tool should have the abifity to extract as many features as possible automatically (for example, from parsed corpus).
</nextsent>
<nextsent>in addition since some features must be hand-marked, the tool must have the ability to compare feature marking between two coders for the same texl 1.2 evaluating anaphora generation and.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL324">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> mo~a~n.  </section>
<citcontext>
<prevsection>
<prevsent>of the dialog act units)must be questioned.
</prevsent>
<prevsent>for the pur- pose.of applying the kappa (g) stnfi~c the segmenta-tion task must be turned into classification task.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
so, we view boundaries between dialog acts as one class and non-boundaries the other (see passonneau &amp; litman (1997) <papid> J97-1005 </papid>for similar practice).</citsent>
<aftsection>
<nextsent>the next step is to classify dialog act units as particular dialog acts.
</nextsent>
<nextsent>for this task the statistic is also appropriate.
</nextsent>
<nextsent>individual and abstract object anaphot~a. since spoken dialog shows high number of discourse deictic and vague anaphora, pronouns and demonstratives have to be classified accusingly.
</nextsent>
<nextsent>thus an additional feature, anaphor type, must be marked in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL325">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> annotating parsed  corpus.  </section>
<citcontext>
<prevsection>
<prevsent>in order to determine that these dimen-sions have been  *reliably marked , we need to measure agreement between two codeas marking the same text.
</prevsent>
<prevsent>one way to increase the relinb dity of the coding (re- gard less of the method used to measure reliability) is to automate part of the coding process.
</prevsent>
</prevsection>
<citsent citstr=" H94-1020 ">
our system can ex-tract number of markings, features and relations from the parsed, part-of-speech-tagged corpora of the type found in in the penn treebank 2 (marcus et al, 1994).<papid> H94-1020 </papid></citsent>
<aftsection>
<nextsent>use of the treebank data means we can find most of the markables and many of the necessary features before giv-ing the task to human coder.
</nextsent>
<nextsent>we do not try to extract any of the co-reference information from the parsed cor-pora.
</nextsent>
<nextsent>56 0 6 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 @ 0 0 0 0 0 0 0 e 0 0 0 o @ o @ @ e @ e, @ @ @ @ @ 0 @ @ @ @ @ @ @ e e e @ @ @ _e @ 2.1 extracting markables.
</nextsent>
<nextsent>in this context, mark able is text span representing discourse ntity which can be anaphoricaily referred to in text or dialog.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL326">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> annotating parsed  corpus.  </section>
<citcontext>
<prevsection>
<prevsent>we require that he two coders have the same set of markables before comparing their co-reference annotations, so achieving mark able agreement of is prerequisite for this calcu-lation.
</prevsent>
<prevsent>as discussed in section 2.3, the co-reference la- tion divides the set of markables into equivalence lasses.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
a model-theoretic algorithm proposed by vilain et al (1995) <papid> M95-1005 </papid>uses these co-referenc lasses to define preci-sion and recall metric which yields intuitively plausible results and is easy to calculate.</citsent>
<aftsection>
<nextsent>the method epends on counting how many co-refereace links must be added to one coder equivalence lasses to wans form the set into that found by the other c~ler.
</nextsent>
<nextsent>we adopt his method and enable the tool to perform l~ computation between any two codings which fully agree on the underlying set of markahies.
</nextsent>
<nextsent>finally.
</nextsent>
<nextsent>we can measure feature-value agreement by viewing the featme assignment task as kind of clas-sification task and then computing kappa (a), which measures how well the coders .g~l compared to their random ejcpected agreemeat4(cahetta, 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL327">
<title id=" W99-0107.xml">building a tool for annotating reference in discourse </title>
<section> annotating parsed  corpus.  </section>
<citcontext>
<prevsection>
<prevsent>finally.
</prevsent>
<prevsent>we can measure feature-value agreement by viewing the featme assignment task as kind of clas-sification task and then computing kappa (a), which measures how well the coders .g~l compared to their random ejcpected agreemeat4(cahetta, 1996).
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
we con-form to the method proposed in poesio &amp; vieira (1998) <papid> J98-2001 </papid>for computing actual and expected agreement.</citsent>
<aftsection>
<nextsent>(again we assume the coders have already agreed on the set of + markables.)
</nextsent>
<nextsent>suppose we are considering given feature sagreemem among aset of   2 coders is usmdly calcu-lated as function of the . ~ pail wise agreements, sowe will discuss only the pa/rwise case here, realizing that the full comlmmion is straightforward.
</nextsent>
<nextsent>58 @ @ @ 0 @ @ 0 @ @ @ @ @ 0 @ 0 0 @ @ 0 @ 0 @ 0 0 @ @ 0 @ 0 @ 0 0 @ 0 @ @ 0 @ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0  0 0 0 e 0 0 0 @ o o 0 , which was marked by two coders on each of ex-pressions in corpus.
</nextsent>
<nextsent>percent agreement is simply the fraction of expressions out of for which the two coders assigned the same value to .f. expected agreement is not computed by assuming that each value is equally likely, though.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL328">
<title id=" W98-1420.xml">a language independent system for generating feature structures from interlingua representations </title>
<section> knowledge resources.  </section>
<citcontext>
<prevsection>
<prevsent>tile developed architecture is language-independent, it takes the information about the target language from three knowledge resources: lexicon, map-rules, and syntactic structure representation formalism of the target language.
</prevsent>
<prevsent>lexicon, besides its other usages, provides information about the relationship between concept instances and word senses of tile target language \[dorr, 1993\].
</prevsent>
</prevsection>
<citsent citstr=" C92-4202 ">
map- rules define how the content of tmr is related to the syntactic structure of the target language \[mitamura and nyberg, 1992\].<papid> C92-4202 </papid></citsent>
<aftsection>
<nextsent>the last knowledge resource provides the information about the structure of the syntactic representation formalism.
</nextsent>
<nextsent>the interface between concept instances in tmr (denoting events and entit ies)and word senses of the target language is established using semantic and pragmatic properties f lexemes that are defined in the lexicon.
</nextsent>
<nextsent>since nouns denote entities and verbs denote events in language, each word that belongs to one of these categories also defined as concept instance in the lexicon.
</nextsent>
<nextsent>so, for every tmr frame that is concept instance, there is set of candidate lexicon entries that are defined using the same concept.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL329">
<title id=" W98-1406.xml">deconstraining text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose data-driven approach to modular ization d illustrate how it elimi-nates ? the previously ubiquitous constraints on combination of evidence across modules and on ? control.
</prevsent>
<prevsent>we also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts.
</prevsent>
</prevsection>
<citsent citstr=" W94-0315 ">
this paper addresses the area of text generation known as micro planning \[levelt1989, panaget1994, <papid> W94-0315 </papid>huang and fiedler1996\], <papid> W96-0403 </papid>or sentence planning \[rambow and korelsky1992\]; [<papid> A92-1006 </papid>wanner and hovy1996\].</citsent>
<aftsection>
<nextsent>micro planning involves low-level discourse structuring and marking, sentence boundary planning, clause internal structuring and all of the varied subtasks involved in lexical choice, these complex tasks are often modular ized and treated separately.
</nextsent>
<nextsent>the general argument is that since sentence planning tasks are not single-step operations, since they do not have to be performed in strict se-quence, and since the planner operation is non-deterministic, each sentence planning task should be implemented by separate module or by several modules (see, e.g., \[wanner and hovy1996\]).
</nextsent>
<nextsent>such an argument is natural if generation is viewed as set of coarse-grained tasks.
</nextsent>
<nextsent>indeed, with the exception of few researchers (\[elhadad et a1.1997\] and the incremental ists li ted below), the task- oriented view is standard in the generation community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL330">
<title id=" W98-1406.xml">deconstraining text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose data-driven approach to modular ization d illustrate how it elimi-nates ? the previously ubiquitous constraints on combination of evidence across modules and on ? control.
</prevsent>
<prevsent>we also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts.
</prevsent>
</prevsection>
<citsent citstr=" W96-0403 ">
this paper addresses the area of text generation known as micro planning \[levelt1989, panaget1994, <papid> W94-0315 </papid>huang and fiedler1996\], <papid> W96-0403 </papid>or sentence planning \[rambow and korelsky1992\]; [<papid> A92-1006 </papid>wanner and hovy1996\].</citsent>
<aftsection>
<nextsent>micro planning involves low-level discourse structuring and marking, sentence boundary planning, clause internal structuring and all of the varied subtasks involved in lexical choice, these complex tasks are often modular ized and treated separately.
</nextsent>
<nextsent>the general argument is that since sentence planning tasks are not single-step operations, since they do not have to be performed in strict se-quence, and since the planner operation is non-deterministic, each sentence planning task should be implemented by separate module or by several modules (see, e.g., \[wanner and hovy1996\]).
</nextsent>
<nextsent>such an argument is natural if generation is viewed as set of coarse-grained tasks.
</nextsent>
<nextsent>indeed, with the exception of few researchers (\[elhadad et a1.1997\] and the incremental ists li ted below), the task- oriented view is standard in the generation community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL331">
<title id=" W98-1406.xml">deconstraining text generation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we propose data-driven approach to modular ization d illustrate how it elimi-nates ? the previously ubiquitous constraints on combination of evidence across modules and on ? control.
</prevsent>
<prevsent>we also briefly overview the constraint-based control architecture that enables such an approach and facilitates near linear-time processing with realistic texts.
</prevsent>
</prevsection>
<citsent citstr=" A92-1006 ">
this paper addresses the area of text generation known as micro planning \[levelt1989, panaget1994, <papid> W94-0315 </papid>huang and fiedler1996\], <papid> W96-0403 </papid>or sentence planning \[rambow and korelsky1992\]; [<papid> A92-1006 </papid>wanner and hovy1996\].</citsent>
<aftsection>
<nextsent>micro planning involves low-level discourse structuring and marking, sentence boundary planning, clause internal structuring and all of the varied subtasks involved in lexical choice, these complex tasks are often modular ized and treated separately.
</nextsent>
<nextsent>the general argument is that since sentence planning tasks are not single-step operations, since they do not have to be performed in strict se-quence, and since the planner operation is non-deterministic, each sentence planning task should be implemented by separate module or by several modules (see, e.g., \[wanner and hovy1996\]).
</nextsent>
<nextsent>such an argument is natural if generation is viewed as set of coarse-grained tasks.
</nextsent>
<nextsent>indeed, with the exception of few researchers (\[elhadad et a1.1997\] and the incremental ists li ted below), the task- oriented view is standard in the generation community.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL332">
<title id=" W98-1406.xml">deconstraining text generation </title>
<section> ontology-based modular ization.  </section>
<citcontext>
<prevsection>
<prevsent>the generation lexicon contains information (such as, for instance, semantics-to-syntax dependency mappings) that drives the generation process, with the help of several dedicated micro theories that deal with issues such as focus and reference (values of which are among the elements of our input representations).
</prevsent>
<prevsent>the mikrokosmos spanish core lexicon is complete with 7000 word senses defined; the english core lexicon is still underdevelopment with projected size of over 10,000 word senses.
</prevsent>
</prevsection>
<citsent citstr=" P96-1005 ">
both of these core lexicons can be expanded with lexical rules to contain around 30,000 entries (\[viegas et al 1996\]).<papid> P96-1005 </papid></citsent>
<aftsection>
<nextsent>lexicon entries in both analysis and generation can be thought of as  objects  or  modules  corresponding to each unit in the input.
</nextsent>
<nextsent>such module has the task of realizing the associated uni , while communicating with other objects around it, if necessary (similar to \[de smedt1990\]).
</nextsent>
<nextsent>1in semantic analysis, the input is set of words, thus the lexicon is indexed on words.
</nextsent>
<nextsent>in generation, the input is concepts, so it is indexed on concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL333">
<title id=" W98-1406.xml">deconstraining text generation </title>
<section> ontology-based modular ization.  </section>
<citcontext>
<prevsection>
<prevsent>he utilizes annotations as feedback mechanism to provide the planning stages with linguistically relevant knowledge.
</prevsent>
<prevsent>another area of research that belies the unnatural task-based ivision widely accepted by.text generation researchers today is the attempts to control sentence planning tasks.
</prevsent>
</prevsection>
<citsent citstr=" W96-0401 ">
\[nirenburg et al  1989\] and more recently, \[wanner and hovy1996\] <papid> W96-0401 </papid>advocate blackboard control mechanism, arguing that the order of sentence planning tasks cannot be pre-determined.</citsent>
<aftsection>
<nextsent>behind this difficulty is the real- ? ity that different linguistic phenomena have different, unpredictable quirements.
</nextsent>
<nextsent>grammatical, stylistic and collocati0nal constraints combine at unexpected times during the various tasks of sen-tence planning.?
</nextsent>
<nextsent>blackboard architectures, theoretically, can be used to allow certain thread of operation to suspend operation until needed bit of information is available.
</nextsent>
<nextsent>unfortunately, in the best case, such an architecture is inefficient and difficult to control.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL334">
<title id=" W99-0304.xml">evaluation of annotation schemes for japanese discourse japanese discourse tagging working group </title>
<section> utterance unit.  </section>
<citcontext>
<prevsection>
<prevsent>in the following, annotation schemes for utterance units, discourse struc-ture, and discourse markers are discussed based on our coding experiments.
</prevsent>
<prevsent>2.1 first annotation scheme.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
based on the survey of existing annota-tion schemes uch as the schemes of sev-eral research groups in japan (kyoto univ., tsukuba univ., waseda univ., atr (na- gata, 1992)) and dri (allen and core, 1996; carletta et al, 1997<papid> J97-1002 </papid>a) for utterances (we call this utterance unit tags), we created the first annotation manual for illocutionary force type, mood information and exchange structures.</citsent>
<aftsection>
<nextsent>illocutionary force types come from speech act theory (searle, 1969), and are one of the most popular set of describ-ing communicative aspects of utterances.
</nextsent>
<nextsent>mood information corresponds to the mean-ing of auxiliary verbs in japanese, which has been hinted that there might be close rela-tions with illocutionary act types?
</nextsent>
<nextsent>exchange structures define minimal interact ional units consisting of initiative, response and follow- up (coulthhard, 1992; stenstrom, 1994).
</nextsent>
<nextsent>we carried out first annotation exper-iments using the above three manuals, and obtained the following lessons for improving the schemes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL340">
<title id=" W99-0304.xml">evaluation of annotation schemes for japanese discourse japanese discourse tagging working group </title>
<section> 35  </section>
<citcontext>
<prevsection>
<prevsent>2(3 m \[\] m \[\] m m m \[\] m mm m data map task group scheduling route direction telephone shopping appointment scheduling total ii \[ umber of utterance ii  a) p(e) table 1: evaluation of utterance unit tagging scheme first version second version agree 3 agree 2 disagree agree 3 60 51 1 41 38 8 0
</prevsent>
<prevsent>86 24 1 26 28 6 30 31 87 29 245 119 \[ i 218 375 agree 2 disagree 54 18 12 4 6 9 20 4 21 11 i13 46 377 0.76 0.68 0.44 0.12 0.57 0.64 3 is course t ruc ture 3.1 first annotation scheme.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
grosz and sidner proposed model of dis-course structure, in which discourse struc-ture is composed of the linguistic structure, the intentional structure, and the attentional state (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>we built the first annotation scheme of discourse struc-ture in dialogue based on this model.
</nextsent>
<nextsent>the written instruction of the scheme describes as follows.
</nextsent>
<nextsent>utterances both at the start and the end of segments are marked.
</nextsent>
<nextsent>discourse segments may be nested.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL341">
<title id=" W99-0304.xml">evaluation of annotation schemes for japanese discourse japanese discourse tagging working group </title>
<section> discourse  markers.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, there are abundant in japanese dis-course.
</prevsent>
<prevsent>kawamori compared english dis- couse markders with japanese.
</prevsent>
</prevsection>
<citsent citstr=" W98-0316 ">
in japanese coupus, half of the turns are started with these words, while english corpus shows that about 25 % of the turns start with corre-sponding expression(kawamori et al, 1998).<papid> W98-0316 </papid></citsent>
<aftsection>
<nextsent>the correlation between japanese dis-course markers and the boundary of dis-course segments has not shown, which can be used to improve the identification of the discourse boundaries.
</nextsent>
<nextsent>in this section, the ex-pressions which can be used for discourse markers, aiduti and fillers are enumerated based on the data survey, and the correlation table 3: aiduti expressions selected by the co ders 4 coders 3 coders 2 coders 1 coder ha 16 26 38 49 soudesuka 0 0 2 0 asoudesuka 0 0 2 0 0 1 1 0 na 0 1 0 0 ha 1 0 1 0 tota 17 30 56 73 table 4: discourse marker expressions se-lected by the coders 4 coders 3 coders 10 26 2 9 1 0 1 1 0 19 0 19 0 6 0 1 0 1 14 87 2 coders 17 6 5 2 7 9 3 1 0 60 anode dewa cto ja aja iya total 1 coder 4 4 7 2 15 10 0 0 0 108 between discourse markers and the discourse boundaries in japanese is shown.
</nextsent>
<nextsent>4.1 surface expressions of discourse.
</nextsent>
<nextsent>markers discourse markers and speech related phe-nomena redefined as utterances that func-tion as lubricant rather than contributing to achieving some task-related goals in con-versations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL342">
<title id=" W99-0630.xml">automatically merging lexicons that have incompatible partofspeech categories </title>
<section> anti- lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>in general, we have observed ifferent be-haviors depending on factors such as the granularity of the tagsets, the linguistic the-ories behind the tagsets, and the coverage of the lexicons.
</prevsent>
<prevsent>finally, in addition to lexicon merging, pos mapping table is also useful in other applications.
</prevsent>
</prevsection>
<citsent citstr=" P98-2230 ">
wu and wong apply them in their sitg channel model to give bet-ter performance in their translation applica-tion (wu and wong, 1998).<papid> P98-2230 </papid></citsent>
<aftsection>
<nextsent>there is serious problem of low recall on our anti-lexicon model.
</nextsent>
<nextsent>this is because our model prunes out many possible pos map-ping rules which results in very conservative lexeme selection during the lexicon merg-ing process.
</nextsent>
<nextsent>moreover, our model cannot discover which pos tags in original lexicon have no corresponding tag in the additional lexicon.
</nextsent>
<nextsent>our model took pos mapping rules as natural starting point since this repre- 256 sen tation has been used in earlier related work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL343">
<title id=" W99-0902.xml">the applications of unsupervised learning to japanese graphemephoneme alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>grapheme-phoneme ( g-b ) alignment is defined as the task of maximally segmenting grapheme compound into morpho-phonic units, and aligning each unit to the corresponding substring in the phoneme compound (bilac et al, 1999).
</prevsent>
<prevsent>its main use is in portrayal of the phonological interaction between adjoining grapheme segments, and also implicit description of the range of readings each grapheme segment can take.
</prevsent>
</prevsection>
<citsent citstr=" J97-4001 ">
we further suggest that large-scale database of maximally aligned g-p tu- ples has applications within the more conventional task of g-p translation (klatt, 1987; huang et al, 1994; divay and vitale, 1997).<papid> J97-4001 </papid></citsent>
<aftsection>
<nextsent>our particular interest in developing database of g-p tuples is to apply it in the development of kanji tester which can dynamically predict plausi-bly incorrect readings forgiven grapheme string.
</nextsent>
<nextsent>for this purpose, we require as great coverage of grapheme strings as possible, and the proposed sys-tem has thus been designed to exhaustively align the input set of g-p tuples, sacrificing precision for 100% recall.
</nextsent>
<nextsent> grapheme string  in this research refers to the maximal kanji representation of given word or compound, and  phoneme string  refers to the kana (hiragana and/or katakana) mora correlate.
</nextsent>
<nextsent>1 by  maximal  segmentation is meant hat the grapheme string must be segmented to the degree that each segment corresponds to self-contained component of the phonemic description of that compound, and that no segment can be further segmented into align-ing sub-segments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL344">
<title id=" W99-0902.xml">the applications of unsupervised learning to japanese graphemephoneme alignment </title>
<section> scoring method.  </section>
<citcontext>
<prevsection>
<prevsent>the first discriminative metric is heuristic, and based on the intuition that we are after maxi-mum disparity in score between the first and sec-ond ranked candidates.
</prevsent>
<prevsent>the second discriminative metric, on the other hand, is designed to balance up maximisation of both sl and the relative disparity between sl and s2.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
note that, unlike dagan and itai (1994), <papid> J94-4003 </papid>we give no consideration to statistical confi-dence as we are after 100% recall, whatever the cost to precision.</citsent>
<aftsection>
<nextsent>to this point, the only difference over method-1 is the sequence in which solutions are output.
</nextsent>
<nextsent>how- ever, by singling out g-p alignment candidate of maximum discrimination on each iteration, it now becomes possible to refine the statistical model by training it on aligned output (i.e. g-p tuples stored in in fig.
</nextsent>
<nextsent>2), hence: (a) alleviating statistics deriv-ing from less-plausible alignments, and (b) weight-ing up term frequencies found in final disambiguated alignments.
</nextsent>
<nextsent>neither of these processes are possible under the simple statistical model as all alignments are processed in parallel, and the system is unable to commit itself to the plausibility of any given align-ment in scoring others.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL345">
<title id=" W99-0902.xml">the applications of unsupervised learning to japanese graphemephoneme alignment </title>
<section> other applications of this.  </section>
<citcontext>
<prevsection>
<prevsent>note also that whereas the discriminative curve is monotonic ally decreasing when averaged over the given corridor, in practice lo-cal maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the maximum discriminative value.
</prevsent>
<prevsent>research other than the constraints described in section 2 and frequency determination techniques, the pro-posed methodology is theoretically scalable to any domain where two streams of chunked information require alignment.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
this suggests applications to the extraction of translation pairs from aligned bilin-gual corpora (gale and church, 1991; <papid> H91-1026 </papid>kupiec, 1993; <papid> P93-1003 </papid>smadja et al, 1996), <papid> J96-1001 </papid>where the system input would be made up of aligned strings (generally sentences) in the two languages.</citsent>
<aftsection>
<nextsent>given that we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scor-ing and learning methods proposed herein in their existing forms.
</nextsent>
<nextsent>note, however, that in the case of translation pair extraction, there is real possibil-ity of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily.
</nextsent>
<nextsent>in fact, it may occur that there is residue of unaligned segments in either or both languages, as could easily occur if one language included zero anaphora.
</nextsent>
<nextsent>it may, therefore, be desirable to apply dynamic threshold on the discriminative ratio (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL346">
<title id=" W99-0902.xml">the applications of unsupervised learning to japanese graphemephoneme alignment </title>
<section> other applications of this.  </section>
<citcontext>
<prevsection>
<prevsent>note also that whereas the discriminative curve is monotonic ally decreasing when averaged over the given corridor, in practice lo-cal maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the maximum discriminative value.
</prevsent>
<prevsent>research other than the constraints described in section 2 and frequency determination techniques, the pro-posed methodology is theoretically scalable to any domain where two streams of chunked information require alignment.
</prevsent>
</prevsection>
<citsent citstr=" P93-1003 ">
this suggests applications to the extraction of translation pairs from aligned bilin-gual corpora (gale and church, 1991; <papid> H91-1026 </papid>kupiec, 1993; <papid> P93-1003 </papid>smadja et al, 1996), <papid> J96-1001 </papid>where the system input would be made up of aligned strings (generally sentences) in the two languages.</citsent>
<aftsection>
<nextsent>given that we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scor-ing and learning methods proposed herein in their existing forms.
</nextsent>
<nextsent>note, however, that in the case of translation pair extraction, there is real possibil-ity of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily.
</nextsent>
<nextsent>in fact, it may occur that there is residue of unaligned segments in either or both languages, as could easily occur if one language included zero anaphora.
</nextsent>
<nextsent>it may, therefore, be desirable to apply dynamic threshold on the discriminative ratio (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL347">
<title id=" W99-0902.xml">the applications of unsupervised learning to japanese graphemephoneme alignment </title>
<section> other applications of this.  </section>
<citcontext>
<prevsection>
<prevsent>note also that whereas the discriminative curve is monotonic ally decreasing when averaged over the given corridor, in practice lo-cal maximums do exist, attributable to the situation where re-training of the statistical model produces inflation of the maximum discriminative value.
</prevsent>
<prevsent>research other than the constraints described in section 2 and frequency determination techniques, the pro-posed methodology is theoretically scalable to any domain where two streams of chunked information require alignment.
</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
this suggests applications to the extraction of translation pairs from aligned bilin-gual corpora (gale and church, 1991; <papid> H91-1026 </papid>kupiec, 1993; <papid> P93-1003 </papid>smadja et al, 1996), <papid> J96-1001 </papid>where the system input would be made up of aligned strings (generally sentences) in the two languages.</citsent>
<aftsection>
<nextsent>given that we can devise some way of creating an alignment paradigm between the two input segments, it is possible to apply the scor-ing and learning methods proposed herein in their existing forms.
</nextsent>
<nextsent>note, however, that in the case of translation pair extraction, there is real possibil-ity of the alignment mapping being many-to-many, and crossing over of alignment is expected to occur readily.
</nextsent>
<nextsent>in fact, it may occur that there is residue of unaligned segments in either or both languages, as could easily occur if one language included zero anaphora.
</nextsent>
<nextsent>it may, therefore, be desirable to apply dynamic threshold on the discriminative ratio (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL349">
<title id=" W98-1306.xml">treatment of emoves in subset construction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>- determinisation and minimisation.
</prevsent>
<prevsent>three different minimisation algorithms are sup- ported: hop croft algorithm (hopcroft, 1971), hop croft and ullmart algorithm (hopcroft and ullman, 1979), and brzozowski algorithm (brzozowski, 1962).
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
- determinisation d minimisation of string-to-string and string-to-weight transducers (mohri, 1996; mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>- visuuli~tion.
</nextsent>
<nextsent>support includes built-in visualisation (tcl/tk, tex+pictex, tex+pstricks, postscript) and interfaces tothird party graph visualisation software (graphviz (dot), vcg, dawmci).
</nextsent>
<nextsent>- random generation of finite automata ( extension of the algorithm in leslie (1995) to al-low the generation of finite automata containing e-moves).
</nextsent>
<nextsent>3 subset construction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL350">
<title id=" W98-1422.xml">fully lexicalized head driven syntactic generation </title>
<section> micro planning and syntactic generation.  </section>
<citcontext>
<prevsection>
<prevsent>in each step, only an abstraction of the multitude of information contained in an alternative needs to be considered.
</prevsent>
<prevsent>another aspect of this architecture is the separation into kernel system, i.e., the language in-dependent core algorithms (a constraint-solver for micro planning and the search and combination algorithms for syntactic generation described in section 5) and declarative knowledge bases, e.g., the language specific word-choice constraints in micro planning and the tag grammars used in syn-tactic realization.
</prevsent>
</prevsection>
<citsent citstr=" P98-1017 ">
this separation allows for an easy adaptation of the system to other languages and domains (see \[becker et al 1998\]).<papid> P98-1017 </papid></citsent>
<aftsection>
<nextsent>all modules of the generator utilize external, declarative knowledge bases.
</nextsent>
<nextsent>for the syntactic gen-erator, extensive off-line preprocessing of the highly declarative hpsg grammar for english 2 is applied.
</nextsent>
<nextsent>the grammar has not even been written exclusively as generation grammar 3.
</nextsent>
<nextsent>it is specialized, however, in that it covers phenomena of spoken language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL351">
<title id=" W98-1422.xml">fully lexicalized head driven syntactic generation </title>
<section> off-line preprocessing: hpsg to tag compilation.  </section>
<citcontext>
<prevsection>
<prevsent>3in fact, most of the testing during grammar development depends on the use of parser.
</prevsent>
<prevsent>209 possible, computationally expensive applications of hpsg schemata, it merely has to find suitable pre computed syntactic structures for each lexical item and combine them appropriately.
</prevsent>
</prevsection>
<citsent citstr=" P95-1013 ">
for this preprocessing of the hpsg grammar, we adapted the  hpsg to tag compilation  process described in \[kasper et al 1995\].<papid> P95-1013 </papid></citsent>
<aftsection>
<nextsent>the basis for the compilation is an identification of syntactically relevant selector features which express ubcategorization requirements of lexical item, e.g. the valence features.
</nextsent>
<nextsent>in general, phrase structure is complete when these selector features are empty.
</nextsent>
<nextsent>starting from the feature structure for lexical item, hpsg schemata reapplied such that the current structure is unified with daughter feature of the schema.
</nextsent>
<nextsent>the resulting structure is again subject to this process: this compilation process tops when certain termination criteria are met, e.g., when all selector features are empty.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL352">
<title id=" W98-1422.xml">fully lexicalized head driven syntactic generation </title>
<section> conclusion and comparison.  </section>
<citcontext>
<prevsection>
<prevsent>we have shown how preprocessing an hpsg grammar can be used to avoid the costly on-line ap-plication (unification) of hpsg schemata in modular ized generation system with micro planner and separate syntactic generator.
</prevsent>
<prevsent>the compilation of an hpsg grammar to tag grammar allows the use of an efficient syntactic generator without sacrificing the declarative nature of the hpsg grammar.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
it is important compare the generation strategy presented here with semantic-head-driven - eration \[shieber et al 1990, <papid> J90-1004 </papid>van noord 1990\] which is direct generation algorithm froni logical form encodings.</citsent>
<aftsection>
<nextsent>it improves previous algorithms inefficiency and in imposing less restrictions on the type of grammar.
</nextsent>
<nextsent>it is also applicable to hpsg and proceeds by applying the hpsg schemata in bottom-up fashion, driven from the lexical heads of the schemata.
</nextsent>
<nextsent>to large ex.tend, the tag-based generation algorithm presented here goes through the same steps as semantic-head-driven eration.
</nextsent>
<nextsent>however, most of these steps will have been made during the off-line preprocessing and are encoded in the elementary trees of the tag grammar thns resulting 6note that the node labels shown in figures 7 are only concession to readability.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL353">
<title id=" W98-1422.xml">fully lexicalized head driven syntactic generation </title>
<section> conclusion and comparison.  </section>
<citcontext>
<prevsection>
<prevsent>215 in an important gain inefficiency.
</prevsent>
<prevsent>note though, that the generation task in the algorithm presented here is shared between the micr0planner and the syntactic generator,-so formal comparison must include both components.
</prevsent>
</prevsection>
<citsent citstr=" P85-1012 ">
work on generation with tag generally assumes that there is one, to~-one mapping between the in-formation in the generator input and the choice of elementary tree \[mcdonald and pustejovsky 1985, <papid> P85-1012 </papid>yang, mccoy, and vijay-shanker 1991, d0ran and stone 1997\].</citsent>
<aftsection>
<nextsent>in general, this will not be the case.
</nextsent>
<nextsent>in particular, in our system the input is not always sufficiently analyzed and the preprocess-ing froman hpsg grammar potentially creates more than one elementary tree that fits the input parameters.
</nextsent>
<nextsent>one possible approach are choice nets-see \[yang, mccoy, and vijay-shanker 1991\] who interpret systemic grammar in this way.
</nextsent>
<nextsent>our approach as some similarity, though we have provided more general algorithm that does not require the specification of grammar specific choice nets but rather executes tree selection and combination from more declarative knowledge bases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL354">
<title id=" W99-0607.xml">applying extra sentential context to maximum entropy based tagging with a large semantic and syntactic tagset </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>it appears intuitively that information from earlier sentences in document ought to help reduce uncertmnty as to word correct part- of-speech tag.
</prevsent>
</prevsection>
<citsent citstr=" C96-1020 ">
this is especially so for large semantic and syntactic tagset such as the roughly-3000-tag atr general english tagset (black et al, 1996; <papid> C96-1020 </papid>black et al, 1998).<papid> P98-1020 </papid></citsent>
<aftsection>
<nextsent>and in fact, (black et al, 1998) <papid> P98-1020 </papid>demonstrate signif-icant  tag trigger-pair  effect.</nextsent>
<nextsent>that is, given that certain  triggering  tags have already oc-curred in document, the probability of oc-currence of specific  triggered  tags is raised significantly--with respect the unigram tag probability model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL355">
<title id=" W99-0607.xml">applying extra sentential context to maximum entropy based tagging with a large semantic and syntactic tagset </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>

<prevsent>it appears intuitively that information from earlier sentences in document ought to help reduce uncertmnty as to word correct part- of-speech tag.
</prevsent>
</prevsection>
<citsent citstr=" P98-1020 ">
this is especially so for large semantic and syntactic tagset such as the roughly-3000-tag atr general english tagset (black et al, 1996; <papid> C96-1020 </papid>black et al, 1998).<papid> P98-1020 </papid></citsent>
<aftsection>
<nextsent>and in fact, (black et al, 1998) <papid> P98-1020 </papid>demonstrate signif-icant  tag trigger-pair  effect.</nextsent>
<nextsent>that is, given that certain  triggering  tags have already oc-curred in document, the probability of oc-currence of specific  triggered  tags is raised significantly--with respect the unigram tag probability model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL362">
<title id=" W99-0607.xml">applying extra sentential context to maximum entropy based tagging with a large semantic and syntactic tagset </title>
<section> j jsystem  </section>
<citcontext>
<prevsection>
<prevsent>2 tagging mode l. 2.1 me mode l. our tagging model is maximum entropy (me) model of the following form: p(tlh) = 7 i~ ~ k(h t)p?
</prevsent>
<prevsent>(1) k=0 where: - is tag we are predicting; - is the history (all prior words and tags) of t; - 7 is normalization coefficient hat en- ~l r-tk \]k(h,t) sures: ~t=otllk=o ak p0 = 1; is the number of tags in our tag set; - ak is the weight of trigger fk; fk are trigger functions and f~e{0, 1}; - p0 is the default agging model (in our case, the uniform distribution, since all of the in-formation in the model is specified using me constraints).
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the model we use is similar to that of (rat- naparkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>our baseline model shares the following features with this tagging model; we will call this set of features the basic n-gram tagger constraints: 1.
</nextsent>
<nextsent>w=x&t;=t 2.
</nextsent>
<nextsent>t _ l=x&t;=t 3.
</nextsent>
<nextsent>t-2t-1 = xy ~: = where: - is word whose tag we are predicting; - is tag we are predicting; - t-1 is tag to the left of tag t; - t-2 is tag to the left of tag - ; our baseline model differs from ratna-parkhi in that it does not use any informa-tion about the occurrence of words in the his-tory or their properties (other than in con-straint 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL363">
<title id=" W99-0607.xml">applying extra sentential context to maximum entropy based tagging with a large semantic and syntactic tagset </title>
<section> j jsystem  </section>
<citcontext>
<prevsection>
<prevsent>t _ l=x&t;=t 3.
</prevsent>
<prevsent>t-2t-1 = xy ~: = where: - is word whose tag we are predicting; - is tag we are predicting; - t-1 is tag to the left of tag t; - t-2 is tag to the left of tag - ; our baseline model differs from ratna-parkhi in that it does not use any informa-tion about the occurrence of words in the his-tory or their properties (other than in con-straint 1).
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
our model exploits the same kind of tag-n-gram information that forms the core of many successful tagging models, for exam-ple, (kupiec, 1992), (merialdo, 1994), (<papid> J94-2001 </papid>ratna-parkhi, 1996).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>we refer to this type of tagger as tag-n-gram tagger.
</nextsent>
<nextsent>2.2 trigger selection.
</nextsent>
<nextsent>we use mutual information (mi) to select the most useful trigger pairs (for more details, see (rosenfeld, 1996)).
</nextsent>
<nextsent>that is, we use the follow-ing formula to gauge feature usefulness to 1-he model: mi( , t) where: = p(,s,t) tog -?~p(tl~) + p(s,t)  p(tls) ~og p(~) + p(~,t) log + p(~,/)lo~ p(tl~) c~ p(\[) - is the tag we are predicting; - can be any kind of triggering feature.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL378">
<title id=" W99-0901.xml">hiding a semantic hierarchy in a markov model </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>5 re iminary resu ts as mentioned above, we tested our trained models on word-sense disambiguation evaluation, reason-ing that if it performed poorly on this evaluation, then it must not be disambiguating the training cor-pus very well.
</prevsent>
<prevsent>the bottom line is that we were not able to advance the state of the art - - the per-formance results are comparable to, but not better 7 than, those obtained by resnik.
</prevsent>
</prevsection>
<citsent citstr=" W97-0209 ">
we used the train-ing sets, test sets, and evaluation method described in (resnik, 1997).<papid> W97-0209 </papid></citsent>
<aftsection>
<nextsent>1 table 3 presents performance results.
</nextsent>
<nextsent>the random method is simply to randomly pick sense with uniform distribution.
</nextsent>
<nextsent>the first sense method is to always pick the most common sense as listed in wordnet.
</nextsent>
<nextsent>the hmm smoothed method is to use models trained with smoothing but no balancing modifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL379">
<title id=" W98-1413.xml">generating warning instructions by planning accidents and injuries </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we will show how knowledge about device that is assumed to already exist as part of the engineering effort, together with adequate, domain-independent knowledge about .the environment, can be used for this.
</prevsent>
<prevsent>we also put forth the notion that actions are performed on the materials that the device operates upon, that the states of these materials may change as result of these actions, and that the goal of the system should be defined in ? terms of the final states of the materials.
</prevsent>
</prevsection>
<citsent citstr=" W93-0203 ">
we take the stand that complete natural anguage instruction generation system for device should have, at the top level, knowledge of the device (as suggested by delin et al (1993)).<papid> W93-0203 </papid></citsent>
<aftsection>
<nextsent>this is one facet of instruction generation that many nlg systems have largely ignored by instead incorporating the knowledge of the task at their top level, i.e., the basic content of the?
</nextsent>
<nextsent>instructions assumed to already exist and does not need to be planned for.
</nextsent>
<nextsent>in our approach, all the knowledge necessary for the planning stage of system s contained (possibly in more abstract form) in the knowledge of the artifact ogether with the world knowledge.
</nextsent>
<nextsent>the kinds of knowledge that should .be sufficient for this planning are device knowledge ?(topological, kinematic, electrical, thermodynamic, and electronic) and world knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL380">
<title id=" W98-1413.xml">generating warning instructions by planning accidents and injuries </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach, all the knowledge necessary for the planning stage of system s contained (possibly in more abstract form) in the knowledge of the artifact ogether with the world knowledge.
</prevsent>
<prevsent>the kinds of knowledge that should .be sufficient for this planning are device knowledge ?(topological, kinematic, electrical, thermodynamic, and electronic) and world knowledge.
</prevsent>
</prevsection>
<citsent citstr=" A92-1009 ">
the idas project of reiter et al (1992), <papid> A92-1009 </papid>reiter et al (1995) served as key motivation for this work.</citsent>
<aftsection>
<nextsent>one of the primary goals of the idas project was to automatically generate technical documentation address correspondence to the second author.
</nextsent>
<nextsent>e-maili gh @cs.toront0.edu ..118 , it i i !1 i .i m ! i i i i i i i i | from domain knowledge base containing design information (such as that produced by an advanced computer-aided design tool) using nlg techniques.
</nextsent>
<nextsent>idas turned outto be successful in demonstrating the usefulness, from cost and benefits perspective, of applying nlg technology to partially automate the generation of documentation.
</nextsent>
<nextsent>if work in qualitative process theory, using functional-specifications such as those in e.g., (iwasaki et al, 1993), can yield the device and world knowledge that are required for text pianning, then the need for cost effectiveness would be met.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL381">
<title id=" W99-0633.xml">improving brills pos tagger for an agglutinative language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in 1992 eric brill presented rule-based tagging system which differs from other rule-based systems because it automatically infers rules from training corpus.
</prevsent>
<prevsent>the tagger does not use hand-crafted rules or pre specified language information, nor does the tagger use external lexicons.
</prevsent>
</prevsection>
<citsent citstr=" H92-1022 ">
according to brill (1992)  <papid> H92-1022 </papid>there is very small amount of general linguistic knowledge built into the system, but no language-specific knowledge .</citsent>
<aftsection>
<nextsent>the grammar is induced directly from the training corpus without human intervention or expert knowledge.
</nextsent>
<nextsent>the only additional component necessary is small, manually and correctly annotated corpus - the training corpus - which serves as input to the tagger.
</nextsent>
<nextsent>the system is then able to derive lexical/morphological nd contextual information from the training corpus and  learns  how to deduce the most likely part of speech tag for word.
</nextsent>
<nextsent>once the training is completed, the tagger can be used to annotate new, unannotated corpora based on the tag set of the training corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL382">
<title id=" W99-0633.xml">improving brills pos tagger for an agglutinative language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system is then able to derive lexical/morphological nd contextual information from the training corpus and  learns  how to deduce the most likely part of speech tag for word.
</prevsent>
<prevsent>once the training is completed, the tagger can be used to annotate new, unannotated corpora based on the tag set of the training corpus.
</prevsent>
</prevsection>
<citsent citstr=" H94-1049 ">
the tagger has been trained for tagging english texts with an accuracy of 97% (brill, 1994).<papid> H94-1049 </papid></citsent>
<aftsection>
<nextsent>in this study brill rule-based part of speech (pos) tagger is tested on hungarian, dissimilar language, concerning both morphology and syntax, to english.
</nextsent>
<nextsent>the main goal is i) to find out if brill system is immediately applicable to language, which greatly differs in structure from english, with high degree of accuracy and (if not) ii) to improve the training strategies to better fit for agglutinative/inflectional languages with complex morphological structure.
</nextsent>
<nextsent>hungarian is basically agglutinative, i.e. grammatical relations are expressed by means of affixes.
</nextsent>
<nextsent>hungarian is also inflectional; it is difficult to assign 275 morphemes precisely to the different parts of the affixes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL383">
<title id=" W99-0633.xml">improving brills pos tagger for an agglutinative language </title>
<section> testing brill original system.  </section>
<citcontext>
<prevsection>
<prevsent>precision (correct_found/retrieved_total) and recall (correct_found~intended_total)for pos categories of both test texts pos tags det (determiner) nm (pronoun) fn (noun) mn (adjective) ige (verb) inf (infinitive) ik (verbal particle) ha (adverb) szn (numeral) nu (postposition) kot (conjunction) isz (interjection) precision recall 1.0 1.0 0.94 0.80 0.83 0.78 0.70 0.75 0.70 0.83 0.90 0.96 0.74 0.85 0.84 0.74 0.73 0.89 0.83 0.97 0.91 0.96 1.0 0.20 to sum up the results, the tagger has greatest difficulties with categories belonging to the open classes because of 279 their morphological structure and homonymy, while grammatical categories are easier to detect and correctly annotate.
</prevsent>
<prevsent>complex and highly developed morphological structure and fairly free word order, i.e. making positional relationships less important, lead to lower accuracy compared to english when using brill tagger on hungarian.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
these results are not very promising when compared with brill results of english test corpora which have an accuracy of 96.5% trained on 88200 words (brill, 1995:<papid> J95-4004 </papid>11).</citsent>
<aftsection>
<nextsent>the difference inaccuracy might depend on i) the type of the training corpus, ii) the type and the size of the test corpus, and iii) the type of language structure, such as morphology and syntax.
</nextsent>
<nextsent>the corpus which was used to train the tagger on hungarian consisted of only one text, fiction with  inventive  language, while brill used training corpus consisting of several types of texts (brill, 1995).<papid> J95-4004 </papid></nextsent>
<nextsent>also, there is difference between the types and the sizes of the test corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL385">
<title id=" W98-1421.xml">towards multilingual protocol generation for spontaneous speech dialogues </title>
<section> an  overview f  verb mobil.  </section>
<citcontext>
<prevsection>
<prevsent>a turn is contribution of one dialogue participant.
</prevsent>
<prevsent>it may be divided into segments, which sometimes resemble linguistic lauses like sentences.
</prevsent>
</prevsection>
<citsent citstr=" A97-1007 ">
basic processing entity for some components is the so-called ialogue act \[bunt 1981, jekat et al 1995, alexandersson et al 1997\].<papid> A97-1007 </papid></citsent>
<aftsection>
<nextsent>for this work we use set of 18 dialogue acts, some purely illocutionary, e.g., requesting proposal for date.
</nextsent>
<nextsent>some comprise propositional content, e.g., proposing date.
</nextsent>
<nextsent>an important property of the dialogue act is its language independence - it should be possible to be used for the annotation of dialogues in any language.
</nextsent>
<nextsent>linguistic information is encoded in an abstract data type, the so called vit ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL386">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> the boosting algorithm ada boost.  </section>
<citcontext>
<prevsection>
<prevsent>let ?(z) {0, 1} denote the value of the pred-icate ? on the example z, and for e {0, 1}, let pb ir be the prediction of the weak hypothe-sis when ?(x) = b. then we can write simply h(x) = pc(z).
</prevsent>
<prevsent>given predicate ?, we choose p0 and pl to minimize z. schapire and singer (1998) show that is minimized when we let pb= ? in ) (1) 39 mf tag 7.66 markov 1-gram 6.74 markov 3-gram 3.7 markov 3-gram 3.64 decision tree 3.5 transformation 3.39 maxent 3.37 maxent 3.11 ~.07 multi-tagger voting 2.84 :t=.03 table 1: tb-wsj testing error previously reported in the literature.
</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
b = (brill and wu, 1998); = (magerman, 1995); = <papid> P95-1037 </papid>our data; = (ratna- parkhi, 1996); = (<papid> W96-0213 </papid>weischedel and others, 1993).</citsent>
<aftsection>
<nextsent>forb {0,1} where ws is thesum of d(i) for examples such that yi = and ?(xi) = b. this choice of p# implies that z: 2 cw+blw-bl  (2) be(o,1} this expression can now be minimized over all choices of ?.
</nextsent>
<nextsent>thus, our weak learner works by searching for the predicate ? that minimizes of eq.
</nextsent>
<nextsent>(2), and the resulting weak hypothesis h(x) predicts pc(z) of eq.
</nextsent>
<nextsent>(1) on example x. in practice, very large values of p0 and pl can cause numerical problems and may also lead to overfitting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL387">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> the boosting algorithm ada boost.  </section>
<citcontext>
<prevsection>
<prevsent>let ?(z) {0, 1} denote the value of the pred-icate ? on the example z, and for e {0, 1}, let pb ir be the prediction of the weak hypothe-sis when ?(x) = b. then we can write simply h(x) = pc(z).
</prevsent>
<prevsent>given predicate ?, we choose p0 and pl to minimize z. schapire and singer (1998) show that is minimized when we let pb= ? in ) (1) 39 mf tag 7.66 markov 1-gram 6.74 markov 3-gram 3.7 markov 3-gram 3.64 decision tree 3.5 transformation 3.39 maxent 3.37 maxent 3.11 ~.07 multi-tagger voting 2.84 :t=.03 table 1: tb-wsj testing error previously reported in the literature.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
b = (brill and wu, 1998); = (magerman, 1995); = <papid> P95-1037 </papid>our data; = (ratna- parkhi, 1996); = (<papid> W96-0213 </papid>weischedel and others, 1993).</citsent>
<aftsection>
<nextsent>forb {0,1} where ws is thesum of d(i) for examples such that yi = and ?(xi) = b. this choice of p# implies that z: 2 cw+blw-bl  (2) be(o,1} this expression can now be minimized over all choices of ?.
</nextsent>
<nextsent>thus, our weak learner works by searching for the predicate ? that minimizes of eq.
</nextsent>
<nextsent>(2), and the resulting weak hypothesis h(x) predicts pc(z) of eq.
</nextsent>
<nextsent>(1) on example x. in practice, very large values of p0 and pl can cause numerical problems and may also lead to overfitting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL388">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> tagging.  </section>
<citcontext>
<prevsection>
<prevsent>the number of rounds may also differ from discriminator to discriminator.
</prevsent>
<prevsent>3.1 corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
to facilitate comparison with previous results, we used the upenn treebank corpus (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>the corpus uses 80 labels, which comprise 45 parts of speech properly so-called, and 35 inde-terminate tags, representing annotator uncertainty.
</nextsent>
<nextsent>we introduce an 81 st label, ##, for paragraph sepa-rators.
</nextsent>
<nextsent>an example of an indeterminate tag is nnio d, which indicates that the annotator could not decide between nn and ,30.
</nextsent>
<nextsent>the  right  thing to do with in-determinate tags would either be to eliminate them or to count the tagger output as correct if it agrees with any of the alternatives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL389">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we discuss the use of boosting for prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the cases of pp-attachment that we address define binary classification problem.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
for example, the sentence congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment ton: accused \[the president of peccadillos\] attachment to v: (4) accused \[the president\] \[of peccadillos\] the upenn treebank-ii parsed wall street jour-nal corpus includes pp-attachment information, and pp-attachment classifiers based on this data have been previously described in ratnaparkhi, reynar, roukos (1994), brill and resnik (1994), <papid> C94-2195 </papid>and collins and brooks (1995).<papid> W95-0103 </papid></citsent>
<aftsection>
<nextsent>we consider how to apply boosting to this classification task.
</nextsent>
<nextsent>we used the same training and test data as collins and brooks (1995).<papid> W95-0103 </papid></nextsent>
<nextsent>the instances of pp-attachment considered are those involving verb immediately followed by simple noun phrase (the direct ob- ject) and prepositional phrase (whose attachment is at issue).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL390">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>in this section, we discuss the use of boosting for prepositional phrase (pp) attachment.
</prevsent>
<prevsent>the cases of pp-attachment that we address define binary classification problem.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
for example, the sentence congress accused the president of peccadillos is classified according to the attachment site of the prepositional phrase: attachment ton: accused \[the president of peccadillos\] attachment to v: (4) accused \[the president\] \[of peccadillos\] the upenn treebank-ii parsed wall street jour-nal corpus includes pp-attachment information, and pp-attachment classifiers based on this data have been previously described in ratnaparkhi, reynar, roukos (1994), brill and resnik (1994), <papid> C94-2195 </papid>and collins and brooks (1995).<papid> W95-0103 </papid></citsent>
<aftsection>
<nextsent>we consider how to apply boosting to this classification task.
</nextsent>
<nextsent>we used the same training and test data as collins and brooks (1995).<papid> W95-0103 </papid></nextsent>
<nextsent>the instances of pp-attachment considered are those involving verb immediately followed by simple noun phrase (the direct ob- ject) and prepositional phrase (whose attachment is at issue).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL394">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>the boosted classifier has the advantage of being much more compact than the large decision list built by collins and brooks using back-off method.
</prevsent>
<prevsent>we also did not take into ac-count the linguistic knowledge used by collins and brooks who, for instance, disallowed tests that ig-nore the preposition.
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
compared to maximum entropy methods (ratna- parkhi et al, 1994), <papid> H94-1048 </papid>although the methods hare similar structure, the boosted classifier achieves an error rate which is significantly lower.</citsent>
<aftsection>
<nextsent>the importance weights that boosting assigns to training examples are very useful for improving data quality.
</nextsent>
<nextsent>mis labelled examples resulting from anno-tator errors tend to be hard examples to classify cor- rectly; hence they tend to have large weight in the 43 prev word tagged word next word (v = was, n1 = decision, ~- of, n2 = people) ( = put, n1 = them, = on, n2 = streets) (v = making, n1 = it, 9 = in, n2 = terms) (v = prompted, n1 = speculation, 19 = in, n2 = market) (v = is, n1 = director, 19 =- at, n2 = bank) prediction +25.41 -23 .08 -22 .89 +25.76 +23.83 table 6: the five weak hypotheses with the highest (absolute) weight after 20,000 rounds.
</nextsent>
<nextsent>be big only at of some   to with the   the - and for most we have - and - - by   but - and were t make have thought will have the first be involved  including as half were in both , said to one to one   the to long-term have called have called with the was his have thought 3o % of have new  in what out the by corpus label nn jj nn jj jj vbn correct label to dt dt cc jjs vbp to big in much the out gold to   only jj cc in dt nnp dt in cc nn cc vb vbp vbp vb vbd vbn test vbp vb rb jj jj vbn nnp pos jj rb dt pdt vb vbp cc (dt) vbn nn prp nn prp nn prp for for big before by more and nn vbd vbd jj prp vbd jj jj (rb) vbn vbn dt prp$ vbn nn table 7: training examples from experiment 4 with greatest weight.
</nextsent>
<nextsent>final distribution dt+i (i).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL395">
<title id=" W99-0606.xml">boosting applied to tagging and pp attachment </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>using output codes to boost multiclass learning problems.
</prevsent>
<prevsent>in machine learning: proceedings of the fourteenth international confer-ence.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
ralph weischedel et al 1993.<papid> J93-2006 </papid></citsent>
<aftsection>
<nextsent>coping with ambigu-ity and unknown words through probabilistic models.
</nextsent>
<nextsent>computational linguistics, 19(2):359-382.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL396">
<title id=" W99-0204.xml">automatic slide presentation from semantically annotated documents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each slide is typically an itemized summary of topic in the original document.
</prevsent>
<prevsent>generating such slides and coordinating them to meet he audience needs involves lot more drastic reformation of the original document than mere extraction of sentences in traditional sum-marization, so that accurate semantic structure of the document is necessary.
</prevsent>
</prevsection>
<citsent citstr=" P98-2151 ">
we hence assume that the input documents come with gda (global document annota- tion) tags \[hasida, 1997; nagao and hasida, 1998\] <papid> P98-2151 </papid>em- bedded.</citsent>
<aftsection>
<nextsent>the gda tagset is an xml (extensible markup language) tagset which allows machines to automati-cally infer the semantic structures (including pragmatic structures) underlying the raw documents.
</nextsent>
<nextsent>under the current state of the art, gda-tagging can be only semiautomatic and calls for manual correction.
</nextsent>
<nextsent>the cost involved here pays, because an annotated docu-ment is generic form of information content from which to compose diverse types of presentations, potentially in-volving summarization, narration, visualization, transla-tion, information retrieval, information extraction, and so forth.
</nextsent>
<nextsent>the slide presentation system reported below addresses core technology in this broad setting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL398">
<title id=" W99-0204.xml">automatic slide presentation from semantically annotated documents </title>
<section> the  gda tagset.  </section>
<citcontext>
<prevsection>
<prevsent>the gda tagset 1 is based on xml, and designed as compat-ible as possible with html, and tei 2, etc., incorporat- lhttp ://~w. etl.
</prevsent>
<prevsent>go.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
jp/et i/nl/gda/t agset, html 2http://www.uic.edu:80/orgs/tei/ 25 ing insights from eagles s, penn treebank \[marcus et al., 1993\], <papid> J93-2004 </papid>and so forth.</citsent>
<aftsection>
<nextsent>described below is minimal outline of the gda tagset necessary for the rest of the discussion.
</nextsent>
<nextsent>parse- tree bracketing, semantic relation, and coreference are essential for slide presentation, as with many other ap-plications uch as translation.
</nextsent>
<nextsent>further details, concern-ing coordination, scoping, illocutionary act, and so on, are omitted.
</nextsent>
<nextsent>2.1 parse-tree bracketing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL401">
<title id=" W99-0905.xml">resolving translation ambiguity using non parallel bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>choosing the correct translation of content word in context, referred to as  translation disambigua-tion (of content word) , is key task in machine translation.
</prevsent>
<prevsent>it is also crucial in cross-language xt processing including cross-language information re-trieval and abstraction.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
due to the recent availability of large text corpora, various tatistical approaches have been tried includ-ing using 1) parallel corpora (brown et al, 1990), (<papid> J90-2002 </papid>brown et al, 1991), (<papid> P91-1034 </papid>brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (yamabana et al, 1998) and 3) un-tagged mono-language cor-pora in the target language (dagan and itai, 1994), (<papid> J94-4003 </papid>tanaka and iwasaki, 1996), (<papid> C96-2098 </papid>kikui, 1998).<papid> P98-1110 </papid></citsent>
<aftsection>
<nextsent>a problem with the first two approaches that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
</nextsent>
<nextsent>although the third approach eases the problem of preparing corpora, it suffers from lack of useful information in the source language.
</nextsent>
<nextsent>for example, suppose the proper name,  dodgers , provides good context identify the usage of  hit  in the training corpus in english.
</nextsent>
<nextsent>if the translation of  dodgers  rarely occurs in the target language corpora, it does not contribute to target word selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL402">
<title id=" W99-0905.xml">resolving translation ambiguity using non parallel bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>choosing the correct translation of content word in context, referred to as  translation disambigua-tion (of content word) , is key task in machine translation.
</prevsent>
<prevsent>it is also crucial in cross-language xt processing including cross-language information re-trieval and abstraction.
</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
due to the recent availability of large text corpora, various tatistical approaches have been tried includ-ing using 1) parallel corpora (brown et al, 1990), (<papid> J90-2002 </papid>brown et al, 1991), (<papid> P91-1034 </papid>brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (yamabana et al, 1998) and 3) un-tagged mono-language cor-pora in the target language (dagan and itai, 1994), (<papid> J94-4003 </papid>tanaka and iwasaki, 1996), (<papid> C96-2098 </papid>kikui, 1998).<papid> P98-1110 </papid></citsent>
<aftsection>
<nextsent>a problem with the first two approaches that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
</nextsent>
<nextsent>although the third approach eases the problem of preparing corpora, it suffers from lack of useful information in the source language.
</nextsent>
<nextsent>for example, suppose the proper name,  dodgers , provides good context identify the usage of  hit  in the training corpus in english.
</nextsent>
<nextsent>if the translation of  dodgers  rarely occurs in the target language corpora, it does not contribute to target word selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL403">
<title id=" W99-0905.xml">resolving translation ambiguity using non parallel bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>choosing the correct translation of content word in context, referred to as  translation disambigua-tion (of content word) , is key task in machine translation.
</prevsent>
<prevsent>it is also crucial in cross-language xt processing including cross-language information re-trieval and abstraction.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
due to the recent availability of large text corpora, various tatistical approaches have been tried includ-ing using 1) parallel corpora (brown et al, 1990), (<papid> J90-2002 </papid>brown et al, 1991), (<papid> P91-1034 </papid>brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (yamabana et al, 1998) and 3) un-tagged mono-language cor-pora in the target language (dagan and itai, 1994), (<papid> J94-4003 </papid>tanaka and iwasaki, 1996), (<papid> C96-2098 </papid>kikui, 1998).<papid> P98-1110 </papid></citsent>
<aftsection>
<nextsent>a problem with the first two approaches that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
</nextsent>
<nextsent>although the third approach eases the problem of preparing corpora, it suffers from lack of useful information in the source language.
</nextsent>
<nextsent>for example, suppose the proper name,  dodgers , provides good context identify the usage of  hit  in the training corpus in english.
</nextsent>
<nextsent>if the translation of  dodgers  rarely occurs in the target language corpora, it does not contribute to target word selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL404">
<title id=" W99-0905.xml">resolving translation ambiguity using non parallel bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>choosing the correct translation of content word in context, referred to as  translation disambigua-tion (of content word) , is key task in machine translation.
</prevsent>
<prevsent>it is also crucial in cross-language xt processing including cross-language information re-trieval and abstraction.
</prevsent>
</prevsection>
<citsent citstr=" C96-2098 ">
due to the recent availability of large text corpora, various tatistical approaches have been tried includ-ing using 1) parallel corpora (brown et al, 1990), (<papid> J90-2002 </papid>brown et al, 1991), (<papid> P91-1034 </papid>brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (yamabana et al, 1998) and 3) un-tagged mono-language cor-pora in the target language (dagan and itai, 1994), (<papid> J94-4003 </papid>tanaka and iwasaki, 1996), (<papid> C96-2098 </papid>kikui, 1998).<papid> P98-1110 </papid></citsent>
<aftsection>
<nextsent>a problem with the first two approaches that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
</nextsent>
<nextsent>although the third approach eases the problem of preparing corpora, it suffers from lack of useful information in the source language.
</nextsent>
<nextsent>for example, suppose the proper name,  dodgers , provides good context identify the usage of  hit  in the training corpus in english.
</nextsent>
<nextsent>if the translation of  dodgers  rarely occurs in the target language corpora, it does not contribute to target word selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL405">
<title id=" W99-0905.xml">resolving translation ambiguity using non parallel bilingual corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>choosing the correct translation of content word in context, referred to as  translation disambigua-tion (of content word) , is key task in machine translation.
</prevsent>
<prevsent>it is also crucial in cross-language xt processing including cross-language information re-trieval and abstraction.
</prevsent>
</prevsection>
<citsent citstr=" P98-1110 ">
due to the recent availability of large text corpora, various tatistical approaches have been tried includ-ing using 1) parallel corpora (brown et al, 1990), (<papid> J90-2002 </papid>brown et al, 1991), (<papid> P91-1034 </papid>brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (yamabana et al, 1998) and 3) un-tagged mono-language cor-pora in the target language (dagan and itai, 1994), (<papid> J94-4003 </papid>tanaka and iwasaki, 1996), (<papid> C96-2098 </papid>kikui, 1998).<papid> P98-1110 </papid></citsent>
<aftsection>
<nextsent>a problem with the first two approaches that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted.
</nextsent>
<nextsent>although the third approach eases the problem of preparing corpora, it suffers from lack of useful information in the source language.
</nextsent>
<nextsent>for example, suppose the proper name,  dodgers , provides good context identify the usage of  hit  in the training corpus in english.
</nextsent>
<nextsent>if the translation of  dodgers  rarely occurs in the target language corpora, it does not contribute to target word selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="ZL409">
<title id=" W99-0105.xml">positing and resolving bridging anaphora in deverba nps </title>
<section> properties of deverbal.  </section>
<citcontext>
<prevsection>
<prevsent>besides, note that pp of the form di np can discharge also other roles, for instance an oblique such as material (e.g.  in costruzione di mattoni  \[the construction made of bricks\]).
</prevsent>
<prevsent>fur-thermore, it can be used to exp~ roles in relations that are not necessarily part of an argument struc-ture, such as possession.
</prevsent>
</prevsection>
<citsent citstr=" W98-0604 ">
the literature offers discussions on patterns for coding nominalisations and their arguments (meyers et al, 1998), (<papid> W98-0604 </papid>badia and sanri, 1998).</citsent>
<aftsection>
<nextsent>it is noted that whether or not position is filled may affect the interpretation other positions.
</nextsent>
<nextsent>it is also dis- ~here modifiers is cover term for complements, adjuncts and so on.
</nextsent>
<nextsent>anote that morphological greement goes with the nomi-nal.
</nextsent>
<nextsent>semantic agreement (with the possessor) is not .allowed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>