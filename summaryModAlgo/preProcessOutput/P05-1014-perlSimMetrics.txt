8	8	
8	10	0.0502025305494265
8	4	0.151894595719466
8	3	0.0414846773353873
8	1	0.233127349633705
8	9	0.0222135198149212
8	2	0.0607918262125129
8	5	0.477392026740274
8	7	0.422587707621246
8	6	0.2208633452228
10	8	0.0502025305494265
10	10	
10	4	0.000773737537970446
10	3	0.0366354782922765
10	1	0.0523899352359537
10	9	0.21804482776238
10	2	0.138865008265734
10	5	0.0215038258349076
10	7	0.376762029852961
10	6	0.000336862447235436
4	8	0.151894595719466
4	10	0.000773737537970446
4	4	
4	3	0.000452001410577402
4	1	0.00035778772177072
4	9	0.000852860694569477
4	2	0.000530676626126044
4	5	0.000435195081914154
4	7	0.000588796236567187
4	6	0.000427835844589505
3	8	0.0414846773353873
3	10	0.0366354782922765
3	4	0.000452001410577402
3	3	
3	1	0.104154191776838
3	9	0.11177872755384
3	2	0.108267718878976
3	5	0.000187844666489981
3	7	0.0416383029692973
3	6	0.000182909879353834
1	8	0.233127349633705
1	10	0.0523899352359537
1	4	0.00035778772177072
1	3	0.104154191776838
1	1	
1	9	0.279857884864495
1	2	0.0630245345646421
1	5	0.247853492814073
1	7	0.247780475147927
1	6	0.115153509703405
9	8	0.0222135198149212
9	10	0.21804482776238
9	4	0.000852860694569477
9	3	0.11177872755384
9	1	0.279857884864495
9	9	
9	2	0.0752495678720767
9	5	0.000384946992443871
9	7	0.34771402834685
9	6	0.0434912542038463
2	8	0.0607918262125129
2	10	0.138865008265734
2	4	0.000530676626126044
2	3	0.108267718878976
2	1	0.0630245345646421
2	9	0.0752495678720767
2	2	
2	5	0.000309669794040011
2	7	0.0519637754204695
2	6	0.000304161375054821
5	8	0.477392026740274
5	10	0.0215038258349076
5	4	0.000435195081914154
5	3	0.000187844666489981
5	1	0.247853492814073
5	9	0.000384946992443871
5	2	0.000309669794040011
5	5	
5	7	0.476108817725616
5	6	0.351821383986426
7	8	0.422587707621246
7	10	0.376762029852961
7	4	0.000588796236567187
7	3	0.0416383029692973
7	1	0.247780475147927
7	9	0.34771402834685
7	2	0.0519637754204695
7	5	0.476108817725616
7	7	
7	6	0.189410076711345
6	8	0.2208633452228
6	10	0.000336862447235436
6	4	0.000427835844589505
6	3	0.000182909879353834
6	1	0.115153509703405
6	9	0.0434912542038463
6	2	0.000304161375054821
6	5	0.351821383986426
6	7	0.189410076711345
6	6	
8	the method for noun entailment acquisition by is based on the idea of distributional inclusion, according to which one noun is entailed by the other if the set of occurrence contexts of the former subsumes that of the latter. 
10	a similar idea relying on word co-occurrence was proposed by geffet and dagan but ourmethodis simpler and we expect it to be applicable to a wider range of vocabularies. 
4	there is also a workshop devoted to thistask . 
3	 , who exploited the distributional similarity approach over the web to address the same task as ours, obtained higher precision but substantially lower recall, considering only distributional candidates. 
1	for example, two verbs willbeconsideredsimilariftheyhavelargecommon sets of modifying subjects, objects, adverbs etc. distributional similarity does not capture directly meaning equivalence and entailment but rather a looser notion of meaning similarity . 
9	it was noted recently by geffet and dagan that distributional similarity captures a quite loose notion of semantic similarity, as exemplified by the pair country party . 
2	previous attempts have used, for instance, the similarities between case frames , anchor words , and a web-based method . 
5	consequently, they proposed a definition for the lexical entailment relation, which conforms to the general framework of applied textual entailment . 
7	a similar idea by geffet and dagan was proposed forcapturing lexical entailment. 
6	recent attention to knowledge-rich problems such as question answering and textual entailment has encouraged natural language processing researchers to develop algorithms for automatically harvesting shallow semantic resources. 
