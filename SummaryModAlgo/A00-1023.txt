examples of using nlp and ie in question answering include shallow parsing [kupiec 1993] [srihari & li 2000], deep parsing [li et al 2002] [litkowski 1999] [voorhees 1999], and ie [abney et al 2000] [srihari & li 2000]. 
assuming that it is very likely that the answer is a named entity, (srihari and li, 2000) describes a ne-supported q&a system that functions quite well when the expected answer type is one of the categories covered by the ne recognizer. 
high performance qa systems (srihari and li 2000; harabagiu et al 2001). 
to find the answer to a question several steps must be taken, as reported in (abney et al, 2000) (moldovan et al, 2000) (srihari and li, 2000): a2 first, the question semantics needs to be captured. 
2 question answering based on ie we use a qa system supported by increasingly sophisticated levels of ie [srihari & li 2000] [li et al 2002]. 
qa is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or urls, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by natural language processing (nlp) and information extraction (ie) (chinchor and marsh 1998, hovy, hermjakob and lin 2001, li and srihari 2000). 
if the expected answer types are typical named entities, information extraction engines (bikel et al 1999, srihari and li 2000) are used to extract candidate answers. 
this shallow approach parallels work in question answering (srihari and li 2000; soubbotin and soubbotin 2002; moldovan et al 1999). 
who, where or how much and eventually one of the question concepts, when the stem is ambiguous (for example what), as described in (harabagiu et al, 2000) (radev et al, 2000) (srihari and li, 2000). 
in response, factoid question answering systems have evolved into two types: â€¢ use-knowledge: extract query words from the input question, perform ir against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (harabagiu et al, 2001; hovy et al, 2001; srihari and li, 2000; abney et al, 2000). 
it is worth noticing that in our experiment, the structural support used for answer-point identification only checks the binary links involving the asking point and the candidate answer points, instead of full template matching as proposed in (srihari and li, 2000). 
typically qa is supported by natural language processing (nlp) and ie [chinchor & marsh 1998] [hovy et al 2001] [srihari & li 2000]. 
examples of the use of nlp and ie in question answering include shallow parsing (kupiec, 1993), semantic parsing (litkowski 1999), named entity tagging (abney et al 2000, srihari and li 1999) and high-level ie (srihari and li, 2000). 
