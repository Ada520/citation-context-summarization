<paper>
<cited id="W0">
<title id=" P98-1080.xml">tagging inlective languages prediction of morphological categories for a rich structured tagset </title>
<section> the  mode l.  </section>
<citcontext>
<prevsection>
<prevsent>1 is typical example of the ambiguities en-countered in running text: little pos ambigu-ity, but lot of gender, number and case ambiguity (columns 3 to 5).
</prevsent>
<prevsent>485
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
instead of employing the source-channel paradigm for tagging (more or less explicitly present e.g. in (merialdo, 1992), (church, 1988), (<papid> A88-1019 </papid>hajji, hladk~, 1997)) used in the past (notwithstanding some ex-ceptions, such as maximum entropy and rule-based taggers), we are using here  direct  approach to modeling, for which we have chosen an exponential probabilistic model.</citsent>
<aftsection>
<nextsent>such model (when predicting an event 5 e in context x) has the general form pac,e (yix) = exp(~-~in----1 aifi (y, x)) z(x) (3) where fi (y, x) is the set (of size n) of binary-valued (yes/no) features of the event value being predicted and its context, hi is  weigth  (in the exponential sense) of the feature fi, and the normalization factor z(x) is defined naturally as z(x) = exp( x)) (4) yey i----1 ~,ve use separate model for each ambiguity class ac (which actually appeared in the training data) of each of the 13 morphological tegories 6.
</nextsent>
<nextsent>the final pac (yix) distribution is further smoothed using unigram distributions onsubtags (again, separately for each category).
</nextsent>
<nextsent>pac(y\[x) = apac,e(yix) q- (1 -- a)pac, i(y) (5) such smoothing takes care of any unseen context; for ambiguity classes not seen in the training data, for which there is no model, we use unigram proba-bilities of subtags, one distribution per category.
</nextsent>
<nextsent>in the general case, features can operate on any imaginable context (such as the speed of the wind over mt. washington, the last word of yesterday tv news, or the absence of noun in the next 1000 words, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W1">
<title id=" P98-1080.xml">tagging inlective languages prediction of morphological categories for a rich structured tagset </title>
<section> t ra in ing   </section>
<citcontext>
<prevsection>
<prevsent>tagging can be seen as  final application  problem for which we assume to have enough data at hand to train and use just one model, abandoning the source-channel paradigm.
</prevsent>
<prevsent>we have therefore used the error rate directly as the objective function which we try to minimize when selecting the model features.
</prevsent>
</prevsection>
<citsent citstr=" P93-1035 ">
this idea is not new, but as far as we know it has been implemented in rule-based taggers and parsers, such as (brill, 1993<papid> P93-1035 </papid>a), (brill, 1993<papid> P93-1035 </papid>b), (brill, 1993<papid> P93-1035 </papid>c) and (ribarov, 1996), but not in models based on proba-bility distributions.</citsent>
<aftsection>
<nextsent>let define the set of contexts of set of features: x(f) = {z: 3~ bf~,-~ 6 f}, (s) where is some set of features of interest.
</nextsent>
<nextsent>the features can therefore be grouped together based on the context hey operate on.
</nextsent>
<nextsent>in the cur-rent implementation, we actually add features in  batches .
</nextsent>
<nextsent>a  batch  of features defined as set of features which share the same context (see the definition below).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W19">
<title id=" P97-1067.xml">choosing the word most typical in context using a lexical cooccurrence network </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) however, such move also would run the risk of cutting deeply into u.s. economic growth, which is why some economists think it would be big {error mistake \[ oversight}.
</prevsent>
<prevsent>(2) the {error mistake oversight} was magnified when the army failed to charge the standard percentage rate for packing and handling.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
evidence-based models represent context as set of fea-tures, say words, that are observed to co-occur with, and thereby predict, word (yarowsky, 1992; <papid> C92-2070 </papid>golding and schabes, 1996; <papid> P96-1010 </papid>karow and edelman, 1996; ng and lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>but, if we use just the context surrounding word, we might not be able to build up representation satisfac-tory to uncover the subtle differences between synonyms, because of the massive volume of text that would be re-quired.
</nextsent>
<nextsent>now, observe that even though word might not co- occur significantly with another given word, it might nev-ertheless predict he use of that word if the two words are mutually related to third word.
</nextsent>
<nextsent>that is, we can treat lexical co-occurrence as though it were moderately tran-sitive.
</nextsent>
<nextsent>for example, in (3), learn provides evidence for task because it co-occurs (in other contexts) with difficult, which in turn co-occurs with task (in other contexts), even though learn is not seen to co-occur significantly with task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W20">
<title id=" P97-1067.xml">choosing the word most typical in context using a lexical cooccurrence network </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) however, such move also would run the risk of cutting deeply into u.s. economic growth, which is why some economists think it would be big {error mistake \[ oversight}.
</prevsent>
<prevsent>(2) the {error mistake oversight} was magnified when the army failed to charge the standard percentage rate for packing and handling.
</prevsent>
</prevsection>
<citsent citstr=" P96-1010 ">
evidence-based models represent context as set of fea-tures, say words, that are observed to co-occur with, and thereby predict, word (yarowsky, 1992; <papid> C92-2070 </papid>golding and schabes, 1996; <papid> P96-1010 </papid>karow and edelman, 1996; ng and lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>but, if we use just the context surrounding word, we might not be able to build up representation satisfac-tory to uncover the subtle differences between synonyms, because of the massive volume of text that would be re-quired.
</nextsent>
<nextsent>now, observe that even though word might not co- occur significantly with another given word, it might nev-ertheless predict he use of that word if the two words are mutually related to third word.
</nextsent>
<nextsent>that is, we can treat lexical co-occurrence as though it were moderately tran-sitive.
</nextsent>
<nextsent>for example, in (3), learn provides evidence for task because it co-occurs (in other contexts) with difficult, which in turn co-occurs with task (in other contexts), even though learn is not seen to co-occur significantly with task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W21">
<title id=" P97-1067.xml">choosing the word most typical in context using a lexical cooccurrence network </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(1) however, such move also would run the risk of cutting deeply into u.s. economic growth, which is why some economists think it would be big {error mistake \[ oversight}.
</prevsent>
<prevsent>(2) the {error mistake oversight} was magnified when the army failed to charge the standard percentage rate for packing and handling.
</prevsent>
</prevsection>
<citsent citstr=" P96-1006 ">
evidence-based models represent context as set of fea-tures, say words, that are observed to co-occur with, and thereby predict, word (yarowsky, 1992; <papid> C92-2070 </papid>golding and schabes, 1996; <papid> P96-1010 </papid>karow and edelman, 1996; ng and lee, 1996).<papid> P96-1006 </papid></citsent>
<aftsection>
<nextsent>but, if we use just the context surrounding word, we might not be able to build up representation satisfac-tory to uncover the subtle differences between synonyms, because of the massive volume of text that would be re-quired.
</nextsent>
<nextsent>now, observe that even though word might not co- occur significantly with another given word, it might nev-ertheless predict he use of that word if the two words are mutually related to third word.
</nextsent>
<nextsent>that is, we can treat lexical co-occurrence as though it were moderately tran-sitive.
</nextsent>
<nextsent>for example, in (3), learn provides evidence for task because it co-occurs (in other contexts) with difficult, which in turn co-occurs with task (in other contexts), even though learn is not seen to co-occur significantly with task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W22">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> morphological disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>ambi~;uity rate ta~s/token 35.11% 1.48 second 40.68% 62.24% third fourth 64.42% 1.57 2.20 3.48 table 2- ambiguity of each level.
</prevsent>
<prevsent>the morphological mbiguity will differ de-pending on the level of tagging used in each case, as shown in table 2.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
there are two kinds of methods for morpho-logical disambiguation: onone hand, statistical methods need little effort and obtain very good results (church, 1988; <papid> A88-1019 </papid>cutting etal., 1992), <papid> A92-1018 </papid>at least when applied to english, but when we try to apply them to basque we encounter addi-tional problems; on the other hand, some rule-based systems (brill, 1992; <papid> A92-1021 </papid>voutilainen et al, 1992) are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.</citsent>
<aftsection>
<nextsent>so, we 381 have selected one of each group: constraint grammar formalism (karlsson et al, 1995) and the hmm based tatoo tagger (armstrong et al, 1995), which has been de-signed to be applied it to the output of mor-phological analyser and the tagset can be switched easily without changing the input text.
</nextsent>
<nextsent>second \[\] third 70 m* m+cg m*+cg figure 1-initial ambiguity3.
</nextsent>
<nextsent>we have used the second and third levels tagsets for the experiments and small corpus -28,300 words- divided in training corpus of 27,000 words and text of 1,300 words for testing.
</nextsent>
<nextsent>second \[\] third m* m+cg m*+cg figure 2- number of tags per token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W23">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> morphological disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>ambi~;uity rate ta~s/token 35.11% 1.48 second 40.68% 62.24% third fourth 64.42% 1.57 2.20 3.48 table 2- ambiguity of each level.
</prevsent>
<prevsent>the morphological mbiguity will differ de-pending on the level of tagging used in each case, as shown in table 2.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
there are two kinds of methods for morpho-logical disambiguation: onone hand, statistical methods need little effort and obtain very good results (church, 1988; <papid> A88-1019 </papid>cutting etal., 1992), <papid> A92-1018 </papid>at least when applied to english, but when we try to apply them to basque we encounter addi-tional problems; on the other hand, some rule-based systems (brill, 1992; <papid> A92-1021 </papid>voutilainen et al, 1992) are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.</citsent>
<aftsection>
<nextsent>so, we 381 have selected one of each group: constraint grammar formalism (karlsson et al, 1995) and the hmm based tatoo tagger (armstrong et al, 1995), which has been de-signed to be applied it to the output of mor-phological analyser and the tagset can be switched easily without changing the input text.
</nextsent>
<nextsent>second \[\] third 70 m* m+cg m*+cg figure 1-initial ambiguity3.
</nextsent>
<nextsent>we have used the second and third levels tagsets for the experiments and small corpus -28,300 words- divided in training corpus of 27,000 words and text of 1,300 words for testing.
</nextsent>
<nextsent>second \[\] third m* m+cg m*+cg figure 2- number of tags per token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W24">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> morphological disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>ambi~;uity rate ta~s/token 35.11% 1.48 second 40.68% 62.24% third fourth 64.42% 1.57 2.20 3.48 table 2- ambiguity of each level.
</prevsent>
<prevsent>the morphological mbiguity will differ de-pending on the level of tagging used in each case, as shown in table 2.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
there are two kinds of methods for morpho-logical disambiguation: onone hand, statistical methods need little effort and obtain very good results (church, 1988; <papid> A88-1019 </papid>cutting etal., 1992), <papid> A92-1018 </papid>at least when applied to english, but when we try to apply them to basque we encounter addi-tional problems; on the other hand, some rule-based systems (brill, 1992; <papid> A92-1021 </papid>voutilainen et al, 1992) are at least as good as statistical systems and are better adapted to free-order languages and agglutinative languages.</citsent>
<aftsection>
<nextsent>so, we 381 have selected one of each group: constraint grammar formalism (karlsson et al, 1995) and the hmm based tatoo tagger (armstrong et al, 1995), which has been de-signed to be applied it to the output of mor-phological analyser and the tagset can be switched easily without changing the input text.
</nextsent>
<nextsent>second \[\] third 70 m* m+cg m*+cg figure 1-initial ambiguity3.
</nextsent>
<nextsent>we have used the second and third levels tagsets for the experiments and small corpus -28,300 words- divided in training corpus of 27,000 words and text of 1,300 words for testing.
</nextsent>
<nextsent>second \[\] third m* m+cg m*+cg figure 2- number of tags per token.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W25">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> morphological disambiguation.  </section>
<citcontext>
<prevsection>
<prevsent>3(s+b*).
</prevsent>
<prevsent>this is not great improvement when trying to decrease an error rate greater than 10%, but the enrichment of the lexicon may be good way to improve the system.
</prevsent>
</prevsection>
<citsent citstr=" A94-1024 ">
the logical conclusions of these experiments are: ? the statistical approach might not be good approach for agglutinative and free-order languages -as pointed out by oflazer and kuruoz (1994).<papid> A94-1024 </papid></citsent>
<aftsection>
<nextsent>writing good disambiguation rules may real-ly improve the accuracy of the disambigua-tion task.
</nextsent>
<nextsent>as we mentioned above, it is difficult define accurate rules using stochastic models, so we use the constraint grammar for basque 6 (aduriz et al, 1997) for this purpose.
</nextsent>
<nextsent>the morphological disambiguator uses around 800 constraint rules that discard illegitimate analyses on the basis of local or global context methods to compare the results, the latter performed better using larger corpus.
</nextsent>
<nextsent>these biases were written taking into account he errors made in the first experiment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W26">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> combining methods.  </section>
<citcontext>
<prevsection>
<prevsent>2- and the error rate in terms of theta ~sets approximately 1%.
</prevsent>
<prevsent>r.) figure 3- accuracy of the experiments 8.
</prevsent>
</prevsection>
<citsent citstr=" C94-1103 ">
there have been some approaches tothe com-bination of statistical and linguistic methods applied to pos disambiguation (leech et al, 1994; <papid> C94-1103 </papid>tapanainen and voutilainen, 1994; <papid> A94-1008 </papid>oflazer and tiar, 1997) to improve the accuracy of the systems.</citsent>
<aftsection>
<nextsent>oflazer and  for (1997) <papid> P97-1029 </papid>use simple statistical in-formation and constraint rules.</nextsent>
<nextsent>they include constraint application paradigm to make the disambiguation dependent of the rule se- quence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W27">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> combining methods.  </section>
<citcontext>
<prevsection>
<prevsent>2- and the error rate in terms of theta ~sets approximately 1%.
</prevsent>
<prevsent>r.) figure 3- accuracy of the experiments 8.
</prevsent>
</prevsection>
<citsent citstr=" A94-1008 ">
there have been some approaches tothe com-bination of statistical and linguistic methods applied to pos disambiguation (leech et al, 1994; <papid> C94-1103 </papid>tapanainen and voutilainen, 1994; <papid> A94-1008 </papid>oflazer and tiar, 1997) to improve the accuracy of the systems.</citsent>
<aftsection>
<nextsent>oflazer and  for (1997) <papid> P97-1029 </papid>use simple statistical in-formation and constraint rules.</nextsent>
<nextsent>they include constraint application paradigm to make the disambiguation dependent of the rule se- quence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W28">
<title id=" P98-1063.xml">combining stochastic and rule based methods for disambiguation in agglutinative languages </title>
<section> combining methods.  </section>
<citcontext>
<prevsection>
<prevsent>r.) figure 3- accuracy of the experiments 8.
</prevsent>
<prevsent>there have been some approaches tothe com-bination of statistical and linguistic methods applied to pos disambiguation (leech et al, 1994; <papid> C94-1103 </papid>tapanainen and voutilainen, 1994; <papid> A94-1008 </papid>oflazer and tiar, 1997) to improve the accuracy of the systems.</prevsent>
</prevsection>
<citsent citstr=" P97-1029 ">
oflazer and  for (1997) <papid> P97-1029 </papid>use simple statistical in-formation and constraint rules.</citsent>
<aftsection>
<nextsent>they include constraint application paradigm to make the disambiguation dependent of the rule se-quence.
</nextsent>
<nextsent>the approach of tapanainen and voutilainen (1994) <papid> A94-1008 </papid>disambiguates the text using xt and engcg independently; then the ambiguities remaining in engcg are solved using there- suits of xt.</nextsent>
<nextsent>we propose similar combination, applying both disambiguation methods one after the other, but training the stochastic tagger on the output of the cg disambiguator.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W34">
<title id=" P96-1012.xml">another facet of lig parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (vijay-shanker and weir, 1994) tags are transformed into equivalent ligs.
</prevsent>
<prevsent>though context-sensitive linguistic phenomena seem to be more naturally expressed in tag formalism, from computational point of view, many authors think that ligs play central role and therefore theun- derstanding of ligs and lig parsing is of impor-tance.
</prevsent>
</prevsection>
<citsent citstr=" J94-1004 ">
for example, quoted from (schabes and shieber, 1994)  <papid> J94-1004 </papid>the lig version of tag can be used for recognition and parsing.</citsent>
<aftsection>
<nextsent>because the lig for-malism is based on augmented rewriting, the pars-ing algorithms can be much simpler to understand 1see (boullier, 1996) for an extended version.
</nextsent>
<nextsent>87 and easier to modify, and no loss of generality is in-curred .
</nextsent>
<nextsent>in (vijay-shanker and weir, 1993) ligs are used to express the derivations of sentence in tags.
</nextsent>
<nextsent>in (vijay-shanker, weir and rainbow, 1995) the approach used for parsing new formalism, the d-tree grammars (dtg), is to translate dtg into linear prioritized multi set grammar which is similar to lig but uses multi sets in place of stacks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W35">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the fully automatic on struction of translation lnodels offers benefits in terms of development effort and potentially in robustness over meth-ods requiring hand-coding of linguistic informa-tion.
</prevsent>
<prevsent>however, there are disadvantages to the automatic approaches proposed so far.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the var-ious methods described by brown et. al (1990),  <papid> J90-2002 </papid>al (1993) do not take into account he natural struc-turing of strings into phrases.</citsent>
<aftsection>
<nextsent>example-based translation, exemplified by the work of sumita and iida (1995), requires very large amounts of training material.
</nextsent>
<nextsent>the number of states in simple finite state model such as those used by vilar et al (1996) becomes extremely large when faced with languages with large word order differences.
</nextsent>
<nextsent>the work reported in wu (1997), <papid> J97-3002 </papid>which uses an inside-outside type of training algorithm to learn statistical context- free transduction, has similar motivation to the current work, but the models we describe here, being fully lexical, are more suitable for direct statistical modelling.</nextsent>
<nextsent>in this paper, we show that both the net-work topology and parameters of head trans-ducer translation model (alshawi, 1996<papid> P96-1023 </papid>b) can be learned fully automatically from bilingual corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W36">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>example-based translation, exemplified by the work of sumita and iida (1995), requires very large amounts of training material.
</prevsent>
<prevsent>the number of states in simple finite state model such as those used by vilar et al (1996) becomes extremely large when faced with languages with large word order differences.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
the work reported in wu (1997), <papid> J97-3002 </papid>which uses an inside-outside type of training algorithm to learn statistical context- free transduction, has similar motivation to the current work, but the models we describe here, being fully lexical, are more suitable for direct statistical modelling.</citsent>
<aftsection>
<nextsent>in this paper, we show that both the net-work topology and parameters of head trans-ducer translation model (alshawi, 1996<papid> P96-1023 </papid>b) can be learned fully automatically from bilingual corpus.</nextsent>
<nextsent>it has already been shown (alshawi et al., 1997) <papid> P97-1046 </papid>that head transducer model with hand-coded structure can be trained to give bet-ter accuracy than comparable transfer-based system, with smaller model size, computational requirements, and development effort.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W37">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the number of states in simple finite state model such as those used by vilar et al (1996) becomes extremely large when faced with languages with large word order differences.
</prevsent>
<prevsent>the work reported in wu (1997), <papid> J97-3002 </papid>which uses an inside-outside type of training algorithm to learn statistical context- free transduction, has similar motivation to the current work, but the models we describe here, being fully lexical, are more suitable for direct statistical modelling.</prevsent>
</prevsection>
<citsent citstr=" P96-1023 ">
in this paper, we show that both the net-work topology and parameters of head trans-ducer translation model (alshawi, 1996<papid> P96-1023 </papid>b) can be learned fully automatically from bilingual corpus.</citsent>
<aftsection>
<nextsent>it has already been shown (alshawi et al., 1997) <papid> P97-1046 </papid>that head transducer model with hand-coded structure can be trained to give bet-ter accuracy than comparable transfer-based system, with smaller model size, computational requirements, and development effort.</nextsent>
<nextsent>we have applied the learning method to cre-ate an english-spanish translation model for limited domain, with word accuracy of over 75% measured by string distance comparison (as used in speech recognition) to three reference translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W38">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the work reported in wu (1997), <papid> J97-3002 </papid>which uses an inside-outside type of training algorithm to learn statistical context- free transduction, has similar motivation to the current work, but the models we describe here, being fully lexical, are more suitable for direct statistical modelling.</prevsent>
<prevsent>in this paper, we show that both the net-work topology and parameters of head trans-ducer translation model (alshawi, 1996<papid> P96-1023 </papid>b) can be learned fully automatically from bilingual corpus.</prevsent>
</prevsection>
<citsent citstr=" P97-1046 ">
it has already been shown (alshawi et al., 1997) <papid> P97-1046 </papid>that head transducer model with hand-coded structure can be trained to give bet-ter accuracy than comparable transfer-based system, with smaller model size, computational requirements, and development effort.</citsent>
<aftsection>
<nextsent>we have applied the learning method to cre-ate an english-spanish translation model for limited domain, with word accuracy of over 75% measured by string distance comparison (as used in speech recognition) to three reference translations.
</nextsent>
<nextsent>the resulting translation model has been used as component of an english- spanish speech translation system.
</nextsent>
<nextsent>we first present the steps of the transduc-tion training method in section 2.
</nextsent>
<nextsent>in section 3 we describe how we obtain an alignment func-tion from source word sub sequences to target word sub sequences for each transcribed utter-ance and its translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W40">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> a ignment   </section>
<citcontext>
<prevsection>
<prevsent>3.1 l ignment mode the cost of pairing is composed of weighted combination of cost functions.
</prevsent>
<prevsent>we currently use two.
</prevsent>
</prevsection>
<citsent citstr=" H91-1026 ">
the first cost function is the ? correlation measure (cf the use of 2 in gale and church (1991)) <papid> H91-1026 </papid>computed as follows: = (bc - ad) x/(a + b)(c + d)(a + c)(b + d) where = nv -- n~,i~v = nw, c = - nv - nw + nw, d = nw - nw, n is the total number of bitexts, nv the number of bitexts in which appears in the target, nw the number of bitexts in which appears in the source, and nw, the number of bitexts in which appears in the source and appears in the target.</citsent>
<aftsection>
<nextsent>we tried using the log probabilities of tar-get sub sequences given source sub sequences (cf brown et al (1990)) <papid> J90-2002 </papid>as cost function instead of ? but ? resulted in better performance of our translation models.</nextsent>
<nextsent>the second cost function used is distance measure which penalizes pairings in which the source sub sequence and target sub sequence are in very different positions in their respective sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W42">
<title id=" P98-1006.xml">automatic acquisition of hierarchical transduction models for machine translation </title>
<section> head se lec ion.  </section>
<citcontext>
<prevsection>
<prevsent>we have been using the following monolingual metrics which can be applied to either the source or target language to predict the likeli-hood of word being the head word of string.
</prevsent>
<prevsent>distance: the distance between dependent and its head.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
in general, the likelihood of head-dependent relation decreases as distance increases (collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>word frequency: the frequency of occurrence of word in the training corpus.
</nextsent>
<nextsent>ivord  complezity : for languages with pho-netic orthography such as english,  complexity  of word can be measured in terms of number of characters in that word.
</nextsent>
<nextsent>optionality: this metric is intended to iden-tify optional modifiers which are less likely to be heads.
</nextsent>
<nextsent>for each word we find trigrams with the word of interest as the middle word and compare the distribution of these trigrams with the distribution of the bigrams formed from the outer pairs of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W43">
<title id=" P98-1015.xml">semiautomatic recognition of noun modifier relationships </title>
<section> noun modifier bracketing.  </section>
<citcontext>
<prevsection>
<prevsent>before assigning nmrs, the system must bracket the head noun and the pre modifier sequence into modifier-head pairs.
</prevsent>
<prevsent>example (2) shows the bracketing for noun phrase (1).
</prevsent>
</prevsection>
<citsent citstr=" J93-2005 ">
(1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) the bracketing problem for noun-noun-noun compounds has been investigated by liberrnan &amp; sproat (1992), pustejovsky et al (1993), <papid> J93-2005 </papid>resnik (1993) and lauer (1995) <papid> P95-1007 </papid>among others.</citsent>
<aftsection>
<nextsent>since the nmr analyzer must handle pre modifier se-quences of any length with both nouns and adjec-tives, it requires more general techniques.
</nextsent>
<nextsent>our semi-automatic bracketer (barker, 1998) allows for any number of adjective or noun premodifiers.
</nextsent>
<nextsent>after bracketing, each non-atomic element of bracketed pair is considered subphrase of the original phrase.
</nextsent>
<nextsent>the subphrases for the bracketing in (2) appear in (3), (4) and (5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W44">
<title id=" P98-1015.xml">semiautomatic recognition of noun modifier relationships </title>
<section> noun modifier bracketing.  </section>
<citcontext>
<prevsection>
<prevsent>before assigning nmrs, the system must bracket the head noun and the pre modifier sequence into modifier-head pairs.
</prevsent>
<prevsent>example (2) shows the bracketing for noun phrase (1).
</prevsent>
</prevsection>
<citsent citstr=" P95-1007 ">
(1) dynamic high impedance microphone (2) (dynamic ((high impedance) microphone)) the bracketing problem for noun-noun-noun compounds has been investigated by liberrnan &amp; sproat (1992), pustejovsky et al (1993), <papid> J93-2005 </papid>resnik (1993) and lauer (1995) <papid> P95-1007 </papid>among others.</citsent>
<aftsection>
<nextsent>since the nmr analyzer must handle pre modifier se-quences of any length with both nouns and adjec-tives, it requires more general techniques.
</nextsent>
<nextsent>our semi-automatic bracketer (barker, 1998) allows for any number of adjective or noun premodifiers.
</nextsent>
<nextsent>after bracketing, each non-atomic element of bracketed pair is considered subphrase of the original phrase.
</nextsent>
<nextsent>the subphrases for the bracketing in (2) appear in (3), (4) and (5).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W45">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of sentence alignment is critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin- gum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase align-ment, and extraction of parameters for statistical translation models.
</prevsent>
<prevsent>many software products which aid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
various methods have been developed for sentence alignment which we can categorise as either lexical such as (chen, 1993), <papid> P93-1002 </papid>based on large-scale bilin-gual lexicon; statistical such as (brown et al, 1991) (<papid> P91-1022 </papid>church, 1993)(<papid> P93-1001 </papid>gale and church, 1903)(kay and rssheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing bilingual exicon as by-product, or hybrid such as (utsuro et al, 1994) (<papid> C94-2175 </papid>wu, 1994), <papid> P94-1012 </papid>based on some combination of the other two.</citsent>
<aftsection>
<nextsent>neither of the pure approaches entirely satisfactory for the fol-lowing reasons: ? text volume limits the usefulness of statistical approaches.
</nextsent>
<nextsent>we would often like to be able to align small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.
</nextsent>
<nextsent>bilingual dictionary coverage limitations mean that we will often encounter problems establish-ing correspondence in non-general domains.
</nextsent>
<nextsent>dictionary-based approaches are founded on an assumption of lexicul correspondence tween language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W46">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of sentence alignment is critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin- gum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase align-ment, and extraction of parameters for statistical translation models.
</prevsent>
<prevsent>many software products which aid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
various methods have been developed for sentence alignment which we can categorise as either lexical such as (chen, 1993), <papid> P93-1002 </papid>based on large-scale bilin-gual lexicon; statistical such as (brown et al, 1991) (<papid> P91-1022 </papid>church, 1993)(<papid> P93-1001 </papid>gale and church, 1903)(kay and rssheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing bilingual exicon as by-product, or hybrid such as (utsuro et al, 1994) (<papid> C94-2175 </papid>wu, 1994), <papid> P94-1012 </papid>based on some combination of the other two.</citsent>
<aftsection>
<nextsent>neither of the pure approaches entirely satisfactory for the fol-lowing reasons: ? text volume limits the usefulness of statistical approaches.
</nextsent>
<nextsent>we would often like to be able to align small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.
</nextsent>
<nextsent>bilingual dictionary coverage limitations mean that we will often encounter problems establish-ing correspondence in non-general domains.
</nextsent>
<nextsent>dictionary-based approaches are founded on an assumption of lexicul correspondence tween language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W47">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of sentence alignment is critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin- gum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase align-ment, and extraction of parameters for statistical translation models.
</prevsent>
<prevsent>many software products which aid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.
</prevsent>
</prevsection>
<citsent citstr=" P93-1001 ">
various methods have been developed for sentence alignment which we can categorise as either lexical such as (chen, 1993), <papid> P93-1002 </papid>based on large-scale bilin-gual lexicon; statistical such as (brown et al, 1991) (<papid> P91-1022 </papid>church, 1993)(<papid> P93-1001 </papid>gale and church, 1903)(kay and rssheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing bilingual exicon as by-product, or hybrid such as (utsuro et al, 1994) (<papid> C94-2175 </papid>wu, 1994), <papid> P94-1012 </papid>based on some combination of the other two.</citsent>
<aftsection>
<nextsent>neither of the pure approaches entirely satisfactory for the fol-lowing reasons: ? text volume limits the usefulness of statistical approaches.
</nextsent>
<nextsent>we would often like to be able to align small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.
</nextsent>
<nextsent>bilingual dictionary coverage limitations mean that we will often encounter problems establish-ing correspondence in non-general domains.
</nextsent>
<nextsent>dictionary-based approaches are founded on an assumption of lexicul correspondence tween language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W48">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of sentence alignment is critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin- gum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase align-ment, and extraction of parameters for statistical translation models.
</prevsent>
<prevsent>many software products which aid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.
</prevsent>
</prevsection>
<citsent citstr=" C94-2175 ">
various methods have been developed for sentence alignment which we can categorise as either lexical such as (chen, 1993), <papid> P93-1002 </papid>based on large-scale bilin-gual lexicon; statistical such as (brown et al, 1991) (<papid> P91-1022 </papid>church, 1993)(<papid> P93-1001 </papid>gale and church, 1903)(kay and rssheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing bilingual exicon as by-product, or hybrid such as (utsuro et al, 1994) (<papid> C94-2175 </papid>wu, 1994), <papid> P94-1012 </papid>based on some combination of the other two.</citsent>
<aftsection>
<nextsent>neither of the pure approaches entirely satisfactory for the fol-lowing reasons: ? text volume limits the usefulness of statistical approaches.
</nextsent>
<nextsent>we would often like to be able to align small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.
</nextsent>
<nextsent>bilingual dictionary coverage limitations mean that we will often encounter problems establish-ing correspondence in non-general domains.
</nextsent>
<nextsent>dictionary-based approaches are founded on an assumption of lexicul correspondence tween language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W49">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the task of sentence alignment is critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilin- gum vocabulary, extraction of translation templates, word sense disambiguation, word and phrase align-ment, and extraction of parameters for statistical translation models.
</prevsent>
<prevsent>many software products which aid human translators now contain sentence align-ment tools as an aid to speeding up editing and ter-minology searching.
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
various methods have been developed for sentence alignment which we can categorise as either lexical such as (chen, 1993), <papid> P93-1002 </papid>based on large-scale bilin-gual lexicon; statistical such as (brown et al, 1991) (<papid> P91-1022 </papid>church, 1993)(<papid> P93-1001 </papid>gale and church, 1903)(kay and rssheheisen, 1993), based on distributional regular-ities of words or byte-length ratios and possibly in-ducing bilingual exicon as by-product, or hybrid such as (utsuro et al, 1994) (<papid> C94-2175 </papid>wu, 1994), <papid> P94-1012 </papid>based on some combination of the other two.</citsent>
<aftsection>
<nextsent>neither of the pure approaches entirely satisfactory for the fol-lowing reasons: ? text volume limits the usefulness of statistical approaches.
</nextsent>
<nextsent>we would often like to be able to align small amounts of text, or texts from var-ious domains which do not share the same sta-tistical properties.
</nextsent>
<nextsent>bilingual dictionary coverage limitations mean that we will often encounter problems establish-ing correspondence in non-general domains.
</nextsent>
<nextsent>dictionary-based approaches are founded on an assumption of lexicul correspondence tween language pairs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W55">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> a ignment  models   </section>
<citcontext>
<prevsection>
<prevsent>the lexical approach is perhaps the most robust for aligning texts in cognate language pairs, or where there is large amount of reformatting in trans-lation.
</prevsent>
<prevsent>it has also been shown to be particularly successful within the vector space model in multilin-gual information retrieval tasks, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P98-1041 ">
(collier et al, 1998<papid> P98-1041 </papid>a),(collier et al, 1998<papid> P98-1041 </papid>b), for aligning texts in non-cognate languages at the article level.</citsent>
<aftsection>
<nextsent>the major limitation with lexical matching is clearly the assumption of lexical correspondence - 269 el.
</nextsent>
<nextsent>taiwan ruling party sees power struggle in china e2.
</nextsent>
<nextsent>taipei , feb 9 ( reuter ) - taiwan ruling nationalist party said struggle to succeed deng xiaoping as china most powerful man may have already begun.
</nextsent>
<nextsent>e3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W59">
<title id=" P98-1042.xml">an experiment in hybrid dictionary and statistical sentence alignment </title>
<section> a ignment  models   </section>
<citcontext>
<prevsection>
<prevsent>for asian language pairs we cannot rely entirely on dictionary term matching.
</prevsent>
<prevsent>moreover, algorithms which relyon matching cognates cannot be applied easily to english and some asian language.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
we were motivated by statistical alignment models such as (gale and church, 1991) <papid> P91-1023 </papid>to investigate whether byte-length probabilities could improve or replace the lexical matching based method.</citsent>
<aftsection>
<nextsent>the underlying assumption is that characters in an english sentence are responsible for generating some fraction of each character in the corresponding japanese sentence.
</nextsent>
<nextsent>we derived probability density function by mak-ing the assumption that english .and japanese sen-tence length ratios are normally distributed.
</nextsent>
<nextsent>the parameters required for the model are the mean, and variance, ~, which we calculated from training set of 450 hand-aligned sentences.
</nextsent>
<nextsent>these are then entered into equation 2 to find the probability of any two sentences (or combinations of sentences for multiple alignments) being in an alignment relation given that they have length ratio of x. the byte length ratios were calculated as the length of the japanese text segment divided by the length of the english text segment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W62">
<title id=" P98-2126.xml">a test environment for natural language understanding systems </title>
<section> the engine test environment.  </section>
<citcontext>
<prevsection>
<prevsent>large and time-consuming batch runs can be carried out on several machines and imported into the database simultaneously.
</prevsent>
<prevsent>tests conducted on other platforms, such as unix~ can be transferred into the ete database and analyzed as well.
</prevsent>
</prevsection>
<citsent citstr=" P98-2161 ">
the key functions of ete are described below: manage test resources: ete provides an graphical interface to manage various resources needed for tests, including corpora, nle versions and parameter settings, and connections to linguistic servers (norton et al (1998)).<papid> P98-2161 </papid></citsent>
<aftsection>
<nextsent>the interface also enforces the constraints on each test.
</nextsent>
<nextsent>for example, two tests with different corpora cannot be compared.
</nextsent>
<nextsent>compare various types of analysis data.
</nextsent>
<nextsent>ete employs different algorithms to compute the difference between different ypes of data and display the disparate regions graphically the comparison routines are implemented in prolog except for trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W63">
<title id=" P98-1115.xml">compacting the penn treebank grammar </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" H94-1020 ">
the penn treebank (ptb) (marcus et al, 1994) <papid> H94-1020 </papid>has been used for rather simple approach to deriving large grammars automatically: one where the grammar ules are simply  read off  the parse trees in the corpus, with each local subtree providing the left and right hand sides of rule.</citsent>
<aftsection>
<nextsent>charniak (charniak, 1996) reports precision and recall figures of around 80% for parser employing such grammar.
</nextsent>
<nextsent>in this paper we show that the huge size of such tree- bank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved.
</nextsent>
<nextsent>our approach can be generalised in terms of data-oriented parsing (dop) methods (see (bonnema et al, 1997)) <papid> P97-1021 </papid>with the tree depth of 1.</nextsent>
<nextsent>however, the number of trees produced with.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W64">
<title id=" P98-1115.xml">compacting the penn treebank grammar </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>charniak (charniak, 1996) reports precision and recall figures of around 80% for parser employing such grammar.
</prevsent>
<prevsent>in this paper we show that the huge size of such tree- bank grammar (see below) can be reduced in size without appreciable loss in performance, and, in fact, an improvement in recall can be achieved.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
our approach can be generalised in terms of data-oriented parsing (dop) methods (see (bonnema et al, 1997)) <papid> P97-1021 </papid>with the tree depth of 1.</citsent>
<aftsection>
<nextsent>however, the number of trees produced with.
</nextsent>
<nextsent>a general dop method is so large that bonnema (bonnema et al, 1997) <papid> P97-1021 </papid>has to resort to restrict-ing the tree depth, using very domain-specific corpus such as atis or ovis, and parsing very short sentences of average length 4.74 words.</nextsent>
<nextsent>our compaction algorithm can be easily exten-ded for the use within the dop framework but, because of the huge size of the derived grammar (see below), we chose to use the simplest pcfg framework for our experiments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W66">
<title id=" P98-1115.xml">compacting the penn treebank grammar </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>this, we believe, gives further support to the utility of treebank grammars and to the compaction method.
</prevsent>
<prevsent>for example, compaction methods can be applied within the dop frame-work to reduce the number of trees.
</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
also, by partially lexical ising the rule extraction process (i.e., by using some more frequent words as well as the part-of-speech tags), we may be able to achieve parsing performance similar to the best results in the field obtained in (collins, 1996).<papid> P96-1025 </papid></citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W67">
<title id=" P96-1037.xml">mechanisms for mixed initiative human computer collaborative discourse </title>
<section> motivation: initiative in.  </section>
<citcontext>
<prevsection>
<prevsent>this means that the di-alogue in t ia ive should always pass immediately to the participant who is best able to handle the current task.
</prevsent>
<prevsent>an agent is said to have dialogue ini-tiative over mutual goal when that agent controls how that goal will be solved by the collaborators.
</prevsent>
</prevsection>
<citsent citstr=" A92-1002 ">
in the implemented voice dialogue system  the circuit fix-it shop  (smith et al, 1992; <papid> A92-1002 </papid>smith and hipp, 1995), the dialogue fragment given in figure 1 occurs between human user and computer collaborator.</citsent>
<aftsection>
<nextsent>utterances 2 and 4 indicate that the computer is di-recting the search for the missing wire in the faulty circuit.
</nextsent>
<nextsent>however, in utterance 5 the user takes the initiative and suggests different wire than has been proposed by the computer.
</nextsent>
<nextsent>in this paper we will present theory explaining how initiative changes between participants and how computational gents can evaluate who should be in control of solving goal.
</nextsent>
<nextsent>center for digital systems engineering research triangle institute box 12194 research triangle park, nc 27709 cig~rt?, org 1 the led is displaying an alter-nately flashing one and the top corner of seven.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W68">
<title id=" P96-1037.xml">mechanisms for mixed initiative human computer collaborative discourse </title>
<section> u it is there.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 the set ing f in t ia ive levels.
</prevsent>
<prevsent>in our model of dialogue, initiative levels for each goal are defined during the interaction based on 1) explicit and implicit initiative-changing utterances and 2) competency evaluation.
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
expl ic t and imp i i in t ia ive -chang ing ut - te rances several researchers (whittaker and sten-ton, 1988; walker and whittaker, 1990) <papid> P90-1010 </papid>have noted that dialogue control can be exchanged through overt cues in the discourse.</citsent>
<aftsection>
<nextsent>our model concentrates on two specific dialogue cues: questions and answers.
</nextsent>
<nextsent>when an agent ai asks another agent a2 to satisfy goal g, agent a2 gains initiative over goal and all sub goals of until agent a2 passes control of one of those sub goals back to agent a1.
</nextsent>
<nextsent>a similar initiative-setting mechanism is fired if agent a1 an-nounces that it cannot satisfy goal g. when goal has been answered (satisfied) the problem-solving stack is popped.
</nextsent>
<nextsent>the initiative will now belong to whomever the initiative is for the goal on top of the stackj.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W69">
<title id=" P96-1037.xml">mechanisms for mixed initiative human computer collaborative discourse </title>
<section> computer simulations.  </section>
<citcontext>
<prevsection>
<prevsent>the large gap between oracle and continuous is due to the fact that continuous initiative selection is only using limited probabilistic information about the knowledge of each agent.
</prevsent>
<prevsent>the dialogue model outlined in this paper has been implemented, and computer-computer dia-logues have been carried out to evaluate the model and judge the effectiveness of various dialogue initia-tive schemes.
</prevsent>
</prevsection>
<citsent citstr=" C92-3134 ">
in methodology similar to that used by power (1979), carletta (1992) <papid> C92-3134 </papid>and walker (1993), knowledge is distributed by random process be-tween agents, and the resulting interaction between these collaborating agents is observed.</citsent>
<aftsection>
<nextsent>this method-ology allows investigators to test different aspects of dialogue theory.
</nextsent>
<nextsent>details of this experimental strat-egy are given by guinn (1995).
</nextsent>
<nextsent>4.1 the usage of computer-computer.
</nextsent>
<nextsent>dialogues the use of computer-computer simulations to study and build human-computer dialogue systems is controversial.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W70">
<title id=" P98-1102.xml">unification based multimodal parsing </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>in order to realize their full potential, multimodal systems need to support not just input from multiple modes, but also synchronized integration of modes.
</prevsent>
</prevsection>
<citsent citstr=" P97-1036 ">
johnston et al  (1997) <papid> P97-1036 </papid>model this integration using unification opera-tion over typed feature structures.</citsent>
<aftsection>
<nextsent>this is an effective so-lution for broad class of systems, but limits multimodal utterances to combinations of single spoken phrase with single gesture.
</nextsent>
<nextsent>we show how the unification-based ap-proach can be scaled up to provide full multimodal grammar formalism.
</nextsent>
<nextsent>in conjunction with multidimen-sional chart parser, this approach supports integration of multiple lements distributed across the spatial, temporal, and acoustic dimensions of multimodal interaction.
</nextsent>
<nextsent>in-tegration strategies are stated in high level unification- based rule formalism supporting rapid prototyping and it-erative development of multimodal systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W77">
<title id=" P98-1102.xml">unification based multimodal parsing </title>
<section> parsing in mult id imensional  space.  </section>
<citcontext>
<prevsection>
<prevsent>complex con-straints can be formed using the basic logical op-erators , a, and =?,.
</prevsent>
<prevsent>the temporal constraint in figure 7, overlap(j7\], \[10\]) follow(\[7\],\[lo\], 4), states that the time of the speech \[7\] must either overlap with or start within four seconds of the time of the gesture \[10\].
</prevsent>
</prevsection>
<citsent citstr=" W97-1401 ">
this temporal constraint is based on empirical investigation of multimodal in-teraction (oviatt et al 1997).<papid> W97-1401 </papid></citsent>
<aftsection>
<nextsent>spatial constraints are used for combinations of gestural inputs.
</nextsent>
<nextsent>forex- ample, close_to(x, y) requires two gestures to be limited distance apart (see figure 12 below) and contact(x, y) determines whether the regions oc-cupied by two objects are in contact.
</nextsent>
<nextsent>the remaining constraints in figure 7 do not constrain the inputs perse, rather they are used to calculate the time, prob, and modality features for the resulting edge.
</nextsent>
<nextsent>for example, the constraint combine_prob(\[8\], \[11\], \[4\]) is used to combine the probabilities of two inputs and assign joint probability to the resulting edge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W78">
<title id=" P98-1102.xml">unification based multimodal parsing </title>
<section> parsing in mult id imensional  space.  </section>
<citcontext>
<prevsection>
<prevsent>this basic back-tracking con-straint satisfaction strategy is simplistic but adequate for current purposes.
</prevsent>
<prevsent>it could readily be substi-tuted with more sophisticated constraint solving strategy allowing for more interaction among con-straints, default constraints, optimization among series of constraints, and so on.
</prevsent>
</prevsection>
<citsent citstr=" P93-1029 ">
the addition of functional constraints is common in hpsg and other unification grammar formalisms (wittenburg 1993).<papid> P93-1029 </papid></citsent>
<aftsection>
<nextsent>627 4 multimodal subcategorization.
</nextsent>
<nextsent>given that multimodal grammar rules are required to be binary, how can the wide variety of commands in which speech combines with more than one gestural element be accounted for?
</nextsent>
<nextsent>the solution to this prob-lem draws on the lexical ist treatment ofcomplemen- tation in hpsg.
</nextsent>
<nextsent>hpsg utilizes sophisticated the-ory of subcategorization account for the different complementation patterns that verbs and other lexi-cal items require.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W80">
<title id=" P98-1102.xml">unification based multimodal parsing </title>
<section> parsing in mult id imensional  space.  </section>
<citcontext>
<prevsection>
<prevsent>the declarative statement of multimodal integration strategies nables rapid prototyping and iterative de-velopment of multimodal systems.
</prevsent>
<prevsent>the system has undergone form of pro-active evaluation that its design is informed by detailed predictive modeling of how users interact multi- modally, and incorporates the results of empirical studies of multimodal interaction (oviatt 1996, ovi-att et al 1997).<papid> W97-1401 </papid></prevsent>
</prevsection>
<citsent citstr=" P98-2136 ">
it is currently undergoing extensive user testing and evaluation (mcgee et al 1998).<papid> P98-2136 </papid></citsent>
<aftsection>
<nextsent>previous work on grammars and parsing for mul-tidimensional languages has focused on two dimen-sional graphical expressions such as mathematical equations, flow charts, and visual programming lan-guages.
</nextsent>
<nextsent>lakin (1986) lays out many of the ini-tial issues in parsing for two-dimensional draw-ings and utilizes pecialized parsers implemented in lisp to parse specific graphical languages.
</nextsent>
<nextsent>helmet al (1991) employ grammatical framework, con-strained set grammars, in which constituent ruc- ture rules are augmented with spatial constraints.
</nextsent>
<nextsent>visual language parsers are build by translation of these rules into constraint logic programming lan-guage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W83">
<title id=" P95-1022.xml">the intersection of finite state automata and definite clause grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fsa of course generalizes such word lattices.
</prevsent>
<prevsent>as another example, certain techniques to deal with ill-formed input can be characterized as finite state transducers (lang, 1989); the composition of an input string with such finite state transducer results in fsa that can then be input for syntac-tic parsing.
</prevsent>
</prevsection>
<citsent citstr=" C88-2118 ">
such an approach allows for the treat-ment of missing, extraneous, interchanged or mis-used words (teitelbaum, 1973; saito and tomita, 1988; <papid> C88-2118 </papid>nederhof and bertsch, 1994).</citsent>
<aftsection>
<nextsent>such techniques might be of use both in the case of written and spoken language input.
</nextsent>
<nextsent>in the latter case another possible application concerns the treat-ment of phenomena such as repairs (carter, 1994).
</nextsent>
<nextsent>note that we allow the input to be full fsa (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cy-cles.
</nextsent>
<nextsent>whereas an ordinary word-graph always de-fines finite language, fsa of course can easily de-fine an infinite number of sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W84">
<title id=" P95-1022.xml">the intersection of finite state automata and definite clause grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>note that we allow the input to be full fsa (possibly including cycles, etc.) since some of the above-mentioned techniques indeed result in cy-cles.
</prevsent>
<prevsent>whereas an ordinary word-graph always de-fines finite language, fsa of course can easily de-fine an infinite number of sentences.
</prevsent>
</prevsection>
<citsent citstr=" C88-1075 ">
cycles might emerge to treat unknown sequences of words, i.e. sentences with unknown parts of unknown lengths (lang, 1988).<papid> C88-1075 </papid></citsent>
<aftsection>
<nextsent>as suggested by an acl reviewer, one could also try to model haplology phenomena (such as the  in english sentences like  the chef at joe hat , where  joe  is the name of restaurant) using finite state transducer.
</nextsent>
<nextsent>in straightforward approach this would also lead to finite-state automaton with cycles.
</nextsent>
<nextsent>it can be shown that the computation of the in-tersection of fsa and cfg requires only rain- 159 imal generalization existing parsing algorithms.
</nextsent>
<nextsent>we simply replace the usual string positions with the names of the states in the fsa.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W85">
<title id=" P95-1022.xml">the intersection of finite state automata and definite clause grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ai bi 1 + 111 a1 1 b1 111 a3 10 + b3 = 101111110 = 101111110 figure 3: illustration of solution for the pcp problem of figure 2.
</prevsent>
<prevsent>nal actions defined in curly braces).
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
but if we use existing techniques for parsing dcgs, then we are also confronted with an undecid- ability problem: the recognition problem for dcgs is undecidable (pereira and warren, 1983).<papid> P83-1021 </papid></citsent>
<aftsection>
<nextsent>a for-tiori the problem of deciding whether the intersec-tion of fsa and dcg is empty or not is undecid- able.
</nextsent>
<nextsent>this undecidability result is usually circum-vented by considering subsets of dcgs which can be recognized effectively.
</nextsent>
<nextsent>for example, we can restrict he attention to dcgs of which the context- free skeleton does not contain cycles.
</nextsent>
<nextsent>recognition for such  off-line parsable  grammars is decidable (pereira nd warren, 1983).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W86">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous work on mixed-initiative dialogues focused on tracking and allocating single thread of control, the conversational lead, among participants.
</prevsent>
<prevsent>novick (1988) developed computational model that utilizes meta- locutionary acts, such as repeat and give-turn, to cap-ture mixed-initiative behavior in dialogues.
</prevsent>
</prevsection>
<citsent citstr=" P88-1015 ">
whittaker and stenton (1988) <papid> P88-1015 </papid>devised rules for allocating dialogue control based on utterance types, and walker and whit-taker (1990) utilized these rules for an analytical study on discourse segmentation.</citsent>
<aftsection>
<nextsent>kitano and van ess-dykema (1991) developed plan-based ialogue understanding model that tracks the conversational initiative based on the domain and discourse plans behind the utterances.
</nextsent>
<nextsent>smith and hipp (1994) developed dialogue system that varies its responses to user utterances based on four di= alogue modes which model different levels of initiative exhibited by dialogue participants.
</nextsent>
<nextsent>however, the dia-logue mode is determined at the outset and cannot be changed uring the dialogue.
</nextsent>
<nextsent>guinn (1996) <papid> P96-1037 </papid>subsequently developed system that allows change in the level of ini- 262 tiative based on initiative-changing utterances and each agent competency in completing the current subtask.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W87">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>smith and hipp (1994) developed dialogue system that varies its responses to user utterances based on four di= alogue modes which model different levels of initiative exhibited by dialogue participants.
</prevsent>
<prevsent>however, the dia-logue mode is determined at the outset and cannot be changed uring the dialogue.
</prevsent>
</prevsection>
<citsent citstr=" P96-1037 ">
guinn (1996) <papid> P96-1037 </papid>subsequently developed system that allows change in the level of ini- 262 tiative based on initiative-changing utterances and each agent competency in completing the current subtask.</citsent>
<aftsection>
<nextsent>however, we contend that merely maintaining the con-versational lead is insufficient for modeling complex be-havior commonly found in naturally-occurring collabo-rative dialogues (sri transcripts, 1992; gross, allen, and tram, 1993; heeman and allen, 1995).
</nextsent>
<nextsent>for in-stance, consider the alternative responses in utterances (3a)-(3c), given by an advisor to student question: (1) s: want to take nlp to satisfy my seminar course requirement.
</nextsent>
<nextsent>(2) who is teaching nlp?
</nextsent>
<nextsent>(3a) a: dr. smith is teaching nlp.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W90">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ti: system 37 (3.5%) ti: manager 274 (26.3%) 727 (69.8%) di: system di: manager 4 (0.4%) table 1: distribution of task and dialogue initiatives agents should perform.
</prevsent>
<prevsent>the utterances may propose domain actions (litman and allen, 1987) that directly contribute to achieving the agents  goal, such as  let send engine e2 to coming.
</prevsent>
</prevsection>
<citsent citstr=" H91-1064 ">
on the other hand, they may propose problem-solving actions (allen, 1991; <papid> H91-1064 </papid>lambert and carberry, 1991; <papid> P91-1007 </papid>ramshaw, 1991) <papid> P91-1006 </papid>that con-tribute not directly to the agents  domain goal, but to how they would go about achieving this goal, such as  let look at the first \[problem\]first. </citsent>
<aftsection>
<nextsent>an agent is said to have the dialogue initiative if she takes the conversational lead in order to establish mutual beliefs, such as mutual beliefs about piece of domain knowledge or about he validity of proposal, between the agents.
</nextsent>
<nextsent>for instance, in responding to agent xs proposal of sending boxcar to coming via dans ville, agent may take over the dia-logue initiative (but not the task initiative) by saying  we can go by dans ville because we ve got engine going on that track.
</nextsent>
<nextsent>thus, when an agent akes over the task initiative, she also takes over the dialogue initiative, since proposal of actions can be viewed as an attempt to es-tablish the mutual belief that set of actions be adopted.
</nextsent>
<nextsent>on the other hand, an agent may take over the dialogue initiative but not the task initiative, as in (3b) above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W91">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ti: system 37 (3.5%) ti: manager 274 (26.3%) 727 (69.8%) di: system di: manager 4 (0.4%) table 1: distribution of task and dialogue initiatives agents should perform.
</prevsent>
<prevsent>the utterances may propose domain actions (litman and allen, 1987) that directly contribute to achieving the agents  goal, such as  let send engine e2 to coming.
</prevsent>
</prevsection>
<citsent citstr=" P91-1007 ">
on the other hand, they may propose problem-solving actions (allen, 1991; <papid> H91-1064 </papid>lambert and carberry, 1991; <papid> P91-1007 </papid>ramshaw, 1991) <papid> P91-1006 </papid>that con-tribute not directly to the agents  domain goal, but to how they would go about achieving this goal, such as  let look at the first \[problem\]first. </citsent>
<aftsection>
<nextsent>an agent is said to have the dialogue initiative if she takes the conversational lead in order to establish mutual beliefs, such as mutual beliefs about piece of domain knowledge or about he validity of proposal, between the agents.
</nextsent>
<nextsent>for instance, in responding to agent xs proposal of sending boxcar to coming via dans ville, agent may take over the dia-logue initiative (but not the task initiative) by saying  we can go by dans ville because we ve got engine going on that track.
</nextsent>
<nextsent>thus, when an agent akes over the task initiative, she also takes over the dialogue initiative, since proposal of actions can be viewed as an attempt to es-tablish the mutual belief that set of actions be adopted.
</nextsent>
<nextsent>on the other hand, an agent may take over the dialogue initiative but not the task initiative, as in (3b) above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W92">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ti: system 37 (3.5%) ti: manager 274 (26.3%) 727 (69.8%) di: system di: manager 4 (0.4%) table 1: distribution of task and dialogue initiatives agents should perform.
</prevsent>
<prevsent>the utterances may propose domain actions (litman and allen, 1987) that directly contribute to achieving the agents  goal, such as  let send engine e2 to coming.
</prevsent>
</prevsection>
<citsent citstr=" P91-1006 ">
on the other hand, they may propose problem-solving actions (allen, 1991; <papid> H91-1064 </papid>lambert and carberry, 1991; <papid> P91-1007 </papid>ramshaw, 1991) <papid> P91-1006 </papid>that con-tribute not directly to the agents  domain goal, but to how they would go about achieving this goal, such as  let look at the first \[problem\]first. </citsent>
<aftsection>
<nextsent>an agent is said to have the dialogue initiative if she takes the conversational lead in order to establish mutual beliefs, such as mutual beliefs about piece of domain knowledge or about he validity of proposal, between the agents.
</nextsent>
<nextsent>for instance, in responding to agent xs proposal of sending boxcar to coming via dans ville, agent may take over the dia-logue initiative (but not the task initiative) by saying  we can go by dans ville because we ve got engine going on that track.
</nextsent>
<nextsent>thus, when an agent akes over the task initiative, she also takes over the dialogue initiative, since proposal of actions can be viewed as an attempt to es-tablish the mutual belief that set of actions be adopted.
</nextsent>
<nextsent>on the other hand, an agent may take over the dialogue initiative but not the task initiative, as in (3b) above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W93">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it shows that while in the majority of turns, the task and dialogue initiatives are held by the same agent, in approximately 1/4 of the turns, the agents  behavior can be better accounted forby tracking the two types of initiatives eparately.
</prevsent>
<prevsent>to assess the reliability of our annotations, approxi-mately 10% of the dialogues were annotated by two ad-ditional coders.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
we then used the kappa statistic (siegel and castel lan, 1988; carletta, 1996) <papid> J96-2004 </papid>to assess the level of agreement between the three coders with respect the some utterance during the turn directly proposes how the agents should accomplish eir goal, as in utterance (3c).</citsent>
<aftsection>
<nextsent>263 task and dialogue initiative holders.
</nextsent>
<nextsent>in this experiment, is 0,57 for the task initiative holder agreement and is 0.69 for the dialogue initiative holder agreement.
</nextsent>
<nextsent>carletta suggests that content analysis researchers consider  .8 as good reliability, with .67  /~   .8 allowing tentative conclusions to be drawn (carletta, 1996).<papid> J96-2004 </papid></nextsent>
<nextsent>strictly based on this metric, our results indicate that the three coders have reasonable vel of agree-ment with respect the dialogue initiative holders, but do not have reliable agreement with respect to the task initiative holders.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W97">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3.1 cues for tracking initiative.
</prevsent>
<prevsent>in order to utilize the dempster-shafer theory for mod-eling initiative, we must first identify the cues that pro-vide evidence for initiative shifts.
</prevsent>
</prevsection>
<citsent citstr=" P90-1010 ">
whittaker, stenton, and walker (whittaker and stenton, 1988; <papid> P88-1015 </papid>walker and whittaker, 1990) <papid> P90-1010 </papid>have previously identified set of ut-terance intentions that serve as cues to indicate shifts or lack of shifts in initiative, such as prompts and questions.</citsent>
<aftsection>
<nextsent>we analyzed our annotated trains91 corpus and iden-tified additional cues that may have contributed to the shift or lack of shift in task/dialogue initiatives during the interactions.
</nextsent>
<nextsent>this results in eight cue types, which are grouped into three classes, based on the kind of knowl-edge needed to recognize them.
</nextsent>
<nextsent>table 2 shows the three classes, the eight cue types, their sub types if any, whether cue may affect merely the dialogue initiative or both the task and dialogue initiatives, and the agent expected to hold the initiative in the next turn.
</nextsent>
<nextsent>the first cue class, explicit cues, includes explicit re-quests by the speaker to give up or take over the initiative.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W98">
<title id=" P97-1034.xml">tracking initiative in collaborative dialogue interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the first type is perceptible silence at the end of an utterance, which suggests that the speaker has nothing more to say and may intend to give up her initiative.
</prevsent>
<prevsent>the second type includes utterances that do not contribute information that has not been conveyed earlier in the dialogue.
</prevsent>
</prevsection>
<citsent citstr=" C92-1054 ">
it can be further classified into two groups: repetitions, sub-set of the information ally redundant utterances (walker, 1992), <papid> C92-1054 </papid>in which the speaker paraphrases an utterance by the hearer or repeats the utterance verbatim, and prompts, in which the speaker merely acknowledges the bearer previous utterance(s).</citsent>
<aftsection>
<nextsent>repetitions and prompts also suggest that the speaker has nothing more to say and indicate that the hearer should take over the initiative (whittaker and stenton, 1988).<papid> P88-1015 </papid></nextsent>
<nextsent>the third type includes questions which, based on anticipated responses, are divided into domain and evaluation questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W104">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>after the orange juice is at - the oranges are at the oj factory (d93-19.3 utt59) an abridged repair is where the repair consists olely of fragment and/or editing terms.
</prevsent>
<prevsent>we need to -um manage to get the bananas to dans ville more quickly (d93-14.3 utt50) these examples also illustrate how speech repairs can be divided into three intervals: the removed text, the editing terms, and the resumed text (cf.
</prevsent>
</prevsection>
<citsent citstr=" P93-1007 ">
levelt, (1983), nakatani and hirschberg, (1993)).<papid> P93-1007 </papid></citsent>
<aftsection>
<nextsent>the removed text, which might end in word fragment, is the text that the speaker intends to replace.
</nextsent>
<nextsent>the end of the removed text is called the interruption point, which is marked in the above examples as  - .
</nextsent>
<nextsent>this is then followed by editing terms, which can either be filled pauses, such as  urn ,  uh , and  er , or cue phrases, such as  mean ,  guess , and  well .
</nextsent>
<nextsent>the last interval is the resumed text, the text that is intended to replace the removed text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W105">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>such an algorithm could precede aparser, or even operate in lockstep with it.
</prevsent>
<prevsent>an ulterior motive for not using higher level syntactic or semantic knowledge is that the coverage of parsers and se-mantic interpreters not sufficient for unrestricted dialogs.
</prevsent>
</prevsection>
<citsent citstr=" P93-1008 ">
recently, dowding et al (1993) <papid> P93-1008 </papid>reported syntactic and se-mantic overage of 86% for the darpa airline reservation corpus (dowding et al, 1993).<papid> P93-1008 </papid></citsent>
<aftsection>
<nextsent>unrestricted ialogs will present even more difficulties; not only will the speech be less grammatical, but there is also the problem of segmenting the dialog into utterance units (cf.
</nextsent>
<nextsent>wang and hirschberg, 1992)?
</nextsent>
<nextsent>if speech repairs can be detected and corrected before pars-ing and semantic interpretation, this should simplify those modules as well as make them more robust.
</nextsent>
<nextsent>in this paper, we present an algorithm that detects and corrects modification and abridged speech repairs without doing syntactic and semantic processing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W108">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>2 in addition, levelt showed that different editing terms make different predictions about whether repair is fresh start or not.
</prevsent>
<prevsent>for instance,  uh  strongly signals an abridged or modification repair, whereas word like  sorry  signals fresh start.
</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
hindle (1983) <papid> P83-1019 </papid>addressed the problem of correcting self- repairs by adding rules to deterministic parser that would remove the necessary text.</citsent>
<aftsection>
<nextsent>hindle assumed the presence of an edit signal that would mark the interruption point, and was able to achieve arecall rate of 97% in finding the correct repair.
</nextsent>
<nextsent>for modification repairs, hindle used three rules for  expuncting  text.
</nextsent>
<nextsent>the first rule  is essentially non- syntactic rule  that matches repetitions (of any length); the second matches repeated constituents, both complete; and the third, matches repeated constituents, in which the first is not complete, but the second is. however, hindle results are difficult to translate into actual performance.
</nextsent>
<nextsent>first, his parsing strategy depends upon the  successful disambiguation the syntactic ategories.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W110">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>labov, 1966) has been difficult to find, and it is unlikely to be represented as binary feature (cf.
</prevsent>
<prevsent>nakatani and hirschberg, 1993).<papid> P93-1007 </papid></prevsent>
</prevsection>
<citsent citstr=" P92-1008 ">
the sri group (bear et al, 1992) <papid> P92-1008 </papid>employed simple pattern matching techniques for detecting and correcting modifica-tion repairs.</citsent>
<aftsection>
<nextsent>3for detection, they were able to achieve arecall rate of 76%, and precision of 62%, and they were able to find the correct repair 57% of the time, leading to an over- all correction recall of 43% and correction precision of 50%.
</nextsent>
<nextsent>they also tried combining syntactic and semantic knowledge in  parser-first  approach--first try to parse the input and if that fails, invoke repair strategies based on word patterns in the input.
</nextsent>
<nextsent>in test set containing 26 repairs (dowding et al, 1993), <papid> P93-1008 </papid>they obtained adetection recall rate of 42% and precision of 84.6%; for correction, they obtained recall rate of 30% and recall rate of 62%.</nextsent>
<nextsent>nakatani and hirschberg (1993) <papid> P93-1007 </papid>investigated using acous-tic information to detect he interruption point of speech re- pairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W114">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>4 the entire corpus consists of 112 dialogs totaling almost eight hours in length and containing about 62,000 words, 6300 speaker turns, and 40 different speakers.
</prevsent>
<prevsent>these dialogs have been segmented into utterance files (cf.
</prevsent>
</prevsection>
<citsent citstr=" H94-1034 ">
heeman and allen, 1994<papid> H94-1034 </papid>b); words 3they referred to modification repairs as non trivial repairs, and to abridged repairs as trivial repairs; however, these terms are mis- leading.</citsent>
<aftsection>
<nextsent>consider the utterance  send it back to elmira uh to make oj .
</nextsent>
<nextsent>determining that he corrected text should be  send it back to elmira to make oj  rather than  send it back to make oj  is non trivial.
</nextsent>
<nextsent>4gross, allen and traum (1992) discuss the manner in which the first set of dialogues were collected, and provide transcriptions.
</nextsent>
<nextsent>296 have been transcribed and the speech repairs have been an-notated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W119">
<title id=" P94-1041.xml">detecting and correcting speech repairs </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the word replacement of  we  by the second instance of   , does not violate any of the rules, including rule (10), so it is added, resulting in the following labeling.
</prevsent>
<prevsent>i think we need to uh i need r m et r\] i algorithm our algorithm for labeling potential repair patterns encodes the assumption that speech repairs can be processed one at time.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
the algorithm runs in lockstep with part-of-speech tagger (church, 1988), <papid> A88-1019 </papid>which is used for deciding possible word replacements.</citsent>
<aftsection>
<nextsent>words are fed in one at time.
</nextsent>
<nextsent>the detection clues are checked first.
</nextsent>
<nextsent>if one of them succeeds, and there is not repair being processed, then new repair pattern is started.
</nextsent>
<nextsent>otherwise, if the clue is consistent with the current repair pattern, then the pattern is updated; otherwise, the current one is sent off to be judged, and new repair pattern is started.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W126">
<title id=" P96-1009.xml">a robust system for natural spoken dialogue </title>
<section> the system.  </section>
<citcontext>
<prevsection>
<prevsent>while the corrected transcriptions are not perfect, they are typically better approximation of the actual utterance.
</prevsent>
<prevsent>as the first example shows, some recognition errors are simple word-for-word confusions: hyp: go b_x syracuse at buffalo hyp : go via syracuse via buffalo ref: go via syracuse and buffalo in the next example, single word was replaced by more than one smaller word: hyp: let go p_m to try hyp : let go p_m to detroit ref: let go via detroit the post-processor yields fewer errors by effectively refining and tuning the vocabulary used by the speech recognizer.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
to achieve this, we adapted some techniques from statistical machine translation (such as brown et al., 1990) <papid> J90-2002 </papid>in order to model the errors that sphinx-ii makes in our domain.</citsent>
<aftsection>
<nextsent>briefly, the model consists of two parts: channel model, which accounts for errors made by the sr, and the language model, which accounts for the likelihood of sequence of words being uttered in the first place.
</nextsent>
<nextsent>more precisely, given an observed word sequence from the speech recognizer, speechpp finds the most likely original word sequence by finding the sequence that maximizes prob(ols) * prob(s), where ? prob(s) is the probability that the user would utter sequence s, and ? prob(ols) is the probability that the sr produces the sequence when was actually spoken.
</nextsent>
<nextsent>for efficiency, it is necessary to estimate these distributions with relatively simple models by making independence assumptions.
</nextsent>
<nextsent>for prob(s), we train word-bigram  back-offf language model (katz, 87) from hand-transcribed dialogues previously collected with the trains-95 system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W128">
<title id=" P96-1009.xml">a robust system for natural spoken dialogue </title>
<section> the system.  </section>
<citcontext>
<prevsection>
<prevsent>there are, for instance, numerous rules that encode specific conventional speech acts (e.g., that g o is confirm, k y is confirm/acknowledge, let go to chicago is suggest, and so on).
</prevsent>
<prevsent>simply classifying such utterances as sentences would miss the point.
</prevsent>
</prevsection>
<citsent citstr=" P89-1026 ">
thus the parser computes set of plausible speech act interpretation based on the surface form, similar to the model described in hinkelman &amp; allen (1989).<papid> P89-1026 </papid></citsent>
<aftsection>
<nextsent>we use hierarchy of speech acts that encode different levels of vagueness, including tell act that indicates content without an identifiable illocutionary force.
</nextsent>
<nextsent>this allows us to always have an illocutionary force that can be refined as more of the utterance is processed.
</nextsent>
<nextsent>the final interpretation of an utterance is the sequence of speech acts that provides the  minimal covering  of the input - i.e., the shortest sequence that accounts for the input.
</nextsent>
<nextsent>even if an utterance was completely uninterpretable, the parser would still produce output - tell act with no content.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W129">
<title id=" P96-1009.xml">a robust system for natural spoken dialogue </title>
<section> the system.  </section>
<citcontext>
<prevsection>
<prevsent>robust speech act processing.
</prevsent>
<prevsent>the dialogue manager is responsible for interpreting the speech acts in context, formulating responses, and maintaining the system idea of the state of the discourse.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
it maintains adiscourse state that consists of goal stack with similarities to the plan stack of litman &amp; allen (1987) and the attentional state of grosz &amp; sidner (1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>each element of the stack captures 1.
</nextsent>
<nextsent>the domain or discourse goal motivating the segment 2.
</nextsent>
<nextsent>the object focus and history list for the segment 3.
</nextsent>
<nextsent>information on the status of problem solving activity (e.g., has the goal been achieved yet or not).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W130">
<title id=" P96-1009.xml">a robust system for natural spoken dialogue </title>
<section> the system.  </section>
<citcontext>
<prevsection>
<prevsent>all we conclude from this experiment is that our robust processing techniques are sufficiently good that speech is viable interface in such tasks even with high word error rates.
</prevsent>
<prevsent>in fact, it appears to be more efficient in this application than keyboard.
</prevsent>
</prevsection>
<citsent citstr=" H93-1074 ">
in contrast to the results of rudnicky (1993), <papid> H93-1074 </papid>who found users preferred speech even when less efficient, our subjects generally preferred the most efficient modality for them (which in majority of cases was speech).</citsent>
<aftsection>
<nextsent>despite the limitations of the current evaluation, we are encouraged by this first step.
</nextsent>
<nextsent>it seems obvious to us that progress in dialogue systems is intimately tied to finding suitable evaluation measures.
</nextsent>
<nextsent>and task-based evaluation seems one of the most promising candidates.
</nextsent>
<nextsent>it measures the impact of proposed techniques directly rather than indirectly with an abstract accuracy figure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W131">
<title id=" P98-1117.xml">methods and practical issues in evaluating alignment techniques </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>these techniques attempt to map various tex-tual units to their translation and have proven useful for wide range of applicatious and tools.
</prevsent>
<prevsent>a simple example of such tool is probably the trans search bilingual concordancing system (isabelle et al, 1993), which allows user to query large archive of existing translations in order to find ready-made solutions to specific translation problems.
</prevsent>
</prevsection>
<citsent citstr=" A94-1006 ">
such tool has proved ex-tremely useful not only for translators, but also for bilingual lexicographers (langlois, 1996) and terminologists (dagan and church, 1994).<papid> A94-1006 </papid></citsent>
<aftsection>
<nextsent>more sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilin-gual lexical resources (melamed, 1996; klavans and tzoukermann, 1995), the automatic verifi-cation of translations (macklovitch, 1995), the automatic dictation of translations (brousseau et al, 1995) and even interactive machine trans-lation (foster et al, 1997).
</nextsent>
<nextsent>enthusiasm for this relatively new field was sparked early on by the apparent demonstra-tion that very simple techniques could yield al-most perfect results.
</nextsent>
<nextsent>for instance, to produce sentence alignments, brown et al (1991) <papid> P91-1022 </papid>and gale and church (1991) <papid> P91-1023 </papid>both proposed meth-ods that completely ignored the lexical content of the texts and both reported accuracy lev-els exceeding 98%.</nextsent>
<nextsent>unfortunately performance tends to deteriorate significantly when align ers are applied to corpora which are widely differ-ent from the training corpus, and/or where the alignments are not straightforward.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W132">
<title id=" P98-1117.xml">methods and practical issues in evaluating alignment techniques </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilin-gual lexical resources (melamed, 1996; klavans and tzoukermann, 1995), the automatic verifi-cation of translations (macklovitch, 1995), the automatic dictation of translations (brousseau et al, 1995) and even interactive machine trans-lation (foster et al, 1997).
</prevsent>
<prevsent>enthusiasm for this relatively new field was sparked early on by the apparent demonstra-tion that very simple techniques could yield al-most perfect results.
</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
for instance, to produce sentence alignments, brown et al (1991) <papid> P91-1022 </papid>and gale and church (1991) <papid> P91-1023 </papid>both proposed meth-ods that completely ignored the lexical content of the texts and both reported accuracy lev-els exceeding 98%.</citsent>
<aftsection>
<nextsent>unfortunately performance tends to deteriorate significantly when align ers are applied to corpora which are widely differ-ent from the training corpus, and/or where the alignments are not straightforward.
</nextsent>
<nextsent>for instance graphics, tables,  floating  notes and missing segments, which are very common in real texts, all result in dramatic loss of efficiency.
</nextsent>
<nextsent>the truth is that, while text alignment is mostly an easy problem, especially when consid-ered at the sentence level, there are situations where even humans have hard time making the right decision.
</nextsent>
<nextsent>in fact, it could be argued that, ultimately, text alignment isno easier than the more general problem of natural anguage understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W133">
<title id=" P98-1117.xml">methods and practical issues in evaluating alignment techniques </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>more sophisticated applications based on alignment technology have also been the object of recent work, such as the automatic building of bilin-gual lexical resources (melamed, 1996; klavans and tzoukermann, 1995), the automatic verifi-cation of translations (macklovitch, 1995), the automatic dictation of translations (brousseau et al, 1995) and even interactive machine trans-lation (foster et al, 1997).
</prevsent>
<prevsent>enthusiasm for this relatively new field was sparked early on by the apparent demonstra-tion that very simple techniques could yield al-most perfect results.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
for instance, to produce sentence alignments, brown et al (1991) <papid> P91-1022 </papid>and gale and church (1991) <papid> P91-1023 </papid>both proposed meth-ods that completely ignored the lexical content of the texts and both reported accuracy lev-els exceeding 98%.</citsent>
<aftsection>
<nextsent>unfortunately performance tends to deteriorate significantly when align ers are applied to corpora which are widely differ-ent from the training corpus, and/or where the alignments are not straightforward.
</nextsent>
<nextsent>for instance graphics, tables,  floating  notes and missing segments, which are very common in real texts, all result in dramatic loss of efficiency.
</nextsent>
<nextsent>the truth is that, while text alignment is mostly an easy problem, especially when consid-ered at the sentence level, there are situations where even humans have hard time making the right decision.
</nextsent>
<nextsent>in fact, it could be argued that, ultimately, text alignment isno easier than the more general problem of natural anguage understanding.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W136">
<title id=" P98-1117.xml">methods and practical issues in evaluating alignment techniques </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, it could be argued that, ultimately, text alignment isno easier than the more general problem of natural anguage understanding.
</prevsent>
<prevsent>in addition, most research efforts were directed towards the easiest problem, that of sentence-to-sentence alignment (brown et al, 1991; <papid> P91-1022 </papid>gale and church, 1991; <papid> P91-1023 </papid>debili, 1992; kay and l~scheisen, 1993; simard et al, 1992; simard and plamondon, 1996).</prevsent>
</prevsection>
<citsent citstr=" P97-1039 ">
alignment at the word and term level, which is extremely useful for applications uch as lexieal resource extraction, is still largely unexplored research area(melamed, 1997).<papid> P97-1039 </papid></citsent>
<aftsection>
<nextsent>in order to live up to the expectations of the 711 various application fields, alignment technology will therefore have to improve substantially.
</nextsent>
<nextsent>as was the case with several other language processing techniques (such as information retrieval, document understanding or speech recognition), it is likely that systematic evalu-ation will enable such improvements.
</nextsent>
<nextsent>however, before the arcade project started, no for- real evaluation exercise was underway; and worse still, there was no multilingnal aligned reference corpus to serve as  gold standard  (as the brown corpus did, for example, for part of speech tagging), nor any established methodology for the evaluation of alignment systems.
</nextsent>
<nextsent>arcade is an evaluation exercise financed by aupelf-uref, network of (at least partially) french-speaking universities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W138">
<title id=" P98-1117.xml">methods and practical issues in evaluating alignment techniques </title>
<section> systems tested.  </section>
<citcontext>
<prevsection>
<prevsent>theun- derlying principle is the automatic detection of isolated cognates (i.e. for which no other similar word exists in window of given size).
</prevsent>
<prevsent>once the search space is reduced, the system aligns the sentences using the well-known sentence-length model described in (gale and church, 1991).<papid> P91-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
ral /sa lgn the second method proposed by rali is based on dynamic programming scheme which uses score function derived from translation model similar to that of (brown et al, 1990).<papid> J90-2002 </papid></citsent>
<aftsection>
<nextsent>the search space is reduced to beam of fixed width around the diagonal (which would represent the alignment if the two texts were perfectly synchronized).
</nextsent>
<nextsent>loria the strategy adopted in this system differs from that of the other systems ince sen-tence alignment is performed after the prelim-inary alignment of larger units (whenever pos-sible, using mark-up), such as paragraphs and divisions, on the basis of the sgml structure.
</nextsent>
<nextsent>a dynamic programming scheme is applied to all alignment levels in successive steps.
</nextsent>
<nextsent>irmc this system involves preliminary, rough word alignment step which uses trans-fer dictionary and measure of the proximity of words (d~bili et al, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W139">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>researchers of natural language have repeatedly ac-knowledged that texts are not just sequence of words nor even sequence of clauses and sentences.
</prevsent>
<prevsent>however, despite the impressive number of discourse-related theo-ries that have been proposed so far, there have emerged no algorithms capable of deriving the discourse struc-ture of an unrestricted text.
</prevsent>
</prevsection>
<citsent citstr=" P94-1003 ">
on one hand, efforts such as those described by asher (1993), lascarides, asher, and oberlander (1992), kamp and reyle (1993), grover et al (1994), <papid> P94-1003 </papid>and pr0st, scha, and vanden berg (1994) take the position that discourse structures can be built only in conjunction with fully specified clause and sen-tence structures.</citsent>
<aftsection>
<nextsent>and hobbs theory (1990) assumes that sophisticated knowledge bases and inference mech-anisms are needed for determining the relations between discourse units.
</nextsent>
<nextsent>despite the formal elegance of these approaches, they are very domain dependent and, there-fore, unable to handle more than few restricted exam- pies.
</nextsent>
<nextsent>on the other hand, although the theories described by grosz and sidner (1986), <papid> J86-3001 </papid>polanyi (1988), and mann and thompson (1988) are successfully applied manually, they ,are too informal to support an automatic approach to discourse analysis.</nextsent>
<nextsent>in contrast with this previous work, the rhetorical parser that we present builds discourse trees for unre-stricted texts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W140">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and hobbs theory (1990) assumes that sophisticated knowledge bases and inference mech-anisms are needed for determining the relations between discourse units.
</prevsent>
<prevsent>despite the formal elegance of these approaches, they are very domain dependent and, there-fore, unable to handle more than few restricted exam- pies.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
on the other hand, although the theories described by grosz and sidner (1986), <papid> J86-3001 </papid>polanyi (1988), and mann and thompson (1988) are successfully applied manually, they ,are too informal to support an automatic approach to discourse analysis.</citsent>
<aftsection>
<nextsent>in contrast with this previous work, the rhetorical parser that we present builds discourse trees for unre-stricted texts.
</nextsent>
<nextsent>we first discuss the key concepts on which our approach relies (section 2) and the corpus analysis (section 3) that provides the empirical data for our rhetor-ical parsing algorithm.
</nextsent>
<nextsent>we discuss then an algorithm that recognizes discourse usages of cue phrases and that de-termines clause boundaries within sentences.
</nextsent>
<nextsent>lastly, we present the rhetorical parser and an example of its opera-tion (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W141">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W142">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" J88-2006 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W143">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" J81-4001 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W147">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" C92-1045 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W151">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" J87-1002 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W152">
<title id=" P97-1013.xml">the rhetorical parsing of unrestricted natural language texts </title>
<section> foundation.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, if one is to build discourse trees for unrestricted texts, the problems that remain to be solved are the automatic determination of the tex-tual units and the rhetorical relations that hold between them.
</prevsent>
<prevsent>in this paper, we show how one can find and ex-ploit approximate solutions for both of these problems by capitalizing on the occurrences of certain lexico gram matical constructs.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
such constructs can include tense 96 and aspect (moens and steedman, 1988; <papid> J88-2003 </papid>webber, 1988; <papid> J88-2006 </papid>lascarides and asher, 1993), certain patterns of pronom- inal ization and anaphoric usages (sidner, 1981; <papid> J81-4001 </papid>grosz and sidner, 1986; <papid> J86-3001 </papid>sumita et al, 1992; grosz, joshi, and weinstein, 1995),/t-clefts (delin and oberlander, 1992), <papid> C92-1045 </papid>and discourse markers or cue phrases (ballard, conrad, and long acre, 1971; halliday and hasan, 1976; van dijk, 1979; long acre, 1983; grosz and sidner, 1986; <papid> J86-3001 </papid>schiff rin, 1987; cohen, 1987; <papid> J87-1002 </papid>redeker, 1990; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; fraser, 1996; moser and moore, 1997).</citsent>
<aftsection>
<nextsent>in the work described here, we investigate how far we can get by focusing our attention only on discourse markers and lexico grammatical constructs that can be detected by shallow analysis of natural language texts.
</nextsent>
<nextsent>the intuition behind our choice relies on the following facts: ? psycho linguistic and other empirical research (kintsch, 1977; schiff rin, 1987; segal, duchan, and scott, 1991; cahn, 1992; sanders, spooren, and noordman, 1992; hirschberg and litman, 1993; <papid> J93-3003 </papid>knott, 1995; coster mans and fayol, 1997) has shown that discourse markers are consistently used by human subjects both as cohesive ties between adjacent clauses and as  macro connectors  between larger textual units.</nextsent>
<nextsent>therefore, we can use them as rhetorical indica-tors at any of the following levels: clause, sen-tence, paragraph, and text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W165">
<title id=" P98-2131.xml">a multi neuro tagger using variable lengths of contexts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we document the use of this ar-chitecture and its components in several prototypes, and also discuss its potential ap-plication to spoken dialogue systems defined in the near-future scenario.
</prevsent>
<prevsent>we present an architecture for spoken dialogue systems for both human-computer interaction and computer mediation or analysis of human dialogue.
</prevsent>
</prevsection>
<citsent citstr=" A97-1001 ">
the architecture shares many compo-nents with those of existing spoken dialogue systems, such as command talk (moore et al 1997), <papid> A97-1001 </papid>galaxy (goddeau et al 1994), trains (allen et al 1995), verb mobil (wahlster 1993), waxholm (carlson 1996), and others.</citsent>
<aftsection>
<nextsent>our ar-chitecture is distinguished from these in its treatment of discourse-level processing.
</nextsent>
<nextsent>most architectures, including ours, contain mod-ules for speech recognition and natural language interpretation (such as morphology, syntax, and sentential semantics).
</nextsent>
<nextsent>many include module for interfacing with the back-end application.
</nextsent>
<nextsent>if the dialogue is two-way, the architectures also include modules for natural language generation and speech synthesis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W166">
<title id=" P98-2131.xml">a multi neuro tagger using variable lengths of contexts </title>
<section> implementations of the architecture.  </section>
<citcontext>
<prevsection>
<prevsent>~ i   ? nl !nterpre~ion/generation imulated :i~,  !,,l ~:~i   back-end interface ~:~~:~-::~:  ::simulated ~  =~ ~.~: context trackin\[ (luperfo~, 1992) pra\[\[matic adaptation currently, simulated dialo\[\[ue manager current development table 5.
</prevsent>
<prevsent>components oftrs system, with discourse components in white components not in our focus (shaded in gray) are either commercial or simulated software.
</prevsent>
</prevsection>
<citsent citstr=" P92-1004 ">
for context tracking, we use an algorithm based on (luperfoy 1992).<papid> P92-1004 </papid></citsent>
<aftsection>
<nextsent>for dialogue management, we developed simple agent able to control system-initiated dialogue, as well as handle non- linguistic events from the back-end.
</nextsent>
<nextsent>the third discourse component, pragmatic adaptation, awaits future research, and was simulated for this system.
</nextsent>
<nextsent>figure 2 presents sample trs dialogue.
</nextsent>
<nextsent>system: welcome.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W167">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent advances in dialogue modeling, speech recogni-tion, and natural language processing have made it possi-ble to build spoken dialogue agents for wide variety of applications, potential benefits of such agents include remote or hands-free access, ease of use, naturalness, and greater efficiency of interaction.
</prevsent>
<prevsent>however, acritical obstacle to progress in this area is the lack of general framework for evaluating and comparing the performance of different dialogue agents.
</prevsent>
</prevsection>
<citsent citstr=" H90-1023 ">
one widely used approach to evaluation isbased on the notion of reference answer (hirschman et al, 1990).<papid> H90-1023 </papid></citsent>
<aftsection>
<nextsent>an agent responses to query are compared with prede-fined key of minimum and maximum reference answers; performance is the proportion of responses that match the key.
</nextsent>
<nextsent>this approach as many widely acknowledged lim-itations (hirschman and pao, 1993; danieli et al, 1992; bates and ayuso, 1993), e.g., although there may be many potential dialogue strategies for carrying out task, the key is tied to one particular dialogue strategy.
</nextsent>
<nextsent>in contrast, agents using different dialogue strategies can be compared with measures uch as inappropri-ate utterance ratio, turn correction ratio, concept accu-racy, implicit recovery and transaction success (danieli lwe use the term agent emphasize the fact that we are evaluating speaking entity that may have personality.
</nextsent>
<nextsent>read-ers who wish to may substitute the word  system  wherever  agent  is used.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W168">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in contrast, agents using different dialogue strategies can be compared with measures uch as inappropri-ate utterance ratio, turn correction ratio, concept accu-racy, implicit recovery and transaction success (danieli lwe use the term agent emphasize the fact that we are evaluating speaking entity that may have personality.
</prevsent>
<prevsent>read-ers who wish to may substitute the word  system  wherever  agent  is used.
</prevsent>
</prevsection>
<citsent citstr=" H92-1005 ">
and gerbino, 1995; hirschman and pao, 1993; po-lifroni et al, 1992; <papid> H92-1005 </papid>simpson and fraser, 1993; shriberg, wade, and price, 1992).</citsent>
<aftsection>
<nextsent>consider comparison of two train timetable information agents (danieli and gerbino, 1995), where agent in dialogue uses an explicit con-firmation strategy, while agent in dialogue 2 uses an implicit confirmation strategy: (1) user: want to go from torino to milano.
</nextsent>
<nextsent>agent a: do you want to go from trento to milano?
</nextsent>
<nextsent>yes or no? user: no.
</nextsent>
<nextsent>(2) user: want to travel from torino to milano.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W171">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> 2  </section>
<citcontext>
<prevsection>
<prevsent>as shown in figure 1, performance is also function of combination of cost measures.
</prevsent>
<prevsent>intuitively, cost measures should be calculated on the basis of any user or agent dialogue behaviors that should be minimized.
</prevsent>
</prevsection>
<citsent citstr=" J97-1006 ">
a wide range of cost measures have been used in previous work; these include pure efficiency measures such as the num-ber of turns or elapsed time to complete the task (abella, brown, and buntschuh, 1996; hirschman et al, 1990; <papid> H90-1023 </papid>smith and gordon, 1997; <papid> J97-1006 </papid>walker, 1996), as well as mea-sures of qualitative phenomena such as inappropriate or repair utterances (danieli and gerbino, 1995; hirschman and pao, 1993; simpson and fraser, 1993).</citsent>
<aftsection>
<nextsent>paradise represents each cost measure as function ci that can be applied to any (sub)dialogue.
</nextsent>
<nextsent>first, consider the simplest case of calculating efficiency measures over whole dialogue.
</nextsent>
<nextsent>for example, let cl be the total number of utterances.
</nextsent>
<nextsent>for the whole dialogue d1 in figure 2, el(d1) is 23 utterances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W172">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> 2  </section>
<citcontext>
<prevsection>
<prevsent>for the task.
</prevsent>
<prevsent>9this tagging can be hand generated, or system generated and hand corrected.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
preliminary studies indicate that reliability for human tagging is higher for avm attribute tagging than for other types of discourse segment tagging (passonneau and litman, 1997; <papid> J97-1005 </papid>hirschberg and nakatani, 1996).<papid> P96-1038 </papid></citsent>
<aftsection>
<nextsent>~:e.ac, dr, ~:a . .a9 seg~cr: s3 s~ml~cr: s4 g0~: i?
</nextsent>
<nextsent>goals: ac rr~ces: a3...u5 0ti/~es: a6...u6 figure 4: task-defined is course structure of agent dialogue interaction utterances that contribute to the success of the whole dia-logue, such as greetings, are tagged with all the attributes.
</nextsent>
<nextsent>since the structure of dialogue reflects the structure of the task (carberry, 1989; grosz and sidner, 1986; <papid> J86-3001 </papid>litman and allen, 1990), the tagging of dialogue by the avm attributes can be used to generate hierarchical discourse structure such as that shown in figure 4 for dialogue 1 (figure 2).</nextsent>
<nextsent>for example, segment (subdialogue) $2 in figure 4 is about both depart-city (dc) and arrival- city (ac).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W173">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> 2  </section>
<citcontext>
<prevsection>
<prevsent>for the task.
</prevsent>
<prevsent>9this tagging can be hand generated, or system generated and hand corrected.
</prevsent>
</prevsection>
<citsent citstr=" P96-1038 ">
preliminary studies indicate that reliability for human tagging is higher for avm attribute tagging than for other types of discourse segment tagging (passonneau and litman, 1997; <papid> J97-1005 </papid>hirschberg and nakatani, 1996).<papid> P96-1038 </papid></citsent>
<aftsection>
<nextsent>~:e.ac, dr, ~:a . .a9 seg~cr: s3 s~ml~cr: s4 g0~: i?
</nextsent>
<nextsent>goals: ac rr~ces: a3...u5 0ti/~es: a6...u6 figure 4: task-defined is course structure of agent dialogue interaction utterances that contribute to the success of the whole dia-logue, such as greetings, are tagged with all the attributes.
</nextsent>
<nextsent>since the structure of dialogue reflects the structure of the task (carberry, 1989; grosz and sidner, 1986; <papid> J86-3001 </papid>litman and allen, 1990), the tagging of dialogue by the avm attributes can be used to generate hierarchical discourse structure such as that shown in figure 4 for dialogue 1 (figure 2).</nextsent>
<nextsent>for example, segment (subdialogue) $2 in figure 4 is about both depart-city (dc) and arrival- city (ac).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W174">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> 2  </section>
<citcontext>
<prevsection>
<prevsent>~:e.ac, dr, ~:a . .a9 seg~cr: s3 s~ml~cr: s4 g0~: i?
</prevsent>
<prevsent>goals: ac rr~ces: a3...u5 0ti/~es: a6...u6 figure 4: task-defined is course structure of agent dialogue interaction utterances that contribute to the success of the whole dia-logue, such as greetings, are tagged with all the attributes.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
since the structure of dialogue reflects the structure of the task (carberry, 1989; grosz and sidner, 1986; <papid> J86-3001 </papid>litman and allen, 1990), the tagging of dialogue by the avm attributes can be used to generate hierarchical discourse structure such as that shown in figure 4 for dialogue 1 (figure 2).</citsent>
<aftsection>
<nextsent>for example, segment (subdialogue) $2 in figure 4 is about both depart-city (dc) and arrival- city (ac).
</nextsent>
<nextsent>it contains egments $3 and $4 within it, and consists of utterances u1 . . .
</nextsent>
<nextsent>u6.
</nextsent>
<nextsent>tagging by avm attributes is required to calculate costs over sub dialogues, ince for any sub dialogue, task attributes define the subdialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W180">
<title id=" P97-1035.xml">paradise a framework for evaluating spoken dialogue agents </title>
<section> u: rsl  1.  </section>
<citcontext>
<prevsection>
<prevsent>a domain-specific task-based success measure in the per-formance model for n. the evaluation model presented here has many applica-tions in apoken dialogue processing.
</prevsent>
<prevsent>we believe that the framework is also applicable to other dialogue modal-ities, and to human-human task-oriented dialogues.
</prevsent>
</prevsection>
<citsent citstr=" C82-1066 ">
in addition, while there are many proposals in the litera-ture for algorithms for dialogue strategies that are co-operative, collaborative or helpful to the user (webber and joshi, 1982; <papid> C82-1066 </papid>pollack, hirschberg, and webber, 1982; joshi, webber, and weischedel, 1984; chu-carrol and carberry, 1995), very few of these strategies have been evaluated as to whether they improve any measurable as-pect of dialogue interaction.</citsent>
<aftsection>
<nextsent>as we have demonstrated here, any dialogue strategy can be evaluated, so it should be possible to show that cooperative sponse, or other cooperative strategy, actually improves task performance by reducing costs or increasing task success.
</nextsent>
<nextsent>we hope that this framework will be broadly applied in future di-alogue research.
</nextsent>
<nextsent>we would like to thank james allen, jennifer chu- carroll, morena danieli, wieland eckert, giuseppe di fabbrizio, don hindle, julia hirschberg, shri narayanan, jay wilpon, steve whittaker and three anonymous re-views for helpful discussion and comments on earlier versions of this paper.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W181">
<title id=" P96-1044.xml">linguistic structure as composition and perturbation </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>this suggests us-ing compression as means of acquiring under- lying properties of language from surface signals.
</prevsent>
<prevsent>the general methodology of language-learning-by- compression is not new.
</prevsent>
</prevsection>
<citsent citstr=" P95-1031 ">
some notable early propo-nents included chomsky (1955), solomon off (1960) and harris (1968), and compression has been used as the basis for wide variety of computer programs that attack unsupervised learning in language; see (olivier, 1968; wolff, 1982; ellison, 1992; stolcke, 1994; chen, 1995; <papid> P95-1031 </papid>cartwright and brent, 1994) among others.</citsent>
<aftsection>
<nextsent>1.1 patterns and language.
</nextsent>
<nextsent>unfortunately, while surface patterns often reflect interesting linguistic mechanisms and parameters, they do not always do so.
</nextsent>
<nextsent>three classes of exam-ples serve to illustrate this.
</nextsent>
<nextsent>1.1.1 extra lngu ls l patterns the sequence it was dark and stormy night is pattern in the sense it occurs in text far more frequently than the frequencies of its letters would suggest, but that does not make it lexical or gram-matical primitive: it is the product of complex mixture of linguistic and extra-linguistic processes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W182">
<title id=" P96-1044.xml">linguistic structure as composition and perturbation </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>sim-ilar heuristics can be used to estimate the benefit of deleting words.
</prevsent>
<prevsent>4 3.2 search proper ies.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
a significant source of problems in traditional gram-mar induction techniques local minima (de mar-cken, 1995a; pereira and schabes, 1992; <papid> P92-1017 </papid>carroll and charniak, 1992).</citsent>
<aftsection>
<nextsent>the search algorithm described above avoids many of these problems.
</nextsent>
<nextsent>the reason is that hidden structure is largely  compile-time  phenomena.
</nextsent>
<nextsent>during parsing all that is important about word is its surface form and codelength.
</nextsent>
<nextsent>the internal representation does not matter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W183">
<title id=" P98-1050.xml">multexteast parallel and comparable corpora and lexicons for six central and eastern european languages </title>
<section> the multext-east corpora.  </section>
<citcontext>
<prevsection>
<prevsent>1.1 encoding format.
</prevsent>
<prevsent>based on the principle that its corpus encoding format should be standardized and homogeneous both for interchange and for facilitating open- ended retrieval tasks, multext-east adopted the lhttp://nl.ijs.si/me 2http://www.
</prevsent>
</prevsection>
<citsent citstr=" W98-1102 ">
lpl.univ-aix.fr/ projects/multext/ 315 corpus encoding standard (ces) 3 (ide, 1998), <papid> W98-1102 </papid>which has been developed to be optimally suited for use in language engineering and corpus- based work.</citsent>
<aftsection>
<nextsent>the ces is an application of sgml (iso-8879, standard generalized markup language) and is based on the tel guidelines for electronic text encoding and interchange.
</nextsent>
<nextsent>in addition to providing encoding conventions for elements relevant to corpus-based work, the ces provides data architecture for linguistic corpora and their annotations.
</nextsent>
<nextsent>each corpus component, comprising single text and its annotations, is organized as hyper-document, with various levels of annotation stored in separate sgml documents (each with separate dtd).
</nextsent>
<nextsent>low-density (i.e., above the token level) annotation is expressed indirectly in terms of inter-document links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W184">
<title id=" P93-1007.xml">a speech first model for repair detection and correction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>delta leaving boston seventeen twenty one ar-riving fort worth twenty two twenty one forty.
</prevsent>
<prevsent>here,  twenty two twenty one forty  must be interpreted as flight arrival time; the system must somehow choose among  21:40 ,  22:21 , and  22:40 .
</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
although studies of large speech corpora have found that approximately 10% of spontaneous tter- ances contain disfluencies involving self-correction, or repairs (hindle, 1983; <papid> P83-1019 </papid>shriberg et al, 1992), <papid> H92-1085 </papid>little is known about how to integrate repair processing with real-time speech recognition.</citsent>
<aftsection>
<nextsent>in particular, the speech signal itself has been relatively unexplored as source of processing cues for the detection and correction of repairs.
</nextsent>
<nextsent>in this paper, we present results from study of the acoustic and prosodic haracteristics of 334 repair utterances, containing 368 repair instances, from the aroa air travel information system (atis) database.
</nextsent>
<nextsent>our results are interpreted within our  speech-first  framework for investigating repairs, the repair in-terval model (rim).
</nextsent>
<nextsent>rim builds upon labov (1966) and hindle (1983) <papid> P83-1019 </papid>by conceptually extending the edit signal hypothesis - - that repairs are acoustically or phonetic ally marked at the point of interruption of flu-ent speech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W185">
<title id=" P93-1007.xml">a speech first model for repair detection and correction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>delta leaving boston seventeen twenty one ar-riving fort worth twenty two twenty one forty.
</prevsent>
<prevsent>here,  twenty two twenty one forty  must be interpreted as flight arrival time; the system must somehow choose among  21:40 ,  22:21 , and  22:40 .
</prevsent>
</prevsection>
<citsent citstr=" H92-1085 ">
although studies of large speech corpora have found that approximately 10% of spontaneous tter- ances contain disfluencies involving self-correction, or repairs (hindle, 1983; <papid> P83-1019 </papid>shriberg et al, 1992), <papid> H92-1085 </papid>little is known about how to integrate repair processing with real-time speech recognition.</citsent>
<aftsection>
<nextsent>in particular, the speech signal itself has been relatively unexplored as source of processing cues for the detection and correction of repairs.
</nextsent>
<nextsent>in this paper, we present results from study of the acoustic and prosodic haracteristics of 334 repair utterances, containing 368 repair instances, from the aroa air travel information system (atis) database.
</nextsent>
<nextsent>our results are interpreted within our  speech-first  framework for investigating repairs, the repair in-terval model (rim).
</nextsent>
<nextsent>rim builds upon labov (1966) and hindle (1983) <papid> P83-1019 </papid>by conceptually extending the edit signal hypothesis - - that repairs are acoustically or phonetic ally marked at the point of interruption of flu-ent speech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W189">
<title id=" P93-1007.xml">a speech first model for repair detection and correction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the first stage, lexical pattern 46 matching rules operating on orthographic transcrip-tions would be used to retrieve candidate repair utter-ances.
</prevsent>
<prevsent>in the second, syntactic, semantic, and acoustic information would filter true repairs from false posi-tives found by the pattern matcher.
</prevsent>
</prevsection>
<citsent citstr=" P92-1008 ">
results of testing the first stage of this model, the lexical pattern matcher, are reported in (bear et al, 1992): <papid> P92-1008 </papid>309 of 406 utterance containing  non trivial  repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utter-ances were incorrectly identified as containing repairs.</citsent>
<aftsection>
<nextsent>this represents recall of 76% with precision of 62%.
</nextsent>
<nextsent>of the repairs correctly identified, the appropriate cor-rection was found for 57%.
</nextsent>
<nextsent>repaj candidates were filtered and corrected by deleting portion of the ut-terance based on the pattern matched, and then check-ing the syntactic and semantic acceptability of the cor-rected version using the syntactic and semantic om- ponents of the gemini nlp system.
</nextsent>
<nextsent>bear et al (1992) <papid> P92-1008 </papid>also speculate that acoustic information might be used to filter out false positives for candidates matching two of their lexical patterns - - repetitions of single words and cases of single inserted words - - but do not report such experimentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W197">
<title id=" P93-1007.xml">a speech first model for repair detection and correction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>crucially, in rim, we extend the notion of edit signal to include any phenomenon which may contribute to the perception of an  abrupt cut-off  of the speech signal - - including cues such as coartic- ulation phenomena, word fragments, interruption glot- tal ization, pause, and prosodic ues which occur in the vicinity of the dis fluency interval.
</prevsent>
<prevsent>rim thus acknowl-edges the edit signal hypothesis, that some aspect of the speech signal may demarcate the computationally key juncture between the reparandum and repair inter-vals, while extending its possible acoustic and prosodic manifestations.
</prevsent>
</prevsection>
<citsent citstr=" H93-1066 ">
acoustic-prosodic characteristics of repairs we studied the acoustic and prosodic correlates of repair events as defined in the rim framework with the aim of identifying potential cues for automatic re-pair processing, extending pilot study reported in (nakatani and hirschberg, 1993).<papid> H93-1066 </papid></citsent>
<aftsection>
<nextsent>our corpus for the current study consisted of 6,414 utterances produced by 123 speakers from the arpa airline travel and in-formation system (atis) database (madcow, 1992) collected at at&t;, bbn, cmu, sri, and tl 334 (5.2%) of these utterances contain at least one repair~ where repair is defined as the self-correction of one or more phonemes (up to and including sequences of words) in an utterance) orthographic transcriptions of the utterances were prepared by arpa contractors accord-ing to standardized conventions.
</nextsent>
<nextsent>the utterances were labeled at bell laboratories for word boundaries and into national prominences and phrasing following pier-rehumbert description of english intonation (pierre- humbert, 1980).
</nextsent>
<nextsent>also, each of the three rim intervals and prosodic and acoustic events within those intervals were labeled.
</nextsent>
<nextsent>identifying the reparandum interval our acoustic and prosodic analysis of the reparan- dum interval focuses on acoustic-phonetic properties of word fragments, as well as additional phonetic ues marking the reparandum offset.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W203">
<title id=" P96-1050.xml">a synopsis of learning to recognize names across languages </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>next, the delimited names are categorized.
</prevsent>
<prevsent>the approach taken here is to utilize data-driven knowledge acquisition strategy based on decision trees which uses contextual information.
</prevsent>
</prevsection>
<citsent citstr=" W93-0104 ">
this differs from other approaches (farwell et al, 1994; kitani &amp; mita-mura, 1994; mcdonald, 1993; <papid> W93-0104 </papid>rau, 1992) which attempt to achieve this task by: (1) hand-coded heuris-tics, (2) list-based matching schemes, (3) human-gen- erated knowledge bases, and (4) combinations thereof.</citsent>
<aftsection>
<nextsent>delimitation occurs through the application of phrasal templates.
</nextsent>
<nextsent>these templates, built by hand, use logical operators (and, or, etc.) to combine features strongly associated with proper names, including: proper noun, ampersand, hyphen, and comma.
</nextsent>
<nextsent>in addi-tion, ambiguities with delimitation are handled by in-cluding other predictive features within the templates.
</nextsent>
<nextsent>to acquire the knowledge required for classifica-tion, each word is tagged with all of its associated fea-tures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W204">
<title id=" P96-1050.xml">a synopsis of learning to recognize names across languages </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>(person), and  county  (location).
</prevsent>
<prevsent>features are derived through automated and manual techniques.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
on-line lists can quickly provide useful features uch as cities, family names, nationalities, etc. proven pos taggers (farwell et al, 1994; brill, 1992; <papid> A92-1021 </papid>matsumoto et al, 1992) predetermine pos features.</citsent>
<aftsection>
<nextsent>other features are derived through statistical measures and hand analysis.
</nextsent>
<nextsent>a decision tree is built (for each name class) from the initial feature set using recursive partitioning al-gorithm (quinlan, 1986; breiman et al, 1984) that uses the following function as its splitting criterion: -p*log2(p) - (1-p)*log2(1-p) (1) where represents the proportion of names within tree node belonging to the class for which the tree is built.
</nextsent>
<nextsent>the feature which minimizes the weighted sum of this function across both child nodes resulting from split is chosen.
</nextsent>
<nextsent>a multi tree approach was chosen over learning single tree for all name classes because it allows for the straightforward association of features within the tree with specific name classes, and facili-tates troubleshooting.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W209">
<title id=" P98-1075.xml">growing semantic grammars </title>
<section> system arch tec ture.  </section>
<citcontext>
<prevsection>
<prevsent>if the end-user is sat-.
</prevsent>
<prevsent>isfied with one of the options, new grammar rule is dynamically created and becomes part of the end-user grammar until further notice.
</prevsent>
</prevsection>
<citsent citstr=" E93-1027 ">
each new rule is annotated with the learning episode that gave rise to it, including end-user id, time stamp, and counter that will keep track of how many times the new rule fires in successful parses, 3.2.2 parser predictions as suggested by kiyono and tsujii (1993), <papid> E93-1027 </papid>one can make use of parse failures to acquire new knowledge, both about the nature of the un parsed words and about he inadequacy of the existing rammar rules.</citsent>
<aftsection>
<nextsent>gsg uses incomplete parses to predict what can come next (i.e. after the partially-parsed sequence 7i.e., parse trees containing concept-subconcept relations that are inconsistent with the stipulations of the dm.
</nextsent>
<nextsent>sthe degree of generalization level o.f abstraction that new rule should exhibit is an open question but currently principle of maximal abstraction is followed: (a) parse the lexical items of the new rule right-hand-side with all concepts granted top-level status, i.e., able to stand at the root of parse tree.
</nextsent>
<nextsent>(b) if word is not covered by any tree, take it as is into the final right-hand side.
</nextsent>
<nextsent>else, take the root of the parse tree with largest span; if tie, prefer the root that ranks higher in the dm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W210">
<title id=" P98-1075.xml">growing semantic grammars </title>
<section> system arch tec ture.  </section>
<citcontext>
<prevsection>
<prevsent>z..j .........
</prevsent>
<prevsent>; lst~a~llj,  ~ }  - -  ,,mat about \[ume\] {i figure 6: ...and new rule is acquired.
</prevsent>
</prevsection>
<citsent citstr=" P94-1004 ">
of un parsed words, the following stochastic models, inspired in miller et al (1994) <papid> P94-1004 </papid>and seneff (1992), <papid> J92-1004 </papid>and collectively referred to as hidden understanding model (hum), are employed.</citsent>
<aftsection>
<nextsent>speech-act n-gram.
</nextsent>
<nextsent>top-level concepts can be seen as speech acts of the domain.
</nextsent>
<nextsent>for instance, in the dm in fig.
</nextsent>
<nextsent>2 top-level concepts uch as \[greeting\], cfarewell\] or \[suggestion\], correspond to discourse speech acts, and in normally-occurring conversation, they follow distribution that is clearly non-uniform.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W211">
<title id=" P98-1075.xml">growing semantic grammars </title>
<section> system arch tec ture.  </section>
<citcontext>
<prevsection>
<prevsent>z..j .........
</prevsent>
<prevsent>; lst~a~llj,  ~ }  - -  ,,mat about \[ume\] {i figure 6: ...and new rule is acquired.
</prevsent>
</prevsection>
<citsent citstr=" J92-1004 ">
of un parsed words, the following stochastic models, inspired in miller et al (1994) <papid> P94-1004 </papid>and seneff (1992), <papid> J92-1004 </papid>and collectively referred to as hidden understanding model (hum), are employed.</citsent>
<aftsection>
<nextsent>speech-act n-gram.
</nextsent>
<nextsent>top-level concepts can be seen as speech acts of the domain.
</nextsent>
<nextsent>for instance, in the dm in fig.
</nextsent>
<nextsent>2 top-level concepts uch as \[greeting\], cfarewell\] or \[suggestion\], correspond to discourse speech acts, and in normally-occurring conversation, they follow distribution that is clearly non-uniform.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W212">
<title id=" P98-1075.xml">growing semantic grammars </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>potential applications of gsg are many, most no-tably very fast development of nlu components for variety of tasks including speech recognition and nl interfaces.
</prevsent>
<prevsent>also, the idiga environment enhances the usability of any system or application that incorporates it, for the end-users are able to eas-ily  teach the computer  their individual anguage patterns and preferences.
</prevsent>
</prevsection>
<citsent citstr=" W97-0410 ">
current and future work includes further develop-ment of the learning methods and their integration, design of rule-merging mechanism, comparison of individual vs. collective grammars, distributed grammar development over the world wide web, and integration of gsg run-time stage into the janus speech recognition system (lavie et al 1997).<papid> W97-0410 </papid></citsent>
<aftsection>
<nextsent>acknowledgements the work reported in this paper was funded in part by grant from atr interpreting telecommunications research laboratories of japan.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W213">
<title id=" P95-1020.xml">a uniform treatment of pragmatic inferences in simple and complex utterances and sequences of utterances </title>
<section> pragmatics and defeasibility.  </section>
<citcontext>
<prevsection>
<prevsent>it is widely acknowledged that full account of na-tural language utterances cannot be given in terms of only syntactic or semantic phenomena.
</prevsent>
<prevsent>forex- ample, hirschberg (1985) has shown that in order to understand scalar implicature, one must analyze the conversants  beliefs and intentions.
</prevsent>
</prevsection>
<citsent citstr=" P90-1012 ">
to recognize normal state implicatures one must consider mutual beliefs and plans (green, 1990).<papid> P90-1012 </papid></citsent>
<aftsection>
<nextsent>to understand con- versationm implicatures associated with indirect re-plies one must consider discourse xpectations, dis-course plans, and discourse relations (green, 1992; green and carberry, 1994).
</nextsent>
<nextsent>some pre suppositions are inferrable when certain lexical constructs (fac- tives, aspect uals, etc) or syntactic onstructs (cleft and pseudo-cleft sentences) are used.
</nextsent>
<nextsent>despite all the complexities that individualize the recognition stage for each of these inferences, all of them can be de-feated by context, by knowledge, beliefs, or plans of the agents that constitute part of the context, or by other pragmatic rules.
</nextsent>
<nextsent>defeasibili~y is notion that is tricky to deal with, and scholars in logics and pragmatics have learned to circumvent it or live with it.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W214">
<title id=" P95-1020.xml">a uniform treatment of pragmatic inferences in simple and complex utterances and sequences of utterances </title>
<section> pragmatics and defeasibility.  </section>
<citcontext>
<prevsection>
<prevsent>3.2.2 or - cance la ion consider now utterance (10).
</prevsent>
<prevsent>the stratified se-mantic tableau that corresponds to its logical theory yields 16 models, but only chris is an adult satisfies definition 2.2 and is projected as pre supposition for the utterance.
</prevsent>
</prevsection>
<citsent citstr=" E93-1033 ">
3.3 ragmat ic inferences in sequences of t te rances we have already mentioned that speech repairs con-stitute good benchmark for studying the genera- 148 tion and cancellation of pragmatic inferences along sequences of utterances (mcroy and hirst, 1993).<papid> E93-1033 </papid></citsent>
<aftsection>
<nextsent>suppose, for example, that jane has two friends - - john smith and john pevler - - and that her room-mate mary has met only john smith, married fel-low.
</nextsent>
<nextsent>assume now that jane has conversation with mary in which jane mentions only the name john because she is not aware that mary does not know about the other john, who is five-year-old boy.
</nextsent>
<nextsent>in this context, it is natural for mary to become confu-sed and to come to wrong conclusions.
</nextsent>
<nextsent>for example, mary may reply that john is not bachelor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W215">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yaari (1997) segmented text into hierarchical structure, identifying sub-segments of larger segments.
</prevsent>
<prevsent>ponte and croft (1997) used word co-occurrences to expand the number of terms for matching.
</prevsent>
</prevsection>
<citsent citstr=" P94-1050 ">
reynar (1994) <papid> P94-1050 </papid>compared all words across text rather than the more usual nearest neighbours.</citsent>
<aftsection>
<nextsent>a problem with using word repetition is that inappropriate matches can be made because of the lack of contextual information (salton et al, 1994).
</nextsent>
<nextsent>another approach to text segmentation is the detection of semantically related words.
</nextsent>
<nextsent>hearst (1993) incorporated semantic information derived from wordnet but in later work reported that this information actually degraded word repetition results (hearst, 1994).
</nextsent>
<nextsent>related words have been located using spreading activation on semantic network (kozima, 1993), <papid> P93-1041 </papid>although only one text was segmented.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W217">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another approach to text segmentation is the detection of semantically related words.
</prevsent>
<prevsent>hearst (1993) incorporated semantic information derived from wordnet but in later work reported that this information actually degraded word repetition results (hearst, 1994).
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
related words have been located using spreading activation on semantic network (kozima, 1993), <papid> P93-1041 </papid>although only one text was segmented.</citsent>
<aftsection>
<nextsent>another approach extracted semantic information from roget thesaurus (rt).
</nextsent>
<nextsent>lexical cohesion relations (halliday and hasan, 1976) between words were identified in rt and used to construct lexical chains of related words in five texts (morris and hirst, 1991).<papid> J91-1002 </papid></nextsent>
<nextsent>it was reported that the lexical chains closely correlated to the intentional structure (grosz and sidner, 1986) <papid> J86-3001 </papid>of the texts, where the start and end of chains coincided with the intention ranges.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W218">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>related words have been located using spreading activation on semantic network (kozima, 1993), <papid> P93-1041 </papid>although only one text was segmented.</prevsent>
<prevsent>another approach extracted semantic information from roget thesaurus (rt).</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
lexical cohesion relations (halliday and hasan, 1976) between words were identified in rt and used to construct lexical chains of related words in five texts (morris and hirst, 1991).<papid> J91-1002 </papid></citsent>
<aftsection>
<nextsent>it was reported that the lexical chains closely correlated to the intentional structure (grosz and sidner, 1986) <papid> J86-3001 </papid>of the texts, where the start and end of chains coincided with the intention ranges.</nextsent>
<nextsent>however, rt does not capture all types of lexical cohesion relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W219">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another approach extracted semantic information from roget thesaurus (rt).
</prevsent>
<prevsent>lexical cohesion relations (halliday and hasan, 1976) between words were identified in rt and used to construct lexical chains of related words in five texts (morris and hirst, 1991).<papid> J91-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
it was reported that the lexical chains closely correlated to the intentional structure (grosz and sidner, 1986) <papid> J86-3001 </papid>of the texts, where the start and end of chains coincided with the intention ranges.</citsent>
<aftsection>
<nextsent>however, rt does not capture all types of lexical cohesion relations.
</nextsent>
<nextsent>in previous work, it was found that collocation (a lexical cohesion relation) was under-represented in the thesaurus.
</nextsent>
<nextsent>furthermore, this process was not automated and relied on subjective decisionmaking.
</nextsent>
<nextsent>following morris and hirst work, segmentation algorithm was developed based on identifying lexical cohesion relations across text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W220">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> identifying lexical cohesion.  </section>
<citcontext>
<prevsection>
<prevsent>word repetition: word repetition ties in lexical cohesion are identified by same word matches and matches on inflections derived from the same stem.
</prevsent>
<prevsent>an inflected word was reduced to its stem by look- up in lexicon (keenan and evett, 1989) comprising inflection and stem word pair records (e.g.  orange oranges ).
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
collocation: collocations were extracted from seven million word sample of the longman english language corpus using the association ratio (church and hanks, 1990) <papid> J90-1003 </papid>and out putted to lexicon.</citsent>
<aftsection>
<nextsent>collocations were automatically located in text by looking up pairwise words in this lexicon.
</nextsent>
<nextsent>figure 1 shows the record for the headword orange followed by its collocates.
</nextsent>
<nextsent>for example, the pairwise words orange and peel form collocation.
</nextsent>
<nextsent>i orange free green lemon peel red \] state yellow figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W222">
<title id=" P98-1100.xml">text segmentation using reiteration and collocation </title>
<section> experiment 2: test subject evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>due to that success, these five approaches were applied in this experiment.
</prevsent>
<prevsent>to evaluate the results, the information retrieval metrics precision and recall were used.
</prevsent>
</prevsection>
<citsent citstr=" W97-0304 ">
these metrics have tended to be adopted for the assessment of text segmentation algorithms, but they do not provide scale of correctness (beeferman et al, 1997).<papid> W97-0304 </papid></citsent>
<aftsection>
<nextsent>the degree to which segmentation point was  missed  by trough, for instance, isnot considered.
</nextsent>
<nextsent>allowing an error margin provides some degree of flexibility.
</nextsent>
<nextsent>an error margin of two sentences either side of segmentation point was used by hearst (1993) and reynar (1994) <papid> P94-1050 </papid>allowed three sentences.</nextsent>
<nextsent>in this investigation, an error margin of two sentences was considered.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W225">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus, the centering model is scaled up to the level of the global referential structure of discourse.
</prevsent>
<prevsent>an empiri-cal evaluation of the algorithm is supplied.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the centering model (grosz et al, 1995) <papid> J95-2003 </papid>has evolved as major methodology for computational discourse analy- sis.</citsent>
<aftsection>
<nextsent>it provides imple, yet powerful data structures, con-straints and rules for the local coherence of discourse.
</nextsent>
<nextsent>as far as anaphora resolution is concerned, e.g., the model requires to consider those discourse ntities as potential antecedents for anaphoric expressions in the current ut-terance ui, which are available in the forward-looking centers of the immediately preceding utterance ui- 1.
</nextsent>
<nextsent>no constraints or rules are formulated, however, that ac-count for anaphoric relationships which spread out over non-adjacent terances.
</nextsent>
<nextsent>hence, it is unclear how dis-course elements which appear in utterances preceding utterance ui-1 are taken into consideration as potential antecedents for anaphoric expressions in ui.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W226">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, it is unclear how dis-course elements which appear in utterances preceding utterance ui-1 are taken into consideration as potential antecedents for anaphoric expressions in ui.
</prevsent>
<prevsent>the extension of the search space for antecedents is by no means trivial enterprise.
</prevsent>
</prevsection>
<citsent citstr=" J96-2005 ">
a simple linear backward search of all preceding centering structures, e.g., may not only turn out to establish illegal references but also contradicts the cognitive principles underlying the lim-ited attention constraint (walker, 1996<papid> J96-2005 </papid>b).</citsent>
<aftsection>
<nextsent>the solution we propose starts from the observation that additional constraints on valid antecedents are placed by the global discourse structure previous utterances are embedded in.
</nextsent>
<nextsent>we want to emphasize from the beginning that our pro-posal considers only the referential properties underlying the global discourse structure.
</nextsent>
<nextsent>accordingly, we define the extension of referential discourse segments (over sev-eral utterances) and hierarchy of referential discourse segments (structuring the entire discourse).
</nextsent>
<nextsent>1 the algo- rithmic procedure we propose for creating and manag-ing such segments receives local centering data as input and generates sort of superimposed index structure by which the reach ability of potential antecedents, in par-ticular those prior to the immediately preceding utter-ance, is made explicit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W232">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> global discourse structure.  </section>
<citcontext>
<prevsection>
<prevsent>two recent studies deal with this topic in order to relate attentional and intentional structures on larger scale of global discourse coher-ence.
</prevsent>
<prevsent>passonneau (1996) proposes an algorithm for the generation of referring expressions and walker (1996<papid> J96-2005 </papid>a) integrates centering into cache model of attentional state.</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
both studies, among other things, deal with the supposition whether correlation exists between partic-ular centering transitions (which were first introduced by brennan et al (1987); <papid> P87-1022 </papid>cf.</citsent>
<aftsection>
<nextsent>table 1) and intention- based discourse segments.
</nextsent>
<nextsent>in particular, the role of shift-type transitions examined from the perspective of whether they not only indicate shift of the topic be-tween two immediately successive utterances but also signal (intention-based) segment boundaries.
</nextsent>
<nextsent>the data in both studies reveal that only weak correlation be-tween the shift transitions and segment boundaries can be observed.
</nextsent>
<nextsent>this finding precludes reliable predic-tion of segment boundaries based on the occurrence of 1 our notion of referential discourse segment should not be.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W233">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> global discourse structure.  </section>
<citcontext>
<prevsection>
<prevsent>the data in both studies reveal that only weak correlation be-tween the shift transitions and segment boundaries can be observed.
</prevsent>
<prevsent>this finding precludes reliable predic-tion of segment boundaries based on the occurrence of 1 our notion of referential discourse segment should not be.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
confounded with the intentional one originating from grosz &amp; sidner (1986), <papid> J86-3001 </papid>for reasons discussed in section 2.</citsent>
<aftsection>
<nextsent>104 shifts and vice versa.
</nextsent>
<nextsent>in order to accommodate to these empirical results divergent solutions are proposed.
</nextsent>
<nextsent>pas-sonneau suggests that the centering data structures need to be modified appropriately, while walker concludes that the local centering data should be left as they are and further be complemented by cache mechanism.
</nextsent>
<nextsent>she thus intends to extend the scope of centering in ac-cordance with cognitively plausible limits of the atten- tional span.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W239">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> global discourse structure.  </section>
<citcontext>
<prevsection>
<prevsent>we diverge from walker (1996<papid> J96-2005 </papid>a), however, in that we propose an al-ternative to the caching mechanism, which we consider to be methodologically more parsimonious and, at least, to be equally effective (for an elaboration of this claim, cf.</prevsent>
<prevsent>section 6).</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
the proposed extension of the centering model builds on the methodological framework of functional center-ing (strube &amp; hahn, 1996).<papid> P96-1036 </papid></citsent>
<aftsection>
<nextsent>this is an approach to cen-tering in which issues such as thematicity or topicality are already inherent.
</nextsent>
<nextsent>its linguistic foundations relate the ranking of the forward-looking centers and the functional information structure of the utterances, notion origi-nally developed by dane~ (1974).
</nextsent>
<nextsent>strube &amp; hahn (1996) <papid> P96-1036 </papid>use the centering data structures to redefine dane~ tri- chotomy between given information, theme and rheme in terms of the centering model.</nextsent>
<nextsent>the cb(un), the most highly ranked element of c!</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W256">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the efforts of sidner (1983), e.g., have provided variety of different focus data structures to be used for reference resolution.
</prevsent>
<prevsent>this multiplicity and the on-going rowth of the number of different entities (cf.
</prevsent>
</prevsection>
<citsent citstr=" J94-2006 ">
suri &amp; mccoy (1994)) <papid> J94-2006 </papid>mirrors an increase in explanatory constructs that we consider methodological drawback to this approach because they can hardly be kept control of.</citsent>
<aftsection>
<nextsent>our model, due to its hier-archical nature implements stack behavior that is also inherent the abovementioned proposals.
</nextsent>
<nextsent>we refrain, however, from establishing new data type (even worse, different ypes of stacks) that has to be managed on its own.
</nextsent>
<nextsent>there is no need for extra computations to deter-mine the  segment focus , since that is implicitly given in the local centering data already available in our model.
</nextsent>
<nextsent>a recent attempt at introducing lobal discourse no-tions into the centering framework considers the use of cache model (walker, 1996<papid> J96-2005 </papid>b).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W263">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, centered segmentation may replace the 110 cache model entirely, since both are competing models of the attentional state.
</prevsent>
<prevsent>centered segmentation has also the additional advantage of restricting the search space of anaphoric antecedents tothose discourse ntities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory.
</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
text segmentation procedures (more with an informa-tion retrieval motivation, rather than being related to ref-erence resolution tasks) have also been proposed for coarse-grained partitioning of texts into contiguous, non-overlapping blocks and assigning content labels to these blocks (hearst, 1994).<papid> P94-1002 </papid></citsent>
<aftsection>
<nextsent>the methodological basis of these studies are lexical cohesion indicators (morris &amp; hirst, 1991) <papid> J91-1002 </papid>combined with word-level co-occurrence statis- tics.</nextsent>
<nextsent>since the labelling is one-dimensional, this approxi-mates our use of preferred centers of discourse segments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W264">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>centered segmentation has also the additional advantage of restricting the search space of anaphoric antecedents tothose discourse ntities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory.
</prevsent>
<prevsent>text segmentation procedures (more with an informa-tion retrieval motivation, rather than being related to ref-erence resolution tasks) have also been proposed for coarse-grained partitioning of texts into contiguous, non-overlapping blocks and assigning content labels to these blocks (hearst, 1994).<papid> P94-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
the methodological basis of these studies are lexical cohesion indicators (morris &amp; hirst, 1991) <papid> J91-1002 </papid>combined with word-level co-occurrence statis- tics.</citsent>
<aftsection>
<nextsent>since the labelling is one-dimensional, this approxi-mates our use of preferred centers of discourse segments.
</nextsent>
<nextsent>these studies, however, lack the fine-grained informa-tion of the contents of cf lists also needed for proper reference resolution.
</nextsent>
<nextsent>finally, many studies on discourse segmentation high- light the role of cue words for signaling segment bound-aries (cf., e.g., the discussion in passonneau &amp; litman (1993)).<papid> P93-1020 </papid></nextsent>
<nextsent>however useful this strategy might be, we see the danger that such surface-level description may actu-ally hide structural regularities at deeper levels of inves-tigation illustrated by access mechanisms for centering data at different levels of discourse segmentation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W265">
<title id=" P97-1014.xml">centered segmentation scaling up the centering model to global discourse structure </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>since the labelling is one-dimensional, this approxi-mates our use of preferred centers of discourse segments.
</prevsent>
<prevsent>these studies, however, lack the fine-grained informa-tion of the contents of cf lists also needed for proper reference resolution.
</prevsent>
</prevsection>
<citsent citstr=" P93-1020 ">
finally, many studies on discourse segmentation high- light the role of cue words for signaling segment bound-aries (cf., e.g., the discussion in passonneau &amp; litman (1993)).<papid> P93-1020 </papid></citsent>
<aftsection>
<nextsent>however useful this strategy might be, we see the danger that such surface-level description may actu-ally hide structural regularities at deeper levels of inves-tigation illustrated by access mechanisms for centering data at different levels of discourse segmentation.
</nextsent>
<nextsent>we have developed proposal for extending the cen-tering model to incorporate the global referential struc-ture of discourse for reference resolution.
</nextsent>
<nextsent>the hierarchy of discourse segments we compute realizes certain con-straints on the reach ability of antecedents.
</nextsent>
<nextsent>moreover, the claim is made that the hierarchy of discourse segments implements an intuitive notion of the limited attention constraint, as we avoid simplistic, cognitively implausi-ble linear backward search for potent ional discourse ref-erents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W266">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for reasons of affordability, their designers appear to have made no attempt to tackle the well-known problems in mt, such as how to ensure the lear nabil ity of correct ranslations and facilitate customiza-tion.
</prevsent>
<prevsent>as result, users are forced to see the same kinds of translation errors over and over again, ex-cept they in cases where they involve merely adding missing word or compound to user dictionary, or specifying one of several word-to-word translations as correct choice.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
there are several alternative approaches that might eventually liberate us from this limitation on the usability of mt systems: unification-based grammar for-malisms and lexical-semantics formalisms (see lfg (kaplan and bresnan, 1982), hpsg (pollard and sag, 1987), and generative lexicon (pustejovsky, 1991), <papid> J91-4003 </papid>for example) have been proposed to facili-tate computationally precise description of natural- language syntax and semantics.</citsent>
<aftsection>
<nextsent>it is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct rans- lations.
</nextsent>
<nextsent>practical implementation mt systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and method for building lexicon that is easy even for novices to use.
</nextsent>
<nextsent>corpus-based or example-based mt (sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991) <papid> P91-1024 </papid>and statisti-cal mt (brown et al, 1993) <papid> J93-2003 </papid>systems provide the easiest customizability, since users have only to sup-ply collection of source and target sentence pairs (a bilingual corpus).</nextsent>
<nextsent>two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial mt systems based on these approaches: ? can the system be used for various domains without showing severe degradation of transla-tion accuracy?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W267">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct rans- lations.
</prevsent>
<prevsent>practical implementation mt systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and method for building lexicon that is easy even for novices to use.
</prevsent>
</prevsection>
<citsent citstr=" C90-3044 ">
corpus-based or example-based mt (sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991) <papid> P91-1024 </papid>and statisti-cal mt (brown et al, 1993) <papid> J93-2003 </papid>systems provide the easiest customizability, since users have only to sup-ply collection of source and target sentence pairs (a bilingual corpus).</citsent>
<aftsection>
<nextsent>two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial mt systems based on these approaches: ? can the system be used for various domains without showing severe degradation of transla-tion accuracy?
</nextsent>
<nextsent>what is the minimum number of examples (or training data) required to achieve reasonable mt quality for new domain?
</nextsent>
<nextsent>tag-based mt (abeill~, schabes, and joshi, 1990) 1 and pattern-based translation (maruyama, 1993) share many important properties for successful implementation in practical mt systems, namely: ? the existence of polynomial-time parsing al-gorithm ? capability for describing larger domain of locality (schabes, abeill~, and joshi, 1988) ? synchronization (shieber and schabes, 1990) <papid> C90-3045 </papid>of the source and target language structures readers should note, however, that the pars- 1 see ltag (schabes, abeiu~, and joshi, 1988) (lex-.</nextsent>
<nextsent>icalized tag) and stag (shieber and schabes, 1990) <papid> C90-3045 </papid>(synchronized tag) for each member of the tag (tree adjoining grammar) family.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W268">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct rans- lations.
</prevsent>
<prevsent>practical implementation mt systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and method for building lexicon that is easy even for novices to use.
</prevsent>
</prevsection>
<citsent citstr=" P91-1024 ">
corpus-based or example-based mt (sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991) <papid> P91-1024 </papid>and statisti-cal mt (brown et al, 1993) <papid> J93-2003 </papid>systems provide the easiest customizability, since users have only to sup-ply collection of source and target sentence pairs (a bilingual corpus).</citsent>
<aftsection>
<nextsent>two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial mt systems based on these approaches: ? can the system be used for various domains without showing severe degradation of transla-tion accuracy?
</nextsent>
<nextsent>what is the minimum number of examples (or training data) required to achieve reasonable mt quality for new domain?
</nextsent>
<nextsent>tag-based mt (abeill~, schabes, and joshi, 1990) 1 and pattern-based translation (maruyama, 1993) share many important properties for successful implementation in practical mt systems, namely: ? the existence of polynomial-time parsing al-gorithm ? capability for describing larger domain of locality (schabes, abeill~, and joshi, 1988) ? synchronization (shieber and schabes, 1990) <papid> C90-3045 </papid>of the source and target language structures readers should note, however, that the pars- 1 see ltag (schabes, abeiu~, and joshi, 1988) (lex-.</nextsent>
<nextsent>icalized tag) and stag (shieber and schabes, 1990) <papid> C90-3045 </papid>(synchronized tag) for each member of the tag (tree adjoining grammar) family.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W269">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is possible that, with the descriptive power of these grammars and lexicons, individual usages of words and phrases may be defined specifically enough to give correct rans- lations.
</prevsent>
<prevsent>practical implementation mt systems based on these formalisms, on the other hand, would not be possible without much more efficient parsing and disambiguation algorithms for these formalisms and method for building lexicon that is easy even for novices to use.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
corpus-based or example-based mt (sato and nagao, 1990; <papid> C90-3044 </papid>sumita and iida, 1991) <papid> P91-1024 </papid>and statisti-cal mt (brown et al, 1993) <papid> J93-2003 </papid>systems provide the easiest customizability, since users have only to sup-ply collection of source and target sentence pairs (a bilingual corpus).</citsent>
<aftsection>
<nextsent>two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial mt systems based on these approaches: ? can the system be used for various domains without showing severe degradation of transla-tion accuracy?
</nextsent>
<nextsent>what is the minimum number of examples (or training data) required to achieve reasonable mt quality for new domain?
</nextsent>
<nextsent>tag-based mt (abeill~, schabes, and joshi, 1990) 1 and pattern-based translation (maruyama, 1993) share many important properties for successful implementation in practical mt systems, namely: ? the existence of polynomial-time parsing al-gorithm ? capability for describing larger domain of locality (schabes, abeill~, and joshi, 1988) ? synchronization (shieber and schabes, 1990) <papid> C90-3045 </papid>of the source and target language structures readers should note, however, that the pars- 1 see ltag (schabes, abeiu~, and joshi, 1988) (lex-.</nextsent>
<nextsent>icalized tag) and stag (shieber and schabes, 1990) <papid> C90-3045 </papid>(synchronized tag) for each member of the tag (tree adjoining grammar) family.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W270">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>two open questions, however, have yet to be satisfactorily answered before we can confidently build commercial mt systems based on these approaches: ? can the system be used for various domains without showing severe degradation of transla-tion accuracy?
</prevsent>
<prevsent>what is the minimum number of examples (or training data) required to achieve reasonable mt quality for new domain?
</prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
tag-based mt (abeill~, schabes, and joshi, 1990) 1 and pattern-based translation (maruyama, 1993) share many important properties for successful implementation in practical mt systems, namely: ? the existence of polynomial-time parsing al-gorithm ? capability for describing larger domain of locality (schabes, abeill~, and joshi, 1988) ? synchronization (shieber and schabes, 1990) <papid> C90-3045 </papid>of the source and target language structures readers should note, however, that the pars- 1 see ltag (schabes, abeiu~, and joshi, 1988) (lex-.</citsent>
<aftsection>
<nextsent>icalized tag) and stag (shieber and schabes, 1990) <papid> C90-3045 </papid>(synchronized tag) for each member of the tag (tree adjoining grammar) family.</nextsent>
<nextsent>144 ing algorithm for tags has o(igin6) 2 worst case time complexity (vijay-shanker, 1987), and that the  patterns  in maruyama approach are merely context-free grammar (cfg) rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W272">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> pattern -based  context -f ree.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>l ink const ra in ts : nonterminal symbols in source and target cfg rules are linked if they 2where \]g\] stands for the size of grammar g, and is the length of an input string.
</prevsent>
</prevsection>
<citsent citstr=" J95-4002 ">
3lexicalized cfg, or tree insertion grammar (tig) (schabes and waters, 1995), <papid> J95-4002 </papid>has been recently intro-duced to achieve such efficiency and lexicalization.</citsent>
<aftsection>
<nextsent>4and its inflectional variants - - we will discuss inflec-tions and agreement issues later.
</nextsent>
<nextsent>5the meaning of the word  synchronized  here is ex-actly the same as in stag (shieber and schabes, 1990).<papid> C90-3045 </papid></nextsent>
<nextsent>see also bilingual signs (tsujii and fujita, 1991) <papid> E91-1048 </papid>for discussion of the importance of combining the appropri-ate domain of locality and synchronization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W274">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> pattern -based  context -f ree.  </section>
<citcontext>
<prevsection>
<prevsent>4and its inflectional variants - - we will discuss inflec-tions and agreement issues later.
</prevsent>
<prevsent>5the meaning of the word  synchronized  here is ex-actly the same as in stag (shieber and schabes, 1990).<papid> C90-3045 </papid></prevsent>
</prevsection>
<citsent citstr=" E91-1048 ">
see also bilingual signs (tsujii and fujita, 1991) <papid> E91-1048 </papid>for discussion of the importance of combining the appropri-ate domain of locality and synchronization.</citsent>
<aftsection>
<nextsent>are given the same index  :i .
</nextsent>
<nextsent>linked nonter-minal must be derived from sequence of syn-chronized pairs.
</nextsent>
<nextsent>thus, the first np (np:i) in the source rule corresponds to the second np (np:i) in the target rule, the vs in both rules correspond to each other, and the second np (np:3) in the source rule corresponds to the first np (np:3) in the target rule.
</nextsent>
<nextsent>the source and target rules are called cfg skele-ton of the pattern.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W275">
<title id=" P96-1020.xml">pattern based context free grammars for machine translation </title>
<section> for each pre terminal rule.  </section>
<citcontext>
<prevsection>
<prevsent>in 1our prototype implementation was based on the earley algorithm, since this does not require lexicaliza-tion of cfg rules.
</prevsent>
<prevsent>this sense, the entire translation algorithm is not guaranteed to run in polynomial time.
</prevsent>
</prevsection>
<citsent citstr=" A83-1015 ">
practically, timeout mechanism and process for recovery from unsuccessful translation (e.g., applying the idea of fitted parse (jensen and heidorn, 1983) <papid> A83-1015 </papid>to target cfg rules) should be incorporated into the transla-tion algorithm.</citsent>
<aftsection>
<nextsent>some restrictions on patterns must be imposed to avoid infinitely many ambiguities and arbitrarily long translations.
</nextsent>
<nextsent>the following patterns are there-fore not allowed: 1.
</nextsent>
<nextsent>a - - *xy~- -b 2.
</nextsent>
<nextsent>a + y ~-c1 . . .b . . .c~ if there is cycle of synchronized erivation such that a--+ . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W276">
<title id=" P98-2132.xml">lilfes  towards a practical hpsg parser </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>words are often ambiguous in terms of their part of speech (pos).
</prevsent>
<prevsent>pos tagging disam- biguates them, i.e., it assigns to each word the correct pos in the context of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" H90-1055 ">
several kinds of pos taggers using rule-based (e.g., brill et al, 1990), <papid> H90-1055 </papid>statistical (e.g., meri-aldo, 1994), <papid> J94-2001 </papid>memory-based (e.g., daelemans, 1996), and neural network (e.g., schmid, 1994) <papid> C94-1027 </papid>models have been proposed for some languages.</citsent>
<aftsection>
<nextsent>the correct rate of tagging of these models has reached 95%, in part by using very large amount of training data (e.g., 1,000,000 words in schmid, 1994).<papid> C94-1027 </papid></nextsent>
<nextsent>for many other languages (e.g., thai, which we deal with in this paper), however, the corpora have not been prepared and there is not large amount of training data available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W277">
<title id=" P98-2132.xml">lilfes  towards a practical hpsg parser </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>words are often ambiguous in terms of their part of speech (pos).
</prevsent>
<prevsent>pos tagging disam- biguates them, i.e., it assigns to each word the correct pos in the context of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
several kinds of pos taggers using rule-based (e.g., brill et al, 1990), <papid> H90-1055 </papid>statistical (e.g., meri-aldo, 1994), <papid> J94-2001 </papid>memory-based (e.g., daelemans, 1996), and neural network (e.g., schmid, 1994) <papid> C94-1027 </papid>models have been proposed for some languages.</citsent>
<aftsection>
<nextsent>the correct rate of tagging of these models has reached 95%, in part by using very large amount of training data (e.g., 1,000,000 words in schmid, 1994).<papid> C94-1027 </papid></nextsent>
<nextsent>for many other languages (e.g., thai, which we deal with in this paper), however, the corpora have not been prepared and there is not large amount of training data available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W278">
<title id=" P98-2132.xml">lilfes  towards a practical hpsg parser </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>words are often ambiguous in terms of their part of speech (pos).
</prevsent>
<prevsent>pos tagging disam- biguates them, i.e., it assigns to each word the correct pos in the context of the sentence.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
several kinds of pos taggers using rule-based (e.g., brill et al, 1990), <papid> H90-1055 </papid>statistical (e.g., meri-aldo, 1994), <papid> J94-2001 </papid>memory-based (e.g., daelemans, 1996), and neural network (e.g., schmid, 1994) <papid> C94-1027 </papid>models have been proposed for some languages.</citsent>
<aftsection>
<nextsent>the correct rate of tagging of these models has reached 95%, in part by using very large amount of training data (e.g., 1,000,000 words in schmid, 1994).<papid> C94-1027 </papid></nextsent>
<nextsent>for many other languages (e.g., thai, which we deal with in this paper), however, the corpora have not been prepared and there is not large amount of training data available.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W283">
<title id=" P98-1014.xml">processing unknown words in hpsg </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it focusses on extracting linguistic proper-ties, as compared to e.g. general concept learning (hahn, klenner &amp; schnattinger 1996).
</prevsent>
<prevsent>unlike er- bach (1990), however, it is not confined to sim-ple morpho-syntactic information but can also han-dle selectional restrictions, emantic types and argu-ment structure.
</prevsent>
</prevsection>
<citsent citstr=" P91-1027 ">
finally, while statistical approaches like brent (1991) <papid> P91-1027 </papid>can gather e.g. valence informa-tion from large corpora, we are more interested in full grammatical processing of individual sentences to maximally exploit each context.</citsent>
<aftsection>
<nextsent>the following three goals serve to structure our model.
</nextsent>
<nextsent>it should i) incorporate gradual, information-based conceptualization  unknown-ness .
</nextsent>
<nextsent>words are not unknown as whole, but may contain unlmown, i.e. revisable pieces of infor-mation.
</nextsent>
<nextsent>consequently, even known words can un-dergo revision to e.g. acquire new senses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W284">
<title id=" P95-1029.xml">using higher order logic programming for semantic interpretation of coordinate constructs </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>likewise, bill gets the lf ap.(p bi l ) , and coordination results in the fol-lowing lf for john and bill: (2) ap.(and (p john) (p bill)) when (2) is applied to the predicate, (15) will re-sult after 13-reduction.
</prevsent>
<prevsent>however, under first-order unification, this needs to simulated by having the variable in az.run(z) unify both with bill and john, and this is not possible.
</prevsent>
</prevsection>
<citsent citstr=" P89-1005 ">
see (jowsey, 1990) and (moore, 1989) <papid> P89-1005 </papid>for thorough discussion.</citsent>
<aftsection>
<nextsent>(moore, 1989) <papid> P89-1005 </papid>suggests that the way to overcome this problem is to use explicit a-terms and encode /~-reduction to perform the needed reduction.</nextsent>
<nextsent>for example, the logical form in (3) would be produced, where x\rtm(x) is the representation az.run (z).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W288">
<title id=" P95-1029.xml">using higher order logic programming for semantic interpretation of coordinate constructs </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>also, if at some point it was desired to determine if the semantic forms of two different sentences were the same, predicate would be needed to compare two lambda forms for a-equivalence, which again is not simple task.
</prevsent>
<prevsent>es-sentially, the logic variable is meant be inter-preted as bound variable, which requires an addi-tional ayer of programming.
</prevsent>
</prevsection>
<citsent citstr=" P92-1027 ">
213 (park, 1992) <papid> P92-1027 </papid>proposes asolution within first-order unification that can handle not only sentence (la), but also more complex examples with determiners.</citsent>
<aftsection>
<nextsent>the method used is to introduce spurious bindings that subsequently get removed.
</nextsent>
<nextsent>for example, the semantics of (4a) would be (4b), which would then get simplified to (4c).
</nextsent>
<nextsent>(4a) farmer and every senator talk (4b) exists(x1 , fanaer( i1) a( exists (x2, (x2=xl) atazk (x2)) ) ) &f; ora lc l3 , senator (x3) =  (exists (x2, (12=13) &talk; (x2)) ) ) (4c) exists (xl,fanaercxl)ktalk(xl)) &forall; (13, senator (13) = talk (13)) while this pushes first-order unification beyond what it had been previously shown capable of, there are two disadvantages to this technique: (1) for ev-ery possible category that can be conjoined, sepa-rate lexical entry for and is required, and (2) as the conjoinable categories become more complex, the and entries become correspondingly more complex and greatly obscure the theoretical background of the grammar formalism.
</nextsent>
<nextsent>the fundamental problem in both cases is that the concept of free and bound occurrences of variables is not supported by prolog, but instead needs to be implemented by additional programming.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W289">
<title id=" P95-1029.xml">using higher order logic programming for semantic interpretation of coordinate constructs </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the solution given in this paper is to use higher-order logic programming language, aprolog, that already implements these concepts, called  abstract syntax  in (miller, 1991) and  higher-order abstract syntax  in (pfenning and elliot, 1988).
</prevsent>
<prevsent>this allows natural and elegant im-plementation of the grammatical theory, with only one lexical entry for and.
</prevsent>
</prevsection>
<citsent citstr=" P86-1037 ">
this paper is meant be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics - see, for example, (miller &amp; nadathur, 1986), (<papid> P86-1037 </papid>pareschi, 1989), and (pereira, 1990).</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W290">
<title id=" P98-1003.xml">towards a single proposal in spelling correction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>constraint grammar (karlsson et al 1995) was chosen to represent syntagmatic knowledge.
</prevsent>
<prevsent>its use as part of speech tagger for english has been highly successful.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
conceptual density (agirre and rigau 1996) <papid> C96-1005 </papid>is the paradigmatic omponent chosen to discriminate semantically among potential noun corrections.</citsent>
<aftsection>
<nextsent>this technique measures  affinity distance  between ouns using wordnet (miller 1990).
</nextsent>
<nextsent>finally, general and document word-occurrence frequency-rates complete the set of knowledge sources combined.
</nextsent>
<nextsent>we knowingly did not use any model of common misspellings, the main reason being that we did not want to use knowledge about he error source.
</nextsent>
<nextsent>this work focuses on language models, not error models (typing errors, common misspellings, ocr mistakes, peech recognition mistakes, etc.).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W291">
<title id=" P98-1003.xml">towards a single proposal in spelling correction </title>
<section> comparison  with other  context-.  </section>
<citcontext>
<prevsection>
<prevsent>although no quantitative sults are given, this is in accord with using document and general frequencies.
</prevsent>
<prevsent>mays et al (1991) present he initial success of applying word trigram conditional probabilities to the problem of context based detection and correction of real-word errors.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
yarowsky (1994) <papid> P94-1013 </papid>experiments with the use of decision lists for lexical ambiguity resolution, using context features like local syntactic patterns and collocational information, so that multiple types of evidence are considered in the context of an ambiguous word.</citsent>
<aftsection>
<nextsent>in addition to word-forms, the patterns involve pos tags and lemmas.
</nextsent>
<nextsent>the algorithm is evaluated in missing accent restoration task for spanish and french text, against predefined set of few words giving an accuracy over 99%.
</nextsent>
<nextsent>golding and schabes (1996) <papid> P96-1010 </papid>propose hybrid method that combines part-of-speech trigrams and context features in order to detect and correct real- word errors.</nextsent>
<nextsent>they present an experiment where their system has substantially higher performance than the grammar checker in ms word, but its coverage is limited to eighteen particular confusion sets composed by two or three similar words (e.g.: weather, whether).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W292">
<title id=" P98-1003.xml">towards a single proposal in spelling correction </title>
<section> comparison  with other  context-.  </section>
<citcontext>
<prevsection>
<prevsent>in addition to word-forms, the patterns involve pos tags and lemmas.
</prevsent>
<prevsent>the algorithm is evaluated in missing accent restoration task for spanish and french text, against predefined set of few words giving an accuracy over 99%.
</prevsent>
</prevsection>
<citsent citstr=" P96-1010 ">
golding and schabes (1996) <papid> P96-1010 </papid>propose hybrid method that combines part-of-speech trigrams and context features in order to detect and correct real- word errors.</citsent>
<aftsection>
<nextsent>they present an experiment where their system has substantially higher performance than the grammar checker in ms word, but its coverage is limited to eighteen particular confusion sets composed by two or three similar words (e.g.: weather, whether).
</nextsent>
<nextsent>27 the last three systems relyon previously collected set of confusion sets (sets of similar words or accentuation ambiguities).
</nextsent>
<nextsent>on the contrary, our system has to choose single proposal for any possible spelling error, and it is therefore impossible to collect the confusion sets (i.e. sets of proposals for each spelling error) beforehand.
</nextsent>
<nextsent>we also need to correct as many errors as possible, even if the amount of data for particular case is scarce.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W293">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another application of automatically extracted similar words is to help solve the problem of data sparseness in statistical natural language process-ing (dagan et al, 1994; essen and stein biss, 1992).
</prevsent>
<prevsent>when the frequency of word does not warrant reli-able maximum likelihood estimation, its probability can be computed as weighted sum of the probabil-ities of words that are similar to it.
</prevsent>
</prevsection>
<citsent citstr=" P97-1008 ">
it was shown in (dagan et al, 1997) <papid> P97-1008 </papid>that similarity-based smooth-ing method achieved much better esults than back- off smoothing methods in word sense disambigua- tion.</citsent>
<aftsection>
<nextsent>the remainder of the paper is organized as fol-lows.
</nextsent>
<nextsent>the next section is concerned with similari-ties between words based on their distributional pat-terns.
</nextsent>
<nextsent>the similarity measure can then be used to create thesaurus.
</nextsent>
<nextsent>in section 3, we evaluate the constructed thesauri by computing the similarity be-tween their entries and entries in manually created thesauri.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W294">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> word  imi lar ty.  </section>
<citcontext>
<prevsection>
<prevsent>finally, section 5 reviews related work and summarize our contributions.
</prevsent>
<prevsent>768
</prevsent>
</prevsection>
<citsent citstr=" P97-1009 ">
our similarity measure is based on proposal in (lin, 1997), <papid> P97-1009 </papid>where the similarity between two ob-jects is defined to be the amount of information con-tained in the commonality between the objects di-vided by the amount of information in the descrip-tions of the objects.</citsent>
<aftsection>
<nextsent>we use broad-coverage parser (lin, 1993; <papid> P93-1016 </papid>lin, 1994) <papid> C94-1079 </papid>to extract dependency triples from the text corpus.</nextsent>
<nextsent>a dependency triple consists of two words and the grammatical relationship between them in the input sentence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W295">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> word  imi lar ty.  </section>
<citcontext>
<prevsection>
<prevsent>768
</prevsent>
<prevsent>our similarity measure is based on proposal in (lin, 1997), <papid> P97-1009 </papid>where the similarity between two ob-jects is defined to be the amount of information con-tained in the commonality between the objects di-vided by the amount of information in the descrip-tions of the objects.</prevsent>
</prevsection>
<citsent citstr=" P93-1016 ">
we use broad-coverage parser (lin, 1993; <papid> P93-1016 </papid>lin, 1994) <papid> C94-1079 </papid>to extract dependency triples from the text corpus.</citsent>
<aftsection>
<nextsent>a dependency triple consists of two words and the grammatical relationship between them in the input sentence.
</nextsent>
<nextsent>for example, the triples ex-tracted from the sentence  have brown dog  are: (2) (have subj i), (i subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) we use the notation iiw, r, ll to denote the fre-quency count of the dependency triple (w, r, ~) in the parsed corpus.
</nextsent>
<nextsent>when w, r, or ~ is the wild card (*), the frequency counts of all the depen-dency triples that matches the rest of the pattern are summed up.
</nextsent>
<nextsent>for example, ilcook, obj, *11 is the to-tal occurrences of cook-object relationships in the parsed corpus, and i1., *, *11 is the total number of dependency triples extracted from the parsed cor-pus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W296">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> word  imi lar ty.  </section>
<citcontext>
<prevsection>
<prevsent>768
</prevsent>
<prevsent>our similarity measure is based on proposal in (lin, 1997), <papid> P97-1009 </papid>where the similarity between two ob-jects is defined to be the amount of information con-tained in the commonality between the objects di-vided by the amount of information in the descrip-tions of the objects.</prevsent>
</prevsection>
<citsent citstr=" C94-1079 ">
we use broad-coverage parser (lin, 1993; <papid> P93-1016 </papid>lin, 1994) <papid> C94-1079 </papid>to extract dependency triples from the text corpus.</citsent>
<aftsection>
<nextsent>a dependency triple consists of two words and the grammatical relationship between them in the input sentence.
</nextsent>
<nextsent>for example, the triples ex-tracted from the sentence  have brown dog  are: (2) (have subj i), (i subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) we use the notation iiw, r, ll to denote the fre-quency count of the dependency triple (w, r, ~) in the parsed corpus.
</nextsent>
<nextsent>when w, r, or ~ is the wild card (*), the frequency counts of all the depen-dency triples that matches the rest of the pattern are summed up.
</nextsent>
<nextsent>for example, ilcook, obj, *11 is the to-tal occurrences of cook-object relationships in the parsed corpus, and i1., *, *11 is the total number of dependency triples extracted from the parsed cor-pus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W297">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> word  imi lar ty.  </section>
<citcontext>
<prevsection>
<prevsent>we then mea-sure the amount of information in the same state-ment when we do know the value of ii w, r,  ii.
</prevsent>
<prevsent>the difference between these two amounts is taken to be the information contained in hw, r,  \[l=c. an occurrence of dependency triple (w, r, ) can be regarded as the co-occurrence of three events: a: randomly selected word is w; b: randomly selected ependency pe is r; c: randomly selected word is ~.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
when the value of ilw, r,w ll is unknown, we assume that and are conditionally indepen-dent given b. the probability of a, and co-occurring is estimated by pmle( ) pmle( a\[b )pmle( c\[b ), where pmle is the maximum likelihood estimation of probability distribution and p.le(b) = i * , * , * l   p.,~e(aib ) = i* ,~,* l   p, le(cib) = when the value of hw, r, w~h is known, we can obtain pmle(a, b, c) directly: pmle(a, b, c) = \[\[w, r, wll/\[\[*, , *h let (w,r ,w ~) denote the amount information contained in hw, r,w~\]\]=c. its value can be corn- 769 simgindze(wl, w2) = ~ ~(r,w)etcwl)ntcw2)are{subj.of.obj-of} min(i(wl, r, w), i(w2, r, w) ) simhindte, (wl, w2) = ~,(r,w)et(w,)nt(w2) min(i(wl, r, w), i(w2, r, w)) \]t(wl)nt(w2)i simcosine(wl,w2) = x/iz(w~)llz(w2)l 2x it(wl)nz(w2)l simdice(wl, w2) = it(wl)l+lt(w2) simjacard (wl, w2) = t(wl )ot(w2)l t(wl) + t(w2)l-it(wl)rlt(w2)l figure 1: other similarity measures puted as follows: i(w,r,w ) = _ iog(pmle(b)pmle(a\]b)pmle(cib)) --(-- log pmle(a, b, c)) - log iiw,r,wflll*,r,*ll - - iiw,r,*ll xll*,r,w ll it is worth noting that i(w,r,w ) is equal to the mutual information between and  (hindle, 1990).<papid> P90-1034 </papid></citsent>
<aftsection>
<nextsent>let t(w) be the set of pairs (r, ) such that log iw w lrll* *ll is positive.
</nextsent>
<nextsent>we define the sim- wlr~* *~r~w ! ilarity sim(wl, w2) between two words wl and w2 as follows: ) ~(r,w)et(w, )nt(w~)(i(wl, w) + i(w2, r, w) ) ~-,(r,w)et(wl) i(wl, r, w) q- ~(r,w)et(w2) i(w2, r, w) we parsed 64-million-word corpus consisting of the wall street journal (24 million words), san jose mercury (21 million words) and ap newswire (19 million words).
</nextsent>
<nextsent>from the parsed corpus, we extracted 56.5 million dependency triples (8.7 mil-lion unique).
</nextsent>
<nextsent>in the parsed corpus, there are 5469 nouns, 2173 verbs, and 2632 adjectives/adverbs that occurred at least 100 times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W302">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> related work  and  conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>evaluation of automatically generated lexical re-sources is difficult problem.
</prevsent>
<prevsent>in (hindle, 1990), <papid> P90-1034 </papid>small set of sample results are presented.</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
in (smadja, 1993), <papid> J93-1007 </papid>automatically extracted colloca-tions are judged by lexicographer.</citsent>
<aftsection>
<nextsent>in (dagan et al., 1993) <papid> P93-1022 </papid>and (pereira et al, ! 993), clusters of sim-ilar words are evaluated by how well they are able to recover data items that are removed from the in- put corpus one at time.</nextsent>
<nextsent>in (alshawi and carter, 1994), <papid> J94-4005 </papid>the collocations and their associated scores were evaluated indirectly by their use in parse tree selection.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W303">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> related work  and  conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>in (hindle, 1990), <papid> P90-1034 </papid>small set of sample results are presented.</prevsent>
<prevsent>in (smadja, 1993), <papid> J93-1007 </papid>automatically extracted colloca-tions are judged by lexicographer.</prevsent>
</prevsection>
<citsent citstr=" P93-1022 ">
in (dagan et al., 1993) <papid> P93-1022 </papid>and (pereira et al, ! 993), clusters of sim-ilar words are evaluated by how well they are able to recover data items that are removed from the in- put corpus one at time.</citsent>
<aftsection>
<nextsent>in (alshawi and carter, 1994), <papid> J94-4005 </papid>the collocations and their associated scores were evaluated indirectly by their use in parse tree selection.</nextsent>
<nextsent>the merits of different measures for as-sociation strength are judged by the differences they make in the precision and the recall of the parser outputs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W304">
<title id=" P98-2127.xml">automatic retrieval and clustering of similar words </title>
<section> related work  and  conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>in (smadja, 1993), <papid> J93-1007 </papid>automatically extracted colloca-tions are judged by lexicographer.</prevsent>
<prevsent>in (dagan et al., 1993) <papid> P93-1022 </papid>and (pereira et al, ! 993), clusters of sim-ilar words are evaluated by how well they are able to recover data items that are removed from the in- put corpus one at time.</prevsent>
</prevsection>
<citsent citstr=" J94-4005 ">
in (alshawi and carter, 1994), <papid> J94-4005 </papid>the collocations and their associated scores were evaluated indirectly by their use in parse tree selection.</citsent>
<aftsection>
<nextsent>the merits of different measures for as-sociation strength are judged by the differences they make in the precision and the recall of the parser outputs.
</nextsent>
<nextsent>the main contribution of this paper is new eval-uation methodology for automatically constructed thesaurus.
</nextsent>
<nextsent>while previous methods relyon indirect tasks or subjective judgments, our method allows direct and objective comparison between automati-cally and manually constructed thesauri.
</nextsent>
<nextsent>the results show that our automatically created thesaurus sig-nificantly closer to wordnet than roget thesaurus is. our experiments also surpasses previous experi-ments on automatic thesaurus construction scale and (possibly) accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W305">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>at the same time, it exploits linguis-tically motivated, eclarative specifications of the discourse functions of syntactic onstruc- tions to make contextually appropriate syntac-tic choices.
</prevsent>
<prevsent>since (meteer, 1991), researchers in natural anguage generation have recognized the need to refine andre- organize content after the rhetorical organization of ar-guments and before the syntactic realization of phrases.
</prevsent>
</prevsection>
<citsent citstr=" A92-1006 ">
this process has been named sentence planning (ram- bow and korelsky, 1992).<papid> A92-1006 </papid></citsent>
<aftsection>
<nextsent>broadly speaking, it involves aggregating content into sentence-sized units, and then selecting the lexical and syntactic elements that are used in realizing each sentence.
</nextsent>
<nextsent>here, we consider this second process.
</nextsent>
<nextsent>the challenge lies in integrating constraints from syn-tax, semantics and pragmatics.
</nextsent>
<nextsent>although most generation systems pipeline decisions (reiter, 1994), <papid> W94-0319 </papid>we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W306">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>here, we consider this second process.
</prevsent>
<prevsent>the challenge lies in integrating constraints from syn-tax, semantics and pragmatics.
</prevsent>
</prevsection>
<citsent citstr=" W94-0319 ">
although most generation systems pipeline decisions (reiter, 1994), <papid> W94-0319 </papid>we believe the most efficient and flexible way to integrate constraints in sentence planning is to synchronize the decisions.</citsent>
<aftsection>
<nextsent>in this paper, we provide natural framework for dealing with interactions and ensuring contextually appropriate output in single pass.
</nextsent>
<nextsent>as in (yang et al, 1991), lex-icalized tree adjoining grammar (ltag) provides an *the authors thank aravind joshi, mark steedman, martha palmer, ellen prince, owen rambow, mike white, betty birner, and the participants of inlg96 for their helpful com-ments on various incarnations of this work.
</nextsent>
<nextsent>this work has been supported by nsf and ircs graduate llowships, nsf grant nsf-stc sbr 8920230, arpa grant n00014-94 and art grant daah04-94-g0426.
</nextsent>
<nextsent>abstraction of the combinatorial properties of words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W307">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> sentences as referring expressions.  </section>
<citcontext>
<prevsection>
<prevsent>given sufficiently rich logical language, the meaning of natural anguage sentence can be represented as description this sense, by assuming sentences refer to entities in discourse model, cf.
</prevsent>
<prevsent>alternative semantics (karttunen and peters, 1979; rooth, 1985).
</prevsent>
</prevsection>
<citsent citstr=" P86-1029 ">
pragmatic analyses of referring expressions model speakers as planning those expressions to achieve sev-eral different kinds of intentions (donellan, 1966; appelt, 198 1985; kronfeld, 1986).<papid> P86-1029 </papid></citsent>
<aftsection>
<nextsent>given set of entities to describe and set of intentions to achieve in describing them, plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied.
</nextsent>
<nextsent>recent work on generating definite referring nps (reiter, 1991 ; dale and haddock, 1991; reiter and dale, 1992; <papid> C92-1038 </papid>horacek, 1995) has emphasized how circumscribed in- stantiations of this procedure can exploit linguistic con-text and convention to arrive quickly at short, unambigu-ous descriptions.</nextsent>
<nextsent>for example, (reiter and dale, 1992) <papid> C92-1038 </papid>apply generalizations about the salience of properties of objects and conventions about what words make base- level attributions to incrementally select words for inclu-sion in description.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W308">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> sentences as referring expressions.  </section>
<citcontext>
<prevsection>
<prevsent>pragmatic analyses of referring expressions model speakers as planning those expressions to achieve sev-eral different kinds of intentions (donellan, 1966; appelt, 198 1985; kronfeld, 1986).<papid> P86-1029 </papid></prevsent>
<prevsent>given set of entities to describe and set of intentions to achieve in describing them, plan is constructed by applying operators that enrich the content of the description until all intentions are satisfied.</prevsent>
</prevsection>
<citsent citstr=" C92-1038 ">
recent work on generating definite referring nps (reiter, 1991 ; dale and haddock, 1991; reiter and dale, 1992; <papid> C92-1038 </papid>horacek, 1995) has emphasized how circumscribed in- stantiations of this procedure can exploit linguistic con-text and convention to arrive quickly at short, unambigu-ous descriptions.</citsent>
<aftsection>
<nextsent>for example, (reiter and dale, 1992) <papid> C92-1038 </papid>apply generalizations about the salience of properties of objects and conventions about what words make base- level attributions to incrementally select words for inclu-sion in description.</nextsent>
<nextsent>(dale and haddock, 1991) use constraint network to represent the dis tractors described by complex referring np, and incrementally select property or relation that rules out as many alternatives as possible.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W310">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> sentences as referring expressions.  </section>
<citcontext>
<prevsection>
<prevsent>treating sentences as referring expressions allows us to encompass the strengths of many disparate proposals.
</prevsent>
<prevsent>incorporating material into descriptions of variety of entities until the addressee can infer desired conclusions allows the sentence planner to enrich input content, so that descriptions refer successfully (dale and haddock, 1991) or reduce it, to eliminate redundancy (mcdonald, 1992).
</prevsent>
</prevsection>
<citsent citstr=" W94-0316 ">
moreover, selecting alternatives on the basis of their syntactic, semantic, and pragmatic ontributions to the sentence using tag allows the sentence planner to choose words in tandem with appropriate syntax (yang et al., 1991), inflexible order (elhadad and robin, 1992), and, if necessary, in conventional combinations (smadja and mckeown, 1991; wanner, 1994).<papid> W94-0316 </papid></citsent>
<aftsection>
<nextsent>realizing this procedure requires declarative specifica-tion of three kinds of information: first, what operators are available and how they may combine; second, how operators pecify the content of description; and third, how operators achieve pragmatic effects.
</nextsent>
<nextsent>we represent operators as elementary trees in ltag, and use tag op-erations to combine them; we give the meaning of each tree as formula in an onto logically promiscuous rep-resentation language; and, we model the pragmatics of operators by associating with each tree set of discourse constraints describing when that operator can and should be used.
</nextsent>
<nextsent>other frameworks have the capability to make com-parable specifications; for example, hpsg (pollard and sag, 1994) feature structures describe syntax (subcat), semantics (conti3nt) and pragmatics (context).
</nextsent>
<nextsent>we choose tag because it enables local specification of syn-tactic dependencies in explicit constructions and flexibil-ity in incorporating modifiers; further, it is constrained grammar formalism with tractable computational proper-ties.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W311">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> linguistic specifications.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 semantics.
</prevsent>
<prevsent>we specify the semantics of trees by applying two prin-ciples to the ltag formalism.
</prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
first, we adopt an onto- logically promiscuous representation (hobbs, 1985) <papid> P85-1008 </papid>that includes wide variety of types of entities.</citsent>
<aftsection>
<nextsent>on- to logical promiscuity offers simple syntax-semantics interface.
</nextsent>
<nextsent>the meaning of tree is just the conjunction of the meanings of the elementary trees used to derive it, once appropriate parameters are recovered.
</nextsent>
<nextsent>such fiat se-mantics is enjoying resurgence in nlp; see (copestake et al, 1997) for an overview and formalism.
</nextsent>
<nextsent>second, we constrain these parameters syntactically, by labeling each syntactic node as supplying information about par-ticular entity or collection of entities, as in jackendoff x-bar semantics (jackendoff, 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W312">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> linguistic specifications.  </section>
<citcontext>
<prevsection>
<prevsent>: hayer vp:  1  np :  2  i /have/ during(r, having) ^ have(having, hayer, havee) (c) figure 2: ltag trees with semantic specifications figure 2.
</prevsent>
<prevsent>onto logical promiscuity makes it possible to explore more complicated analyses in this general frame-work.
</prevsent>
</prevsection>
<citsent citstr=" W96-0410 ">
for example, in (stone and doran, 1996), <papid> W96-0410 </papid>we use reference to properties, actions and belief contexts (bal- lim et al, 1991) to describe semantic ollocations (puste- jovsky, 1991) <papid> J91-4003 </papid>and idiomatic omposition (nunberg et al, 1994).</citsent>
<aftsection>
<nextsent>3.3 pragmatics.
</nextsent>
<nextsent>different constructions make different assumptions about the status of entities and propositions in the discourse, which we model by including in each tree specification of the contextual conditions under which use of the tree is pragmatically icensed.
</nextsent>
<nextsent>we have selected four repre-sentative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of particular theories.
</nextsent>
<nextsent>we use the following distinctions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W313">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> linguistic specifications.  </section>
<citcontext>
<prevsection>
<prevsent>: hayer vp:  1  np :  2  i /have/ during(r, having) ^ have(having, hayer, havee) (c) figure 2: ltag trees with semantic specifications figure 2.
</prevsent>
<prevsent>onto logical promiscuity makes it possible to explore more complicated analyses in this general frame-work.
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
for example, in (stone and doran, 1996), <papid> W96-0410 </papid>we use reference to properties, actions and belief contexts (bal- lim et al, 1991) to describe semantic ollocations (puste- jovsky, 1991) <papid> J91-4003 </papid>and idiomatic omposition (nunberg et al, 1994).</citsent>
<aftsection>
<nextsent>3.3 pragmatics.
</nextsent>
<nextsent>different constructions make different assumptions about the status of entities and propositions in the discourse, which we model by including in each tree specification of the contextual conditions under which use of the tree is pragmatically icensed.
</nextsent>
<nextsent>we have selected four repre-sentative pragmatic distinctions for our implementation; however, the framework does not commit one to the use of particular theories.
</nextsent>
<nextsent>we use the following distinctions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W314">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> linguistic specifications.  </section>
<citcontext>
<prevsection>
<prevsent>first, entities differ in newness (prince, 1981).
</prevsent>
<prevsent>at any point, an entity is ei-ther new or old to the hearer and either new or old to the discourse.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
second, entities differ in salience (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>salience assigns each entity position in partial order that indicates how accessible it is for reference in the current con-text.
</nextsent>
<nextsent>third, entities are related by salient partially- ordered set (poset) relations to other entities in the context (hirschberg, 1985).
</nextsent>
<nextsent>these relations include part and whole, subset and superset, and membership in common class.
</nextsent>
<nextsent>finally, the discourse may distinguish some open propositions (propositions containing free variables) as being under discussion (prince, 1986).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W315">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> linguistic specifications.  </section>
<citcontext>
<prevsection>
<prevsent>first, entities differ in newness (prince, 1981).
</prevsent>
<prevsent>at any point, an entity is ei-ther new or old to the hearer and either new or old to the discourse.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
second, entities differ in salience (grosz and sidner, 1986; <papid> J86-3001 </papid>grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>salience assigns each entity position in partial order that indicates how accessible it is for reference in the current con-text.
</nextsent>
<nextsent>third, entities are related by salient partially- ordered set (poset) relations to other entities in the context (hirschberg, 1985).
</nextsent>
<nextsent>these relations include part and whole, subset and superset, and membership in common class.
</nextsent>
<nextsent>finally, the discourse may distinguish some open propositions (propositions containing free variables) as being under discussion (prince, 1986).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W319">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> comparison with related work.  </section>
<citcontext>
<prevsection>
<prevsent>other researchers (nicolov et al, 1995; rubinoff, 1992) have assumed that this flexibility comes from mismatch between input content and grammatical options.
</prevsent>
<prevsent>in our system, such differences arise from the referential requirements and inferential opportunities that are encountered.
</prevsent>
</prevsection>
<citsent citstr=" P85-1012 ">
previous authors (mcdonald and pustejovsky, 1985; <papid> P85-1012 </papid>joshi, 1987) have noted that tag has many advantages for generation as syntactic formalism, because of its localization of argument structure.</citsent>
<aftsection>
<nextsent>(joshi, 1987) states that adjunction is powerful tool for elaborating descrip-tions.
</nextsent>
<nextsent>these aspects of tags are crucial to spud, as they are to (mcdonald and pustejovsky, 1985; <papid> P85-1012 </papid>joshi, 1987; yang et al, 1991; nicolov et al, 1995; wahlster et al, 1991; danlos, 1996).</nextsent>
<nextsent>what sets spud apart is its simul-taneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifica-tions for constructions it uses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W323">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> comparison with related work.  </section>
<citcontext>
<prevsection>
<prevsent>what sets spud apart is its simul-taneous construction of syntax and semantics, and the tripartite, lexicalized, declarative grammatical specifica-tions for constructions it uses.
</prevsent>
<prevsent>two contrasts hould be emphasized in this regard.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
(shieber et al, 1990; <papid> J90-1004 </papid>shieber and schabes, 1991) construct simultaneous derivation of syntax and semantics but they do not construct he semantics--it an input to their system.</citsent>
<aftsection>
<nextsent>(prevost and steedman, 1993; <papid> E93-1039 </papid>hoffman, 1994) <papid> W94-0314 </papid>represent syntax, se-mantics and pragmatics in lexicalized framework, but concentrate on information structure rather than the prag- matics of particular constructions.</nextsent>
<nextsent>most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (reiter, 1994).<papid> W94-0319 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W324">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> comparison with related work.  </section>
<citcontext>
<prevsection>
<prevsent>two contrasts hould be emphasized in this regard.
</prevsent>
<prevsent>(shieber et al, 1990; <papid> J90-1004 </papid>shieber and schabes, 1991) construct simultaneous derivation of syntax and semantics but they do not construct he semantics--it an input to their system.</prevsent>
</prevsection>
<citsent citstr=" E93-1039 ">
(prevost and steedman, 1993; <papid> E93-1039 </papid>hoffman, 1994) <papid> W94-0314 </papid>represent syntax, se-mantics and pragmatics in lexicalized framework, but concentrate on information structure rather than the prag- matics of particular constructions.</citsent>
<aftsection>
<nextsent>most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (reiter, 1994).<papid> W94-0319 </papid></nextsent>
<nextsent>with the fight formalism, constructing pragmatics, emantics and syntax simultaneously is easier and better.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W325">
<title id=" P97-1026.xml">sentence planning as description using tree adjoining grammar </title>
<section> comparison with related work.  </section>
<citcontext>
<prevsection>
<prevsent>two contrasts hould be emphasized in this regard.
</prevsent>
<prevsent>(shieber et al, 1990; <papid> J90-1004 </papid>shieber and schabes, 1991) construct simultaneous derivation of syntax and semantics but they do not construct he semantics--it an input to their system.</prevsent>
</prevsection>
<citsent citstr=" W94-0314 ">
(prevost and steedman, 1993; <papid> E93-1039 </papid>hoffman, 1994) <papid> W94-0314 </papid>represent syntax, se-mantics and pragmatics in lexicalized framework, but concentrate on information structure rather than the prag- matics of particular constructions.</citsent>
<aftsection>
<nextsent>most generation systems pipeline pragmatic, semantic, lexical and syntactic decisions (reiter, 1994).<papid> W94-0319 </papid></nextsent>
<nextsent>with the fight formalism, constructing pragmatics, emantics and syntax simultaneously is easier and better.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W327">
<title id=" P95-1002.xml">automatic induction of finite state transducers for simple phonological rules </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>this suggests that, the traditional for-malism of context-sensitive rewrite rules contains im-plicit generalizations about how phonological rules usu-ally work that are not present in the transducer system.
</prevsent>
<prevsent>we hope that further experimentation will lead to way of expressing this language bias in our induction system.
</prevsent>
</prevsection>
<citsent citstr=" P84-1070 ">
johnson (1984) <papid> P84-1070 </papid>gives one of the first computational l- gorithms for phonological rule induction.</citsent>
<aftsection>
<nextsent>his algorithm works for rules of the form (5) ---+ b/c where is the feature matrix of the segments around a. johnson algorithm sets up system of constraint equations which must satisfy, by considering both the positive contexts, i.e., all the contexts ci in which b occurs on the surface, as well as all the negative contexts cj in which an occurs on the surface.
</nextsent>
<nextsent>the set of all positive and negative contexts will not generally deter-mine unique rule, but will determine set of possible rules.
</nextsent>
<nextsent>touretzky et al (1990) extended johnson insight by using the version spaces algorithm of mitchell (1981) to induce phonological rules in their many maps architec-ture.
</nextsent>
<nextsent>like johnson s, their system looks at the underly-ing and surface realizations of single segments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W328">
<title id=" P96-1001.xml">higher order coloured unification and natural language semantics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we show that higher-order coloured unification - form of unification developed for automated theorem proving - provides general theory for modeling the interface between the interpretation process and other sources of linguistic, non semantic information.
</prevsent>
<prevsent>in particular, it pro-vides the general theory for the primary occurrence restriction which (dalrymple et al, 1991) analysis called for.
</prevsent>
</prevsection>
<citsent citstr=" C96-1073 ">
it is well known that higher-order unification (hou) can be used to construct he semantics of natural language: (dalrymple al., 1991) - hence-forth, dsp - show that it allows treatment ofvp- ellipsis which successfully captures the interaction of vpe with quantification and nominal anaphora; (pulman, 1995; gardent and kohlhase, 1996) <papid> C96-1073 </papid>use hou to model the interpretation of focus and its interaction with focus sensitive operators, adverbial quantifiers and second occurrence expressions; (gar- dent et al, 1996) shows that hou yields sim-ple but precise treatment of corrections; finally, (pinkal, 1995) uses linear hou to reconstruct under-specified semantic representations.</citsent>
<aftsection>
<nextsent>however, it is also well known that the hou approach to nl semantics ystematically over- generates and that some general theory of the in-terface between the interpretation process and other sources of linguistic information is needed in order to avoid this.
</nextsent>
<nextsent>in their treatment of vp-ellipsis, dsp introduce an informal restriction to avoid over-generation: the primary occurrence restriction (por).
</nextsent>
<nextsent>although this restriction is intuitive and linguistically well- motivated, it does not provide general theoretical framework for extra-semantic constraints.
</nextsent>
<nextsent>in this paper, we argue that higher-order coloured unification (hocu, (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W333">
<title id=" P98-1064.xml">anaphor resolution in unrestricted texts with partial parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our system can be included into the first approach.
</prevsent>
<prevsent>in these integrated approaches the semantic and domain knowledge information is very expensive in relation to computational processing.
</prevsent>
</prevsection>
<citsent citstr=" W97-1306 ">
as consequence, current anaphor resolution implementations mainly relyon constraints and preference heuristics which employ information originated from morphosyntactic or shallow semantic analysis, e.g. in baldwin (1997).<papid> W97-1306 </papid></citsent>
<aftsection>
<nextsent>these approaches, however, perform remarkably well.
</nextsent>
<nextsent>in lappin and leass (1994) <papid> J94-4002 </papid>it is described an algorithm for pronominal naphor resolution with high rate of correct analyses: 85%.</nextsent>
<nextsent>this one operates primarily on syntactic information only.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W334">
<title id=" P98-1064.xml">anaphor resolution in unrestricted texts with partial parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as consequence, current anaphor resolution implementations mainly relyon constraints and preference heuristics which employ information originated from morphosyntactic or shallow semantic analysis, e.g. in baldwin (1997).<papid> W97-1306 </papid></prevsent>
<prevsent>these approaches, however, perform remarkably well.</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
in lappin and leass (1994) <papid> J94-4002 </papid>it is described an algorithm for pronominal naphor resolution with high rate of correct analyses: 85%.</citsent>
<aftsection>
<nextsent>this one operates primarily on syntactic information only.
</nextsent>
<nextsent>in kennedy and boguraev (1996) <papid> C96-1021 </papid>it is proposed an algorithm for anaphor resolution which is modified and extended version of that developed by lappin and leass (1994).<papid> J94-4002 </papid></nextsent>
<nextsent>in contrast that work, this algorithm does not require in-depth, full, syntactic parsing of text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W335">
<title id=" P98-1064.xml">anaphor resolution in unrestricted texts with partial parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in lappin and leass (1994) <papid> J94-4002 </papid>it is described an algorithm for pronominal naphor resolution with high rate of correct analyses: 85%.</prevsent>
<prevsent>this one operates primarily on syntactic information only.</prevsent>
</prevsection>
<citsent citstr=" C96-1021 ">
in kennedy and boguraev (1996) <papid> C96-1021 </papid>it is proposed an algorithm for anaphor resolution which is modified and extended version of that developed by lappin and leass (1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>in contrast that work, this algorithm does not require in-depth, full, syntactic parsing of text.
</nextsent>
<nextsent>the modifications enable the resolution process to work from the output of pos tagger, enriched only with annotations of grammatical function of lexical items in the input text stream.
</nextsent>
<nextsent>the advantage of this algorithm is that anaphor esolution can be realized within nlp frameworks which do not -or cannot- employ robust and reliable parsing components.
</nextsent>
<nextsent>quantitative valuation shows the anaphor esolution algorithm described here to run at rate of 75% accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W339">
<title id=" P98-1064.xml">anaphor resolution in unrestricted texts with partial parsing </title>
<section> partial parsing with sug.  </section>
<citcontext>
<prevsection>
<prevsent>but in our approach we will automatically get the syntactic information from this grammar.
</prevsent>
<prevsent>the ss returned by the parser will consist of sequence of these constituents: pp, np, p, conj, verb and free words.
</prevsent>
</prevsection>
<citsent citstr=" E95-1041 ">
the attachments (e.g. of the pp) will be postponed to the module of resolution of nlp problems, which could work jointly with the algorithm for anaphor resolution (in similar way to the approach proposed in azzam (1995)).<papid> E95-1041 </papid></citsent>
<aftsection>
<nextsent>the free words will consist of constituents that are not covered by the grammar (e.g. adverbs) or words that are not important for the anaphor esolution.
</nextsent>
<nextsent>the output of the whole system will consist of sequence of the logical formulas of each constituent.
</nextsent>
<nextsent>here sentence will be the initial symbol of the grammar and the partial parsing will be applied with the rules shown in figure 4.
</nextsent>
<nextsent>if we want complete parsing, we just have to substitute these rules for the following: sentence ++  np, vp, and obviously we will have to add the grammatical rule for verbal phrase (vp).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W341">
<title id=" P95-1034.xml">two level many paths generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the quality of an nlg system depends on the quality of its inputs and knowledge bases.
</prevsent>
<prevsent>given that perfect kbs do not yet exist, an impor-tant question arises: can we build high-quality nlg systems that are robust against incomplete kbs and inputs?
</prevsent>
</prevsection>
<citsent citstr=" P81-1033 ">
although robustness has been heavily stud-ied in natural anguage understanding (weischedel and black, 1980; hayes, 1981; <papid> P81-1033 </papid>lavie, 1994), <papid> P94-1045 </papid>it has received much less attention in nlg (robin, 1995).</citsent>
<aftsection>
<nextsent>we describe hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in these- mantic input.
</nextsent>
<nextsent>the model comes out of our practi-cal experience in building large japanese-english newspaper machine translation system, japan- gloss (knight et al, 1994; knight et al, 1995).
</nextsent>
<nextsent>this system translates japanese into representations whose terms are drawn from the sensus ontol-ogy (knight and luk, 1994), 70,000-node knowl-edge base skeleton derived from resources like word-net (miller, 1990), longman dictionary (procter, 1978), and the penman upper model (bateman, 1990).<papid> W90-0108 </papid></nextsent>
<nextsent>these representations are turned into en-glish during generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W342">
<title id=" P95-1034.xml">two level many paths generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the quality of an nlg system depends on the quality of its inputs and knowledge bases.
</prevsent>
<prevsent>given that perfect kbs do not yet exist, an impor-tant question arises: can we build high-quality nlg systems that are robust against incomplete kbs and inputs?
</prevsent>
</prevsection>
<citsent citstr=" P94-1045 ">
although robustness has been heavily stud-ied in natural anguage understanding (weischedel and black, 1980; hayes, 1981; <papid> P81-1033 </papid>lavie, 1994), <papid> P94-1045 </papid>it has received much less attention in nlg (robin, 1995).</citsent>
<aftsection>
<nextsent>we describe hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in these- mantic input.
</nextsent>
<nextsent>the model comes out of our practi-cal experience in building large japanese-english newspaper machine translation system, japan- gloss (knight et al, 1994; knight et al, 1995).
</nextsent>
<nextsent>this system translates japanese into representations whose terms are drawn from the sensus ontol-ogy (knight and luk, 1994), 70,000-node knowl-edge base skeleton derived from resources like word-net (miller, 1990), longman dictionary (procter, 1978), and the penman upper model (bateman, 1990).<papid> W90-0108 </papid></nextsent>
<nextsent>these representations are turned into en-glish during generation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W343">
<title id=" P95-1034.xml">two level many paths generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we describe hybrid model for natural language generation which offers improved performance in the presence of knowledge gaps in the generator (the grammar and the lexicon), and of errors in these- mantic input.
</prevsent>
<prevsent>the model comes out of our practi-cal experience in building large japanese-english newspaper machine translation system, japan- gloss (knight et al, 1994; knight et al, 1995).
</prevsent>
</prevsection>
<citsent citstr=" W90-0108 ">
this system translates japanese into representations whose terms are drawn from the sensus ontol-ogy (knight and luk, 1994), 70,000-node knowl-edge base skeleton derived from resources like word-net (miller, 1990), longman dictionary (procter, 1978), and the penman upper model (bateman, 1990).<papid> W90-0108 </papid></citsent>
<aftsection>
<nextsent>these representations are turned into en-glish during generation.
</nextsent>
<nextsent>because we are processing unrestricted newspaper text, all modules in japan- gloss must be robust.
</nextsent>
<nextsent>in addition, we show how the model is useful in simplifying the design of generator and its knowl-edge bases even when perfect knowledge is available.
</nextsent>
<nextsent>this is accomplished by relegating some aspects of lexical choice (such as preposition selection and non- compositional inter lexical constraints) to statisti-cal component.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W344">
<title id=" P95-1034.xml">two level many paths generation </title>
<section> knowledge  gaps.  </section>
<citcontext>
<prevsection>
<prevsent>does b?
</prevsent>
<prevsent>following this reasoning, we are led into statisti-cal language modeling.
</prevsent>
</prevsection>
<citsent citstr=" C94-1058 ">
we built language model 4see also (harbusch et al, 1994) <papid> C94-1058 </papid>for thorough dis-cussion of defaulting in nlg systems.</citsent>
<aftsection>
<nextsent>for the english language by estimating bigram and trigram probabilities from large collection of 46 million words of wall street journal material.
</nextsent>
<nextsent>5 we smoothed these estimates according to class mem-bership for proper names and numbers, and accord-ing to an extended version of the enhanced good- turing method (church and gale, 1991) for there- maining words.
</nextsent>
<nextsent>the latter smoothing operation ot only optimally regresses the probabilities of seen n- grams but also assigns non-zero probability to all unseen n-grams which depends on how likely their component m-grams (m   n, i.e., words and bi- grams) are.
</nextsent>
<nextsent>the resulting conditional probabilities are converted to log-likelihoods for reasons of nu-merical accuracy and used to estimate the overall probability p(s) of any english sentence accord-ing to markov assumption, i.e., log p(s) = log p(w, \[wi_l) for bigrams log p(s) = log p(wilwi_z, i -2 ) for trigrams because both equations would assign lower and lower probabilities to longer sentences and we need to compare sentences of different lengths, heuristic strictly increasing function of sentence length, f( l) = 0.5l, is added to the log-likelihood estimates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W345">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the set of techniques have been applied in combined way to disambiguate the genus terms of two machine-readable dictionaries (mrd), enabling us to con-struct complete taxonomies for spanish and french.
</prevsent>
<prevsent>tested accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that taxonomy build-ing is not limited to structured ictionaries such as ldoce.
</prevsent>
</prevsection>
<citsent citstr=" C94-1042 ">
while in english the  lexical bottleneck  problem (briscoe, 1991) seems to be softened (e.g. wordnet (miller, 1990), alvey lexicon (grover et al, 1993), comlex (grishman et al, 1994), <papid> C94-1042 </papid>etc.) there are no available wide range lexicons for natural anguage processing (nlp) for other languages.</citsent>
<aftsection>
<nextsent>manual con-struction of lexicons is the most reliable technique for obtaining structured lexicons but is costly and highly time-consuming.
</nextsent>
<nextsent>this is the reason for many researchers having focused on the massive acquisi-tion of lexical knowledge and semantic information from pre-existing structured lexical resources as au-tomatically as possible.
</nextsent>
<nextsent>*this research as been partially funded by cicyt tic96-1243-c03-02 (item project) and the european comission le-4003 (eurowordnet project).
</nextsent>
<nextsent>as dictionaries are special texts whose subject matter is language (or pair of languages in the case of bilingual dictionaries) they provide wide range of information about words by giving defini-tions of senses of words, and, doing that, supplying knowledge not just about language, but about the world itself.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W346">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as dictionaries are special texts whose subject matter is language (or pair of languages in the case of bilingual dictionaries) they provide wide range of information about words by giving defini-tions of senses of words, and, doing that, supplying knowledge not just about language, but about the world itself.
</prevsent>
<prevsent>one of the most important relation to be ex-tracted from machine-readable dictionaries (mrd) is the hyponym/hypernym relation among dictio-nary senses (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P81-1030 ">
(amsler, 1981), (<papid> P81-1030 </papid>vossen and serail, 1990) ) not only because of its own importance as the backbone of taxonomies, but also because this rela-tion acts as the support of main inheritance mecha-nisms helping, thus, the acquisition of other relations and semantic features (cohen and lois elle, 1988), providing formal structure and avoiding redundancy in the lexicon (briscoe et al, 1990).<papid> C90-2008 </papid></citsent>
<aftsection>
<nextsent>for instance, following the natural chain of dictionary senses de-scribed in the diccionario general ilustrado de la lengua espadola (dgile, 1987) we can discover that bonsai is cultivated plant or bush.
</nextsent>
<nextsent>bonsai_l_2 planta arbusto asi cultivado.
</nextsent>
<nextsent>(bonsai, plant and bush cultivated in that way) the hyponym/hypernym relation appears be-tween the entry word (e.g. bonsai) and the genus term, or the core of the phrase (e.g. planta and arbusto).
</nextsent>
<nextsent>thus, usually dictionary definition is written to employ genus term combined with dif- ferentia which distinguishes the word being defined from other words with the same genus term 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W347">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as dictionaries are special texts whose subject matter is language (or pair of languages in the case of bilingual dictionaries) they provide wide range of information about words by giving defini-tions of senses of words, and, doing that, supplying knowledge not just about language, but about the world itself.
</prevsent>
<prevsent>one of the most important relation to be ex-tracted from machine-readable dictionaries (mrd) is the hyponym/hypernym relation among dictio-nary senses (e.g.
</prevsent>
</prevsection>
<citsent citstr=" C90-2008 ">
(amsler, 1981), (<papid> P81-1030 </papid>vossen and serail, 1990) ) not only because of its own importance as the backbone of taxonomies, but also because this rela-tion acts as the support of main inheritance mecha-nisms helping, thus, the acquisition of other relations and semantic features (cohen and lois elle, 1988), providing formal structure and avoiding redundancy in the lexicon (briscoe et al, 1990).<papid> C90-2008 </papid></citsent>
<aftsection>
<nextsent>for instance, following the natural chain of dictionary senses de-scribed in the diccionario general ilustrado de la lengua espadola (dgile, 1987) we can discover that bonsai is cultivated plant or bush.
</nextsent>
<nextsent>bonsai_l_2 planta arbusto asi cultivado.
</nextsent>
<nextsent>(bonsai, plant and bush cultivated in that way) the hyponym/hypernym relation appears be-tween the entry word (e.g. bonsai) and the genus term, or the core of the phrase (e.g. planta and arbusto).
</nextsent>
<nextsent>thus, usually dictionary definition is written to employ genus term combined with dif- ferentia which distinguishes the word being defined from other words with the same genus term 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W348">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these dictionaries are very different in number of headwords, polysemy degree, size and length of definitions (c.f. table 1).
</prevsent>
<prevsent>while dgile is good example of large sized dictionary, lppl shows to what extent he smallest dictionary is useful.
</prevsent>
</prevsection>
<citsent citstr=" J92-1001 ">
even if most of the techniques for wsd are pre-sented as stand-alone, it is our belief, following the ideas of (mcroy, 1992), <papid> J92-1001 </papid>that full-fledged lexical am-biguity resolution should combine several informa-tion sources and techniques.</citsent>
<aftsection>
<nextsent>this work does not ad-dress all the heuristics cited in her paper, but prof-its from techniques that were at hand, without any claim of them being complete.
</nextsent>
<nextsent>in fact we use unsu-pervised techniques, i.e. those that do not require hand-coding of any kind, that draw knowledge from variety of sources - the source dictionaries, bilin-gual dictionaries and wordnet - in diverse ways.
</nextsent>
<nextsent>2called also lexical ambiguity resolution, word sense discrimination, word sense selection or word sense identification.
</nextsent>
<nextsent>3in ldoce, dictionary senses are explicitly ordered by frequency, 86% dictionary senses have semantic odes and 44% of dictionary senses have pragmatic codes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W349">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for lppl the match among lemmas proved most useful, while dgile yielded better results when matching the first four characters of words.
</prevsent>
<prevsent>this heuristic uses cooccurrence data collected from the whole dictionary (see section 4.1 for more de- tails).
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
thus, given hyponym definition (o) and set of candidate hypernym definitions, this method selects the candidate hypernym definition (e) which returns the maximum score given by formula (1): sc(o, e) : cw(wi, wj) (i)  wieoawj6e the cooccurrence weight (cw) between two words can be given by cooccurrence frequency, mutual information (church and hanks, 1990) <papid> J90-1003 </papid>or associ-ation ratio (resnik, 1992).</citsent>
<aftsection>
<nextsent>we tested them us-ing different context window sizes.
</nextsent>
<nextsent>best results were obtained in both dictionaries using the association ratio.
</nextsent>
<nextsent>in dgile window size 7 proved the most suitable, whereas in lppl whole definitions were used.
</nextsent>
<nextsent>this heuristic is based on the method presented in (wilks et al, 1993) which also uses cooccurrence data collected from the whole dictionary (c.f. sec-tion 4.1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W350">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this case, given an hyponym (o) and set of possible hypernyms we select he candidate hzy- pernym (e) which yields maximum similarity among semantic vectors: sv(o , e) = sim(vo, (4) where sim can be the dot product, cosine or eu- clidean distance, as before.
</prevsent>
<prevsent>each dictionary sense .has been semantically tagged with vector of se-mantic weights following formula (5).
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
yogi = sw (w,) (5) wiedef the salient word vector (swv) for word contains saliency weight (yarowsky, 1992) <papid> C92-2070 </papid>for each of the 25 semantic tags of wordnet.</citsent>
<aftsection>
<nextsent>again, the best method differs from one dictionary to the other: each one prefers the method used in the previous ection.
</nextsent>
<nextsent>conceptual distance provides basis for determining closeness in meaning among words, taking as refer-ence structured hierarchical net.
</nextsent>
<nextsent>conceptual dis-tance between two concepts is essentially the length 50 of the shortest path that connects the concepts in the hierarchy.
</nextsent>
<nextsent>in order to apply conceptual distance, wordnet was chosen as the hierarchical knowledge base, and bilingual dictionaries were used to link spanish and french words to the english concepts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W351">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(agirre et al, 1994).
</prevsent>
<prevsent>to compute the distance between any two words (wl,w2), all the corresponding concepts in wordnet (el,, e2j) are searched via bilingual dictionary, and the mini-mum of the summa tory for each concept in the path between each possible combination of c1~ and c2~ is returned, as shown below: 1 dist(wl, w2) = rain depth(ck) cl ewl c2j ew2 cke path(c l~ ,c2.i ) (7) formulas (6) and (7) proved the most suitable of several other possibilities for this task, includ-ing those which included full definitions in (6) or those using other conceptual distance formulas, c.f.
</prevsent>
</prevsection>
<citsent citstr=" C96-1005 ">
(agirre and rigau, 1996).<papid> C96-1005 </papid></citsent>
<aftsection>
<nextsent>as outlined in the beginning of this section, the way to combine all the heuristics in one single decision is simple.
</nextsent>
<nextsent>the weights each heuristic assigns to the rivaling senses of one genus are normalized to the interval between 1 (best weight) and 0.
</nextsent>
<nextsent>formula (8) shows the normalized value given heuristic will give to sense of the genus, according to the weight as-signed to the heuristic to sense and the maximum weight of all the sense of the genus ei.
</nextsent>
<nextsent>vote(o, e) = weight(o, e) max e, ( weigth( , ei ) ) (s) the values thus collected from each heuristic, are added up for each competing sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W353">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5 compar son i th rev ious work.
</prevsent>
<prevsent>several approaches have been proposed for attaching the correct sense (from set of prescribed ones) of word in context.
</prevsent>
</prevsection>
<citsent citstr=" P94-1013 ">
some of them have been fully tested in real size texts (e.g. statistical methods (yarowsky, 1992), (<papid> C92-2070 </papid>yarowsky, 1994), (<papid> P94-1013 </papid>miller and teibel, 1991), <papid> H91-1077 </papid>knowledge based methods (sussna, 1993), (agirre and rigau, 1996), <papid> C96-1005 </papid>or mixed methods (richardson et al, 1994), (resnik, 1995)).<papid> W95-0105 </papid></citsent>
<aftsection>
<nextsent>the performance of wsd is reaching high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation (e.g.
</nextsent>
<nextsent>(yarowsky, 1995) <papid> P95-1026 </papid>reports success rate of 96% disambiguating twelve words with two clear sense distinctions each one).</nextsent>
<nextsent>this paper has presented general technique for wsd which is combination of statistical and knowledge based methods, and which has been ap-plied to disambiguate all the genus terms in two dic- tionaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W354">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5 compar son i th rev ious work.
</prevsent>
<prevsent>several approaches have been proposed for attaching the correct sense (from set of prescribed ones) of word in context.
</prevsent>
</prevsection>
<citsent citstr=" H91-1077 ">
some of them have been fully tested in real size texts (e.g. statistical methods (yarowsky, 1992), (<papid> C92-2070 </papid>yarowsky, 1994), (<papid> P94-1013 </papid>miller and teibel, 1991), <papid> H91-1077 </papid>knowledge based methods (sussna, 1993), (agirre and rigau, 1996), <papid> C96-1005 </papid>or mixed methods (richardson et al, 1994), (resnik, 1995)).<papid> W95-0105 </papid></citsent>
<aftsection>
<nextsent>the performance of wsd is reaching high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation (e.g.
</nextsent>
<nextsent>(yarowsky, 1995) <papid> P95-1026 </papid>reports success rate of 96% disambiguating twelve words with two clear sense distinctions each one).</nextsent>
<nextsent>this paper has presented general technique for wsd which is combination of statistical and knowledge based methods, and which has been ap-plied to disambiguate all the genus terms in two dic- tionaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W356">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5 compar son i th rev ious work.
</prevsent>
<prevsent>several approaches have been proposed for attaching the correct sense (from set of prescribed ones) of word in context.
</prevsent>
</prevsection>
<citsent citstr=" W95-0105 ">
some of them have been fully tested in real size texts (e.g. statistical methods (yarowsky, 1992), (<papid> C92-2070 </papid>yarowsky, 1994), (<papid> P94-1013 </papid>miller and teibel, 1991), <papid> H91-1077 </papid>knowledge based methods (sussna, 1993), (agirre and rigau, 1996), <papid> C96-1005 </papid>or mixed methods (richardson et al, 1994), (resnik, 1995)).<papid> W95-0105 </papid></citsent>
<aftsection>
<nextsent>the performance of wsd is reaching high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation (e.g.
</nextsent>
<nextsent>(yarowsky, 1995) <papid> P95-1026 </papid>reports success rate of 96% disambiguating twelve words with two clear sense distinctions each one).</nextsent>
<nextsent>this paper has presented general technique for wsd which is combination of statistical and knowledge based methods, and which has been ap-plied to disambiguate all the genus terms in two dic- tionaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W357">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>some of them have been fully tested in real size texts (e.g. statistical methods (yarowsky, 1992), (<papid> C92-2070 </papid>yarowsky, 1994), (<papid> P94-1013 </papid>miller and teibel, 1991), <papid> H91-1077 </papid>knowledge based methods (sussna, 1993), (agirre and rigau, 1996), <papid> C96-1005 </papid>or mixed methods (richardson et al, 1994), (resnik, 1995)).<papid> W95-0105 </papid></prevsent>
<prevsent>the performance of wsd is reaching high stance, although usually only small sets of words with clear sense distinctions are selected for disambiguation (e.g.</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
(yarowsky, 1995) <papid> P95-1026 </papid>reports success rate of 96% disambiguating twelve words with two clear sense distinctions each one).</citsent>
<aftsection>
<nextsent>this paper has presented general technique for wsd which is combination of statistical and knowledge based methods, and which has been ap-plied to disambiguate all the genus terms in two dic-tionaries.
</nextsent>
<nextsent>although this latter task could be seen easier than general wsd 4, genus are usually frequent and gen-eral words with high ambiguity ~.
</nextsent>
<nextsent>while the average of senses per noun in dgile is 1.8 the average of senses per noun genus is 2.75 (1.30 and 2.29 respec-tively for lppl).
</nextsent>
<nextsent>furthermore, it is not possible to apply the powerful  one sense per discourse  prop-erty (yarowsky, 1995) <papid> P95-1026 </papid>because there is no discourse in dictionaries.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W362">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>5however, in dictionary definitions the headword and the genus term have to be the same part of speech.
</prevsent>
<prevsent>6(wilks et al, 1993) disambiguating 197 occurrences of the word bank in ldoce say  was not an easy task, as some of the usages of bank did not seem to fit any of the definitions very well .
</prevsent>
</prevsection>
<citsent citstr=" H94-1046 ">
also (miller et al, 1994) <papid> H94-1046 </papid>tagging semantically semcor by hand, measure an error rate around 10% for polysemous words.</citsent>
<aftsection>
<nextsent>53 acteristics of ldoce.
</nextsent>
<nextsent>yhrthermore, using only the implicit information contained into the dictionary definitions of ldoce (cowie et al, 1992) <papid> H92-1046 </papid>report success rate of 47% at sense level.</nextsent>
<nextsent>(wilks et al., 1993) reports success rate of 45% disambiguat- ing the word bank (thirteen senses ldoce) using technique similar to heuristic 6.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W363">
<title id=" P97-1007.xml">combining unsupervised lexical knowledge methods for word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>also (miller et al, 1994) <papid> H94-1046 </papid>tagging semantically semcor by hand, measure an error rate around 10% for polysemous words.</prevsent>
<prevsent>53 acteristics of ldoce.</prevsent>
</prevsection>
<citsent citstr=" H92-1046 ">
yhrthermore, using only the implicit information contained into the dictionary definitions of ldoce (cowie et al, 1992) <papid> H92-1046 </papid>report success rate of 47% at sense level.</citsent>
<aftsection>
<nextsent>(wilks et al., 1993) reports success rate of 45% disambiguat- ing the word bank (thirteen senses ldoce) using technique similar to heuristic 6.
</nextsent>
<nextsent>in our case, combin-ing informed heuristics and without explicit seman-tic tags, the success rates are 83% and 82% over- all, and 95% and 75% for two-way ambiguous genus (dgile and lppl data, respectively).
</nextsent>
<nextsent>moreover, 93% and 92% of times the real solution is between the first and second proposed solution.
</nextsent>
<nextsent>6 conc lus ion and future work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W364">
<title id=" P97-1069.xml">generative power of ccgs with generalized type raised categories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proof is based on the idea that any instance of such grammar can be simulated by standard ccg.
</prevsent>
<prevsent>the class of combinatory categorial grammars (ccg- std) was proved to be weakly equivalent to linear index grammars and tree adjoining grammars (joshi, vijay- shanker, and weir, 1991; vijay-shanker and weir, 1994).
</prevsent>
</prevsection>
<citsent citstr=" E93-1039 ">
but ccg-std cannot handle the generalization of type raising that has been used in accounting for various lin-guistic phenomena including: coordination and extrac-tion (steedman, 1985; dowty, 1988; steedman, 1996), prosody (prevost and steedman, 1993), <papid> E93-1039 </papid>and quantifier scope (park, 1995).<papid> P95-1028 </papid></citsent>
<aftsection>
<nextsent>intuitively, all of these phenomena call for non-traditional, more flexible notion of consti-tuency capable of representing surface structures inclu-ding  (subj v) (obj)  in english.
</nextsent>
<nextsent>although lexical type raising involving variables can be introduced to derive such constituent?
</nextsent>
<nextsent>unconstrained use of variables can increase the power.
</nextsent>
<nextsent>for example, grammar involving (t \z ) / (t \v ) can generate language b d  which ccg-std cannot (hoffman, 1993).<papid> P93-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W365">
<title id=" P97-1069.xml">generative power of ccgs with generalized type raised categories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proof is based on the idea that any instance of such grammar can be simulated by standard ccg.
</prevsent>
<prevsent>the class of combinatory categorial grammars (ccg- std) was proved to be weakly equivalent to linear index grammars and tree adjoining grammars (joshi, vijay- shanker, and weir, 1991; vijay-shanker and weir, 1994).
</prevsent>
</prevsection>
<citsent citstr=" P95-1028 ">
but ccg-std cannot handle the generalization of type raising that has been used in accounting for various lin-guistic phenomena including: coordination and extrac-tion (steedman, 1985; dowty, 1988; steedman, 1996), prosody (prevost and steedman, 1993), <papid> E93-1039 </papid>and quantifier scope (park, 1995).<papid> P95-1028 </papid></citsent>
<aftsection>
<nextsent>intuitively, all of these phenomena call for non-traditional, more flexible notion of consti-tuency capable of representing surface structures inclu-ding  (subj v) (obj)  in english.
</nextsent>
<nextsent>although lexical type raising involving variables can be introduced to derive such constituent?
</nextsent>
<nextsent>unconstrained use of variables can increase the power.
</nextsent>
<nextsent>for example, grammar involving (t \z ) / (t \v ) can generate language b d  which ccg-std cannot (hoffman, 1993).<papid> P93-1045 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W366">
<title id=" P97-1069.xml">generative power of ccgs with generalized type raised categories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although lexical type raising involving variables can be introduced to derive such constituent?
</prevsent>
<prevsent>unconstrained use of variables can increase the power.
</prevsent>
</prevsection>
<citsent citstr=" P93-1045 ">
for example, grammar involving (t \z ) / (t \v ) can generate language b d  which ccg-std cannot (hoffman, 1993).<papid> P93-1045 </papid></citsent>
<aftsection>
<nextsent>this paper argues that there is class of grammars which allows the use of linguistically-motivated form of type raising involving variables while it is still weakly equivalent ccg-std.
</nextsent>
<nextsent>a class of grammars, ccg- gtrc, is introduced in the next section as an extension to ccg-std.
</nextsent>
<nextsent>then we show that ccg-gtrc can actually be simulated by ccg-std, proving the equivalence.
</nextsent>
<nextsent>thanks to mark steedman, beryl hoffman, anoop sarkar, and the reviewers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W367">
<title id=" P97-1069.xml">generative power of ccgs with generalized type raised categories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>n66001-94-c6043, and arid grant no.
</prevsent>
<prevsent>daah04-94- g0426.
</prevsent>
</prevsection>
<citsent citstr=" J91-3003 ">
iour lexieal rules to introduce type raising are non-recursive and thus do not suffer from the problem of the over generation discussed in (carpenter, 1991).<papid> J91-3003 </papid></citsent>
<aftsection>
<nextsent>categories in languages like japanese, multiple nps can easily form non-traditional constituent as in  \[(subj objl) &amp; (subj2 obj2)\] verb .
</nextsent>
<nextsent>the proposed ~ammars (ccg-gtrc) admit lexical type-raised categories (ltrc) of the form  1 /(t\a) or \ (t/a) where is variable over categories and is constant category (const).
</nextsent>
<nextsent>2 then, composition of ltrcs can give rise to class of categories having the fo rmt/ (t \a ....
</nextsent>
<nextsent>\at) or t\ (t/a ....
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W368">
<title id=" P97-1069.xml">generative power of ccgs with generalized type raised categories </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the introduction of gtrcs affects the use of combi- natory rules: functional application   : /y + ---,  and generalized functional composition   ~ (x) : /y + ylzt ...\[zk - - - zlzl ...\[z~  where is bounded by grammar-dependent kma~ as in ccg-std.
</prevsent>
<prevsent>3 this paper assumes two constraints defined for the grammars and one condition stipulated to control the formal properties.
</prevsent>
</prevsection>
<citsent citstr=" P91-1010 ">
the following order-preserving constraint, which follows more primitive directionality features (steedman, 1991), <papid> P91-1010 </papid>limits the directions of the slashes in gtrcs.</citsent>
<aftsection>
<nextsent>(1) in gtrc  1 \[o (t\[,a ....
</nextsent>
<nextsent>ira,), the direction of \[0 must be the opposite to any of in, ..., \]b this prohibits functional composition   b?  on  gtrc+gtrc  pairs so that  t/(t\a\b) + u\(u/c/d)  does not result in t\ (t\a\b/c/d) or / (u ic /d\a \b) . that is, no movement of arguments across the functor is allowed.
</nextsent>
<nextsent>the variable constraint states that: (2) variables are limited to the defined positions in gtrcs.
</nextsent>
<nextsent>this prohibits   k (?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W369">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we extend the centering model for the resolution of intia-sentential naphora nd specify how to handle complex sentences.
</prevsent>
<prevsent>an empirical eval-uation indicates that the functional information structure guides the search for an antecedent within the sentence.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the centering model (grosz et al, 1995) <papid> J95-2003 </papid>focuses on the resolution of inter-sentential naphora.</citsent>
<aftsection>
<nextsent>since intra-sentential naphora occur at high rates in real- world texts, the model has to be extended for theres- olution of anaphora the sentence vel.
</nextsent>
<nextsent>however, the centering framework is not fully specified to han- die complex sentences (suri &amp; mccoy, 1994).<papid> J94-2006 </papid></nextsent>
<nextsent>this underspocification corresponds to the lack of pre-cise definition of the expression utterance, term al-ways used but intentionally left undefined 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W371">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the centering model (grosz et al, 1995) <papid> J95-2003 </papid>focuses on the resolution of inter-sentential naphora.</prevsent>
<prevsent>since intra-sentential naphora occur at high rates in real- world texts, the model has to be extended for theres- olution of anaphora the sentence vel.</prevsent>
</prevsection>
<citsent citstr=" J94-2006 ">
however, the centering framework is not fully specified to han- die complex sentences (suri &amp; mccoy, 1994).<papid> J94-2006 </papid></citsent>
<aftsection>
<nextsent>this underspocification corresponds to the lack of pre-cise definition of the expression utterance, term al-ways used but intentionally left undefined 1.
</nextsent>
<nextsent>there-fore, the centering algorithms currently under discus-sion are not able to handle naturally occurring dis-course.
</nextsent>
<nextsent>possible strategies for treating sentence-level anaphora within the centering framework are 1.
</nextsent>
<nextsent>processing sentences linearly one clause at time (as suggested by grosz et al (1995)), <papid> J95-2003 </papid>2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W374">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in my contribution, evidence for mixed-mode strategy is brought forward, which favors particular set of sentence-internal antecedents given by functional criteria.
</prevsent>
<prevsent>1cf.
</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
the sketchy statements by brennan et al (1987, <papid> P87-1022 </papid>p.155):  \[...\] is an utterance (not necessarily full clause) \[...\] , and by grosz et al (1995, <papid> J95-2003 </papid>p.209):  need not to be full clause. </citsent>
<aftsection>
<nextsent>our studies on german texts have revealed that the functional information structure of the sentence, con-sidered in terms of the context-boundedness of dis-course lements, isthe major determinant for the rank-ing on the forward-looking-centers (c!
</nextsent>
<nextsent>(u,)) (strube &amp; hahn, 1996).<papid> P96-1036 </papid></nextsent>
<nextsent>hence, context-bound discourse le- ments are generally ranked higher in the c!</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W376">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> constraints on sentential anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>the sketchy statements by brennan et al (1987, <papid> P87-1022 </papid>p.155):  \[...\] is an utterance (not necessarily full clause) \[...\] , and by grosz et al (1995, <papid> J95-2003 </papid>p.209):  need not to be full clause. </prevsent>
<prevsent>our studies on german texts have revealed that the functional information structure of the sentence, con-sidered in terms of the context-boundedness of dis-course lements, isthe major determinant for the rank-ing on the forward-looking-centers (c!</prevsent>
</prevsection>
<citsent citstr=" P96-1036 ">
(u,)) (strube &amp; hahn, 1996).<papid> P96-1036 </papid></citsent>
<aftsection>
<nextsent>hence, context-bound discourse le- ments are generally ranked higher in the c!
</nextsent>
<nextsent>than any other non-anaphoric element.
</nextsent>
<nextsent>the functional informa-tion structure has impact not only on the resolution of inter-sentential anaphora, but also on the resolution of intra-sentential naphora.
</nextsent>
<nextsent>hence, the most preferred antecedent of an intra-sentential naphor is phrase which is also anaphoric.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W377">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> eva luat ion.  </section>
<citcontext>
<prevsection>
<prevsent>in order to evaluate the functional approach to theres- olution of intra-sentential anaphora within the center-ing model, we compared it to the other approaches mentioned in section 1, employing the test set re-ferred to in table 2.
</prevsent>
<prevsent>note that we tried to eliminate r- ror chaining and false positives (for some remarks on evaluating discourse processing algorithms, cf.
</prevsent>
</prevsection>
<citsent citstr=" P89-1031 ">
walker (1989); <papid> P89-1031 </papid>we consider her results as starting point for our proposal).</citsent>
<aftsection>
<nextsent>first, we examine the errors which all strategies have in common (for the success rate, cf.
</nextsent>
<nextsent>table 4).
</nextsent>
<nextsent>99 errors are caused by under specification at differ-ent levels, e.g., prepositional naphors (16), plural anaphors (8), anaphors which refer to member of set (14), sentence anaphors (21), and anaphors which refer to global focus (12) are not yet included in the mechanism.
</nextsent>
<nextsent>in 9 cases, any strategy will choose the false antecedent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W378">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> eva luat ion.  </section>
<citcontext>
<prevsection>
<prevsent>the approach which prefers intra-sentential an-tecedents causes 27 additional errors.
</prevsent>
<prevsent>these errors occur whenever an inter-sentential anaphor can be re-solved with an incorrect intra-sentential antecedent.
</prevsent>
</prevsection>
<citsent citstr=" E95-1033 ">
4we abstract here from the syntactic riteria for filtering out some elements of the current sentence by applying bind- hag criteria (strube &amp; hahn, 1995).<papid> E95-1033 </papid></citsent>
<aftsection>
<nextsent>syntactic onstraints like control phenomena override the preferences given by the context.
</nextsent>
<nextsent>379 ii r4 linear ex=a   int i int   extra f ctional it 308 237 (6,9%) 229 (4,3%) 240 (77,9%) 247 (80,2%) spiegel 102 82 (80,4%) 76 (74,5%) 82 (80,4%) 86 (84,3%) mtlller 153 105 (68,8%) 99 (64,7%) 115 (75,3%) 128 (83,7%) ii 563 1424 (75,3%)1404 (71,8%)1437 (77,6%)1461 (81,9%) table 4: success rate without semantic constraints the functional approach causes only 3 additional er-rors.
</nextsent>
<nextsent>these errors occur whenever the antecedent of an intra-sentential anaphor is not bound by the context (which is possible but rare) and when the anaphor can be resolved at the text level.
</nextsent>
<nextsent>the results change slightly if semantic/conceptual constraints (type and further admissibility constraints) on anaphora reconsidered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W384">
<title id=" P96-1057.xml">processing complex sentences in the centering framework </title>
<section> comparison to related work.  </section>
<citcontext>
<prevsection>
<prevsent>suri &amp; mccoy (1994) <papid> J94-2006 </papid>argue in the same manner, but we consider the functional approach for languages with free word order superior to their grammatical criteria, while, for languages with fixed word order, both ap-proaches should give the same results.</prevsent>
<prevsent>hence, our ap-proach seems to be more generally applicable.</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
other approaches which integrate the resolution of sentence- and text-level anaphora rebased on salience metrics (haji~ov~i et al, 1992; lappin &amp; leass, 1994).<papid> J94-4002 </papid></citsent>
<aftsection>
<nextsent>we consider such metrics to be method which detracts from the exact linguistic specifications we propose them.
</nextsent>
<nextsent>at first sight, grammar theories like gb (chomsky, 1981) or i-ipsg (pollard &amp; sag, 1994), are the best choice for resolving anaphora the sentence-level.
</nextsent>
<nextsent>but these grammar theories only give filters forex- cluding some elements from consideration.
</nextsent>
<nextsent>neither gives any preference for particular antecedent the sentence-level, nor do they consider text anaphora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W387">
<title id=" P94-1032.xml">extracting noun phrases from largescale texts a hybrid approach and its automatic evaluation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, parsing the text completely is very difficult, since various ambiguities cannot be resolved solely by syntactic or semantic information.
</prevsent>
<prevsent>do we really need to fully parse the texts in every application?
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
some researchers apply shallow or partial parsers (smadja, 1991; hindle, 1990) <papid> P90-1034 </papid>to acquiring specific patterns from texts.</citsent>
<aftsection>
<nextsent>these tell us that it is not necessary to completely parse the texts for some applications.
</nextsent>
<nextsent>this paper will propose probabilistic partial parser and incorporate linguistic knowledge to extract noun phrases.
</nextsent>
<nextsent>the partial parser is motivated by an intuition (abney, 1991): (1) when we read sentence, we read it chunk by chunk.
</nextsent>
<nextsent>abney uses two level grammar ules to implement the parser through pure lr parsing technique.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W388">
<title id=" P94-1032.xml">extracting noun phrases from largescale texts a hybrid approach and its automatic evaluation </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>the possible extensions of the proposed work will be discussed in section 7.
</prevsent>
<prevsent>section 8 will conclude the remarks.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
church (1988) <papid> A88-1019 </papid>proposes part of speech tagger and simple noun phrase extractor.</citsent>
<aftsection>
<nextsent>his noun phrase extractor brackets the noun phrases of input tagged texts according to two probability matrices: one is starting noun phrase matrix; the other is ending noun phrase matrix.
</nextsent>
<nextsent>the methodology is simple version of garside and leech probabilistic parser (1985).
</nextsent>
<nextsent>church lists sample text in the appendix of his paper to show the performance of his work.
</nextsent>
<nextsent>it demonstrates only 5 out of 248 noun phrases are omitted.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W390">
<title id=" P94-1032.xml">extracting noun phrases from largescale texts a hybrid approach and its automatic evaluation </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>it demonstrates only 5 out of 248 noun phrases are omitted.
</prevsent>
<prevsent>because the tested text is too small to assess the results, the experiment for large volume of texts is needed.
</prevsent>
</prevsection>
<citsent citstr=" C92-3150 ">
234 bourigault (1992) <papid> C92-3150 </papid>reports tool, lexter, for extracting terminologies from texts.</citsent>
<aftsection>
<nextsent>lexter triggers two-stage processing: 1) ana lys s (by identification of frontiers), which extracts the maximal-length noun phrase: 2) parsing (the maximal-length noun phrases), which, furthermore, acquires the terminology embedded in the noun phrases.
</nextsent>
<nextsent>bourigault declares the lexter extracts 95?/ 0 maximal-length noun phrases, that is, 43500 out of 46000 from test corpus.
</nextsent>
<nextsent>the result is validated by an expert.
</nextsent>
<nextsent>however, the precision is not reported in the boruigault paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W391">
<title id=" P94-1032.xml">extracting noun phrases from largescale texts a hybrid approach and its automatic evaluation </title>
<section> previous works.  </section>
<citcontext>
<prevsection>
<prevsent>the result is validated by an expert.
</prevsent>
<prevsent>however, the precision is not reported in the boruigault paper.
</prevsent>
</prevsection>
<citsent citstr=" W93-0306 ">
voutilainen (1993) <papid> W93-0306 </papid>announces nptoo for acquisition of maximal-length noun phrases.</citsent>
<aftsection>
<nextsent>nptool applies two finite state mechanisms (one is np-hostile; the other is np-friendly) to the task.
</nextsent>
<nextsent>the two mechanisms produce two np sets and any np candidate with at least one occurrence in both sets will be labeled as the  ok  np.
</nextsent>
<nextsent>the reported recall is 98.5-100% and the precision is 95- 98% validated manually by some 20000 words.
</nextsent>
<nextsent>but from the sample text listed in appendix of his paper, the recall is about 85%, and we can find some inconsistencies among these extracted noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W393">
<title id=" P94-1032.xml">extracting noun phrases from largescale texts a hybrid approach and its automatic evaluation </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>if we cannot decide the prepositional phrase  over husband eyes  is licensed by the verb  pull , we will not know  the wool  and  husband eyes  are two noun phrases or form noun pharse combined by the preposition  over .
</prevsent>
<prevsent>(18) to pull the wool over husband eyes to sell the books of my uncle in contrast, the noun phrase  the books of my uncle  is so called maximal noun phrase in current context.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
as the result, we conclude that if we do not resolve pp- attachment problem (hindle and rooth, 1993), <papid> J93-1005 </papid>to the expected extent, we will not extract he maximal noun phrases.</citsent>
<aftsection>
<nextsent>in our work, the probabilistic hunker decides the implicit boundaries between words and the np- tractor connects the adjacent noun chunks.
</nextsent>
<nextsent>when noun chunk is followed by preposition chunk, we do not connect the two chunks except he preposition chunk is led by  of  preposition.
</nextsent>
<nextsent>comparing with other works, our results are evaluated by parsed corpus automatically and show the high precision.
</nextsent>
<nextsent>although we do not point out the exact recall, we provide estimated values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W398">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>it also constrains the word usage in the language (lyons, 1977), due to its evaluative characteristics (battistella, 1990).
</prevsent>
<prevsent>for example, some nearly syn-onymous words differ in orientation because one im-plies desirability and the other does not (e.g., sim-ple versus simplisfic).
</prevsent>
</prevsection>
<citsent citstr=" C90-3018 ">
in linguistic constructs uch as conjunctions, which impose constraints on these- mantic orientation of their arguments (anscombre and ducrot, 1983; elhadad and mckeown, 1990), <papid> C90-3018 </papid>the choices of arguments and connective are mutu-ally constrained, as illustrated by: the tax proposal was simple and well-received } simplistic but well-received *simplistic and well-received by the public.</citsent>
<aftsection>
<nextsent>in addition, almost all antonyms have different se-mantic orientations3 if we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms.
</nextsent>
<nextsent>given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>hatzivassiloglou and mckeown, 1993), <papid> P93-1023 </papid>identifying the semantic orienta-tion of words would allow system to further refine the retrieved semantic similarity relationships, ex-tracting antonyms.</nextsent>
<nextsent>unfortunately, dictionaries and similar sources (theusari, wordnet (miller et al, 1990)) do not in-clude semantic orientation information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W399">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in linguistic constructs uch as conjunctions, which impose constraints on these- mantic orientation of their arguments (anscombre and ducrot, 1983; elhadad and mckeown, 1990), <papid> C90-3018 </papid>the choices of arguments and connective are mutu-ally constrained, as illustrated by: the tax proposal was simple and well-received } simplistic but well-received *simplistic and well-received by the public.</prevsent>
<prevsent>in addition, almost all antonyms have different se-mantic orientations3 if we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms.</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>hatzivassiloglou and mckeown, 1993), <papid> P93-1023 </papid>identifying the semantic orienta-tion of words would allow system to further refine the retrieved semantic similarity relationships, ex-tracting antonyms.</citsent>
<aftsection>
<nextsent>unfortunately, dictionaries and similar sources (theusari, wordnet (miller et al, 1990)) do not in-clude semantic orientation information.
</nextsent>
<nextsent>2 explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the do- main of discourse; for example, the opposition bear- bull appears only in stockmarket reports, where the two words take specialized meanings.
</nextsent>
<nextsent>in this paper, we present and evaluate method that automatically retrieves emantic orientation - formation using indirect information collected from large corpus.
</nextsent>
<nextsent>because the method relies on the cor-pus, it extracts domain-dependent formation and automatically adapts to new domain when the cor-pus is changed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W400">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in linguistic constructs uch as conjunctions, which impose constraints on these- mantic orientation of their arguments (anscombre and ducrot, 1983; elhadad and mckeown, 1990), <papid> C90-3018 </papid>the choices of arguments and connective are mutu-ally constrained, as illustrated by: the tax proposal was simple and well-received } simplistic but well-received *simplistic and well-received by the public.</prevsent>
<prevsent>in addition, almost all antonyms have different se-mantic orientations3 if we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms.</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>hatzivassiloglou and mckeown, 1993), <papid> P93-1023 </papid>identifying the semantic orienta-tion of words would allow system to further refine the retrieved semantic similarity relationships, ex-tracting antonyms.</citsent>
<aftsection>
<nextsent>unfortunately, dictionaries and similar sources (theusari, wordnet (miller et al, 1990)) do not in-clude semantic orientation information.
</nextsent>
<nextsent>2 explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the do- main of discourse; for example, the opposition bear- bull appears only in stockmarket reports, where the two words take specialized meanings.
</nextsent>
<nextsent>in this paper, we present and evaluate method that automatically retrieves emantic orientation - formation using indirect information collected from large corpus.
</nextsent>
<nextsent>because the method relies on the cor-pus, it extracts domain-dependent formation and automatically adapts to new domain when the cor-pus is changed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W401">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in linguistic constructs uch as conjunctions, which impose constraints on these- mantic orientation of their arguments (anscombre and ducrot, 1983; elhadad and mckeown, 1990), <papid> C90-3018 </papid>the choices of arguments and connective are mutu-ally constrained, as illustrated by: the tax proposal was simple and well-received } simplistic but well-received *simplistic and well-received by the public.</prevsent>
<prevsent>in addition, almost all antonyms have different se-mantic orientations3 if we know that two words relate to the same property (for example, members of the same scalar group such as hot and cold) but have different orientations, we can usually infer that they are antonyms.</prevsent>
</prevsection>
<citsent citstr=" P93-1023 ">
given that semantically similar words can be identified automatically on the basis of distributional properties and linguistic cues (brown et al, 1992; <papid> J92-4003 </papid>pereira et al, 1993; <papid> P93-1024 </papid>hatzivassiloglou and mckeown, 1993), <papid> P93-1023 </papid>identifying the semantic orienta-tion of words would allow system to further refine the retrieved semantic similarity relationships, ex-tracting antonyms.</citsent>
<aftsection>
<nextsent>unfortunately, dictionaries and similar sources (theusari, wordnet (miller et al, 1990)) do not in-clude semantic orientation information.
</nextsent>
<nextsent>2 explicit links between antonyms and synonyms may also be lacking, particularly when they depend on the do- main of discourse; for example, the opposition bear- bull appears only in stockmarket reports, where the two words take specialized meanings.
</nextsent>
<nextsent>in this paper, we present and evaluate method that automatically retrieves emantic orientation - formation using indirect information collected from large corpus.
</nextsent>
<nextsent>because the method relies on the cor-pus, it extracts domain-dependent formation and automatically adapts to new domain when the cor-pus is changed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W402">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> data  co lec ion.  </section>
<citcontext>
<prevsection>
<prevsent>after presenting our results and evaluation, we discuss simulation experiments that show how our method performs under different conditions of sparseness of data.
</prevsent>
<prevsent>for our experiments, we use the 21 million word 1987 wall street journal corpus 4, automatically an-.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
notated with part-of-speech tags using the parts tagger (church, 1988).<papid> A88-1019 </papid></citsent>
<aftsection>
<nextsent>in order to verify our hypothesis about the ori-entations of conjoined adjectives, and also to train and evaluate our subsequent algorithms, we need 3certain words inflected with negative affixes (such as in- or un-) tend to be mostly negative, but this rule applies only to fraction of the negative words.
</nextsent>
<nextsent>further- more, there are words so inflected which have positive orientation, e.g., independent and unbiased.
</nextsent>
<nextsent>4available form the acl data collection initiative as cd rom 1.
</nextsent>
<nextsent>positive: adequate central clever famous intelligent remarkable reputed sensitive slender thriving negative: contagious drunken ignorant lanky listless primitive strident rouble some unresolved unsuspecting figure 1: randomly selected adjectives with positive and negative orientations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W403">
<title id=" P97-1023.xml">predicting the semantic orientation of adjectives </title>
<section> validation of the conjunction.  </section>
<citcontext>
<prevsection>
<prevsent>there are few exceptions, e.g., appositive and conjunc-tions modifying plural nouns are evenly split between same and different orientation.
</prevsent>
<prevsent>but in these exceptional cases the sample is very small, and the observed behavior may be due to chance.
</prevsent>
</prevsection>
<citsent citstr=" J91-1001 ">
further analysis of different-orientation pairs in conjunctions other than but shows that con-joined antonyms are far more frequent han ex-pected by chance, in agreement with (justeson and katz, 1991).<papid> J91-1001 </papid></citsent>
<aftsection>
<nextsent>the analysis in the previous ection suggests base- line method for classifying links between adjectives: since 77.84% of all links from conjunctions indicate same orientation, we can achieve this level of perfor-mance by always guessing that link is of the same- orientation type.
</nextsent>
<nextsent>however, we can improve perfor-mance by noting that conjunctions using but exhibit the opposite pattern, usually involving adjectives of different orientations.
</nextsent>
<nextsent>thus, revised but still sim-ple rule predicts different-orientation link if the two adjectives have been seen in but conjunction, and same-orientation li otherwise, assuming the two adjectives were seen connected by at least one conjunction.
</nextsent>
<nextsent>morphological relationships between adjectives al- so play role.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W404">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>general-purpose natural language (nl) analysis systems have recently started to use declarative unification-based sentence grammar formalisms; systems of this type include sri clare sys-tem (alshawi et al, 1992) and the a1vey nl tools (anlt; briscoe et al, 1987a).
</prevsent>
<prevsent>using declarative formalism helps ease the task of developing and maintaining the grammar (kaplan, 1987).
</prevsent>
</prevsection>
<citsent citstr=" J93-1002 ">
in ad-dition to syntactic processing, the systems incor-porate lexical, morphological, and semantic pro-cessing, and have been applied successfully to the analysis of naturally-occurring texts (e.g. alshawi et al, 1992; briscoe &amp; carroll, 1993).<papid> J93-1002 </papid></citsent>
<aftsection>
<nextsent>evaluations of the grammars in these par-ticular systems have shown them to have wide coverage (alshawi et al, 1992; taylor, grover &amp;= briscoe, 1989) 2.
</nextsent>
<nextsent>however, although the practical throughput of parsers with such realistic gram-mars is important, for example when process- 1this research was supported by serc/dti project 4/1/1261  extensions to the alvey natu-ral language tools  and by ec esprit bra-7315  acquilex-ii .
</nextsent>
<nextsent>i am grateful toted briscoe for com-ments on an earlier version of this paper, to david weir for valuable discussions, and to hiyan alshawi for assistance with the clare system.
</nextsent>
<nextsent>2for example, taylor et al demonstrate hat the anlt grammar is in principle able to analyse 96.8% of corpus of 10,000 noun phrases taken from variety of corpora.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W405">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>2for example, taylor et al demonstrate hat the anlt grammar is in principle able to analyse 96.8% of corpus of 10,000 noun phrases taken from variety of corpora.
</prevsent>
<prevsent>ing large amounts of text or in interactive ap-plications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars.
</prevsent>
</prevsection>
<citsent citstr=" J87-1004 ">
previous comparisons have either fo-cussed on context-free (cf) or augmented cf parsing (tomita, 1987; <papid> J87-1004 </papid>billot &amp; lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (shann, 1989; bouma &amp; van noord, 1993; <papid> E93-1010 </papid>maxwell &amp; kaplan, 1993).<papid> J93-4001 </papid></citsent>
<aftsection>
<nextsent>it is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based gram- mars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be con-strained tightly with relatively simple predictive techniques.
</nextsent>
<nextsent>also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be confounding factor in any general conclusions made.
</nextsent>
<nextsent>other research directed towards improving the throughput of unification-based parsing sys-tems has been concerned with the unification oper-ation itself, which can consume up to 90% of parse time (e.g. tomabechi, 1991) <papid> P91-1041 </papid>in systems using lex- icalist grammar formalisms (e.g. hpsg; pollard &amp; sag, 1987).</nextsent>
<nextsent>however, parsing algorithms as-sume more importance for grammars having more substantial phrase structure components, uch as clare (which although employing some hpsg- like analyses till contains several tens of rules) and the anlt (which uses formalism derived from gpsg; gazdar et al, 1985), incethe more specific rule set can be used to control which uni- fic ations are performed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W407">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>2for example, taylor et al demonstrate hat the anlt grammar is in principle able to analyse 96.8% of corpus of 10,000 noun phrases taken from variety of corpora.
</prevsent>
<prevsent>ing large amounts of text or in interactive ap-plications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars.
</prevsent>
</prevsection>
<citsent citstr=" E93-1010 ">
previous comparisons have either fo-cussed on context-free (cf) or augmented cf parsing (tomita, 1987; <papid> J87-1004 </papid>billot &amp; lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (shann, 1989; bouma &amp; van noord, 1993; <papid> E93-1010 </papid>maxwell &amp; kaplan, 1993).<papid> J93-4001 </papid></citsent>
<aftsection>
<nextsent>it is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based gram- mars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be con-strained tightly with relatively simple predictive techniques.
</nextsent>
<nextsent>also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be confounding factor in any general conclusions made.
</nextsent>
<nextsent>other research directed towards improving the throughput of unification-based parsing sys-tems has been concerned with the unification oper-ation itself, which can consume up to 90% of parse time (e.g. tomabechi, 1991) <papid> P91-1041 </papid>in systems using lex- icalist grammar formalisms (e.g. hpsg; pollard &amp; sag, 1987).</nextsent>
<nextsent>however, parsing algorithms as-sume more importance for grammars having more substantial phrase structure components, uch as clare (which although employing some hpsg- like analyses till contains several tens of rules) and the anlt (which uses formalism derived from gpsg; gazdar et al, 1985), incethe more specific rule set can be used to control which uni- fic ations are performed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W408">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>2for example, taylor et al demonstrate hat the anlt grammar is in principle able to analyse 96.8% of corpus of 10,000 noun phrases taken from variety of corpora.
</prevsent>
<prevsent>ing large amounts of text or in interactive ap-plications, there is little published research that compares the performance of different parsing algorithms using wide-coverage unification-based grammars.
</prevsent>
</prevsection>
<citsent citstr=" J93-4001 ">
previous comparisons have either fo-cussed on context-free (cf) or augmented cf parsing (tomita, 1987; <papid> J87-1004 </papid>billot &amp; lang, 1989), or have used relatively small, limited-coverage unification grammars and lexicons (shann, 1989; bouma &amp; van noord, 1993; <papid> E93-1010 </papid>maxwell &amp; kaplan, 1993).<papid> J93-4001 </papid></citsent>
<aftsection>
<nextsent>it is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based gram- mars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be con-strained tightly with relatively simple predictive techniques.
</nextsent>
<nextsent>also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be confounding factor in any general conclusions made.
</nextsent>
<nextsent>other research directed towards improving the throughput of unification-based parsing sys-tems has been concerned with the unification oper-ation itself, which can consume up to 90% of parse time (e.g. tomabechi, 1991) <papid> P91-1041 </papid>in systems using lex- icalist grammar formalisms (e.g. hpsg; pollard &amp; sag, 1987).</nextsent>
<nextsent>however, parsing algorithms as-sume more importance for grammars having more substantial phrase structure components, uch as clare (which although employing some hpsg- like analyses till contains several tens of rules) and the anlt (which uses formalism derived from gpsg; gazdar et al, 1985), incethe more specific rule set can be used to control which uni- fic ations are performed.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W409">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>it is not clear that these results scale up to reflect accurately the behaviour of parsers using realistic, complex unification-based gram- mars: in particular, with grammars admitting less ambiguity parse time will tend to increase more slowly with increasing input length, and also with smaller grammars rule application can be con-strained tightly with relatively simple predictive techniques.
</prevsent>
<prevsent>also, since none of these studies relate observed performance to that of other comparable parsing systems, implementational oversights may not be apparent and so be confounding factor in any general conclusions made.
</prevsent>
</prevsection>
<citsent citstr=" P91-1041 ">
other research directed towards improving the throughput of unification-based parsing sys-tems has been concerned with the unification oper-ation itself, which can consume up to 90% of parse time (e.g. tomabechi, 1991) <papid> P91-1041 </papid>in systems using lex- icalist grammar formalisms (e.g. hpsg; pollard &amp; sag, 1987).</citsent>
<aftsection>
<nextsent>however, parsing algorithms as-sume more importance for grammars having more substantial phrase structure components, uch as clare (which although employing some hpsg- like analyses till contains several tens of rules) and the anlt (which uses formalism derived from gpsg; gazdar et al, 1985), incethe more specific rule set can be used to control which uni- fic ations are performed.
</nextsent>
<nextsent>in nl analysis, the syntactic information as-sociated with lexical items makes top-down pars-ing less attractive than bottom-up (e.g. cky; kasami, 1965; younger, 1967), although the lat-ter is often augmented with top-down predic- 287 tion to improve performance (e.g. earley, 1970; lang, 1974; pratt, 1975).
</nextsent>
<nextsent>section 2 describes three unification-based parsers which are related to polynomial-complexity bottom-up cf parsing algorithms.
</nextsent>
<nextsent>although incorporating unification increases their complexity to exponential on gram-mar size and input length (section 3), this ap-pears to have little impact on practical perfor-mance (section 4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W410">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> the parsers.  </section>
<citcontext>
<prevsection>
<prevsent>although incorporating unification increases their complexity to exponential on gram-mar size and input length (section 3), this ap-pears to have little impact on practical perfor-mance (section 4).
</prevsent>
<prevsent>sections 5 and 6 discuss these findings and present conclusions.
</prevsent>
</prevsection>
<citsent citstr=" P91-1014 ">
the three parsers in this study are: bottom- up left-corner parser, (non-deterministic) lr parser, and an lr-like parser based on an algo-rithm devised by schabes (1991).<papid> P91-1014 </papid></citsent>
<aftsection>
<nextsent>all three parsers accept grammars written in the anlt formal-ism (briscoe et al, 1987a), and the first two are distributed as part of the anlt package.
</nextsent>
<nextsent>the parsers create parse forests (tomita, 1987) <papid> J87-1004 </papid>that incorporate subtree sharing (in which identical sub-analyses are shared between differing super- ordinate analyses) and node packing (where sub- analyses covering the same portion of input whose root categories are in subsumption relationship are merged into single node).</nextsent>
<nextsent>the bottom-up left-corner parser the bottom-up left-corner (bu-lc) parser oper-ates left-to-right and breadth-first, storing partial (active) constituents in chart; carroll (1993) gives full description.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W418">
<title id=" P94-1040.xml">relating complexity to practical performance in parsing with wide coverage unification grammars </title>
<section> practical  results.  </section>
<citcontext>
<prevsection>
<prevsent>both grammars give total of around 280 analyses at similar level of detail.
</prevsent>
<prevsent>the results show that the lr parser is ap-proximately 35% faster than the bu-lc parser, and allocates about 30% less storage.
</prevsent>
</prevsection>
<citsent citstr=" H91-1036 ">
the mag-nitude of the speed-up is less than might be ex-pected, given the enthusiastic advocation of non- deterministic cf lr parsing for nl by some re-searchers (e.g. tomita, 1987; <papid> J87-1004 </papid>wright, wrigley &amp; sharman, 1991), and in the light of improvements observed for predictive over pure bottom-up ars- ing (e.g. moore &amp; dowding, 1991).<papid> H91-1036 </papid></citsent>
<aftsection>
<nextsent>however, on the assumption that incorrect prediction of gaps is 290 the main avoidable source of performance degra-dation (c.f. moore &amp; dowding), further investiga-tion shows that the speed-up is near the maximum that is possible with the anlt grammar (around 50%).
</nextsent>
<nextsent>the throughput of the ce parser is half that of the lr parser, and also less than that of the bu-lc parser.
</nextsent>
<nextsent>however, it is intermediate be-tween the two in terms of storage allocated.
</nextsent>
<nextsent>part of the difference in performance between it and the lr parser is due to the fact that it performs around 15% more unifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W421">
<title id=" P95-1014.xml">memo ization of coroutined constraints </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>some linguistic constraints cannot be effec-tively resolved during parsing at the loca-tion in which they are most naturally intro-duced.
</prevsent>
<prevsent>this paper shows how constraints can be propagated in memoizing parser (such as chart parser) in much the same way that variable bindings are, providing general treatment of constraint coroutining in memoization.
</prevsent>
</prevsection>
<citsent citstr=" P94-1021 ">
prolog code for sim-ple application of our technique to bouma and van noord (1994) <papid> P94-1021 </papid>categorial gram-mar analysis of dutch is provided.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W422">
<title id=" P95-1016.xml">utilizing statistical dialogue act processing in verb mobil </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as an application example we show how it supports repair when unexpected dialogue states occur.
</prevsent>
<prevsent>extracting and processing communicative intentions behind natural language utterances plays an im-portant role in natural anguage systems (see e.g.
</prevsent>
</prevsection>
<citsent citstr=" C94-2194 ">
(cohen et al, 1990; hinkelman and spackman, 1994)).<papid> C94-2194 </papid></citsent>
<aftsection>
<nextsent>within the speech-to-speech translation sys-tem verb mobil (wahlster, 1993; kay et al, 1994), dialogue acts are used as the basis for the treatment of intentions in dialogues.
</nextsent>
<nextsent>the representation in-tentions in the verb mobil system serves two main purposes: ? utilizing the dialogue act of an utterance as an important knowledge source for transla-tion yields faster and often qualitative better translation than method that depends on sur-face expressions only.
</nextsent>
<nextsent>this is the case especially in the first application of vv.rbmobil, the on- demand translation of appointment scheduling dialogues.
</nextsent>
<nextsent>another use of dialogue act processing in verb- mobil is the prediction of follow-up dialogue acts to narrow down the search space on the analysis ide.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W423">
<title id=" P95-1016.xml">utilizing statistical dialogue act processing in verb mobil </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the information acquired uring di-alogue processing is stored in dialogue memory.
</prevsent>
<prevsent>this contextual information is decomposed into the intentional structure, the referential structure, and the temporal structure which refers to the dates mentioned in the dialogue.
</prevsent>
</prevsection>
<citsent citstr=" E95-1026 ">
116 an overview of the dialogue component is given in (alexandersson et al, 1995).<papid> E95-1026 </papid></citsent>
<aftsection>
<nextsent>in this paper main emphasis is on statistical dialogue act prediction in veftbmobil, with an evaluation of the method, and an example of the interaction between plan recogni-tion and statistical dialogue act prediction.
</nextsent>
<nextsent>main wadoguo gro~ suggea introduce init i=lequoet_cornmqmt requut commont ? commont / thank su99eet requeet_comment con(mrn potonuol additions in any cllelogue clarily_amvo?
</nextsent>
<nextsent>?--.-=  , / i:)igam= coa~y_ou=ry 1-1 initial stw 0 final state ? nc~4iaal sum \[ figure 1: dialogue model for the description of appointment scheduling dialogs
</nextsent>
<nextsent>pred ic ions f ia logue acts like previous approaches for modeling task-oriented dialogues we assume that dialogue can be de-scribed by means of limited but open set of di-alogue acts (see e.g.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W424">
<title id=" P95-1016.xml">utilizing statistical dialogue act processing in verb mobil </title>
<section> the  ia logue  mode  and.  </section>
<citcontext>
<prevsection>
<prevsent>?--.-=  , / i:)igam= coa~y_ou=ry 1-1 initial stw 0 final state ? nc~4iaal sum \[ figure 1: dialogue model for the description of appointment scheduling dialogs
</prevsent>
<prevsent>pred ic ions f ia logue acts like previous approaches for modeling task-oriented dialogues we assume that dialogue can be de-scribed by means of limited but open set of di-alogue acts (see e.g.
</prevsent>
</prevsection>
<citsent citstr=" E91-1015 ">
(bilange, 1991), (<papid> E91-1015 </papid>mast et al, 1992)).</citsent>
<aftsection>
<nextsent>we selected the dialogue acts by examining the verb mobil corpus, which consists of transliter-ated spoken dialogues (german and english) for ap-pointment scheduling.
</nextsent>
<nextsent>we examined this corpus for the occurrence of dialogue acts as proposed by e.g.
</nextsent>
<nextsent>(austin, 1962; searle, 1969) and for the necessity to introduce new, sometimes problem-oriented dialogue acts.
</nextsent>
<nextsent>we first defined 17 dialogue acts together with semi-formal rules for their assignment to utterances (maier, 1994).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W425">
<title id=" P98-1025.xml">deriving the predicate argument structure for a free word order language </title>
<section> grammatical functions, type shifting,.  </section>
<citcontext>
<prevsection>
<prevsent>type shifting an np3 yields (sinp1 inp2)/(sinp1 \]np~ inp3) in which the argu-ment category is not lexically licensed.
</prevsent>
<prevsent>(5) is order- preserving in language-particular way; the result category always corresponds to lexical category in the language if the argument category does too.
</prevsent>
</prevsection>
<citsent citstr=" P95-1044 ">
for arguments requiring non-canonical order, we need type shifting and composition (hence the third clause in (7)): 3as suggested in (bozsahin and gocmen, 1995), <papid> P95-1044 </papid>morpho-logical and syntactic omposition can be distinguished by asso-ciating several attachment calculi with functors and arguments (e.g., affix ation, concatenation, clitics, etc,) np3:x t=~ (sinp1)/(sinpilnp3):tx ~ (sinp, inp2)/(sinp, inp31np2): b(tx) = bbtx once syntactic ategory of the argument is fixed, its semantics uniquely determined by (7).</citsent>
<aftsection>
<nextsent>the combinatory primitives operating on the pas are (7a), (7b--c), and (7c).
</nextsent>
<nextsent>t has the reduction rule tar a, and if f. the use oft or signifies that the term category is functor; its correct place in the pas is yet to be determined.
</nextsent>
<nextsent>i indicates that the term is in the right place in the partially derived pas.
</nextsent>
<nextsent>according to (5), there is unique result- argument combination for higher type np3, com-pared to 24 using (3).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W426">
<title id=" P98-1025.xml">deriving the predicate argument structure for a free word order language </title>
<section> grammatical functions, type shifting,.  </section>
<citcontext>
<prevsection>
<prevsent>this is indeed necessary because the resulting np can be further inflected on case and assume geno type index.
</prevsent>
<prevsent>for turkish, the type shifted category is c(5) =npagr/(npagr\nps).
</prevsent>
</prevsection>
<citsent citstr=" C88-1018 ">
hence the genitive suffix bears the category c(5)\n. agreement features enforce the possessor- possessed agreement on person and number via uni-fication (as in ucg (calder et al, 1988)): <papid> C88-1018 </papid>kalem -in uc -u pencil -gen.3s tip -poss.3s n. --ff c(5)\n: n:t ---~ (ne=g~\nps \n:p.oss npag~/(np~g~\nps):~p 5 np~,~\nps:posst   npo:: :rp  cposst  ) _(posst  )p   the tip of the pencil </citsent>
<aftsection>
<nextsent>due to space limitations, the following abbre-viated categories are employed in derivations: iv = sinpz tv = sinpiinp 2 dv = sinpiinp3inp2 the categories licensed by (5) can then be written as iv/tv and iv~tv for np2, tv/dv and tv~dv for np3, etc.
</nextsent>
<nextsent>(10a-b) show the verb-final variations in the word order.
</nextsent>
<nextsent>the bracketings in the pas and juxtaposition are left-associative; (fa)b is same as lab.
</nextsent>
<nextsent>b. kitab-t mehmet oku-du iv/tv: tb  s\iv: tm ~ tv: i  bx  s/tv: b(tm  )(tb  ) s: b(tm  )(tb  )r     a~  (10a) exhibits spurious ambiguity.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W427">
<title id=" P98-1025.xml">deriving the predicate argument structure for a free word order language </title>
<section> word order and scrambling.  </section>
<citcontext>
<prevsection>
<prevsent>this problem is resolved by grammar ewriting in the sense proposed by eisner 4 (1996).
</prevsent>
<prevsent>grammar rewriting can be done using predictive combinators (wittenburg, 1987), but they cannot handle crossing compositions that are essential to our method.
</prevsent>
</prevsection>
<citsent citstr=" E89-1002 ">
other normal form parsers, e.g. that of hepple and morrill (1989), <papid> E89-1002 </papid>have the same problem.</citsent>
<aftsection>
<nextsent>all grammar ules in (4) in fact check the labels of the constituent cate-gories, which show how the category is derived.
</nextsent>
<nextsent>the labels are as in (eisner, 1996).<papid> P96-1011 </papid></nextsent>
<nextsent>-fc: output of forward composition, of which forward cross-ing composition is special case.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W428">
<title id=" P98-1025.xml">deriving the predicate argument structure for a free word order language </title>
<section> word order and scrambling.  </section>
<citcontext>
<prevsection>
<prevsent>other normal form parsers, e.g. that of hepple and morrill (1989), <papid> E89-1002 </papid>have the same problem.</prevsent>
<prevsent>all grammar ules in (4) in fact check the labels of the constituent cate-gories, which show how the category is derived.</prevsent>
</prevsection>
<citsent citstr=" P96-1011 ">
the labels are as in (eisner, 1996).<papid> P96-1011 </papid></citsent>
<aftsection>
<nextsent>-fc: output of forward composition, of which forward cross-ing composition is special case.
</nextsent>
<nextsent>-bc: output of backward composition, of which backward cross-ing composition is special case.
</nextsent>
<nextsent>-ot: lexical or type shifted category.
</nextsent>
<nextsent>the goal is to block e.g., x/y-fc y/z-{fc, bc, ot} =~b  x/z and x/y-fc y-{fc, bc, ot} =~a  in (10a).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W433">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for unrestricted text, however, the context tends to be very diverse and difficult to capture with lexicalized model, therefore corpus-trained system is unlikely to port to new domains and run off the shelf.
</prevsent>
<prevsent>generality and adaptive ness are therefore key to robust and portable wsd system.
</prevsent>
</prevsection>
<citsent citstr=" W93-0303 ">
a concept-based model for wsd requires less parameter and has an element of generality built in (liddy and paik 1993).<papid> W93-0303 </papid></citsent>
<aftsection>
<nextsent>conceptual classes make it possible to generalize from word- specific context in order to disambiguate word sense appearing in particularly unfamiliar context in term of word recurrences.
</nextsent>
<nextsent>an adaptive system armed with an initial lexical and conceptual knowledge base extracted from machine-readable dictionaries (mrds), has two strong advantages over static lexicalized models trained using corpus.
</nextsent>
<nextsent>first, the initial 237 knowledge is rich and unbiased such that substantial portion of text can be disambiguated precisely.
</nextsent>
<nextsent>second, based on the result of initial disambiguated text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W434">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> acquiring conceptual knowledge.  </section>
<citcontext>
<prevsection>
<prevsent>the adaptation step adds deer and animal to the contextual representation for geo-bank.
</prevsent>
<prevsent>the enriched cr therefore contains information capable of disambiguating the instance of bank in the context of vole to produce final disambiguation result.
</prevsent>
</prevsection>
<citsent citstr=" J98-1003 ">
from mrd in this section we apply so-called top sense algorithm (chen and chang 1998) <papid> J98-1003 </papid>to acquire cr for mrd senses.</citsent>
<aftsection>
<nextsent>the current implementation of top sense uses the topical information in longman lexicon of contemporary english (mcarthur 1992, lloce) to represent wsd knowledge for ldoce senses.
</nextsent>
<nextsent>in the following subsections we describe how that is done.
</nextsent>
<nextsent>2.1 contextual representation from.
</nextsent>
<nextsent>mrds dictionary is text whose subject matter is language.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W435">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> experiments  and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>table 1 provides further details.
</prevsent>
<prevsent>however, there are still room for improvement in the area of precision.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
evidence have shown that by exploiting the constraint of so-called  one sense per discourse,  (gale, church and yarowsky 1992b) and the strategy of bootstrapping (yarowsky 1995), <papid> P95-1026 </papid>it is possible to boost coverage, while maintaining about he same level of precision.</citsent>
<aftsection>
<nextsent>4.2 discussions.
</nextsent>
<nextsent>although it is often difficult to compare studies on different ext domain, genre and experimental setup, the approach presented here seems to compare favorably with the experimental results reported in previous wsd research.
</nextsent>
<nextsent>luk (1995) <papid> P95-1025 </papid>experiments with the same words we use except the word bank and reports that there are totally 616 instances of these words in the brown corpus, (slightly less than the 749 instances we have experimented on).</nextsent>
<nextsent>the author eports that 60% of instances are resolved correctly using the definition-based concept co-occurrence (dbcc) approach.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W436">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> experiments  and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>4.2 discussions.
</prevsent>
<prevsent>although it is often difficult to compare studies on different ext domain, genre and experimental setup, the approach presented here seems to compare favorably with the experimental results reported in previous wsd research.
</prevsent>
</prevsection>
<citsent citstr=" P95-1025 ">
luk (1995) <papid> P95-1025 </papid>experiments with the same words we use except the word bank and reports that there are totally 616 instances of these words in the brown corpus, (slightly less than the 749 instances we have experimented on).</citsent>
<aftsection>
<nextsent>the author eports that 60% of instances are resolved correctly using the definition-based concept co-occurrence (dbcc) approach.
</nextsent>
<nextsent>leacock et al (1996) report that precision rate of 76% for disambiguating the word line in sample of wsj articles.
</nextsent>
<nextsent>one of the limiting factors of this approach is the quality of sense definition in the mrd.
</nextsent>
<nextsent>short and vague definitions tend to lead to inclusion of inappropriate topics in the contextual representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W437">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> experiments  and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>the model attacks the problem of lexical ambiguity and produces satisfactory results, under some strong assumption.
</prevsent>
<prevsent>a major problem with trigram model is that of long distance dependency.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
dagan and itai (1994) <papid> J94-4003 </papid>indicate that two languages are more informative than one; an english corpus is very helpful in disambiguating polysemous words in hebrew text.</citsent>
<aftsection>
<nextsent>local context in the form of lexical relations are identified in very large corpus.
</nextsent>
<nextsent>brown, et al (1991) <papid> P91-1034 </papid>describe statistical algorithm for partitioning word senses into two groups.</nextsent>
<nextsent>the authors use mutual information to find contextual feature that most reliably indicates which of the senses of the french ambiguous word is used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W438">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> experiments  and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>dagan and itai (1994) <papid> J94-4003 </papid>indicate that two languages are more informative than one; an english corpus is very helpful in disambiguating polysemous words in hebrew text.</prevsent>
<prevsent>local context in the form of lexical relations are identified in very large corpus.</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
brown, et al (1991) <papid> P91-1034 </papid>describe statistical algorithm for partitioning word senses into two groups.</citsent>
<aftsection>
<nextsent>the authors use mutual information to find contextual feature that most reliably indicates which of the senses of the french ambiguous word is used.
</nextsent>
<nextsent>the authors report 20% improvement in the performance of machine translation system when the words are first disambiguated this way.
</nextsent>
<nextsent>of the recent wsd systems proposed in the literature, almost all have the property that the knowledge is fixed when the system completes the training phase.
</nextsent>
<nextsent>that means the acquired knowledge never expands during the course of disambiguation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W439">
<title id=" P98-1037.xml">a concept based adaptive approach to word sense disambiguation </title>
<section> experiments  and discussions.  </section>
<citcontext>
<prevsection>
<prevsent>of the recent wsd systems proposed in the literature, almost all have the property that the knowledge is fixed when the system completes the training phase.
</prevsent>
<prevsent>that means the acquired knowledge never expands during the course of disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
gale, et al (1992<papid> H92-1045 </papid>a) report hat if one had obtained set of training materials with errors no more than twenty to thirty percent, one could iterate training materials election just once or twice and have training sets that had less than ten percent errors.</citsent>
<aftsection>
<nextsent>the adaptive approach is somehow similar to their idea of incremental learning and to the bootstrap approach proposed by yarowsky (1995).<papid> P95-1026 </papid></nextsent>
<nextsent>however, both approaches are still considered static models which are changed only in the training phase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W441">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>text segmentation according to topical criterion is useful process in many applications, such as text summarization or information extraction task.
</prevsent>
<prevsent>approaches that address this problem can be classified in knowledge-based approaches or word-based approaches.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
knowledge-based systems as grosz and sidner (1986) <papid> J86-3001 </papid>require an extensive manual knowledge engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains.</citsent>
<aftsection>
<nextsent>to overcome this limitation, and to process large amount of texts, word-based approaches have been developed.
</nextsent>
<nextsent>hearst (1997) <papid> J97-1003 </papid>and masson (1995) make use of the word distribution in text to find thematic segmentation.</nextsent>
<nextsent>these works are well adapted to technical or scientific texts characterized by specific vocabulary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W442">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>knowledge-based systems as grosz and sidner (1986) <papid> J86-3001 </papid>require an extensive manual knowledge engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains.</prevsent>
<prevsent>to overcome this limitation, and to process large amount of texts, word-based approaches have been developed.</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
hearst (1997) <papid> J97-1003 </papid>and masson (1995) make use of the word distribution in text to find thematic segmentation.</citsent>
<aftsection>
<nextsent>these works are well adapted to technical or scientific texts characterized by specific vocabulary.
</nextsent>
<nextsent>to process narrative or expository texts such as newspaper articles, kozima (1993) <papid> P93-1041 </papid>and morris and hirst (1991) <papid> J91-1002 </papid>approaches are based on lexical cohesion computed from lexical network.</nextsent>
<nextsent>these methods depend on the presence of the text vocabulary inside their network.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W444">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>hearst (1997) <papid> J97-1003 </papid>and masson (1995) make use of the word distribution in text to find thematic segmentation.</prevsent>
<prevsent>these works are well adapted to technical or scientific texts characterized by specific vocabulary.</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
to process narrative or expository texts such as newspaper articles, kozima (1993) <papid> P93-1041 </papid>and morris and hirst (1991) <papid> J91-1002 </papid>approaches are based on lexical cohesion computed from lexical network.</citsent>
<aftsection>
<nextsent>these methods depend on the presence of the text vocabulary inside their network.
</nextsent>
<nextsent>so, to avoid any restriction about domains in such kinds of texts, we present here mixed method that augments masson system (1995), based on word distribution, by using knowledge represented by lexical co-occurrence network automatically built from corpus.
</nextsent>
<nextsent>by making some experiments with these two latter systems, we show that adding lexical knowledge is not sufficient on its own to have an all-purpose method, able to process either technical texts or narratives.
</nextsent>
<nextsent>we will then propose some solutions to choose the more suitable method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W445">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> int roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>hearst (1997) <papid> J97-1003 </papid>and masson (1995) make use of the word distribution in text to find thematic segmentation.</prevsent>
<prevsent>these works are well adapted to technical or scientific texts characterized by specific vocabulary.</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
to process narrative or expository texts such as newspaper articles, kozima (1993) <papid> P93-1041 </papid>and morris and hirst (1991) <papid> J91-1002 </papid>approaches are based on lexical cohesion computed from lexical network.</citsent>
<aftsection>
<nextsent>these methods depend on the presence of the text vocabulary inside their network.
</nextsent>
<nextsent>so, to avoid any restriction about domains in such kinds of texts, we present here mixed method that augments masson system (1995), based on word distribution, by using knowledge represented by lexical co-occurrence network automatically built from corpus.
</nextsent>
<nextsent>by making some experiments with these two latter systems, we show that adding lexical knowledge is not sufficient on its own to have an all-purpose method, able to process either technical texts or narratives.
</nextsent>
<nextsent>we will then propose some solutions to choose the more suitable method.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W446">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> pre-processing of the texts.  </section>
<citcontext>
<prevsection>
<prevsent>as compound nouns are less polysemous than single ones, we have added to mtseg the ability to identify 2300 compound nouns.
</prevsent>
<prevsent>we have retained the most frequent compound nouns in 11 years of the french le monde newspaper.
</prevsent>
</prevsection>
<citsent citstr=" C94-1095 ">
they have been collected with the intex tool of silberztein (1994).<papid> C94-1095 </papid></citsent>
<aftsection>
<nextsent>the part of speech tagger tree tagger of schmid (1994) is applied to disambiguate he lexical category of the words and to provide their lemmatized form.
</nextsent>
<nextsent>the selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing.
</nextsent>
<nextsent>this one is applied to the texts both for building the collocation network and for their thematic segmentation.
</nextsent>
<nextsent>our segmentation mechanism relies on semantic relations between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W447">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> building the collocation network.  </section>
<citcontext>
<prevsection>
<prevsent>in order to evaluate it, we have built network of lexical collocations from large corpus.
</prevsent>
<prevsent>our corpus, whose size is around 39 million words, is made up of 24 months of the le monde newspaper taken from 1990 to 1994.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
the collocations have been calculated according to the method described in church and hanks (1990) <papid> J90-1003 </papid>by moving window on the texts.</citsent>
<aftsection>
<nextsent>the corpus was pre-processed as described above, which induces 63% cut.
</nextsent>
<nextsent>the window in which the collocations have been collected is 20 words wide and takes into account he boundaries of the texts.
</nextsent>
<nextsent>moreover, the collocations here are indifferent to order.
</nextsent>
<nextsent>these three choices are motivated by our task point of view.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W453">
<title id=" P98-1065.xml">thematic segmentation of texts two methods for two kind of texts </title>
<section> related works.  </section>
<citcontext>
<prevsection>
<prevsent>according to our actual studies, has been settled to 25.
</prevsent>
<prevsent>395
</prevsent>
</prevsection>
<citsent citstr=" C94-2187 ">
without taking into account the collocation network, the methods described above relyon the same principles as hearst (1997) <papid> J97-1003 </papid>and nomoto and nitta (1994).<papid> C94-2187 </papid></citsent>
<aftsection>
<nextsent>although hearst considers that paragraph breaks are sometimes invoked only for lightening the physical appearance of texts, we have chosen paragraphs as basic units because they are more natural thematic units than somewhat arbitrary sets of words.
</nextsent>
<nextsent>we assume that paragraph breaks that indicate topic changes are always present in texts.
</nextsent>
<nextsent>those which are set for visual reasons are added between them and the segmentation algorithm is able to join them again.
</nextsent>
<nextsent>of course, the size of actual paragraphs are sometimes irregular.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W455">
<title id=" P96-1033.xml">magic for filter optimization in dynamic bottom up processing </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>this can lead to non termination as the tree fragments enumer-ated in bottom-up evaluation of magic compiled grammars are connected (johnson, forthcoming).
</prevsent>
<prevsent>more specifically,  magic generation  falls prey to non-termination in the face of head recur sion, i.e., the generation analog of left recur sion in parsing.
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
this necessitates dynamic processing strategy, i.e., memo ization, extended with an abstraction function like, e.g., restriction (shieber, 1985), <papid> P85-1018 </papid>to weaken fil-tering and subsumption check to discard redun-dant results.</citsent>
<aftsection>
<nextsent>it is shown that for large class of grammars the subsumption check which often influ-ences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic pred-icates derived for particular grammar after apply-ing an abstraction function in an off-line fashion.
</nextsent>
<nextsent>unfolding can be used to eliminate superfluous fil-tering steps.
</nextsent>
<nextsent>given an off-line optimization of the order in which the right-hand side categories in the rules of logic grammar are processed (minnen et al., 1996) the resulting processing behavior can be considered generalization the head corner gen-eration approach (shieber et al, 1990): <papid> J90-1004 </papid>without he need to relyon notions such as semantic head and chain rule, head corner behavior can be mimicked in strict bottom-up fashion.</nextsent>
<nextsent>247</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W457">
<title id=" P96-1033.xml">magic for filter optimization in dynamic bottom up processing </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>it is shown that for large class of grammars the subsumption check which often influ-ences processing efficiency rather dramatically can be eliminated through fine-tuning of the magic pred-icates derived for particular grammar after apply-ing an abstraction function in an off-line fashion.
</prevsent>
<prevsent>unfolding can be used to eliminate superfluous fil-tering steps.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
given an off-line optimization of the order in which the right-hand side categories in the rules of logic grammar are processed (minnen et al., 1996) the resulting processing behavior can be considered generalization the head corner gen-eration approach (shieber et al, 1990): <papid> J90-1004 </papid>without he need to relyon notions such as semantic head and chain rule, head corner behavior can be mimicked in strict bottom-up fashion.</citsent>
<aftsection>
<nextsent>247
</nextsent>
<nextsent>of filtering many approaches focus on exploiting specific knowl-edge about grammars and/or the computational task(s) that one is using them for by making filter-ing explicit and extending the processing strategy such that this information can be made effective.
</nextsent>
<nextsent>in generation, examples of such extended process-ing strategies are head corner generation with its semantic linking (shieber et al, 1990) <papid> J90-1004 </papid>or bottom-up (earley) generation with semantic filter (shieber, 1988).<papid> C88-2128 </papid></nextsent>
<nextsent>even though these approaches often accom-plish considerable improvements with respect ef-ficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W459">
<title id=" P96-1033.xml">magic for filter optimization in dynamic bottom up processing </title>
<section> definite clause characterization.  </section>
<citcontext>
<prevsection>
<prevsent>247
</prevsent>
<prevsent>of filtering many approaches focus on exploiting specific knowl-edge about grammars and/or the computational task(s) that one is using them for by making filter-ing explicit and extending the processing strategy such that this information can be made effective.
</prevsent>
</prevsection>
<citsent citstr=" C88-2128 ">
in generation, examples of such extended process-ing strategies are head corner generation with its semantic linking (shieber et al, 1990) <papid> J90-1004 </papid>or bottom-up (earley) generation with semantic filter (shieber, 1988).<papid> C88-2128 </papid></citsent>
<aftsection>
<nextsent>even though these approaches often accom-plish considerable improvements with respect ef-ficiency or termination behavior, it remains unclear how these optimizations relate to each other and what comprises the logic behind these specialized forms of filtering.
</nextsent>
<nextsent>by bringing filtering into the logic underlying the grammar it is possible to show in perspicuous and logically clean way how and why fil-tering can be optimized in particular fashion and how various approaches relate to each other.
</nextsent>
<nextsent>2.1 magic compilation.
</nextsent>
<nextsent>magic makes filtering explicit hrough characterizing it as definite clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W466">
<title id=" P96-1049.xml">on reversing the generation process in optimality theory </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>optimality theory, constraint-based phonol- ogy and morphology paradigm, has allowed linguists to make elegant analyses of many phenomena, including in fixation and redupli-cation.
</prevsent>
</prevsection>
<citsent citstr=" C94-2163 ">
in this work-in-progress, we build on the work of ellison (1994) <papid> C94-2163 </papid>to investigate the possibility of using ot as parsing tool that derives underlying forms from surface forms.</citsent>
<aftsection>
<nextsent>a. derivational phonology b. optimality theory figure i: search spaces within different paradigms
</nextsent>
<nextsent>optimality theory (prince and smolensky, 1993) is constraint-based phonological nd morphological system that allows viola ble constraints in deriving output sur-face forms from underlying forms.
</nextsent>
<nextsent>in ot system of constraints selects an  optimal  surface output from set of candidates.
</nextsent>
<nextsent>the methodology allows succinct anal-yses of phenomena such as in fixation and reduplication that were difficult to describe under sets of transforma-tional rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W471">
<title id=" P98-1076.xml">one tokenization per source </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for instance, the character string todayissunday would normally be tokenized as 457   today is sunday  but can also reasonably be   today is sun day .
</prevsent>
<prevsent>in terms of possibility, it has been argued that no lexically possible tokenization can not be grammatically and meaningfully realized in at least some special contexts, as every token can be assigned to bear any meaning without any orthographic means.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
consequently, the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts, either linguistically in rule-based framework or statistically in searching and optimization set-up (gan, palmer and lua 1996; sproat, shih, gale and chang 1996; wu 1997; <papid> J97-3002 </papid>gut 1997).</citsent>
<aftsection>
<nextsent>hence, it was really surprise when we first observed the regularity of one tokenization per source.
</nextsent>
<nextsent>nevertheless, the regularity turns out to be very helpful in sentence tokenization practice, and to be with far-reaching implications in natural language processing.
</nextsent>
<nextsent>retrospectively, we now understand that it is by no means an isolated special phenomenon but another display of the postulated general law of one realization per expression.
</nextsent>
<nextsent>in the rest of the paper, we will first present concrete corpus verification (section 2), clarify its meaning and scope (section 3), display its striking utility value in tokenization (section 4), and then disclose its implication for the notion of words/tokens (section 5), and associate the hypothesis with the general aw of one realization per expression through examination of related works in the literature (section 6).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W472">
<title id=" P98-1076.xml">one tokenization per source </title>
<section> corpus investigation.  </section>
<citcontext>
<prevsection>
<prevsent>to ensure consistency, the manual tokenization reported in this paper has been independently done twice under the following three criteria, applied in that order: (1) dictionary existence: the tokenization contains no non-dictionary-entry character fragment.
</prevsent>
<prevsent>(2) structural consistency: the tokenization has no crossing-brackets (black, garside and leech 1993) with at least one correct and complete structural analysis of its underlying sentence.
</prevsent>
</prevsection>
<citsent citstr=" J97-4004 ">
(3) maximum tokenization: the tokenization is critical tokenization (guo 1997).<papid> J97-4004 </papid></citsent>
<aftsection>
<nextsent>the basic idea behind is to regard sentence tokenization as (shallow) type of (phrase- structure-like) morpho-syntactic parsing which is to assign tree-like structure to sentence.
</nextsent>
<nextsent>the tokenization of sentence is taken to be the single-layer bracketing corresponding to the highest-possible cross-section of the sentence tree, with each bracket token in dictionary.
</nextsent>
<nextsent>among the three criteria, both the criterion of dictionary existence and that of maximum tokenization are well-defined without any uncertainty, as long as the tokenization dictionary is specified.
</nextsent>
<nextsent>however, the criterion of structural consistency is somewhat under-specified since the same linguistic expression may have different sentence structural analyses under different grammatical theories and/or formalisms, and it may be read differently by different people.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W483">
<title id=" P98-1076.xml">one tokenization per source </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, zheng and liu (1997) themselves merely took the apparent regularity as special case, and focused on the development of local- context-oriented disambiguation rules.
</prevsent>
<prevsent>moreover, while they constructed for tokenization disambiguation annotated  phrase base  of all ambiguous fragments in the large corpus, they still concluded that good results can not come solely from corpus but have to relyon the utilization of syntactic, semantic, pragmatic and other information.
</prevsent>
</prevsection>
<citsent citstr=" J96-3004 ">
the actual implementation the weighted finite- state transducer by sproat et al (1996) <papid> J96-3004 </papid>can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.</citsent>
<aftsection>
<nextsent>while the primary strength of such transducer is its effectiveness in representing and 6 roughly three-character fragment abc where a, b, c,.
</nextsent>
<nextsent>ab, and bc are all tokens in the tokenization dictionary.
</nextsent>
<nextsent>c, d, ab, bc, and cd are all tokens in the tokenization dictionary.
</nextsent>
<nextsent>utilizing local and sentential constraints, what sproat et al (1996) <papid> J96-3004 </papid>implemented was simply token unigram scoring function.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W487">
<title id=" P98-1076.xml">one tokenization per source </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>under this setting, no critical fragment can realize different tokenizations in different local sentential context, since no local constraints other than the identity of token together with its associated token score can be utilized.
</prevsent>
<prevsent>that is, the requirement of one tokenization per source has actually been implicitly obeyed.
</prevsent>
</prevsection>
<citsent citstr=" H93-1052 ">
we admit here that, while we have been aware of the fact for long time, only after the dissemination of the closely related hypotheses of one sense per discourse (gale, church and yarowsky 1992)  and one sense per collocation (yarowsky 1993), <papid> H93-1052 </papid>we are able to articulate the hypothesis of one tokenization per source.</citsent>
<aftsection>
<nextsent>the point here is that, one tokenization per source is unlikely an isolated phenomenon.
</nextsent>
<nextsent>rather, there must exist general law that covers all the related linguistic phenomena.
</nextsent>
<nextsent>let us speculate that, for proper linguistic expression in proper scope, there always exists the regularity of one realization per expression.
</nextsent>
<nextsent>that is, only one of the multiple values on one aspect of linguistic expression can be realized in the specified scope.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W488">
<title id=" P95-1023.xml">tal recognition in omn2 time </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>they have been found to be good grammatical systems for natural lan-guages (kroch, joshi, 1985).
</prevsent>
<prevsent>the first polynomial time parsing algorithm for tals was given by vi- jaya shanker and :loshi (1986), which had run time of o(n6), for an input of size n. their algorithm had flavor similar to the cocke-younger-kasami (cyk) algorithm for context-free grammars.
</prevsent>
</prevsection>
<citsent citstr=" P88-1032 ">
an earley-type parsing algorithm has been given by schabes and joshi (1988).<papid> P88-1032 </papid></citsent>
<aftsection>
<nextsent>an optimal inear time parallel parsing algorithm for tals was given by palls, shende and wei (1990).
</nextsent>
<nextsent>in recent paper, rajasekaran (1995) shows how tals can be parsed in time o(n3m(n)).
</nextsent>
<nextsent>in this paper, we propose an o(m(n2)) time recognition algorithm for tals, where m(k) is the time needed to multiply two x boolean matri-ces.
</nextsent>
<nextsent>the best known value for m(k) is o(n 2 3vs) (coppersmith, winograd, 1990).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W489">
<title id=" P98-1036.xml">proper name translation in cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2) words in query may be ambiguous, thus disambiguation is required.
</prevsent>
<prevsent>(3) queries are usually short, thus expansion is required.
</prevsent>
</prevsection>
<citsent citstr=" C96-1039 ">
(4) word boundary in queries of some languages (chen and lee, 1996) <papid> C96-1039 </papid>is not clear, thus segmentation is required.</citsent>
<aftsection>
<nextsent>(5) document may be in more than one language, thus language identification is required.
</nextsent>
<nextsent>this paper focuses on query translation issue, proper name in particular.
</nextsent>
<nextsent>the percentage of user queries containing proper names is very high.
</nextsent>
<nextsent>the paper (thompson and dozier, 1997) <papid> W97-0315 </papid>reported an experiment over periods of several days in 1995.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W490">
<title id=" P98-1036.xml">proper name translation in cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper focuses on query translation issue, proper name in particular.
</prevsent>
<prevsent>the percentage of user queries containing proper names is very high.
</prevsent>
</prevsection>
<citsent citstr=" W97-0315 ">
the paper (thompson and dozier, 1997) <papid> W97-0315 </papid>reported an experiment over periods of several days in 1995.</citsent>
<aftsection>
<nextsent>it showed 67.8%, 83.4%, and 38.8% of queries to wall street journal, los angeles times, and washington post, respectively, involve name searching.
</nextsent>
<nextsent>in clir, three tasks are needed: name identification, ame translation, and name searching.
</nextsent>
<nextsent>because proper names are usually unknown words, it is hard to find in monolingual dictionary not to mention bilingual dictionary.
</nextsent>
<nextsent>coverage is one of the major problems in dictionary-based approaches (ballesteros and croft, 1996; davis, 1997; hull and grefenstette, 1996).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W491">
<title id=" P98-1036.xml">proper name translation in cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>because proper names are usually unknown words, it is hard to find in monolingual dictionary not to mention bilingual dictionary.
</prevsent>
<prevsent>coverage is one of the major problems in dictionary-based approaches (ballesteros and croft, 1996; davis, 1997; hull and grefenstette, 1996).
</prevsent>
</prevsection>
<citsent citstr=" C96-1030 ">
corpus-based approaches (brown, 1996; <papid> C96-1030 </papid>oard 1996; sheridan and ballerini, 1996) set up thesaurus from large- scale corpora.</citsent>
<aftsection>
<nextsent>they provide narrow but specific coverage of the language, and are complementary to broad and shallow coverage in dictionaries.
</nextsent>
<nextsent>however, domain shifts and term align accuracy are major limitations of corpus-based approaches.
</nextsent>
<nextsent>besides, proper names are infrequent words relative to other content words in corpora.
</nextsent>
<nextsent>in information retrieval, most frequent and less frequent words are regarded as unimportant words and may be neglected.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W492">
<title id=" P98-1036.xml">proper name translation in cross language information retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus name extraction is indispensable for both natural anguage understanding and information retrieval.
</prevsent>
<prevsent>in famous message understanding system evaluation and message understanding conferences (muc) and the related multilingual entity tasks (met), named entity, which covers named organizations, people, and locations, along with date/time xpressions and monetary and percentage expressions, is one of tasks for evaluating technologies.
</prevsent>
</prevsection>
<citsent citstr=" M95-1018 ">
in muc-6 named entity task, the systems developed by sra (krupka, 1995) <papid> M95-1018 </papid>and bbn (weischedel, 1995) on the person name recognition portion have very high recall and precision scores (over 94%).</citsent>
<aftsection>
<nextsent>in chinese language processing, chert and lee (1996) present various strategies to identify and classify three types of proper nouns, i.e., chinese person names, chinese transliterated person names and organization names.
</nextsent>
<nextsent>in large-scale xperiments, the average precision rate is 88.04% and the average recall rate is 92.56% for the identification of chinese person names.
</nextsent>
<nextsent>the above approaches can be employed to collect chinese and english proper name sets from www (very large-scale corpora).
</nextsent>
<nextsent>identification of proper names in queries is different from that in large-scale texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W493">
<title id=" P93-1001.xml">char align a program for aligning parallel texts at the character level </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P91-1022 ">
there have been number of recent papers on aligning parallel texts at the sentence vel, e.g., brown et al (1991), <papid> P91-1022 </papid>gale and church (to appear), isabelle (1992), kay and r/ssenschein (to appear), simard et al (1992), warwick- armstrong and russell (1990).</citsent>
<aftsection>
<nextsent>on clean inputs, such as the canadian hansa rds, these methods have been very successful (at least 96% correct by sentence).
</nextsent>
<nextsent>unfortunately, if the input is noisy (due to ocr and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let al ne sentences.
</nextsent>
<nextsent>this paper describes new program, charal ign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by simard et al
</nextsent>
<nextsent>parallel texts have recently received considerable attention in machine translation (e.g., brown et al  1990), <papid> J90-2002 </papid>bilingual lexicography (e.g., klavans and tzoukermann, 1990), and terminology research for human translators (e.g., isabelle, 1992).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W494">
<title id=" P93-1001.xml">char align a program for aligning parallel texts at the character level </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, if the input is noisy (due to ocr and/or unknown markup conventions), then these methods tend to break down because the noise can make it difficult to find paragraph boundaries, let al ne sentences.
</prevsent>
<prevsent>this paper describes new program, charal ign, that aligns texts at the character level rather than at the sentence/paragraph level, based on the cognate approach proposed by simard et al
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
parallel texts have recently received considerable attention in machine translation (e.g., brown et al  1990), <papid> J90-2002 </papid>bilingual lexicography (e.g., klavans and tzoukermann, 1990), and terminology research for human translators (e.g., isabelle, 1992).</citsent>
<aftsection>
<nextsent>we have been most interested in the terminology application.
</nextsent>
<nextsent>translators find it extremely embarrassing when  store  (in the computer sense) is translated as  grocery,  or when  magnetic fields  is translated as  magnetic meadows.
</nextsent>
<nextsent>terminology errors of this kind are all too common because the translator is generally not as familiar with the subject domain as the author of the source text or the readers of the target text.
</nextsent>
<nextsent>parallel texts could be used to help translators overcome their lack of domain expertise by providing them with the ability to search previously translated documents for examples of potentially difficult expressions and see how they were translated in the past.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W495">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a set of rules which on the basis of ending characters of unknown words, assign them with sets of possible pos-tags is supplied with the xerox tag-ger (kupiec, 1992).
</prevsent>
<prevsent>a similar approach was taken in (weischedel al., 1993) where an unknown word was guessed given the probabilities for an unknown word to be of particular pos, its capitalisation fea-ture and its ending.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
in (brill, 1995) <papid> J95-4004 </papid>system of rules which uses both ending-guessing and more morpho-logically motivated rules is described.</citsent>
<aftsection>
<nextsent>the best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g.
</nextsent>
<nextsent>(brill, 1995; <papid> J95-4004 </papid>weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>the major topic in the development of word-pos guess ers is the strategy which is to be used for the acquisition of the guessing rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W498">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in (brill, 1995) <papid> J95-4004 </papid>system of rules which uses both ending-guessing and more morpho-logically motivated rules is described.</prevsent>
<prevsent>the best of these methods are reported to achieve 82-85% of tagging accuracy on unknown words, e.g.</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
(brill, 1995; <papid> J95-4004 </papid>weischedel et al, 1993).<papid> J93-2006 </papid></citsent>
<aftsection>
<nextsent>the major topic in the development of word-pos guess ers is the strategy which is to be used for the acquisition of the guessing rules.
</nextsent>
<nextsent>a rule-based tag-ger described in (voutilainen, 1995) <papid> E95-1022 </papid>is equipped with set of guessing rules which has been hand-crafted using knowledge of english morphology and intu- ition.</nextsent>
<nextsent>a more appealing approach is an empiri-cal automatic acquisition of such rules using avail-able lexical resources.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W499">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(brill, 1995; <papid> J95-4004 </papid>weischedel et al, 1993).<papid> J93-2006 </papid></prevsent>
<prevsent>the major topic in the development of word-pos guess ers is the strategy which is to be used for the acquisition of the guessing rules.</prevsent>
</prevsection>
<citsent citstr=" E95-1022 ">
a rule-based tag-ger described in (voutilainen, 1995) <papid> E95-1022 </papid>is equipped with set of guessing rules which has been hand-crafted using knowledge of english morphology and intu- ition.</citsent>
<aftsection>
<nextsent>a more appealing approach is an empiri-cal automatic acquisition of such rules using avail-able lexical resources.
</nextsent>
<nextsent>in (zhang&kim;, 1990) <papid> C90-2074 </papid>system for the automated learning of morphologi-cal word-formation rules is described.</nextsent>
<nextsent>this system divides string into three regions and from train-ing examples infers their correspondence to under- lying morphological features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W500">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a rule-based tag-ger described in (voutilainen, 1995) <papid> E95-1022 </papid>is equipped with set of guessing rules which has been hand-crafted using knowledge of english morphology and intu- ition.</prevsent>
<prevsent>a more appealing approach is an empiri-cal automatic acquisition of such rules using avail-able lexical resources.</prevsent>
</prevsection>
<citsent citstr=" C90-2074 ">
in (zhang&kim;, 1990) <papid> C90-2074 </papid>system for the automated learning of morphologi-cal word-formation rules is described.</citsent>
<aftsection>
<nextsent>this system divides string into three regions and from train-ing examples infers their correspondence to under- lying morphological features.
</nextsent>
<nextsent>brill (brill, 1995) <papid> J95-4004 </papid>out-lines transformation-based arner which learns guessing rules from pre-tagged training corpus.</nextsent>
<nextsent>a statistical-based suffix learner is presented in (schmid, 1994).<papid> C94-1027 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W503">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this system divides string into three regions and from train-ing examples infers their correspondence to under- lying morphological features.
</prevsent>
<prevsent>brill (brill, 1995) <papid> J95-4004 </papid>out-lines transformation-based arner which learns guessing rules from pre-tagged training corpus.</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
a statistical-based suffix learner is presented in (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>from pre-tagged training cor-pus it constructs the suffix tree where every suf-fix is associated with its information measure.
</nextsent>
<nextsent>al-though the learning process in these and some other systems is fully unsupervised and the accuracy of obtained rules reaches current state-of-the-art, they require specially prepared training data - - pre- tagged training corpus, training examples, etc. in this paper we describe new fully automatic technique for learning part-of-speech guessing rules.
</nextsent>
<nextsent>this technique does not require specially prepared training data and employs fully unsupervised statis-tical learning using the lexicon supplied with the tag-ger and word-frequencies obtained from raw cor-pus.
</nextsent>
<nextsent>the learning is implemented as two-staged process with feedback.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W505">
<title id=" P96-1043.xml">unsupervised learning of word category guessing rules </title>
<section> tagging unknown words.  </section>
<citcontext>
<prevsection>
<prevsent>so we did not worry too much about tuning the taggers for the texts and used the brown corpus model instead.
</prevsent>
<prevsent>we tagged several texts of different origins, except from the brown corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
these texts were not seen at the training phase which means that neither the 6since brill tagger was trained on the penn tag-set (marcus et al, 1993) <papid> J93-2004 </papid>we provided an additional mapping.</citsent>
<aftsection>
<nextsent>331 taggers nor the guess ers had been trained on these texts and they naturally had words unknown to the lexicon.
</nextsent>
<nextsent>for each text we performed two tagging experiments.
</nextsent>
<nextsent>in the first experiment we tagged the text with the brown corpus lexicon supplied with the taggers and hence had only those unknown words which naturally occur in this text.
</nextsent>
<nextsent>in the second ex-periment we tagged the same text with the lexicon which contained only closed-class 7 and short 8 words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W506">
<title id=" P97-1030.xml">mistake driven mixture of hierarchical tag context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, the method incorporates not only frequent connec-tions but also infrequent ones that are of- ten considered to be collocationah we evaluate several tag models by implement-ing japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.
</prevsent>
<prevsent>the experimental results show the proposed method significantly outper-forms both hand-crafted and conventional statistical methods.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
the last few years have seen the great success of stochastic part-of-speech (pos) taggers (church, 1988: <papid> A88-1019 </papid>kupiec, 1992; charniak et m., 1993; brill, 1992; <papid> A92-1021 </papid>nagata, 1994).<papid> C94-1032 </papid></citsent>
<aftsection>
<nextsent>the stochastic approach gen-erally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.
</nextsent>
<nextsent>however, 1ntt is an abbreviation of nippon telegraph and telephone corporation.
</nextsent>
<nextsent>practical systems require more accuracy because pos tagging is an inevitable pre-processing step for all practical systems.
</nextsent>
<nextsent>to derive new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W507">
<title id=" P97-1030.xml">mistake driven mixture of hierarchical tag context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, the method incorporates not only frequent connec-tions but also infrequent ones that are of- ten considered to be collocationah we evaluate several tag models by implement-ing japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.
</prevsent>
<prevsent>the experimental results show the proposed method significantly outper-forms both hand-crafted and conventional statistical methods.
</prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
the last few years have seen the great success of stochastic part-of-speech (pos) taggers (church, 1988: <papid> A88-1019 </papid>kupiec, 1992; charniak et m., 1993; brill, 1992; <papid> A92-1021 </papid>nagata, 1994).<papid> C94-1032 </papid></citsent>
<aftsection>
<nextsent>the stochastic approach gen-erally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.
</nextsent>
<nextsent>however, 1ntt is an abbreviation of nippon telegraph and telephone corporation.
</nextsent>
<nextsent>practical systems require more accuracy because pos tagging is an inevitable pre-processing step for all practical systems.
</nextsent>
<nextsent>to derive new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W508">
<title id=" P97-1030.xml">mistake driven mixture of hierarchical tag context trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in other words, the method incorporates not only frequent connec-tions but also infrequent ones that are of- ten considered to be collocationah we evaluate several tag models by implement-ing japanese part-of-speech taggers that share all other conditions (i.e.,dictionary and word model) other than their tag models.
</prevsent>
<prevsent>the experimental results show the proposed method significantly outper-forms both hand-crafted and conventional statistical methods.
</prevsent>
</prevsection>
<citsent citstr=" C94-1032 ">
the last few years have seen the great success of stochastic part-of-speech (pos) taggers (church, 1988: <papid> A88-1019 </papid>kupiec, 1992; charniak et m., 1993; brill, 1992; <papid> A92-1021 </papid>nagata, 1994).<papid> C94-1032 </papid></citsent>
<aftsection>
<nextsent>the stochastic approach gen-erally attains 94 to 96% accuracy and replaces the labor-intensive compilation of linguistics rules by using an automated learning algorithm.
</nextsent>
<nextsent>however, 1ntt is an abbreviation of nippon telegraph and telephone corporation.
</nextsent>
<nextsent>practical systems require more accuracy because pos tagging is an inevitable pre-processing step for all practical systems.
</nextsent>
<nextsent>to derive new stochastic tagger, we have two options since stochastic taggers generally comprise two components: word model and tag model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W509">
<title id=" P97-1030.xml">mistake driven mixture of hierarchical tag context trees </title>
<section> p re iminar ies   </section>
<citcontext>
<prevsection>
<prevsent>bi-grams of subdivision are too general to selectively detect exceptions.
</prevsent>
<prevsent>6 re la ted work.
</prevsent>
</prevsection>
<citsent citstr=" W95-0108 ">
although statistical natural anguage processing has mainly focused on maximum likelihood estimators, (pereira et al, 1995) <papid> W95-0108 </papid>proposed mixture approach to predict next words by using the context tree weighting (ctw) method .(willems et al, 1995).</citsent>
<aftsection>
<nextsent>the ctw method computes probability by mixing subtrees in single context ree in bayesian fashion.
</nextsent>
<nextsent>although the method is very efficient, it cannot be used to construct hierarchical tag context rees.
</nextsent>
<nextsent>various kinds of re-sampling techniques have been studied in statistics (efron, 1979; efron and tibshi-rani, 1993) and machine learning (breiman, 1996; hull et al, 1996; freund and schapire, 1996a).
</nextsent>
<nextsent>in particular, the mistake-driven mixture algorithm 235 g- f_ 97 95 94 93  92 91 90 i mixture of bikjrarns .e- mixture of context trees -+--- - . -   . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W510">
<title id=" P98-1098.xml">combining a chinese thesaurus with a chinese dictionary </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>a manual effort has been made to build resource for english, i.e., wordnet, which contains both definition and classification information (miller et al, 1990), but such resources are not available for many other languages, e.g. chinese.
</prevsent>
<prevsent>this paper presents an automatic method to combine the chinese thesaurus with the chinese dictionary into such resource, by tagging the entries in the thesaurus with appropriate senses in the dictionary, meanwhile assigning appropriate semantic odes, which stand for semantic categories in the thesaurus, to the senses in the dictionary.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
d.yarowsky has considered similar problem to link roget categories, an english thesaurus, with the senses in cobuild, an english dictionary (yarowsky, 1992).<papid> C92-2070 </papid></citsent>
<aftsection>
<nextsent>he treats the problem as sense disambiguation e, with the definitions in the dictionary taken as kind of contexts in which the headwords occur, and deals with it based on statistical model of roget categories trained on large corpus.
</nextsent>
<nextsent>in our opinion, the method, for specific word, neglects the difference between its definitions and the ordinary contexts: definitions generally contain its synonyms, hyponyms or hypernyms, etc., while ordinary contexts generally its collocations.
</nextsent>
<nextsent>so the trained model on ordinary contexts may be not appropriate for the disambiguation problem in definition contexts.
</nextsent>
<nextsent>a seemingly reasonable method to the problem would be common word strategy, which has been extensively studied by many researchers (e.g., knight, 1993; <papid> H93-1036 </papid>lesk, 1986).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W511">
<title id=" P98-1098.xml">combining a chinese thesaurus with a chinese dictionary </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in our opinion, the method, for specific word, neglects the difference between its definitions and the ordinary contexts: definitions generally contain its synonyms, hyponyms or hypernyms, etc., while ordinary contexts generally its collocations.
</prevsent>
<prevsent>so the trained model on ordinary contexts may be not appropriate for the disambiguation problem in definition contexts.
</prevsent>
</prevsection>
<citsent citstr=" H93-1036 ">
a seemingly reasonable method to the problem would be common word strategy, which has been extensively studied by many researchers (e.g., knight, 1993; <papid> H93-1036 </papid>lesk, 1986).</citsent>
<aftsection>
<nextsent>the solution 600 would be, for category, to select those senses whose definitions hold most number of common words among all those for its member words.
</nextsent>
<nextsent>but the words in category in the chinese thesaurus may be not similar in strict way, although similar to some extend, so their definitions may only contain some similar words at most, rather than share many words.
</nextsent>
<nextsent>as result, the common word strategy may be not appropriate for the problem we study here.
</nextsent>
<nextsent>in this paper, we extend the idea of common word strategy further to similar word method based on the intuition that definitions for similar senses generally contain similar words, if not the same ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W512">
<title id=" P98-1032.xml">automated scoring using a hybrid feature identification technique </title>
<section> hybrid feature methodology.  </section>
<citcontext>
<prevsection>
<prevsent>this is part of the reason that human readers must also relyon cue words to identify new arguments in an essay.
</prevsent>
<prevsent>literature in the field of discourse analysis supports our approach.
</prevsent>
</prevsection>
<citsent citstr=" P84-1055 ">
it points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (cohen (1984), (<papid> P84-1055 </papid>mann and thompson (1988), hovy, et al(1992), hirschberg and litman (1993), <papid> J93-3003 </papid>vander linden and martin (1995), and knott (1996)).</citsent>
<aftsection>
<nextsent>e-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information.
</nextsent>
<nextsent>we adapted the conceptual framework of conjunctive relations from quirk, et ai (1985) in which cue terms, such as  in summary  and  in conclusion,  are classified as conjuncts used for summarizing.
</nextsent>
<nextsent>cue words such as  perhaps,  and  possibly  are considered to be  belief  words used by the writer to express belief in developing an argument in the essay.
</nextsent>
<nextsent>words like  this  and  these  may often be used to flag that the writer has not changed topics (sidner (1986)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W513">
<title id=" P98-1032.xml">automated scoring using a hybrid feature identification technique </title>
<section> hybrid feature methodology.  </section>
<citcontext>
<prevsection>
<prevsent>this is part of the reason that human readers must also relyon cue words to identify new arguments in an essay.
</prevsent>
<prevsent>literature in the field of discourse analysis supports our approach.
</prevsent>
</prevsection>
<citsent citstr=" J93-3003 ">
it points out that rhetorical cue words and structures can be identified and used for computer-based discourse analysis (cohen (1984), (<papid> P84-1055 </papid>mann and thompson (1988), hovy, et al(1992), hirschberg and litman (1993), <papid> J93-3003 </papid>vander linden and martin (1995), and knott (1996)).</citsent>
<aftsection>
<nextsent>e-rater follows this approach by using rhetorical cue words and structure features, in addition to other topical and syntactic information.
</nextsent>
<nextsent>we adapted the conceptual framework of conjunctive relations from quirk, et ai (1985) in which cue terms, such as  in summary  and  in conclusion,  are classified as conjuncts used for summarizing.
</nextsent>
<nextsent>cue words such as  perhaps,  and  possibly  are considered to be  belief  words used by the writer to express belief in developing an argument in the essay.
</nextsent>
<nextsent>words like  this  and  these  may often be used to flag that the writer has not changed topics (sidner (1986)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W514">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>each of these stages is modeled as statistical process.
</prevsent>
<prevsent>the models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
a recent trend in natural anguage processing has been toward greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (church 1988), <papid> A88-1019 </papid>and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum (weischedel et al 1993) and nyu proteus (grishman and sterling 1993).</citsent>
<aftsection>
<nextsent>more recently, statistical methods have been applied to domain-specific semantic parsing (miller et al 1994), <papid> P94-1004 </papid>and to the more difficult problem of wide-coverage syntactic parsing (magerman 1995).<papid> P95-1037 </papid></nextsent>
<nextsent>nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as at&t; chronus (levin and pieraccini 1995), continue to require significant rule based component.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W515">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.
</prevsent>
<prevsent>a recent trend in natural anguage processing has been toward greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (church 1988), <papid> A88-1019 </papid>and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum (weischedel et al 1993) and nyu proteus (grishman and sterling 1993).</prevsent>
</prevsection>
<citsent citstr=" P94-1004 ">
more recently, statistical methods have been applied to domain-specific semantic parsing (miller et al 1994), <papid> P94-1004 </papid>and to the more difficult problem of wide-coverage syntactic parsing (magerman 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as at&t; chronus (levin and pieraccini 1995), continue to require significant rule based component.
</nextsent>
<nextsent>development of complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including (miller et al 1995) and (koppelman et al 1995).
</nextsent>
<nextsent>in this paper, we present such system.
</nextsent>
<nextsent>the overall structure of our approach is conventional, consisting of parser, semantic interpreter, and discourse module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W516">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.
</prevsent>
<prevsent>a recent trend in natural anguage processing has been toward greater emphasis on statistical approaches, beginning with the success of statistical part-of-speech tagging programs (church 1988), <papid> A88-1019 </papid>and continuing with other work using statistical part-of-speech tagging programs, such as bbn plum (weischedel et al 1993) and nyu proteus (grishman and sterling 1993).</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
more recently, statistical methods have been applied to domain-specific semantic parsing (miller et al 1994), <papid> P94-1004 </papid>and to the more difficult problem of wide-coverage syntactic parsing (magerman 1995).<papid> P95-1037 </papid></citsent>
<aftsection>
<nextsent>nevertheless, most natural language systems remain primarily rule based, and even systems that do use statistical techniques, such as at&t; chronus (levin and pieraccini 1995), continue to require significant rule based component.
</nextsent>
<nextsent>development of complete end-to-end statistical understanding system has been the focus of several ongoing research efforts, including (miller et al 1995) and (koppelman et al 1995).
</nextsent>
<nextsent>in this paper, we present such system.
</nextsent>
<nextsent>the overall structure of our approach is conventional, consisting of parser, semantic interpreter, and discourse module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W518">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the focus of this work is primarily to extract sufficient information from each utterance to give an appropriate response to user request.
</prevsent>
<prevsent>a variety of problems regarded as standard in computational inguistics, such as quantification, reference and the like, are thus ignored.
</prevsent>
</prevsection>
<citsent citstr=" H90-1022 ">
to evaluate our approach, we trained an experimental system using data from the air travel information (atis) domain (bates et al 1990; <papid> H90-1022 </papid>price 1990).<papid> H90-1020 </papid></citsent>
<aftsection>
<nextsent>the selection of atis was motivated by three concerns.
</nextsent>
<nextsent>first, large corpus of atis sentences already exists and is readily available.
</nextsent>
<nextsent>second, atis provides an existing evaluation methodology, complete with independent training and test corpora, and scoring programs.
</nextsent>
<nextsent>finally, evaluating on common corpus makes it easy to compare the performance of the system with those based on different approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W519">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the focus of this work is primarily to extract sufficient information from each utterance to give an appropriate response to user request.
</prevsent>
<prevsent>a variety of problems regarded as standard in computational inguistics, such as quantification, reference and the like, are thus ignored.
</prevsent>
</prevsection>
<citsent citstr=" H90-1020 ">
to evaluate our approach, we trained an experimental system using data from the air travel information (atis) domain (bates et al 1990; <papid> H90-1022 </papid>price 1990).<papid> H90-1020 </papid></citsent>
<aftsection>
<nextsent>the selection of atis was motivated by three concerns.
</nextsent>
<nextsent>first, large corpus of atis sentences already exists and is readily available.
</nextsent>
<nextsent>second, atis provides an existing evaluation methodology, complete with independent training and test corpora, and scoring programs.
</nextsent>
<nextsent>finally, evaluating on common corpus makes it easy to compare the performance of the system with those based on different approaches.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W520">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 statistical parsing model.
</prevsent>
<prevsent>the parsing model is probabilistic recursive transition network similar to those described in (miller et ai.
</prevsent>
</prevsection>
<citsent citstr=" J92-1004 ">
1994) and (seneff 1992).<papid> J92-1004 </papid></citsent>
<aftsection>
<nextsent>the probability of parse tree given word string wis rewritten using bayes role as: p(t) p(w t) p(tiw) = p(w) since p(w) is constant for any given word string, candidate parses can be ranked by considering only the product p(t) p(w 7 ).
</nextsent>
<nextsent>the probability p(t) is modeled by state transition probabilities in the recursive transition etwork, and p(w t) is modeled by word transition probabilities.
</nextsent>
<nextsent>* state transition probabilities have the form p(state i staten_l, stateup) . for example, p(location/pp arrival/vp-head, arrival/vp) is the probability of location/pp following an arrival/vp- head within an arrival/vp constituent.
</nextsent>
<nextsent>word transition probabilities have the form p(word i wordn_ l,tag) . for example, p( class   first , class-of-service/npr) is the probability of the word sequence  first class  given the tag class-of-service/npr.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W521">
<title id=" P96-1008.xml">a fully statistical approach to natural language interfaces </title>
<section> parsing.  </section>
<citcontext>
<prevsection>
<prevsent>3.3 searching the parsing model.
</prevsent>
<prevsent>in order to explore the space of possible parses efficiently, the parsing model is searched using decoder based on an adaptation of the earley parsing algorithm (earley 1970).
</prevsent>
</prevsection>
<citsent citstr=" J95-2002 ">
this adaptation, related to that of (stolcke 1995), <papid> J95-2002 </papid>involves reformulating the earley algorithm to work with probabilistic recursive transition etworks rather than with deterministic production rules.</citsent>
<aftsection>
<nextsent>for details of the decoder, see (miller 1996).
</nextsent>
<nextsent>both pre-discourse and post-discourse meanings in our current system are represented using simple frame representation.
</nextsent>
<nextsent>figure 3 shows sample semantic frame corresponding to the parse in figure 2.
</nextsent>
<nextsent>air-transportation show: (arrival-time) origin: (city  boston ) destination: (city  atlanta ) figure 3: sample semantic frame.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W525">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>keywords: alignment, bilingual corpus, image processing
</prevsent>
<prevsent>aligned corpora have proved very useful in many tasks, including statistical machine translation, bilingual lexicography (daille, gaussier and lange 1993), and word sense disambiguation (gale, church and yarowsky 1992; chen, ker, sheng, and chang 1997).
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
several methods have recently been proposed for sentence alignment of the hansa rds, an english-french corpus of canadian parliamentary debates (brown, lai and mercer 1991; gale and church 1991<papid> P91-1023 </papid>a; simard, foster and isabelle 1992; chen 1993), <papid> P93-1002 </papid>and for other language pairs such as english-german, english-chinese, and english-japanese (church, dagan, gale, fung, helfman and satish 1993; kay and rtischeisen 1993; <papid> J93-1006 </papid>wu 1994).<papid> P94-1012 </papid></citsent>
<aftsection>
<nextsent>the statistical approach to machine translation (smt) can be understood as word-by-word model consisting of two sub-models: language model for generating source text segment and translation model for mapping to its translation t. brown et al (1993) <papid> J93-2003 </papid>also recommend using bilingual corpus to train the parameters ofpr(s 73, translation probability (tp) in the translation model.</nextsent>
<nextsent>in the context of smt, brown et al (1993) <papid> J93-2003 </papid>present series of five models of pr(s 73 for word alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W529">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>keywords: alignment, bilingual corpus, image processing
</prevsent>
<prevsent>aligned corpora have proved very useful in many tasks, including statistical machine translation, bilingual lexicography (daille, gaussier and lange 1993), and word sense disambiguation (gale, church and yarowsky 1992; chen, ker, sheng, and chang 1997).
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
several methods have recently been proposed for sentence alignment of the hansa rds, an english-french corpus of canadian parliamentary debates (brown, lai and mercer 1991; gale and church 1991<papid> P91-1023 </papid>a; simard, foster and isabelle 1992; chen 1993), <papid> P93-1002 </papid>and for other language pairs such as english-german, english-chinese, and english-japanese (church, dagan, gale, fung, helfman and satish 1993; kay and rtischeisen 1993; <papid> J93-1006 </papid>wu 1994).<papid> P94-1012 </papid></citsent>
<aftsection>
<nextsent>the statistical approach to machine translation (smt) can be understood as word-by-word model consisting of two sub-models: language model for generating source text segment and translation model for mapping to its translation t. brown et al (1993) <papid> J93-2003 </papid>also recommend using bilingual corpus to train the parameters ofpr(s 73, translation probability (tp) in the translation model.</nextsent>
<nextsent>in the context of smt, brown et al (1993) <papid> J93-2003 </papid>present series of five models of pr(s 73 for word alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W530">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>keywords: alignment, bilingual corpus, image processing
</prevsent>
<prevsent>aligned corpora have proved very useful in many tasks, including statistical machine translation, bilingual lexicography (daille, gaussier and lange 1993), and word sense disambiguation (gale, church and yarowsky 1992; chen, ker, sheng, and chang 1997).
</prevsent>
</prevsection>
<citsent citstr=" J93-1006 ">
several methods have recently been proposed for sentence alignment of the hansa rds, an english-french corpus of canadian parliamentary debates (brown, lai and mercer 1991; gale and church 1991<papid> P91-1023 </papid>a; simard, foster and isabelle 1992; chen 1993), <papid> P93-1002 </papid>and for other language pairs such as english-german, english-chinese, and english-japanese (church, dagan, gale, fung, helfman and satish 1993; kay and rtischeisen 1993; <papid> J93-1006 </papid>wu 1994).<papid> P94-1012 </papid></citsent>
<aftsection>
<nextsent>the statistical approach to machine translation (smt) can be understood as word-by-word model consisting of two sub-models: language model for generating source text segment and translation model for mapping to its translation t. brown et al (1993) <papid> J93-2003 </papid>also recommend using bilingual corpus to train the parameters ofpr(s 73, translation probability (tp) in the translation model.</nextsent>
<nextsent>in the context of smt, brown et al (1993) <papid> J93-2003 </papid>present series of five models of pr(s 73 for word alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W531">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>keywords: alignment, bilingual corpus, image processing
</prevsent>
<prevsent>aligned corpora have proved very useful in many tasks, including statistical machine translation, bilingual lexicography (daille, gaussier and lange 1993), and word sense disambiguation (gale, church and yarowsky 1992; chen, ker, sheng, and chang 1997).
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
several methods have recently been proposed for sentence alignment of the hansa rds, an english-french corpus of canadian parliamentary debates (brown, lai and mercer 1991; gale and church 1991<papid> P91-1023 </papid>a; simard, foster and isabelle 1992; chen 1993), <papid> P93-1002 </papid>and for other language pairs such as english-german, english-chinese, and english-japanese (church, dagan, gale, fung, helfman and satish 1993; kay and rtischeisen 1993; <papid> J93-1006 </papid>wu 1994).<papid> P94-1012 </papid></citsent>
<aftsection>
<nextsent>the statistical approach to machine translation (smt) can be understood as word-by-word model consisting of two sub-models: language model for generating source text segment and translation model for mapping to its translation t. brown et al (1993) <papid> J93-2003 </papid>also recommend using bilingual corpus to train the parameters ofpr(s 73, translation probability (tp) in the translation model.</nextsent>
<nextsent>in the context of smt, brown et al (1993) <papid> J93-2003 </papid>present series of five models of pr(s 73 for word alignment.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W532">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>aligned corpora have proved very useful in many tasks, including statistical machine translation, bilingual lexicography (daille, gaussier and lange 1993), and word sense disambiguation (gale, church and yarowsky 1992; chen, ker, sheng, and chang 1997).
</prevsent>
<prevsent>several methods have recently been proposed for sentence alignment of the hansa rds, an english-french corpus of canadian parliamentary debates (brown, lai and mercer 1991; gale and church 1991<papid> P91-1023 </papid>a; simard, foster and isabelle 1992; chen 1993), <papid> P93-1002 </papid>and for other language pairs such as english-german, english-chinese, and english-japanese (church, dagan, gale, fung, helfman and satish 1993; kay and rtischeisen 1993; <papid> J93-1006 </papid>wu 1994).<papid> P94-1012 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the statistical approach to machine translation (smt) can be understood as word-by-word model consisting of two sub-models: language model for generating source text segment and translation model for mapping to its translation t. brown et al (1993) <papid> J93-2003 </papid>also recommend using bilingual corpus to train the parameters ofpr(s 73, translation probability (tp) in the translation model.</citsent>
<aftsection>
<nextsent>in the context of smt, brown et al (1993) <papid> J93-2003 </papid>present series of five models of pr(s 73 for word alignment.</nextsent>
<nextsent>the authors propose using an adaptive expectation and maximization (em) algorithm to estimate parameters for lexical translation probability (ltp) and distortion probability (dp), two factors in the tp, from an aligned bitext.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W536">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the authors propose using an adaptive expectation and maximization (em) algorithm to estimate parameters for lexical translation probability (ltp) and distortion probability (dp), two factors in the tp, from an aligned bitext.
</prevsent>
<prevsent>the em algorithm ite rates between two phases to estimate ltp and dp until both functions converge.
</prevsent>
</prevsection>
<citsent citstr=" P93-1001 ">
church (1993) <papid> P93-1001 </papid>observes that reliably distinguishing sentence boundaries for noisy bitext obtained from an ocr device is quite difficult.</citsent>
<aftsection>
<nextsent>dagan, church and gale (1993) recommend aligning words directly without he preprocessing phase of sentence alignment.
</nextsent>
<nextsent>they propose using char_align to produce rough character-level alignment first.
</nextsent>
<nextsent>the rough alignment provides basis for estimating the translation probability based on position, as well as limits the range of target words being considered for each source word.
</nextsent>
<nextsent>char_align (church 1993) <papid> P93-1001 </papid>is based on the observation that there are many instances f . 297 ? : . - , . , - - - . . , ~-: : .~ ? ?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W538">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>an example of dotplot of alignment showing only likely dots which lie within short distance from the diagonal.
</prevsent>
<prevsent>cognates among the languages in the indo- european family.
</prevsent>
</prevsection>
<citsent citstr=" C94-2178 ">
however, fung and church (1994) <papid> C94-2178 </papid>point out that such constraint does not exist between languages across language groups such as chinese and english.</citsent>
<aftsection>
<nextsent>the authors propose k-vec approach which is based on k- way partition of the bilingual corpus.
</nextsent>
<nextsent>fung and mckeown (1994) propose using similar measure based on dynamic time warping (dtw) between occurrence cency sequences to improve on the k- vec method.
</nextsent>
<nextsent>the char-align, k-vec and dtw approaches relyon dynamic programming strategy to reach rough alignment.
</nextsent>
<nextsent>as chen (1993) <papid> P93-1002 </papid>points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W540">
<title id=" P97-1038.xml">an alignment method for noisy parallel corpora based on image processing techniques </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the char-align, k-vec and dtw approaches relyon dynamic programming strategy to reach rough alignment.
</prevsent>
<prevsent>as chen (1993) <papid> P93-1002 </papid>points out, dynamic programming is particularly susceptible to deletions occurring in one of the two languages.</prevsent>
</prevsection>
<citsent citstr=" P91-1022 ">
thus, dynamic programming based sentence alignment algorithms relyon paragraph anchors (brown et al 1991) <papid> P91-1022 </papid>or lexical information, such as cognates (simard 1992), to maintain high accuracy rate.</citsent>
<aftsection>
<nextsent>these methods are not robust with respect to non-literal translations and large deletions (simard 1996).
</nextsent>
<nextsent>this paper presents new approach based on image processing (ip) techniques, which is immune to such predicaments.
</nextsent>
<nextsent>2.1 estimation of ltp.
</nextsent>
<nextsent>a wide variety of ways of ltp estimation have been proposed in the literature of computational linguistics, including dice coefficient (kay and r6scheisen 1993), mutual information, ~2 (gale and church 1991<papid> P91-1023 </papid>b), dictionary and thesaurus table 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W546">
<title id=" P91-1032.xml">finite state approximation of phrase structure grammars </title>
<section> mot ivat ion.  </section>
<citcontext>
<prevsection>
<prevsent>current recognition algorithms can most directly use finite-state ac-ceptor (fsa) language models.
</prevsent>
<prevsent>however, these models are inadequate for language interpreta-tion, since they cannot express the relevant syntac-tic and semantic regularities.
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
augmented phrase structure grammar (apsg) formalisms, such as unification-based grammars (shieber, 1985<papid> P85-1018 </papid>a), can express many of those regularities, but they are computationally ess suitable for language mod-eling, because of the inherent cost of computing state transitions in apsg parsers.</citsent>
<aftsection>
<nextsent>the above problems might be circumvented by using separate grammars for language modeling and language interpretation.
</nextsent>
<nextsent>ideally, the recog-nition grammar should not reject sentences ac-ceptable by the interpretation grammar and it should contain as much as reasonable of the con-straints built into the interpretation grammar.
</nextsent>
<nextsent>however, if the two grammars are built indepen-dently, those goals are difficult to maintain.
</nextsent>
<nextsent>for this reason, we have developed method for con-structing automatically finite-state approxima-tion for an apsg.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W551">
<title id=" P91-1032.xml">finite state approximation of phrase structure grammars </title>
<section> the algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>the final state is  --~ s..
</prevsent>
<prevsent>the other states are all the possible dotted rules of g. ? there is transition labeled x, where is terminal or nonterminal symbol, from dotted rule -+ . x~ to --+ c~x.//.
</prevsent>
</prevsection>
<citsent citstr=" J89-4001 ">
there is an e-transition from --~ ? b/~ to --~  7, where is nonterminal symbol and -+ 7 rule in g. 2unification-based grammars not in this class would have to be weakened first, using techniques akin to those of sato and tamaki (1984), shieber (1985<papid> P85-1018 </papid>b) and haas (1989).<papid> J89-4001 </papid></citsent>
<aftsection>
<nextsent>i  -  . s -  . ab - .
</nextsent>
<nextsent>a a- .
</nextsent>
<nextsent>1 is - s.\]  aqk~ sa  .ba ja~\ [a . aa.
</nextsent>
<nextsent>j figure 1: characteristic machine for g1 .a~(g) can be seen as the finite state control for non deterministic shift-reduce push down recog-nizer to(g) for g. state transition labeled by terminal symbol from state to state  licenses shift move, pushing onto the stack of the recog-nizer the pair (s, z).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W552">
<title id=" P91-1032.xml">finite state approximation of phrase structure grammars </title>
<section> informal analysis.  </section>
<citcontext>
<prevsection>
<prevsent>substantially larger grammars, with thousands of instantiated rules, have been developed for speech-to-speech translation project.
</prevsent>
<prevsent>compilation times vary widely, but very long compilations ap-pear to be caused by combinatorial explosion in the unfolding of right recur sions that will be dis-cussed further in the next section.
</prevsent>
</prevsection>
<citsent citstr=" J82-3004 ">
in addition to the cases of left-linear and right- linear grammars discussed in section 3, our algo- ithm is exact in variety of interesting cases, in-cluding the examples of church and patil (1982), <papid> J82-3004 </papid>which illustrate how typical attachment ambigu-ities arise as structural ambiguities on regular string sets.</citsent>
<aftsection>
<nextsent>the algorithm is also exact for some self- embedding rammars 4 of regular languages, such as --+ as sb c defining the regular language a*eb*.
</nextsent>
<nextsent>a more interesting example is the following sim-plified grammar for the structure of english noun 4 grammar isself-embedding if and only if licenses the derivation ~ c~x~ for non empty c~ and/3.
</nextsent>
<nextsent>a language is regular if and only if it can be described by some non- self-embedding grammar.
</nextsent>
<nextsent>252 figure 4: acceptor for noun phrases phrases: np -+ det nom \[ pn det -+ art \] np nom -+ i nom pp adj nom pp --* np the symbols art, n, pn and correspond to the parts of speech article, noun, proper noun and preposition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W555">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J90-2002 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W557">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P91-1023 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W558">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P93-1001 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W559">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" J93-2003 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W560">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W93-0301 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W562">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" A94-1006 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W563">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C94-2178 ">
parallel corpora have been shown to provide an extremely rich source of constraints for statistical analysis (e.g., brown et al  1990; <papid> J90-2002 </papid>gale &amp; church 1991; <papid> P91-1023 </papid>gale et al  1992; church 1993; <papid> P93-1001 </papid>brown et al  1993; <papid> J93-2003 </papid>dagan et al  1993; <papid> W93-0301 </papid>dagan &amp; church 1994; <papid> A94-1006 </papid>fung &amp; church 1994; <papid> C94-2178 </papid>wu &amp; xia 1994; fung &amp; mckeown 1994).</citsent>
<aftsection>
<nextsent>our thesis in this paper is that he lexical information actually gives suffi-cient information to extract not merely word alignments, but also bracketing constraints for both parallel texts.
</nextsent>
<nextsent>aside from purely linguistic interest, bracket structure has been empirically shown to be highly effective at con-straining subsequent training of, for example, stochas-tic context-free grammars (pereira &amp; ~ 1992; black et al  1993).
</nextsent>
<nextsent>previous algorithms for automatic bracketing operate on monolingual texts and hence re-quire more grammatical constraints; for example, tac-tics employing mutual information have been applied to tagged text (magerumn &amp; marcus 1990).
</nextsent>
<nextsent>algorithms for word alignment attempt to find the matching words between parallel sentences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W570">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> stochastic bracketing transduction.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 simultaneous segmentation.
</prevsent>
<prevsent>we often find the same concept realized using different numbers of words in the two languages, creating potential difficulties for word alignment; what is single word in english may be realized as compound inchinese.
</prevsent>
</prevsection>
<citsent citstr=" P94-1010 ">
since chinese text is not ortho graphically separated into words, the standard methodology is to first preproce~ input exts through segmentation module (chiang et al  1992; inet al  1992; chang &amp; chert 1993; inet al  1993; wu &amp; tseng 1993; sproat et al  1994).<papid> P94-1010 </papid></citsent>
<aftsection>
<nextsent>however, this se- rionsly degrades our algorithm performance, since the the segmenter may encounter ambiguities that are un-resolvable monolingually and thereby introduce rrors.
</nextsent>
<nextsent>even if the chinese segmentation is acceptable moaolin- gually, it may not agree with the division of words present in the english sentence.
</nextsent>
<nextsent>moreover, conventional com-pounds are frequently and unlmxlictably missing from translation lexicons, and this can furllu~ degrade perfor- inane.
</nextsent>
<nextsent>to avoid such problems we have extended the algo-rithm to optimize the segmentation the chinese sen-tence in parallel with the ~t ing lm~:ess.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W571">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> sink-singleton(chi/d).  </section>
<citcontext>
<prevsection>
<prevsent>notice that in contrast, the linguistic evaluation criterion is in-sensitive to whether the bracketings of the two sentences match each other in any semantic way, as long as the monolingual bracketings in each sentence are correct.
</prevsent>
<prevsent>in either case, the bracket precision gives the proportion of found br~&ets; that agree with the chosen correctness criterion.
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
all experiments reported in this paper were performed on sentence-pairs om the hkust english-chinese par-allel bilingual corpus, which consists of governmental transcripts (wu 1994).<papid> P94-1012 </papid></citsent>
<aftsection>
<nextsent>the translation lexicon was au-tomatically learned from the same corpus via statisti-cal sentence alignment (wu 1994) <papid> P94-1012 </papid>and statistical chi-nese word and collocation extraction (fung &amp; wu 1994; <papid> P94-1012 </papid>wu &amp; fung 1994), <papid> A94-1030 </papid>followed by an em word-translation learning procedure (wu &amp; xia 1994).</nextsent>
<nextsent>the translation lexicon contains an english vocabulary of approximately 6,500 words and chinese vocabulary of approximately 5,500 words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W574">
<title id=" P95-1033.xml">an algorithm for simultaneously bracketing parallel texts by aligning words </title>
<section> sink-singleton(chi/d).  </section>
<citcontext>
<prevsection>
<prevsent>in either case, the bracket precision gives the proportion of found br~&ets; that agree with the chosen correctness criterion.
</prevsent>
<prevsent>all experiments reported in this paper were performed on sentence-pairs om the hkust english-chinese par-allel bilingual corpus, which consists of governmental transcripts (wu 1994).<papid> P94-1012 </papid></prevsent>
</prevsection>
<citsent citstr=" A94-1030 ">
the translation lexicon was au-tomatically learned from the same corpus via statisti-cal sentence alignment (wu 1994) <papid> P94-1012 </papid>and statistical chi-nese word and collocation extraction (fung &amp; wu 1994; <papid> P94-1012 </papid>wu &amp; fung 1994), <papid> A94-1030 </papid>followed by an em word-translation learning procedure (wu &amp; xia 1994).</citsent>
<aftsection>
<nextsent>the translation lexicon contains an english vocabulary of approximately 6,500 words and chinese vocabulary of approximately 5,500 words.
</nextsent>
<nextsent>the mapping is many-to-many, with an average of 2.25 chinese translations per english word.
</nextsent>
<nextsent>the translation accuracy is imperfect (about 86% percent weighted precision), which turns out to cause many of the bracketing errors.
</nextsent>
<nextsent>approximately 2,000 sentence-pairs with both english and chinese lengths of 30 words or less were extracted from our corpus and bracketed using the algorithm de-scribed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W575">
<title id=" P97-1043.xml">the complexity of recognition of linguistically adequate dependency grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given this equivalence, interest in dg as linguistic framework diminished considerably, although many de-pendency grammar ians view gaifman conception as an unfortunate one (cf.
</prevsent>
<prevsent>section 2).
</prevsent>
</prevsection>
<citsent citstr=" C96-2122 ">
to our knowledge, there has been no other formal study of dg.this is reflected by recent study (lombardo &amp; lesmo, 1996), <papid> C96-2122 </papid>which applies the earley parsing technique (earley, 1970) to dg, and thereby achieves cubic time complexity for the analysis of dg.</citsent>
<aftsection>
<nextsent>in their discussion, lombardo &amp; lesmo express their hope that slight increases in generative ca-pacity will correspond to equally slight increases incom- putational complexity.
</nextsent>
<nextsent>it is this claim that we challenge here.
</nextsent>
<nextsent>after motivating non-projective analyses for dg, we investigate various variants of dg and identify the sep-aration of dominance and precedence as major part of current dg theorizing.
</nextsent>
<nextsent>thus, no current variant of dg (not even tesni~re original formulation) is compatible with gaifman  conception, which seems to be motivated by formal considerations only (viz., the proof of equiva- lence).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W576">
<title id=" P97-1043.xml">the complexity of recognition of linguistically adequate dependency grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is il-lustrated in the fourth section, where we give simple en-coding of an a/p-complete problem in discontinuous dg.
</prevsent>
<prevsent>our proof of a/79-completeness, however, does not relyon discontinuity, but only requires unordered trees.
</prevsent>
</prevsection>
<citsent citstr=" J85-4001 ">
it is adapted from similar proof for unordered context- free grammars (ucfgs) by barton (1985).<papid> J85-4001 </papid></citsent>
<aftsection>
<nextsent>the growing interest in the dependency concept (which roughly corresponds to the o-roles of gb, subcatego-rization in hpsg, and the so-called omain of locality of tag) again raises the issue whether non-lexical cat-egories are necessary for linguistic analysis.
</nextsent>
<nextsent>after re-viewing several proposals in this section, we argue in the next section that word order - - the description of which is the most prominent difference between psgs and dgs - - can adequately be described without reference to non- lexical categories.
</nextsent>
<nextsent>standard psg trees are projective, i.e., no branches cross when the terminal nodes are projected onto the input string.
</nextsent>
<nextsent>in contrast psg approaches, dg re-quires non-projective analyses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W578">
<title id=" P97-1043.xml">the complexity of recognition of linguistically adequate dependency grammars </title>
<section> versions of dependency grammar.  </section>
<citcontext>
<prevsection>
<prevsent>lexicase (starosta, 1988; 1992) employs complex fea-ture structures to represent lexical and syntactic enti-ties.
</prevsent>
<prevsent>its word order description is much like that of word grammar (at least at some level of abstraction), and shares the above inconsistency.
</prevsent>
</prevsection>
<citsent citstr=" C88-1049 ">
dependency unification grammar (hellwig, 1988) <papid> C88-1049 </papid>defines tree-like data structure for the representation syntactic analyses.</citsent>
<aftsection>
<nextsent>using morphosyntactic features with special interpretations, word defines abstract positions into which modifiers are mapped.
</nextsent>
<nextsent>partial orderings and even discontinuities can thus be described by allowing modifier to occupy position defined by some transitive head.
</nextsent>
<nextsent>the approach cannot restrict discontinuities prop-erly, however.
</nextsent>
<nextsent>slot grammar (mccord, 1990) employs number of rule types, some of which are exclusively concerned with precedence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W582">
<title id=" P97-1043.xml">the complexity of recognition of linguistically adequate dependency grammars </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>this feature leads to a/ 7% complete recognition problem.
</prevsent>
<prevsent>the necessity of this ex-tension approved by most current dgs relates to the fact that dg must directly characterize dependencies which in psg are captured by projective structure and addi-tional processes such as co indexing or structure sharing (most easily seen in treatments of so-called unbounded 342 dependencies).
</prevsent>
</prevsection>
<citsent citstr=" E89-1014 ">
the dissociation of tree structure and linear order, as we have done in section 3, nevertheless seems to be promising approach for psg as well; see very similar proposal for hpsg (reape, 1989).<papid> E89-1014 </papid></citsent>
<aftsection>
<nextsent>the .n 79-completeness result also holds for the dis-continuous dg presented in section 3.
</nextsent>
<nextsent>this dg can characterize at least some context-sensitive languages such as anbnc n, i.e., the increase in complexity corre-sponds to an increase of generative capacity.
</nextsent>
<nextsent>we conjec-ture that, provided proper formalization ofthe other dg versions presented in section 2, their .a/p-completeness can be similarly shown.
</nextsent>
<nextsent>with respect to parser design, this result implies that the well known polynomial time complexity of chart- or tabular-based parsing techniques cannot be achieved for these dg formalisms in gen-eral.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W583">
<title id=" P97-1043.xml">the complexity of recognition of linguistically adequate dependency grammars </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>we conjec-ture that, provided proper formalization ofthe other dg versions presented in section 2, their .a/p-completeness can be similarly shown.
</prevsent>
<prevsent>with respect to parser design, this result implies that the well known polynomial time complexity of chart- or tabular-based parsing techniques cannot be achieved for these dg formalisms in gen-eral.
</prevsent>
</prevsection>
<citsent citstr=" C96-1085 ">
this is the reason why the parse talk text under-standing system (neuhaus &amp; hahn, 1996) <papid> C96-1085 </papid>utilizes pecial heuristics in heterogeneous chart- and backtracking- based parsing approach.</citsent>
<aftsection>




</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W584">
<title id=" P98-1043.xml">alignment of multiple languages for historical comparison </title>
<section> background </section>
<citcontext>
<prevsection>
<prevsent>275 regular correspondence, once discovered, can be used to refine one choice of alignments and even putative cognates.
</prevsent>
<prevsent>parts of the comparative method have been computerized by frantz (1970), hewson (1974), wimbish (1989), and lowe and mazandon (1994), but none of them have tackled the align-ment step.
</prevsent>
</prevsection>
<citsent citstr=" J96-4002 ">
covington (1996) <papid> J96-4002 </papid>presents work- able alignment algorithm for comparing two lan- guages.</citsent>
<aftsection>
<nextsent>in this paper extend that algorithm to handle more than two languages at once.
</nextsent>
<nextsent>the alignment step is hard to automate be- cause there are too many possible alignments to choose from.
</nextsent>
<nextsent>for example, french le \[l~\] and spanish el \[el can be lined up at least three ways: l l - -e 12 -1~ 12- of these, the second is etymologically correct, and the third would merit consideration if one did not know the etymology.
</nextsent>
<nextsent>the number of alignments rises exponentially with the length of the strings and the number of strings being aligned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W608">
<title id=" P97-1028.xml">applying explanation based learning to control and speeding up natural language generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P96-1030 ">
in recent years, machine learning tech-nique known as explanation-based learning ebl (mitchell, keller, and kedar-cabelli, 1986; van harmelen and bundy, 1988; minton et al, 1989) has successfully been applied to control and speeding-up natural anguage parsing (rayner, 1988; samuelsson and rayner, 1991; neumann, 1994a; samuelsson, 1994; srinivas and joshi, 1995; rayner and carter, 1996).<papid> P96-1030 </papid></citsent>
<aftsection>
<nextsent>the core idea of ebl is to transform the derivations (or explanations) computed by prob-lem solver (e.g., parser) to some generalized and compact forms, which can be used very efficiently for solving similar problems in the future.
</nextsent>
<nextsent>ebl has primarily been used for parsing to automatically spe-cialize given source grammar to specific domain.
</nextsent>
<nextsent>in that case, ebl is used as method for adapting general grammar and/or parser to the sub-language defined by suitable training corpus (rayner and carter, 1996).<papid> P96-1030 </papid></nextsent>
<nextsent>a specialized grammar can be seen as describ-ing domain-specific set of prototypical construc- tions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W612">
<title id=" P97-1028.xml">applying explanation based learning to control and speeding up natural language generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present novel method for the automatic extraction of sub grammars for the control and speeding-up of natural anguage generation.
</prevsent>
<prevsent>its main advantage for nlg is that the complexity of the (linguistically oriented) decisionmaking process during natural anguage generation can be vastly re-duced, because the ebl method supports adaption of nlg system to particular language use.
</prevsent>
</prevsection>
<citsent citstr=" P96-1027 ">
the core properties of this new method are: ? prototypical occuring grammatical construc-tions can automatically be extracted; ? generation of these constructions vastly sped up using simple but efficient mechanisms; the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained exam- ples; ? it can easily be integrated with recently de-veloped chart-based generators as described in, 214 e.g., (neumann, 1994b; kay, 1996; <papid> P96-1027 </papid>shemtov, 1996).<papid> C96-2155 </papid></citsent>
<aftsection>
<nextsent>the method has been completely implemented and tested with broad-coverage hpsg-based grammar for english (see sec.
</nextsent>
<nextsent>5 for more details).
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W613">
<title id=" P97-1028.xml">applying explanation based learning to control and speeding up natural language generation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we present novel method for the automatic extraction of sub grammars for the control and speeding-up of natural anguage generation.
</prevsent>
<prevsent>its main advantage for nlg is that the complexity of the (linguistically oriented) decisionmaking process during natural anguage generation can be vastly re-duced, because the ebl method supports adaption of nlg system to particular language use.
</prevsent>
</prevsection>
<citsent citstr=" C96-2155 ">
the core properties of this new method are: ? prototypical occuring grammatical construc-tions can automatically be extracted; ? generation of these constructions vastly sped up using simple but efficient mechanisms; the new method supports partial matching, in the sense that new semantic input need not be completely covered by previously trained exam- ples; ? it can easily be integrated with recently de-veloped chart-based generators as described in, 214 e.g., (neumann, 1994b; kay, 1996; <papid> P96-1027 </papid>shemtov, 1996).<papid> C96-2155 </papid></citsent>
<aftsection>
<nextsent>the method has been completely implemented and tested with broad-coverage hpsg-based grammar for english (see sec.
</nextsent>
<nextsent>5 for more details).
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W614">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments indicate that the context-sensitive dop model obtains better accuracy than the original model, allowing for fast and robust processing of spoken input.
</prevsent>
<prevsent>the data-oriented parsing (dop) model (cf.
</prevsent>
</prevsection>
<citsent citstr=" C92-3126 ">
bod 1992, <papid> C92-3126 </papid>1995; bod &amp; kaplan 1998; <papid> P98-1022 </papid>scha 1992; sima an 1995, 1997; rajman 1995) is probabilistic parsing model which does not single out narrowly predefined set of structures as the statistically significant ones.</citsent>
<aftsection>
<nextsent>it accomplishes this by maintaining large corpus of analyses of previously occurring utterances.
</nextsent>
<nextsent>new utterances are analyzed by combining subtrees from the corpus.
</nextsent>
<nextsent>the occurrence-frequencies of the subtrees are used to estimate the most probable analysis of an utterance.
</nextsent>
<nextsent>to date, dop has mainly been applied to corpora of trees labeled with syntactic annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W616">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments indicate that the context-sensitive dop model obtains better accuracy than the original model, allowing for fast and robust processing of spoken input.
</prevsent>
<prevsent>the data-oriented parsing (dop) model (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-1022 ">
bod 1992, <papid> C92-3126 </papid>1995; bod &amp; kaplan 1998; <papid> P98-1022 </papid>scha 1992; sima an 1995, 1997; rajman 1995) is probabilistic parsing model which does not single out narrowly predefined set of structures as the statistically significant ones.</citsent>
<aftsection>
<nextsent>it accomplishes this by maintaining large corpus of analyses of previously occurring utterances.
</nextsent>
<nextsent>new utterances are analyzed by combining subtrees from the corpus.
</nextsent>
<nextsent>the occurrence-frequencies of the subtrees are used to estimate the most probable analysis of an utterance.
</nextsent>
<nextsent>to date, dop has mainly been applied to corpora of trees labeled with syntactic annotations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W618">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>bod (1992) <papid> C92-3126 </papid>demonstrated that dop can be implemented using conventional context-free parsing techniques.</prevsent>
<prevsent>however, the computation of the most probable parse of sentence is np-hard (sima an 1996).</prevsent>
</prevsection>
<citsent citstr=" W96-0214 ">
the most probable parse can be estimated by iterative monte carlo sampling (bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of sentence (bod 1995, sima an 1995) or the  labelled recall parse  of sentence (goodman 1996).<papid> W96-0214 </papid></citsent>
<aftsection>
<nextsent>so far, the syntactic dop model has been tested on the atis corpus and the wall street journal corpus, obtaining significantly better test results than other stochastic parsers (charniak 1996).
</nextsent>
<nextsent>for example, goodman (1998) compares the results of his dop parser to replication of pereira &amp; schabes (1992) <papid> P92-1017 </papid>on the same training and test data.</nextsent>
<nextsent>while the pereira &amp; schabes method achieves 79.2% zero-crossing brackets accuracy, dop obtains 86.1% on the same data (goodman 1998: p. 179, table 4.4).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W619">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most probable parse can be estimated by iterative monte carlo sampling (bod 1995), but efficient algorithms exist only for sub-optimal solutions such as the most likely derivation of sentence (bod 1995, sima an 1995) or the  labelled recall parse  of sentence (goodman 1996).<papid> W96-0214 </papid></prevsent>
<prevsent>so far, the syntactic dop model has been tested on the atis corpus and the wall street journal corpus, obtaining significantly better test results than other stochastic parsers (charniak 1996).</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
for example, goodman (1998) compares the results of his dop parser to replication of pereira &amp; schabes (1992) <papid> P92-1017 </papid>on the same training and test data.</citsent>
<aftsection>
<nextsent>while the pereira &amp; schabes method achieves 79.2% zero-crossing brackets accuracy, dop obtains 86.1% on the same data (goodman 1998: p. 179, table 4.4).
</nextsent>
<nextsent>thus the dop method outperforms the pereira &amp; schabes method with an accuracy-increase of 6.9%, or an error- reduction of 33%.
</nextsent>
<nextsent>goodman also performs statistical analysis using t-test, showing that the differences are statistically significant beyond the 98th percentile.
</nextsent>
<nextsent>in bod et al (1996), it was shown how dop can be generalized to semantic interpretation by using corpora annotated with compositional semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W620">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> the ovis corpus: trees enriched with.  </section>
<citcontext>
<prevsection>
<prevsent>robustness is further achieved by special category, called error.
</prevsent>
<prevsent>this category is used for stutters, false starts, and repairs.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
no grammar is used to determine the correct syntactic annotation; there is small set of guidelines, that has the degree of detail necessary to avoid an  anything goes  attitude in the annotator, but leaves room for the annotator perception of the structure of the utterance (see bonnema et al 1997).<papid> P97-1021 </papid></citsent>
<aftsection>
<nextsent>the semantic annotations are based on the update language defined for the ovis dialogue manager by veldhuijzen van zanten (1996).
</nextsent>
<nextsent>this language consists of hierarchical frame structure with slots and values for the origin and destination of train connection, for the time at which the user wants to arrive or depart, etc. the distinction between slots and values can be regarded as special case of ground and focus distinction (vallduvi 1990).
</nextsent>
<nextsent>updates specify the ground and focus of the user utterances.
</nextsent>
<nextsent>for example, the utterance ik wil niet vandaag maar morgen naar almere (literally:  want not today but tomorrow to almere ) yields the following update: (4) user .wants .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W621">
<title id=" P98-1021.xml">spoken dialogue interpretation with the dop model </title>
<section> the ovis corpus: trees enriched with.  </section>
<citcontext>
<prevsection>
<prevsent>note that the top-node meaning of (5) is compositionally built up out of the meanings of its sub-constituents.
</prevsent>
<prevsent>substituting the meaning represen-tations into the corresponding variables yields the update expression (4).
</prevsent>
</prevsection>
<citsent citstr=" P96-1008 ">
the ovis annotations are in contrast with other corpora and systems (e.g. miller et al. 1996), <papid> P96-1008 </papid>in that our annotation convention exploits the principle of compositionality of meaning.</citsent>
<aftsection>
<nextsent>3 (5) dl.d2 ~ p per 1 .d2 uir / ~ ~ ~ p ik wants wil mp mp mp con mp np / ~ ! tomorrow destination,place town.almere i i adv mp maar rnorgen naar almere # today i niet vamaag figure (6) gives an example of the error category for the annotation of the ill-formed sentence van voor burg naar van venlo naar voor burg ( from voor burg to from venlo to voor burg ): (6) mp error (dl;d2) mp dl.d2 mp (dl;d2) / destinaiion.place np np ,aar rigin place towlvenh origi.place town.v0orburg\] van venlo van worburg mp np destinton.place w,.\]00rbur~ naar tvorburg note that the error category has no semantic annotation; in the top-node semantics of van voor burg phenomena such as non-standard quantifier scope or discontinuous constituents may create complications in the syntactic or semantic analyses assigned to certain sentences and their constituents.
</nextsent>
<nextsent>it is therefore not clear yet whether our current treatment ought to be viewed as completely general, or whether more sophisticated treatment in the vein of vanden berg et al (1994) should be worked out.
</nextsent>
<nextsent>140 naar van venlo naar voor burg, the meaning of the false start van voor burg naar is thus absent: (7) (or ig in.place.town.venlo ; destination, place, town.
</nextsent>
<nextsent>voor burg ) the manual annotation of 10,000 ovis utterances may seem laborious and error-prone process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W625">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>thus we don~ relyon the existence of pos-taggers or lemmatizers for the languages involved, but wish to provide mechanisms that user can easily adapt to new languages.
</prevsent>
<prevsent>the organisation of the paper is as follows: in section 2 we relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others.
</prevsent>
</prevsection>
<citsent citstr=" C88-1016 ">
most algorithms for bilingual word alignment date have been based on the probabilistic translation models first proposed by brown et al (1988), <papid> C88-1016 </papid>brown et al (1990), <papid> J90-2002 </papid>especially model and model 2.</citsent>
<aftsection>
<nextsent>these models explicitly exclude multi-word units from consideration 1.
</nextsent>
<nextsent>melamed (1997<papid> W97-0311 </papid>b), however, proposes method for the recognition of multi-word compounds in bitexts that is based on the predictive value of translation model.</nextsent>
<nextsent>a trial translation model that treat certain multi-word sequences as units is compared with base translation model that treats the same sequences as multiple single-word units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W626">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>thus we don~ relyon the existence of pos-taggers or lemmatizers for the languages involved, but wish to provide mechanisms that user can easily adapt to new languages.
</prevsent>
<prevsent>the organisation of the paper is as follows: in section 2 we relate this approach to previous work, in section 3 we motivate and spell out our assumptions about the behaviour of lexical units in translation, in section 4 we present the basic features of the algorithm, and in section 5 we present results from an evaluation and try to compare these to the results of others.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
most algorithms for bilingual word alignment date have been based on the probabilistic translation models first proposed by brown et al (1988), <papid> C88-1016 </papid>brown et al (1990), <papid> J90-2002 </papid>especially model and model 2.</citsent>
<aftsection>
<nextsent>these models explicitly exclude multi-word units from consideration 1.
</nextsent>
<nextsent>melamed (1997<papid> W97-0311 </papid>b), however, proposes method for the recognition of multi-word compounds in bitexts that is based on the predictive value of translation model.</nextsent>
<nextsent>a trial translation model that treat certain multi-word sequences as units is compared with base translation model that treats the same sequences as multiple single-word units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W627">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>most algorithms for bilingual word alignment date have been based on the probabilistic translation models first proposed by brown et al (1988), <papid> C88-1016 </papid>brown et al (1990), <papid> J90-2002 </papid>especially model and model 2.</prevsent>
<prevsent>these models explicitly exclude multi-word units from consideration 1.</prevsent>
</prevsection>
<citsent citstr=" W97-0311 ">
melamed (1997<papid> W97-0311 </papid>b), however, proposes method for the recognition of multi-word compounds in bitexts that is based on the predictive value of translation model.</citsent>
<aftsection>
<nextsent>a trial translation model that treat certain multi-word sequences as units is compared with base translation model that treats the same sequences as multiple single-word units.
</nextsent>
<nextsent>a drawback with melamed method is that compounds are defined relative to given translation and not with respect to language- internal criteria.
</nextsent>
<nextsent>thus, if the method is used to construct bilingual concordance, there is risk that compounds and idioms that translate compositionally will not be found.
</nextsent>
<nextsent>moreover, it is computationally expensive and, since it constructs compounds incrementally, adding one word at time, requires many iterations and much processing to find linguistic units of the proper size.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W633">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>thus, if the method is used to construct bilingual concordance, there is risk that compounds and idioms that translate compositionally will not be found.
</prevsent>
<prevsent>moreover, it is computationally expensive and, since it constructs compounds incrementally, adding one word at time, requires many iterations and much processing to find linguistic units of the proper size.
</prevsent>
</prevsection>
<citsent citstr=" W96-0107 ">
kitamura and matsumoto (1996) <papid> W96-0107 </papid>present results from aligning multi-word and single word expressions with recall of 80 percent if partially correct ranslations were included.</citsent>
<aftsection>
<nextsent>their method is iterative and is based on the use of the dice coefficient.
</nextsent>
<nextsent>smadja et. al (1996) <papid> J96-1001 </papid>also use the dice model 3-5 includes multi-word units in one direction.</nextsent>
<nextsent>coefficient as their basis for aligning collocations between english and french.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W634">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> p rev ious  work   </section>
<citcontext>
<prevsection>
<prevsent>kitamura and matsumoto (1996) <papid> W96-0107 </papid>present results from aligning multi-word and single word expressions with recall of 80 percent if partially correct ranslations were included.</prevsent>
<prevsent>their method is iterative and is based on the use of the dice coefficient.</prevsent>
</prevsection>
<citsent citstr=" J96-1001 ">
smadja et. al (1996) <papid> J96-1001 </papid>also use the dice model 3-5 includes multi-word units in one direction.</citsent>
<aftsection>
<nextsent>coefficient as their basis for aligning collocations between english and french.
</nextsent>
<nextsent>their evaluation show results of 73 percent accuracy (precision) on average.
</nextsent>
<nextsent>as fung and church (1994) <papid> C94-2178 </papid>we wish to estimate the bilingual lexicon directly.</nextsent>
<nextsent>unlike fung and church our texts are already aligned at sentence level and the lexicon is viewed, not merely as word associations, but as associations between lexical units of the two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W635">
<title id=" P98-1004.xml">a simple hybrid aligner for generating lexical correspondences in parallel texts </title>
<section> underlying assumptions.  </section>
<citcontext>
<prevsection>
<prevsent>coefficient as their basis for aligning collocations between english and french.
</prevsent>
<prevsent>their evaluation show results of 73 percent accuracy (precision) on average.
</prevsent>
</prevsection>
<citsent citstr=" C94-2178 ">
as fung and church (1994) <papid> C94-2178 </papid>we wish to estimate the bilingual lexicon directly.</citsent>
<aftsection>
<nextsent>unlike fung and church our texts are already aligned at sentence level and the lexicon is viewed, not merely as word associations, but as associations between lexical units of the two languages.
</nextsent>
<nextsent>we assume that texts have structure at many different levels.
</nextsent>
<nextsent>at the most concrete level text is simply sequence of characters.
</nextsent>
<nextsent>at the next level text is sequence of word tokens, where word tokens are defined as sequences of alphanumeric character strings that are separated from one another by finite set of delimiters uch as spaces and punctuation marks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W648">
<title id=" P91-1040.xml">constraint projection an efficient treatment of disjunctive feature descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, if disjunctive fea-ture structures were expanded to disjunctive nor-mal form (dnf) 1 as in definite clause grammar (pereira and warren 1980) and kay parser (kay 1985), unification would take exponential time in the number of disjuncts.
</prevsent>
<prevsent>avoiding unnecessary expansion of disjunction is important for efficient disjunctive unification.
</prevsent>
</prevsection>
<citsent citstr=" P87-1033 ">
kasper (1987) <papid> P87-1033 </papid>and eisele and dsrre (1988) <papid> P88-1035 </papid>have tackled this problem and proposed unification methods for disjunctive fea-ture descriptions.</citsent>
<aftsection>
<nextsent>~dnf has form bt vq~ v3 v.-.
</nextsent>
<nextsent>vq~n, where includes no disjunctions.
</nextsent>
<nextsent>these works are based on graph unification rather than on term unification.
</nextsent>
<nextsent>graph unifica-tion has the advantage that the number of argu-ments is free and arguments are selected by la-bels so that it is easy to write grammar and lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W649">
<title id=" P91-1040.xml">constraint projection an efficient treatment of disjunctive feature descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, if disjunctive fea-ture structures were expanded to disjunctive nor-mal form (dnf) 1 as in definite clause grammar (pereira and warren 1980) and kay parser (kay 1985), unification would take exponential time in the number of disjuncts.
</prevsent>
<prevsent>avoiding unnecessary expansion of disjunction is important for efficient disjunctive unification.
</prevsent>
</prevsection>
<citsent citstr=" P88-1035 ">
kasper (1987) <papid> P87-1033 </papid>and eisele and dsrre (1988) <papid> P88-1035 </papid>have tackled this problem and proposed unification methods for disjunctive fea-ture descriptions.</citsent>
<aftsection>
<nextsent>~dnf has form bt vq~ v3 v.-.
</nextsent>
<nextsent>vq~n, where includes no disjunctions.
</nextsent>
<nextsent>these works are based on graph unification rather than on term unification.
</nextsent>
<nextsent>graph unifica-tion has the advantage that the number of argu-ments is free and arguments are selected by la-bels so that it is easy to write grammar and lexicon.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W653">
<title id=" P91-1040.xml">constraint projection an efficient treatment of disjunctive feature descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we adopt term unification for these reasons.
</prevsent>
<prevsent>although eisele and dsrre (1988) <papid> P88-1035 </papid>have men-tioned that their algorithm is applicable to term unification as well as graph unification, this method would lose term unification advantage of not requiring so much copying.</prevsent>
</prevsection>
<citsent citstr=" C86-1018 ">
on the con-trary, constraint unification (cu) (hasida 1986, <papid> C86-1018 </papid>tuda et al 1989), <papid> E89-1013 </papid>disjunctive unification method, makes full use of term unification ad- vantages.</citsent>
<aftsection>
<nextsent>in cu, disjunctive feature structures are represented by logical constraints, particu-larly by horn clauses, and unification is regarded as constraint satisfaction problem.
</nextsent>
<nextsent>further- more, solving constraint satisfaction problem is identical to transforming constraint into an equivalent and satisfiable constraint.
</nextsent>
<nextsent>cu unifies feature structures by transforming the constraints on them.
</nextsent>
<nextsent>the basic idea of cu is to transform constraints in demand-driven way; that is, to transform only those constraints which may not be satisfiable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W655">
<title id=" P91-1040.xml">constraint projection an efficient treatment of disjunctive feature descriptions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we adopt term unification for these reasons.
</prevsent>
<prevsent>although eisele and dsrre (1988) <papid> P88-1035 </papid>have men-tioned that their algorithm is applicable to term unification as well as graph unification, this method would lose term unification advantage of not requiring so much copying.</prevsent>
</prevsection>
<citsent citstr=" E89-1013 ">
on the con-trary, constraint unification (cu) (hasida 1986, <papid> C86-1018 </papid>tuda et al 1989), <papid> E89-1013 </papid>disjunctive unification method, makes full use of term unification ad- vantages.</citsent>
<aftsection>
<nextsent>in cu, disjunctive feature structures are represented by logical constraints, particu-larly by horn clauses, and unification is regarded as constraint satisfaction problem.
</nextsent>
<nextsent>further- more, solving constraint satisfaction problem is identical to transforming constraint into an equivalent and satisfiable constraint.
</nextsent>
<nextsent>cu unifies feature structures by transforming the constraints on them.
</nextsent>
<nextsent>the basic idea of cu is to transform constraints in demand-driven way; that is, to transform only those constraints which may not be satisfiable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W661">
<title id=" P91-1040.xml">constraint projection an efficient treatment of disjunctive feature descriptions </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>cpu time (see.)
</prevsent>
<prevsent>prolog cu cp 3.88 6.88 5.64 29.84 19.54 12.49 (out of memory) 245.34 17.32 65.27 19.34 14.66 table computation time
</prevsent>
</prevsection>
<citsent citstr=" C90-3013 ">
in the context of graph unification, carter (1990) <papid> C90-3013 </papid>proposed bottom-up parsing method which abandons information irrelevant the mother structures.</citsent>
<aftsection>
<nextsent>his method, however, fails to check the inconsistency of the abandoned information.
</nextsent>
<nextsent>furthermore, it abandons irrelevant information after the application of the rule is completed, while cp abandons goal-irrelevant constraints dy-namically in its processes.
</nextsent>
<nextsent>this is another reason why our method is better.
</nextsent>
<nextsent>another advantage of cp is that it does not need much copying.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W662">
<title id=" P98-1092.xml">terminological variation a means of identifying research topics from texts </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, following the empirical approach successfully implemented by bourigault (1994), we split complex nps only after search as been performed in the corpus for occurrences of their sub-segments in unambiguous situations, i.e. when the sub-segments are not included in larger segment.
</prevsent>
<prevsent>this favours the extraction of pre-conceived textual units possibly corresponding to domain terms.
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
however morpho-syntactic features alone cannot verify the terminological status of the units extracted since they can also select non terms (see smadja 1993).<papid> J93-1007 </papid></citsent>
<aftsection>
<nextsent>for instance root nodulation is term in the plant biotechnology field whereas book review also found in the corpus is not.
</nextsent>
<nextsent>thus in the first stage, the terms extracted are only plausible candidates which need to be filtered in order to eliminate the most unlikely ones.
</nextsent>
<nextsent>this filtering takes advantage of lexical information accessible at our level of analysis to fine-tune the statistical occurrence criterion which used alone, inevitably leads to massive limination.
</nextsent>
<nextsent>1.2 splitting complex noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W663">
<title id=" P98-2135.xml">discourse cues for broadcast news segmentation </title>
<section> broadcast news analysis.  </section>
<citcontext>
<prevsection>
<prevsent>for example, traditional text tiling approaches often under segment broadcast news because of rapid topic shifts (mani et al 1997).
</prevsent>
<prevsent>this paper takes corpus-based approach to this problem, building linguistic models based on an analysis of digital collection of broadcast news, exploiting the regularity utilized by humans in signaling topic shifts to detect story segments.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
human communication is characterized by distinct discourse structure (grosz and sidner 1986) <papid> J86-3001 </papid>which is used for variety of purposes including managing interaction between participants, mitigating limited attention, and signaling topic shifts.</citsent>
<aftsection>
<nextsent>in processing enre such as technical or journalistic texts, programs can take advantage of explicit discourse cues (e.g.,  the first ,  the most important ) to perform tasks such as summarization (paice 1981).
</nextsent>
<nextsent>our initial inability to segment topics in closed caption news text using thesaurus based subject assessments (liddy and myaeng 1992) motivated an investigation of explicit turn taking signals (e.g., anchor to reporter handoff).
</nextsent>
<nextsent>we analyzed programs (e.g., cnn primenews) from an over one year corpus of closed caption texts with the intention of creating models of discourse and other cues for segmentation.
</nextsent>
<nextsent>i~ . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W664">
<title id=" P98-2135.xml">discourse cues for broadcast news segmentation </title>
<section> broadcast news analysis.  </section>
<citcontext>
<prevsection>
<prevsent>(word) (word)  , abc news  ?  abc correspondent   (word) (word) the pairs of words in parentheses correspond to the reporter first and last names.
</prevsent>
<prevsent>combining the handoffs with structural cues, such as knowing that the first and last speaker in the program will be the anchor, allow us differentiate anchor segments from reporter segments.
</prevsent>
</prevsection>
<citsent citstr=" M95-1012 ">
by preprocessing the closed caption text with part of speech tagger and named entity detector (aberdeen et al 1995) <papid> M95-1012 </papid>retrained on closed captions, we generalize search of text strings to the following class of patterns: * (proper name)  , abc news  ?  abc correspondent   (proper name)</citsent>
<aftsection>
<nextsent>our discourse cue story segment or has been implemented in the context of multimedia (closed captioned text, audio, video) analysis system for web based broadcast news navigation.
</nextsent>
<nextsent>we employ finite state machine to represent discourse states such as an anchor, reporter, or advertisting segment (see figure 2).
</nextsent>
<nextsent>we further enhance these with multimedia cues (e.g. detected silence, black or logo keyframes) and temporal knowledge (indicated as time in figure 2).
</nextsent>
<nextsent>for example, from statistical analysis of cnn prime news programs, we know that weather segments appear on average 18 minutes after the start of the news.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W665">
<title id=" P95-1051.xml">towards a cognitively plausible model for quantification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consequently, all natural language quantifiers ! the support and guidance of dr. jean-pierre corriveau of carleton university is greatly appreciated.
</prevsent>
<prevsent>are, indirectly, modeled by two logical connectives: negation and either conjunction or disjunction.
</prevsent>
</prevsection>
<citsent citstr=" J92-4002 ">
in such an oversimplified model, quantifier ambiguity has often been translated to scoping ambiguity, and elaborate models were developed to remedy the problem, by semantic ists (cooper, 1983; lepore et al 1983; partee, 1984) as well as computational linguists (harper, 1992; <papid> J92-4002 </papid>alshawi, 1990; <papid> J90-3001 </papid>pereira, 1990; moran, 1988).<papid> P88-1005 </papid></citsent>
<aftsection>
<nextsent>the problem can be illustrated by the following examples: (la) every student in cs404 received grade.
</nextsent>
<nextsent>(lb) every student in cs404 received course outline.
</nextsent>
<nextsent>the syntactic structures of (la) and (lb) are identical, and thus according to montague ptq would have the same translation.
</nextsent>
<nextsent>hence, the translation of (lb) would incorrectly state that students in cs404 received different course outlines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W666">
<title id=" P95-1051.xml">towards a cognitively plausible model for quantification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consequently, all natural language quantifiers ! the support and guidance of dr. jean-pierre corriveau of carleton university is greatly appreciated.
</prevsent>
<prevsent>are, indirectly, modeled by two logical connectives: negation and either conjunction or disjunction.
</prevsent>
</prevsection>
<citsent citstr=" J90-3001 ">
in such an oversimplified model, quantifier ambiguity has often been translated to scoping ambiguity, and elaborate models were developed to remedy the problem, by semantic ists (cooper, 1983; lepore et al 1983; partee, 1984) as well as computational linguists (harper, 1992; <papid> J92-4002 </papid>alshawi, 1990; <papid> J90-3001 </papid>pereira, 1990; moran, 1988).<papid> P88-1005 </papid></citsent>
<aftsection>
<nextsent>the problem can be illustrated by the following examples: (la) every student in cs404 received grade.
</nextsent>
<nextsent>(lb) every student in cs404 received course outline.
</nextsent>
<nextsent>the syntactic structures of (la) and (lb) are identical, and thus according to montague ptq would have the same translation.
</nextsent>
<nextsent>hence, the translation of (lb) would incorrectly state that students in cs404 received different course outlines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W667">
<title id=" P95-1051.xml">towards a cognitively plausible model for quantification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>consequently, all natural language quantifiers ! the support and guidance of dr. jean-pierre corriveau of carleton university is greatly appreciated.
</prevsent>
<prevsent>are, indirectly, modeled by two logical connectives: negation and either conjunction or disjunction.
</prevsent>
</prevsection>
<citsent citstr=" P88-1005 ">
in such an oversimplified model, quantifier ambiguity has often been translated to scoping ambiguity, and elaborate models were developed to remedy the problem, by semantic ists (cooper, 1983; lepore et al 1983; partee, 1984) as well as computational linguists (harper, 1992; <papid> J92-4002 </papid>alshawi, 1990; <papid> J90-3001 </papid>pereira, 1990; moran, 1988).<papid> P88-1005 </papid></citsent>
<aftsection>
<nextsent>the problem can be illustrated by the following examples: (la) every student in cs404 received grade.
</nextsent>
<nextsent>(lb) every student in cs404 received course outline.
</nextsent>
<nextsent>the syntactic structures of (la) and (lb) are identical, and thus according to montague ptq would have the same translation.
</nextsent>
<nextsent>hence, the translation of (lb) would incorrectly state that students in cs404 received different course outlines.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W669">
<title id=" P98-2123.xml">a freely available morphological analyzer disambiguator and context sensitive lemmatizer for german </title>
<section> the disambiguator.  </section>
<citcontext>
<prevsection>
<prevsent>after the success of statistical part-of-speech taggers for english, there have been quite few attempts to apply the same methods to german.
</prevsent>
<prevsent>lezius, rapp &amp; wettler (1996) give an overview on some german tagging projects.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
although we considered number of algorithms, we decided to use the trigram algorithm described by church (1988) <papid> A88-1019 </papid>for tagging.</citsent>
<aftsection>
<nextsent>it is simple, fast, robust, and - among the statistical taggers - still more or less unsurpassed in terms of accuracy.
</nextsent>
<nextsent>conceptually, the church-algorithm works as follows: for each sentence of text, it generates all possible assignments of part-of-speech tags to words.
</nextsent>
<nextsent>it then selects that assignment which optimizes the product of the lexical and contex-tual probabilities.
</nextsent>
<nextsent>the lexical probability for word is the probability of observing part of speech given the (possibly ambiguous) word n. the contextual probability for tag is the probability of observing part of speech given the preceding two parts of speech and y. it is estimated by dividing the trigram frequency xyz by the bigram frequency xy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W671">
<title id=" P98-2123.xml">a freely available morphological analyzer disambiguator and context sensitive lemmatizer for german </title>
<section> the disambiguator.  </section>
<citcontext>
<prevsection>
<prevsent>in practice, computational limitations do not allow the enu-meration of all possible assignments for long sentences, and smoothing is required for infre-quent events.
</prevsent>
<prevsent>this is described in more detail in the original publication (church, 1988).<papid> A88-1019 </papid></prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
although more sophisticated algorithms for unsupervised learning - which can be trained on plain text instead on manually tagged corpora are well established (see e.g. merialdo, 1994), <papid> J94-2001 </papid>we decided not to use them.</citsent>
<aftsection>
<nextsent>the main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.
</nextsent>
<nextsent>more recently, in contrast to the statistical tag-gers, rule-based tagging algorithms have been suggested which were shown to reduce the error rate significantly (samuelsson &amp; voutilainen, 1997).<papid> P97-1032 </papid></nextsent>
<nextsent>we consider this promising approach and have started to develop such system for german with the intention of later inclusion into morphy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W672">
<title id=" P98-2123.xml">a freely available morphological analyzer disambiguator and context sensitive lemmatizer for german </title>
<section> the disambiguator.  </section>
<citcontext>
<prevsection>
<prevsent>although more sophisticated algorithms for unsupervised learning - which can be trained on plain text instead on manually tagged corpora are well established (see e.g. merialdo, 1994), <papid> J94-2001 </papid>we decided not to use them.</prevsent>
<prevsent>the main reason is that with large tag sets, the sparse-data-problem can become so severe that unsupervised training easily ends up in local minima, which can lead to poor results without any indication to the user.</prevsent>
</prevsection>
<citsent citstr=" P97-1032 ">
more recently, in contrast to the statistical tag-gers, rule-based tagging algorithms have been suggested which were shown to reduce the error rate significantly (samuelsson &amp; voutilainen, 1997).<papid> P97-1032 </papid></citsent>
<aftsection>
<nextsent>we consider this promising approach and have started to develop such system for german with the intention of later inclusion into morphy.
</nextsent>
<nextsent>the tag set of morphy tagger is based on the feature system of the morphological analyzer.
</nextsent>
<nextsent>however, some features were discarded for tag-ging.
</nextsent>
<nextsent>for example, the tense of verbs is not con-sidered.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W673">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we describe an approach which overcomes this problem using dictionary definitions.
</prevsent>
<prevsent>using the definition- based conceptual co-occurrence data collected from the relatively small brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.
</prevsent>
</prevsection>
<citsent citstr=" P91-1034 ">
previous corpus-based sense disambiguation methods require substantial mounts of sense-tagged training data (kelly and stone, 1975; black, 1988 and hearst, 1991) or aligned bilingual corpora (brown et al., 1991; <papid> P91-1034 </papid>dagan, 1991 and gale et al 1992).</citsent>
<aftsection>
<nextsent>yarowsky (1992) <papid> C92-2070 </papid>introduces thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without he need for sense-tagged training data.</nextsent>
<nextsent>by collecting statistical data of word occurrences in the context of different thesaurus categories from relatively large corpus (10 million words), the system can identify salient words for each category.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W674">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using the definition- based conceptual co-occurrence data collected from the relatively small brown corpus, our sense disambiguation system achieves an average accuracy comparable to human performance given the same contextual information.
</prevsent>
<prevsent>previous corpus-based sense disambiguation methods require substantial mounts of sense-tagged training data (kelly and stone, 1975; black, 1988 and hearst, 1991) or aligned bilingual corpora (brown et al., 1991; <papid> P91-1034 </papid>dagan, 1991 and gale et al 1992).</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
yarowsky (1992) <papid> C92-2070 </papid>introduces thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without he need for sense-tagged training data.</citsent>
<aftsection>
<nextsent>by collecting statistical data of word occurrences in the context of different thesaurus categories from relatively large corpus (10 million words), the system can identify salient words for each category.
</nextsent>
<nextsent>using these salient words, the system is able to disambiguate polysemous words with respect to thesaurus categories.
</nextsent>
<nextsent>statistical approaches like these generally suffer from the problem of data sparseness.
</nextsent>
<nextsent>to estimate the salience of word with reasonable accuracy, the system needs the word to have significant number of occurrences in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W680">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> statistical sense disambiguation using.  </section>
<citcontext>
<prevsection>
<prevsent>concepts dc and dc  \[6\] i(dc, dc ) is the mutual information 4 (fano, 1961) between the 2 defining concepts dc and dc  given by: i(x,y) --- log p(x,y) p(x).
</prevsent>
<prevsent>p(y) (x ,y ) .n ig2 (x ) . f(y) (using the maximum likelihood estimator).
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
f(x,y) is looked up directly from the conceptual co-occurrence data table, fix) and f(y) are looked up from pre-constructed list off(dc) values, for each defining concept dc: f(oc) = ~_,f(dc, dc ) vdc  4 church and hanks (1989) <papid> P89-1010 </papid>use mutual information.</citsent>
<aftsection>
<nextsent>to measure word association norms.
</nextsent>
<nextsent>183 is taken to be the total number of pairs of words processed, given by ~ ( dc)/2 since for each pair of surface words processed, li( c) v/~c is increased by 2.
</nextsent>
<nextsent>our scoring method is based on probabilistic model at the conceptual level.
</nextsent>
<nextsent>in standard model, the logarlthm of the probability of occurrence of conceptual set {x,, x~ ..... xm} in the context of the conceptual set {y~, y~.....y,} is given by log2 p(xl,x2 ..... x,,lyl,y2 ..... y,)  ~ ~=l (  j~.__ll(x,,yj)+lg2 p(xi)) assuming that each p(x~) is independent of each other given y~, y2...., y, and each p(y.i) is independent of each other given x~, for all x~.s our scoring method deviates from the standard model in number of aspects: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W716">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous attempts to tackle the data sparseness problem in general corpus-based work include the class-based approaches and similarity-based approaches.
</prevsent>
<prevsent>in these approaches, relationships between given pair of words are modelled by analogy with other words that resemble the given pair in some way.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
the class-based approaches (brown et al, 1992; <papid> J92-4003 </papid>resnik, 1992; pereira et al, 1993) <papid> P93-1024 </papid>calculate co-occurrence data of words belonging to different classes,~ rather than individual words, to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies.</citsent>
<aftsection>
<nextsent>dagan et al (1993) <papid> P93-1022 </papid>argue that using relatively small number of classes to model the similarity between words may lead to substantial loss of information.</nextsent>
<nextsent>in the similarity- based approaches (dagan et al, 1993 &amp; <papid> P93-1022 </papid>1994; grishman et al, 1993), rather than class, each word is modelled by its own set of similar words derived from statistical data collected from corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W717">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>previous attempts to tackle the data sparseness problem in general corpus-based work include the class-based approaches and similarity-based approaches.
</prevsent>
<prevsent>in these approaches, relationships between given pair of words are modelled by analogy with other words that resemble the given pair in some way.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
the class-based approaches (brown et al, 1992; <papid> J92-4003 </papid>resnik, 1992; pereira et al, 1993) <papid> P93-1024 </papid>calculate co-occurrence data of words belonging to different classes,~ rather than individual words, to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies.</citsent>
<aftsection>
<nextsent>dagan et al (1993) <papid> P93-1022 </papid>argue that using relatively small number of classes to model the similarity between words may lead to substantial loss of information.</nextsent>
<nextsent>in the similarity- based approaches (dagan et al, 1993 &amp; <papid> P93-1022 </papid>1994; grishman et al, 1993), rather than class, each word is modelled by its own set of similar words derived from statistical data collected from corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W718">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in these approaches, relationships between given pair of words are modelled by analogy with other words that resemble the given pair in some way.
</prevsent>
<prevsent>the class-based approaches (brown et al, 1992; <papid> J92-4003 </papid>resnik, 1992; pereira et al, 1993) <papid> P93-1024 </papid>calculate co-occurrence data of words belonging to different classes,~ rather than individual words, to enhance the co-occurrence data collected and to cover words which have low occurrence frequencies.</prevsent>
</prevsection>
<citsent citstr=" P93-1022 ">
dagan et al (1993) <papid> P93-1022 </papid>argue that using relatively small number of classes to model the similarity between words may lead to substantial loss of information.</citsent>
<aftsection>
<nextsent>in the similarity- based approaches (dagan et al, 1993 &amp; <papid> P93-1022 </papid>1994; grishman et al, 1993), rather than class, each word is modelled by its own set of similar words derived from statistical data collected from corpora.</nextsent>
<nextsent>however, deriving these sets of similar words requires substantial mount of statistical data and thus these approaches require relatively large corpora to start with.~ 2 our definition-based approach to statistical sense disambiguation is similar in spirit to the similarity- based approaches, with respect to the  specificity  of modelling individual words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W727">
<title id=" P95-1025.xml">statistical sense disambiguation with relatively small corpora using dictionary definitions </title>
<section> limitation and further work.  </section>
<citcontext>
<prevsection>
<prevsent>in many cases, semantic coherence information is not adequate to select the correct sense, and knowledge about local constraints is needed.
</prevsent>
<prevsent>~3 for disambiguation polysemous nouns, these constraints include the modifiers of these nouns and the verbs which take these nouns as objects, etc. this knowledge has been successfully acquired from corpora in manual or semi-automatic approaches such as that described in hearst (1991).
</prevsent>
</prevsection>
<citsent citstr=" W94-0106 ">
however, fully automatic lexically based approaches 3 hatzivassiloglou (1994) <papid> W94-0106 </papid>shows that the.</citsent>
<aftsection>
<nextsent>introduction of linguistic cues improves the performance of statistical semantic knowledge acquisition system in the context of word grouping.
</nextsent>
<nextsent>187 such as that described in yarowsky (1992) <papid> C92-2070 </papid>are very unlikely to be capable of acquiring this finer knowledge because the problem of data sparseness becomes even more serious with the introduction of syntactic onstraints.</nextsent>
<nextsent>our approach has overcome the data sparseness problem by using the defining concepts of words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W733">
<title id=" P96-1047.xml">sub deletion in verb phrase ellipsis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we have focused on errors in which the program selects the correct head verb as antecedent.
</prevsent>
<prevsent>these cases can be divided into the following categories: 1) too much material included from the antecedent, 2) not enough much material included from the antecedent, 3) discontinuous antecedents, and 4) miscellaneous.
</prevsent>
</prevsection>
<citsent citstr=" J90-4001 ">
for subset of case 1, sub deletion, an algorithm derived from (lappin and mccord, 1990) <papid> J90-4001 </papid>is evaluated 1 this research was supported in part by nsf.</citsent>
<aftsection>
<nextsent>career grant, no.
</nextsent>
<nextsent>iri-9502257.
</nextsent>
<nextsent>in regard to the brown corpus.
</nextsent>
<nextsent>previous studies on evaluating discourse processing (e.g., walker, 1989; <papid> P89-1031 </papid>hobbs, 1978) have involved subjectively examining test cases to determine correctness.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W734">
<title id=" P96-1047.xml">sub deletion in verb phrase ellipsis </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>iri-9502257.
</prevsent>
<prevsent>in regard to the brown corpus.
</prevsent>
</prevsection>
<citsent citstr=" P89-1031 ">
previous studies on evaluating discourse processing (e.g., walker, 1989; <papid> P89-1031 </papid>hobbs, 1978) have involved subjectively examining test cases to determine correctness.</citsent>
<aftsection>
<nextsent>with the development of resources uch as the penn treebank (marcus, santorini, and marcinkiewicz, 1993), it has become possible to automate mpirical tests of discourse processing systems to obtain more objective measure of their success.
</nextsent>
<nextsent>towards this end, an algorithm was implemented in common lisp program called vpeal (verb phrase ellipsis antecedent locator) (hardt, 1995), drawing on the penn treebank as input.
</nextsent>
<nextsent>the portion of the penn treebank examined--the brown corpus, about million words--contains about 400 vpes.
</nextsent>
<nextsent>furthermore, to automatically evaluate the algorithm, utilities were developed to automatically test the output of vpeal for correctness.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W736">
<title id=" P96-1027.xml">chart generation </title>
<section> charts.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C88-2128 ">
shieber (1988) <papid> C88-2128 </papid>showed that parsing charts can be also used in generation and raised the question, which we take up again here, of whether they constitute natural uniform architecture for parsing and generation.</citsent>
<aftsection>
<nextsent>in particular, we will be interested in the extent which they bring to the generation process advantages comparable to those that make them attractive in parsing.
</nextsent>
<nextsent>chart parsing is not well defined notion.
</nextsent>
<nextsent>the usual conception of it involves at least four related ideas: inactive edges.
</nextsent>
<nextsent>in context-free grammar, all phrases of given category that cover given part of the string are equivalent for the purposes of constructing larger phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W738">
<title id=" P96-1027.xml">chart generation </title>
<section> generation.  </section>
<citcontext>
<prevsection>
<prevsent>this view has the apparent disadvantage of putting insignif-icant differences in the syntax of logical forms, such as the relative order of the arguments to symmetric operators, on the same footing as more significant facts about them.
</prevsent>
<prevsent>we know that it will not generally be possible to reduce 200 logical expressions to canonical form but this does not mean that we should expect our generator to be compro-mised, or even greatly delayed, by trivial distinctions.
</prevsent>
</prevsection>
<citsent citstr=" C92-2117 ">
con-siderations of this kind were, in part, responsible for the recent resurgence of interest in  flat  representations of log-ical form (copestake a/.,1996) and for the representa-tions used for transfer in shake-and-bake translation (whitelock, 1992).<papid> C92-2117 </papid></citsent>
<aftsection>
<nextsent>they have made semantic formalisms like those now usually associated with davison (davidson, 1980, parsons, 1990) attractive in artificial intelligence for many years (hobbs 1985, <papid> P85-1008 </papid>kay, 1970).</nextsent>
<nextsent>operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W739">
<title id=" P96-1027.xml">chart generation </title>
<section> generation.  </section>
<citcontext>
<prevsection>
<prevsent>we know that it will not generally be possible to reduce 200 logical expressions to canonical form but this does not mean that we should expect our generator to be compro-mised, or even greatly delayed, by trivial distinctions.
</prevsent>
<prevsent>con-siderations of this kind were, in part, responsible for the recent resurgence of interest in  flat  representations of log-ical form (copestake a/.,1996) and for the representa-tions used for transfer in shake-and-bake translation (whitelock, 1992).<papid> C92-2117 </papid></prevsent>
</prevsection>
<citsent citstr=" P85-1008 ">
they have made semantic formalisms like those now usually associated with davison (davidson, 1980, parsons, 1990) attractive in artificial intelligence for many years (hobbs 1985, <papid> P85-1008 </papid>kay, 1970).</citsent>
<aftsection>
<nextsent>operationally, the attraction is that the notations can be analyzed largely as free word-order languages in the manner outlined above.
</nextsent>
<nextsent>consider the expression ( 1 ) (1) r: run(r), past(r), fast(r), argl (r,j), name(j, john) which we will take as representation the logical form of the sentences john ran fast and john ran quickly.
</nextsent>
<nextsent>it consists of distinguished index (r) and list of predicates whose relative order is immaterial.
</nextsent>
<nextsent>the distinguished index identi-fies this as sentence that makes claim about running event.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W742">
<title id=" P98-1016.xml">redundancy helping semantic disambiguation </title>
<section> source of information and.  </section>
<citcontext>
<prevsection>
<prevsent>4 the importance f semantic.
</prevsent>
<prevsent>re la ions clusters are and have been used in different applications for information retrieval and word sense disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
clustering can be done statistically by analyzing text corpora (wilks et al, 1989; brown et al, 1992; <papid> J92-4003 </papid>pereira et al., 1995) and usually results in set of words or word senses.</citsent>
<aftsection>
<nextsent>in this paper, we are using the clustering method used in (barri~re and popowich, 1996) to present our view on re-dundancy and disambiguation.
</nextsent>
<nextsent>the clustering brings together set of words but also builds cckg which shows the actual links (semantic relations) between the members of the cluster.
</nextsent>
<nextsent>we suggest that those links are essential in an-alyzing and disambiguating texts.
</nextsent>
<nextsent>when links are redundant in graph (that is we find two identical links between two compatible concepts at each end) we are able to reduce semantic am-biguity relating to anaphora nd word sense.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W743">
<title id=" P98-1044.xml">veins theory a model of global discourse cohesion and coherence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using these domains.
</prevsent>
<prevsent>as such, our approach differs from walker (1996), whose account of referentialit~, within the cache memory model does not relyon discourse structure, but rather on cue phrases and matching constraints ogether 281 with constraints on the size of the cache imposed to reflect the plausible limits of the attentional span.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
our approach is closer to that of passonneau (1995) and hahn and strtibe (1997), who both use stack-based model of discourse structure based on grosz and sidner (1986) <papid> J86-3001 </papid>focus spaces.</citsent>
<aftsection>
<nextsent>such model is equivalent a dynamic processing model of tree-like structure reflecting the hierarchical nesting of discourse segments, and thus has significant similarities to discourse structure trees produced by rst (see moser and moore (1996)).<papid> J96-3006 </papid></nextsent>
<nextsent>however, using the rst notion of nuclearity, we go beyond previous work by revealing  hidden  structure in the discourse tree, which we call veins, that enables us to determine the referential accessibility domain for each discourse unit and ultimately to apply ct globally, without extensions to ct or addltional oata structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W744">
<title id=" P98-1044.xml">veins theory a model of global discourse cohesion and coherence </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as such, our approach differs from walker (1996), whose account of referentialit~, within the cache memory model does not relyon discourse structure, but rather on cue phrases and matching constraints ogether 281 with constraints on the size of the cache imposed to reflect the plausible limits of the attentional span.
</prevsent>
<prevsent>our approach is closer to that of passonneau (1995) and hahn and strtibe (1997), who both use stack-based model of discourse structure based on grosz and sidner (1986) <papid> J86-3001 </papid>focus spaces.</prevsent>
</prevsection>
<citsent citstr=" J96-3006 ">
such model is equivalent a dynamic processing model of tree-like structure reflecting the hierarchical nesting of discourse segments, and thus has significant similarities to discourse structure trees produced by rst (see moser and moore (1996)).<papid> J96-3006 </papid></citsent>
<aftsection>
<nextsent>however, using the rst notion of nuclearity, we go beyond previous work by revealing  hidden  structure in the discourse tree, which we call veins, that enables us to determine the referential accessibility domain for each discourse unit and ultimately to apply ct globally, without extensions to ct or addltional oata structures.
</nextsent>
<nextsent>in this paper, we describe veins theory (vt) by showing how veins are defined over discourse structure trees, and how ct can be applied to global discourse by using them.
</nextsent>
<nextsent>we use centering transitions (brennan, friedman and pollard (1987)) to define   smoothness   index, which is used to compare different discourse structures and interpretations.
</nextsent>
<nextsent>because veins define the domains of referential ccess for each discourse unit, we further demonstrate how vt may` be potentially used to determine the   minimal   parts ta text required to resolve references a given utterance or, more generally, to understand it out of the context of the entire discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W745">
<title id=" P98-1044.xml">veins theory a model of global discourse cohesion and coherence </title>
<section> minimal  text.  </section>
<citcontext>
<prevsection>
<prevsent>a graph of cb-unifications can be derived, in which each edge of the graph represents cb computation and therefore unification process.
</prevsent>
<prevsent>the notion that text summaries can be created by extracting the nuclei from rst trees is well known in the literature (mann and thompson, (1988)).
</prevsent>
</prevsection>
<citsent citstr=" P97-1013 ">
most recently, marcu (1997) <papid> P97-1013 </papid>has described method for text summarization based on nuclearity and selective retention of hierarchical fragments.</citsent>
<aftsection>
<nextsent>because his salient units correspond to heads in vt, his results are predicted in our model.
</nextsent>
<nextsent>that is, the union of heads at given level in the tree provides summary of the text at degree of detail dependent on the depth of that level.
</nextsent>
<nextsent>in addition to summarizing entire texts, vt can be used to summarize given unit or sub-tree of that 283 text.
</nextsent>
<nextsent>in effect, we reverse the problem addressed by text summarization efforts so far: instead of attempting to summarize an entire discourse at given level of detail, we select single span of text and abstract the minimal text required to understand this span alone when considered in the context of the entire discourse.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W746">
<title id=" P98-1044.xml">veins theory a model of global discourse cohesion and coherence </title>
<section> 5.2%  </section>
<citcontext>
<prevsection>
<prevsent>corpus analysis.
</prevsent>
<prevsent>because of the lack of large-scale corpora annotated for discourse, our study currently involves only small corpus of english, romanian, and french texts.
</prevsent>
</prevsection>
<citsent citstr=" W98-1102 ">
the corpus was prepared using an encoding scheme for discourse structure (cristea, ide, and romary, 1998) based on the corpus encoding standard (ces) (ide (1998)).<papid> W98-1102 </papid></citsent>
<aftsection>
<nextsent>the following texts were included in our analysis: . three short english texts, rst-analyzed by experts and subsequently annotated for reference and cf lists by the authors; ? fragment from de balzac   le p~re goriot   (french), previously annotated for co-reference (bruneseaux and romary (1997)); rst and cf lists annotation made by the authors; ? fragment from alexandru mitru   legend ele olimpului   4 .
</nextsent>
<nextsent>(romanian); structure, reference, and cf hsts annotated by one of the authors.
</nextsent>
<nextsent>the encoding marks referring expressions, links between referring expressions (co-reference or functional), units, relations between units (if known), nuclearity, and the units  cf lists in terms of refemng expressions.
</nextsent>
<nextsent>we have developed program 5 that does the following: builds the tree structure of units and relations between them, adds to each referring expression the index of the unit it occurs in, computes the heads and veins for all nodes in the structure, determines the accessibility domains of the terminal nodes (units), counts the number of direct and indirect references.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W750">
<title id=" P92-1039.xml">right association revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kimball (1973) proposes the parsing strategy of right association (ra).
</prevsent>
<prevsent>ra resolves modifiers attachment am-biguities by attaching at the lowest syntactically per-missible position along the right frontier.
</prevsent>
</prevsection>
<citsent citstr=" E85-1013 ">
many au-thors (among them wilks 1985, <papid> E85-1013 </papid>schubert 1986, whit-temore et al 1990, <papid> P90-1004 </papid>and weischedel et al 1991) <papid> H91-1037 </papid>in-corporate ra into their parsing systems, yet none relyon it solely, integrating it instead with disambiguation preferences derived from word/constituent/concept o- occurrence based.</citsent>
<aftsection>
<nextsent>on its own, ra performs rather well, given its simplicity, but it is far from adequate: whitte-more et al evaluate ra performance on pp attachment using corpus derived from computer-mediated dialog.
</nextsent>
<nextsent>they find that ra makes correct predictions 55% of the time.
</nextsent>
<nextsent>weischedel et al, using corpus of news sto-ries, report 75% success rate on the general case of attachment using strategy closest attachment which is essentially ra.
</nextsent>
<nextsent>in the work cited above, ra plays relatively minor role, as compared with co-occurrence based preferences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W751">
<title id=" P92-1039.xml">right association revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kimball (1973) proposes the parsing strategy of right association (ra).
</prevsent>
<prevsent>ra resolves modifiers attachment am-biguities by attaching at the lowest syntactically per-missible position along the right frontier.
</prevsent>
</prevsection>
<citsent citstr=" P90-1004 ">
many au-thors (among them wilks 1985, <papid> E85-1013 </papid>schubert 1986, whit-temore et al 1990, <papid> P90-1004 </papid>and weischedel et al 1991) <papid> H91-1037 </papid>in-corporate ra into their parsing systems, yet none relyon it solely, integrating it instead with disambiguation preferences derived from word/constituent/concept o- occurrence based.</citsent>
<aftsection>
<nextsent>on its own, ra performs rather well, given its simplicity, but it is far from adequate: whitte-more et al evaluate ra performance on pp attachment using corpus derived from computer-mediated dialog.
</nextsent>
<nextsent>they find that ra makes correct predictions 55% of the time.
</nextsent>
<nextsent>weischedel et al, using corpus of news sto-ries, report 75% success rate on the general case of attachment using strategy closest attachment which is essentially ra.
</nextsent>
<nextsent>in the work cited above, ra plays relatively minor role, as compared with co-occurrence based preferences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W752">
<title id=" P92-1039.xml">right association revisited </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>kimball (1973) proposes the parsing strategy of right association (ra).
</prevsent>
<prevsent>ra resolves modifiers attachment am-biguities by attaching at the lowest syntactically per-missible position along the right frontier.
</prevsent>
</prevsection>
<citsent citstr=" H91-1037 ">
many au-thors (among them wilks 1985, <papid> E85-1013 </papid>schubert 1986, whit-temore et al 1990, <papid> P90-1004 </papid>and weischedel et al 1991) <papid> H91-1037 </papid>in-corporate ra into their parsing systems, yet none relyon it solely, integrating it instead with disambiguation preferences derived from word/constituent/concept o- occurrence based.</citsent>
<aftsection>
<nextsent>on its own, ra performs rather well, given its simplicity, but it is far from adequate: whitte-more et al evaluate ra performance on pp attachment using corpus derived from computer-mediated dialog.
</nextsent>
<nextsent>they find that ra makes correct predictions 55% of the time.
</nextsent>
<nextsent>weischedel et al, using corpus of news sto-ries, report 75% success rate on the general case of attachment using strategy closest attachment which is essentially ra.
</nextsent>
<nextsent>in the work cited above, ra plays relatively minor role, as compared with co-occurrence based preferences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W753">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>this strategy has been successfully applied for number of se-mantic ambiguities like quantifier and operator scope ambiguities.
</prevsent>
<prevsent>therefore it is not surpris-ing that the usage of underspecified semantic representations have gained much popularity in recent years.
</prevsent>
</prevsection>
<citsent citstr=" C96-1024 ">
work in the literature include the qlf representations (alshawi, 1992), the work on underspecified discourse representa-tion structures (udrs) (reyle, 1993; bos et al., 1996), <papid> C96-1024 </papid>and the collection of papers in van deemter and peters (1996).</citsent>
<aftsection>
<nextsent>for an application of using underspecified semantic representations within mt see alshawi et al (1991), <papid> P91-1021 </papid>copestake et al (1995) and dorna and emele (1996).<papid> C96-1054 </papid></nextsent>
<nextsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W754">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>therefore it is not surpris-ing that the usage of underspecified semantic representations have gained much popularity in recent years.
</prevsent>
<prevsent>work in the literature include the qlf representations (alshawi, 1992), the work on underspecified discourse representa-tion structures (udrs) (reyle, 1993; bos et al., 1996), <papid> C96-1024 </papid>and the collection of papers in van deemter and peters (1996).</prevsent>
</prevsection>
<citsent citstr=" P91-1021 ">
for an application of using underspecified semantic representations within mt see alshawi et al (1991), <papid> P91-1021 </papid>copestake et al (1995) and dorna and emele (1996).<papid> C96-1054 </papid></citsent>
<aftsection>
<nextsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.
</nextsent>
<nextsent>there has been growing interest in developing underspecified or so called packed res presentations to deal with such syn-tactic ambiguities (cf.
</nextsent>
<nextsent>rich et al (1987), seo and simmons (1989), <papid> J89-1002 </papid>bear and hobbs (1988), <papid> A88-1032 </papid>maxwell iii and kaplan (1993), pinkal (1995), egg and lebeth (1995), schiehlen (1996) <papid> C96-2153 </papid>and dsrre (1997)).</nextsent>
<nextsent>the key idea of all these representations is to factor common information as much as pos-sible in parse forest and to represent the at-tachment ambiguities as local dis junctions with- out conversion to disjunctive normal form.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W755">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>therefore it is not surpris-ing that the usage of underspecified semantic representations have gained much popularity in recent years.
</prevsent>
<prevsent>work in the literature include the qlf representations (alshawi, 1992), the work on underspecified discourse representa-tion structures (udrs) (reyle, 1993; bos et al., 1996), <papid> C96-1024 </papid>and the collection of papers in van deemter and peters (1996).</prevsent>
</prevsection>
<citsent citstr=" C96-1054 ">
for an application of using underspecified semantic representations within mt see alshawi et al (1991), <papid> P91-1021 </papid>copestake et al (1995) and dorna and emele (1996).<papid> C96-1054 </papid></citsent>
<aftsection>
<nextsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.
</nextsent>
<nextsent>there has been growing interest in developing underspecified or so called packed res presentations to deal with such syn-tactic ambiguities (cf.
</nextsent>
<nextsent>rich et al (1987), seo and simmons (1989), <papid> J89-1002 </papid>bear and hobbs (1988), <papid> A88-1032 </papid>maxwell iii and kaplan (1993), pinkal (1995), egg and lebeth (1995), schiehlen (1996) <papid> C96-2153 </papid>and dsrre (1997)).</nextsent>
<nextsent>the key idea of all these representations is to factor common information as much as pos-sible in parse forest and to represent the at-tachment ambiguities as local dis junctions with- out conversion to disjunctive normal form.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W756">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.
</prevsent>
<prevsent>there has been growing interest in developing underspecified or so called packed res presentations to deal with such syn-tactic ambiguities (cf.
</prevsent>
</prevsection>
<citsent citstr=" J89-1002 ">
rich et al (1987), seo and simmons (1989), <papid> J89-1002 </papid>bear and hobbs (1988), <papid> A88-1032 </papid>maxwell iii and kaplan (1993), pinkal (1995), egg and lebeth (1995), schiehlen (1996) <papid> C96-2153 </papid>and dsrre (1997)).</citsent>
<aftsection>
<nextsent>the key idea of all these representations is to factor common information as much as pos-sible in parse forest and to represent the at-tachment ambiguities as local dis junctions with- out conversion to disjunctive normal form.
</nextsent>
<nextsent>such representations avoid the exponential explosion which would result if all possible readings are extracted from the parse forest.
</nextsent>
<nextsent>to achieve our overall goal of ambiguity pre-serving mt it requires not only parser which is able to produce such packed representations but also generator which is able to take such packed representation as input and generate all possible paraphrases without explicitly enumer-ating all readings.
</nextsent>
<nextsent>the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W757">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.
</prevsent>
<prevsent>there has been growing interest in developing underspecified or so called packed res presentations to deal with such syn-tactic ambiguities (cf.
</prevsent>
</prevsection>
<citsent citstr=" A88-1032 ">
rich et al (1987), seo and simmons (1989), <papid> J89-1002 </papid>bear and hobbs (1988), <papid> A88-1032 </papid>maxwell iii and kaplan (1993), pinkal (1995), egg and lebeth (1995), schiehlen (1996) <papid> C96-2153 </papid>and dsrre (1997)).</citsent>
<aftsection>
<nextsent>the key idea of all these representations is to factor common information as much as pos-sible in parse forest and to represent the at-tachment ambiguities as local dis junctions with- out conversion to disjunctive normal form.
</nextsent>
<nextsent>such representations avoid the exponential explosion which would result if all possible readings are extracted from the parse forest.
</nextsent>
<nextsent>to achieve our overall goal of ambiguity pre-serving mt it requires not only parser which is able to produce such packed representations but also generator which is able to take such packed representation as input and generate all possible paraphrases without explicitly enumer-ating all readings.
</nextsent>
<nextsent>the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W758">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>another source of ambiguities which might be pre servable between related languages include syntactic ambiguities like the well-known pp at- 365 tachment ambiguities.
</prevsent>
<prevsent>there has been growing interest in developing underspecified or so called packed res presentations to deal with such syn-tactic ambiguities (cf.
</prevsent>
</prevsection>
<citsent citstr=" C96-2153 ">
rich et al (1987), seo and simmons (1989), <papid> J89-1002 </papid>bear and hobbs (1988), <papid> A88-1032 </papid>maxwell iii and kaplan (1993), pinkal (1995), egg and lebeth (1995), schiehlen (1996) <papid> C96-2153 </papid>and dsrre (1997)).</citsent>
<aftsection>
<nextsent>the key idea of all these representations is to factor common information as much as pos-sible in parse forest and to represent the at-tachment ambiguities as local dis junctions with- out conversion to disjunctive normal form.
</nextsent>
<nextsent>such representations avoid the exponential explosion which would result if all possible readings are extracted from the parse forest.
</nextsent>
<nextsent>to achieve our overall goal of ambiguity pre-serving mt it requires not only parser which is able to produce such packed representations but also generator which is able to take such packed representation as input and generate all possible paraphrases without explicitly enumer-ating all readings.
</nextsent>
<nextsent>the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W759">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>such representations avoid the exponential explosion which would result if all possible readings are extracted from the parse forest.
</prevsent>
<prevsent>to achieve our overall goal of ambiguity pre-serving mt it requires not only parser which is able to produce such packed representations but also generator which is able to take such packed representation as input and generate all possible paraphrases without explicitly enumer-ating all readings.
</prevsent>
</prevsection>
<citsent citstr=" P96-1027 ">
the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</citsent>
<aftsection>
<nextsent>what needs to be done to realize our envis-aged goal is transfer system which is able to work on these packed translations without unpacking them or only as much as necessary if ambiguities can only partly be preserved in the target language.
</nextsent>
<nextsent>the rest of this paper is concerned with the extension of shake-and- bake like transfer approach (whitelock, 1992; <papid> C92-2117 </papid>beaven, 1992) <papid> C92-2091 </papid>or the kind of semantic-based transfer approach as described for example in dorna and emele (1996) <papid> C96-1054 </papid>to cope with local am- biguities.</nextsent>
<nextsent>to explain and illustrate the treatment of local ambiguities we show how an underspeci- fled representation of pp attachment ambigu-ities can be utilized in machine translation architecture for providing ambiguity preserving translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W760">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>such representations avoid the exponential explosion which would result if all possible readings are extracted from the parse forest.
</prevsent>
<prevsent>to achieve our overall goal of ambiguity pre-serving mt it requires not only parser which is able to produce such packed representations but also generator which is able to take such packed representation as input and generate all possible paraphrases without explicitly enumer-ating all readings.
</prevsent>
</prevsection>
<citsent citstr=" C96-2155 ">
the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</citsent>
<aftsection>
<nextsent>what needs to be done to realize our envis-aged goal is transfer system which is able to work on these packed translations without unpacking them or only as much as necessary if ambiguities can only partly be preserved in the target language.
</nextsent>
<nextsent>the rest of this paper is concerned with the extension of shake-and- bake like transfer approach (whitelock, 1992; <papid> C92-2117 </papid>beaven, 1992) <papid> C92-2091 </papid>or the kind of semantic-based transfer approach as described for example in dorna and emele (1996) <papid> C96-1054 </papid>to cope with local am- biguities.</nextsent>
<nextsent>to explain and illustrate the treatment of local ambiguities we show how an underspeci- fled representation of pp attachment ambigu-ities can be utilized in machine translation architecture for providing ambiguity preserving translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W761">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</prevsent>
<prevsent>what needs to be done to realize our envis-aged goal is transfer system which is able to work on these packed translations without unpacking them or only as much as necessary if ambiguities can only partly be preserved in the target language.</prevsent>
</prevsection>
<citsent citstr=" C92-2117 ">
the rest of this paper is concerned with the extension of shake-and- bake like transfer approach (whitelock, 1992; <papid> C92-2117 </papid>beaven, 1992) <papid> C92-2091 </papid>or the kind of semantic-based transfer approach as described for example in dorna and emele (1996) <papid> C96-1054 </papid>to cope with local am- biguities.</citsent>
<aftsection>
<nextsent>to explain and illustrate the treatment of local ambiguities we show how an underspeci- fled representation of pp attachment ambigu-ities can be utilized in machine translation architecture for providing ambiguity preserving translations.
</nextsent>
<nextsent>it is illustrated on the basis of lfg f-structure level representations (kaplan and bresnan, 1982).
</nextsent>
<nextsent>however, it could equally well be done on the level of underspecified se-mantic representations shown in (dorna et al., 1998).<papid> P98-1056 </papid></nextsent>
<nextsent>the main reason for choosing the f- structure level representation is due to the fact that we could use the xerox linguistic envi-ronment (xle) system (maxwell iii and ka-plan, 1996) for the analysis and generation of english and german utterances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W762">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the work in kay (1996) <papid> P96-1027 </papid>and the extension to ambiguous input in shemtov (1996) <papid> C96-2155 </papid>and shemtov (1997) describes chart- based generation process which takes packed representations input and generates all para-phrases without expanding first into disjunctive normal form.</prevsent>
<prevsent>what needs to be done to realize our envis-aged goal is transfer system which is able to work on these packed translations without unpacking them or only as much as necessary if ambiguities can only partly be preserved in the target language.</prevsent>
</prevsection>
<citsent citstr=" C92-2091 ">
the rest of this paper is concerned with the extension of shake-and- bake like transfer approach (whitelock, 1992; <papid> C92-2117 </papid>beaven, 1992) <papid> C92-2091 </papid>or the kind of semantic-based transfer approach as described for example in dorna and emele (1996) <papid> C96-1054 </papid>to cope with local am- biguities.</citsent>
<aftsection>
<nextsent>to explain and illustrate the treatment of local ambiguities we show how an underspeci- fled representation of pp attachment ambigu-ities can be utilized in machine translation architecture for providing ambiguity preserving translations.
</nextsent>
<nextsent>it is illustrated on the basis of lfg f-structure level representations (kaplan and bresnan, 1982).
</nextsent>
<nextsent>however, it could equally well be done on the level of underspecified se-mantic representations shown in (dorna et al., 1998).<papid> P98-1056 </papid></nextsent>
<nextsent>the main reason for choosing the f- structure level representation is due to the fact that we could use the xerox linguistic envi-ronment (xle) system (maxwell iii and ka-plan, 1996) for the analysis and generation of english and german utterances.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W764">
<title id=" P98-1060.xml">ambiguity preserving machine translation using packed representations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>to explain and illustrate the treatment of local ambiguities we show how an underspeci- fled representation of pp attachment ambigu-ities can be utilized in machine translation architecture for providing ambiguity preserving translations.
</prevsent>
<prevsent>it is illustrated on the basis of lfg f-structure level representations (kaplan and bresnan, 1982).
</prevsent>
</prevsection>
<citsent citstr=" P98-1056 ">
however, it could equally well be done on the level of underspecified se-mantic representations shown in (dorna et al., 1998).<papid> P98-1056 </papid></citsent>
<aftsection>
<nextsent>the main reason for choosing the f- structure level representation is due to the fact that we could use the xerox linguistic envi-ronment (xle) system (maxwell iii and ka-plan, 1996) for the analysis and generation of english and german utterances.
</nextsent>
<nextsent>the key ar-gument for using this linguistic workbench is the ability to produce packed representations for ambiguous utterances using techniques de-scribed in maxwell iii and kaplan (1993) and the availability of generator which generates utterances from f-structure descriptions.
</nextsent>
<nextsent>the rest of the paper is structured as follows: first, we show how the hierarchical f-structure representations can be converted into flat set of prolog predicates uch that the shake-and- bake like transfer approach can be applied.
</nextsent>
<nextsent>sec-ond, we show how pp attachment ambiguities are represented using packed representation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W765">
<title id=" P98-1108.xml">use of mutual information based character clusters in dictionary less morphological analysis of japanese </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" A88-1019 ">
recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (church 1988; <papid> A88-1019 </papid>cutting et. al. 1992; <papid> A92-1018 </papid>charniak et. al. 1993; brill 1994; nagata 1994; <papid> C94-1032 </papid>yamamoto 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>morphological analysis on japanese, however, is more complex because, unlike european languages, no spaces are inserted between words.
</nextsent>
<nextsent>in fact, even native japanese speakers place word boundaries incon-sistently.
</nextsent>
<nextsent>consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications.
</nextsent>
<nextsent>for practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to co-ordinate the dictionary used, tag sets, and nu-merous other parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W766">
<title id=" P98-1108.xml">use of mutual information based character clusters in dictionary less morphological analysis of japanese </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" A92-1018 ">
recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (church 1988; <papid> A88-1019 </papid>cutting et. al. 1992; <papid> A92-1018 </papid>charniak et. al. 1993; brill 1994; nagata 1994; <papid> C94-1032 </papid>yamamoto 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>morphological analysis on japanese, however, is more complex because, unlike european languages, no spaces are inserted between words.
</nextsent>
<nextsent>in fact, even native japanese speakers place word boundaries incon-sistently.
</nextsent>
<nextsent>consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications.
</nextsent>
<nextsent>for practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to co-ordinate the dictionary used, tag sets, and nu-merous other parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W767">
<title id=" P98-1108.xml">use of mutual information based character clusters in dictionary less morphological analysis of japanese </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C94-1032 ">
recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (church 1988; <papid> A88-1019 </papid>cutting et. al. 1992; <papid> A92-1018 </papid>charniak et. al. 1993; brill 1994; nagata 1994; <papid> C94-1032 </papid>yamamoto 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>morphological analysis on japanese, however, is more complex because, unlike european languages, no spaces are inserted between words.
</nextsent>
<nextsent>in fact, even native japanese speakers place word boundaries incon-sistently.
</nextsent>
<nextsent>consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications.
</nextsent>
<nextsent>for practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to co-ordinate the dictionary used, tag sets, and nu-merous other parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W768">
<title id=" P98-1108.xml">use of mutual information based character clusters in dictionary less morphological analysis of japanese </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" W96-0113 ">
recent papers have reported cases of successful part-of-speech tagging with statistical language modeling techniques (church 1988; <papid> A88-1019 </papid>cutting et. al. 1992; <papid> A92-1018 </papid>charniak et. al. 1993; brill 1994; nagata 1994; <papid> C94-1032 </papid>yamamoto 1996).<papid> W96-0113 </papid></citsent>
<aftsection>
<nextsent>morphological analysis on japanese, however, is more complex because, unlike european languages, no spaces are inserted between words.
</nextsent>
<nextsent>in fact, even native japanese speakers place word boundaries incon-sistently.
</nextsent>
<nextsent>consequently, individual researchers have been adopting different word boundaries and tag sets based on their own theory-internal justifications.
</nextsent>
<nextsent>for practical system to utilize the different word boundaries and tag sets according to the demands of an application, it is necessary to co-ordinate the dictionary used, tag sets, and nu-merous other parameters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W769">
<title id=" P98-1108.xml">use of mutual information based character clusters in dictionary less morphological analysis of japanese </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>3 mutua in format ion-based.
</prevsent>
<prevsent>character luster ing one idea is to sort words out in terms of neigh-boring contexts.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
accordingly research as been carried out on n-gram models of word cluster-ing (brown et. al. 1992) <papid> J92-4003 </papid>to obtain hierarchical clusters of words by classifying words in such way so as to minimizes the reduction of mi.</citsent>
<aftsection>
<nextsent>this idea is general in the clustering of any kind of list of items into hierarchical classes.
</nextsent>
<nextsent>4 we therefore have adopted this approach not only to compute word classes but also to com-pute character clusterings in japanese.
</nextsent>
<nextsent>the basic algorithm for clustering items based on the amount of mi is as follows: 1) assign singleton class to every item in the set.
</nextsent>
<nextsent>2) choose two appropriate classes to create new class which subsumes them.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W773">
<title id=" P97-1027.xml">an algorithm for generating referential descriptions with flexible interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, the main achievement of our approach lies in providing core algorithm that makes few assumptions about other processing components and improves the flow of control between modules.
</prevsent>
<prevsent>generating referential descriptions requires electing aset of descriptors according to criteria which reflect humans preferences and verbalizing these descriptors while meeting natural anguage constraints.
</prevsent>
</prevsection>
<citsent citstr=" P89-1009 ">
over the last decade, (dale, 1989, <papid> P89-1009 </papid>dale, haddock, 1991, reiter, 1990<papid> P90-1013 </papid>b, dale, reiter, 1995), and others 2 have contributed to this issue the term  referential description  is due to donell an (donellan, 1966).</citsent>
<aftsection>
<nextsent>this notion signifies referring expression that serves the purpose of letting the hearer identify particular object out of set of objects assumed to be in the current focus of attention.
</nextsent>
<nextsent>the approach undertaken by appelt and kronfeld (appelt, 1985<papid> P85-1025 </papid>a, appelt, 1985<papid> P85-1025 </papid>b, kronfeld, 1986, <papid> P86-1029 </papid>appelt, kronfeld, 1987) is very elaborate but it suffers from very limited coverage, missing assessments of the relative benefit of alternatives, and notorious inef- ficiency.</nextsent>
<nextsent>(see the systems naos (novak, 1988), epicure (dale, 1988), fn (reiter, 1990<papid> P90-1013 </papid>a), and idas (reiter, dale, 1992)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W774">
<title id=" P97-1027.xml">an algorithm for generating referential descriptions with flexible interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence, the main achievement of our approach lies in providing core algorithm that makes few assumptions about other processing components and improves the flow of control between modules.
</prevsent>
<prevsent>generating referential descriptions requires electing aset of descriptors according to criteria which reflect humans preferences and verbalizing these descriptors while meeting natural anguage constraints.
</prevsent>
</prevsection>
<citsent citstr=" P90-1013 ">
over the last decade, (dale, 1989, <papid> P89-1009 </papid>dale, haddock, 1991, reiter, 1990<papid> P90-1013 </papid>b, dale, reiter, 1995), and others 2 have contributed to this issue the term  referential description  is due to donell an (donellan, 1966).</citsent>
<aftsection>
<nextsent>this notion signifies referring expression that serves the purpose of letting the hearer identify particular object out of set of objects assumed to be in the current focus of attention.
</nextsent>
<nextsent>the approach undertaken by appelt and kronfeld (appelt, 1985<papid> P85-1025 </papid>a, appelt, 1985<papid> P85-1025 </papid>b, kronfeld, 1986, <papid> P86-1029 </papid>appelt, kronfeld, 1987) is very elaborate but it suffers from very limited coverage, missing assessments of the relative benefit of alternatives, and notorious inef- ficiency.</nextsent>
<nextsent>(see the systems naos (novak, 1988), epicure (dale, 1988), fn (reiter, 1990<papid> P90-1013 </papid>a), and idas (reiter, dale, 1992)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W777">
<title id=" P97-1027.xml">an algorithm for generating referential descriptions with flexible interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over the last decade, (dale, 1989, <papid> P89-1009 </papid>dale, haddock, 1991, reiter, 1990<papid> P90-1013 </papid>b, dale, reiter, 1995), and others 2 have contributed to this issue the term  referential description  is due to donell an (donellan, 1966).</prevsent>
<prevsent>this notion signifies referring expression that serves the purpose of letting the hearer identify particular object out of set of objects assumed to be in the current focus of attention.</prevsent>
</prevsection>
<citsent citstr=" P85-1025 ">
the approach undertaken by appelt and kronfeld (appelt, 1985<papid> P85-1025 </papid>a, appelt, 1985<papid> P85-1025 </papid>b, kronfeld, 1986, <papid> P86-1029 </papid>appelt, kronfeld, 1987) is very elaborate but it suffers from very limited coverage, missing assessments of the relative benefit of alternatives, and notorious inef- ficiency.</citsent>
<aftsection>
<nextsent>(see the systems naos (novak, 1988), epicure (dale, 1988), fn (reiter, 1990<papid> P90-1013 </papid>a), and idas (reiter, dale, 1992)).</nextsent>
<nextsent>nevertheless, these approaches still suffer from some crucial deficits, including limited coverage (see (horacek, 1995, horacek, 1996) for an improved algo- rithm), and too strong assumptions about adjacent processing components, namely: ? the instant availability of all descriptors for an object to be described, ? the adequate expressibility of chosen set of descriptors in terms of lexical items.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W781">
<title id=" P97-1027.xml">an algorithm for generating referential descriptions with flexible interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>over the last decade, (dale, 1989, <papid> P89-1009 </papid>dale, haddock, 1991, reiter, 1990<papid> P90-1013 </papid>b, dale, reiter, 1995), and others 2 have contributed to this issue the term  referential description  is due to donell an (donellan, 1966).</prevsent>
<prevsent>this notion signifies referring expression that serves the purpose of letting the hearer identify particular object out of set of objects assumed to be in the current focus of attention.</prevsent>
</prevsection>
<citsent citstr=" P86-1029 ">
the approach undertaken by appelt and kronfeld (appelt, 1985<papid> P85-1025 </papid>a, appelt, 1985<papid> P85-1025 </papid>b, kronfeld, 1986, <papid> P86-1029 </papid>appelt, kronfeld, 1987) is very elaborate but it suffers from very limited coverage, missing assessments of the relative benefit of alternatives, and notorious inef- ficiency.</citsent>
<aftsection>
<nextsent>(see the systems naos (novak, 1988), epicure (dale, 1988), fn (reiter, 1990<papid> P90-1013 </papid>a), and idas (reiter, dale, 1992)).</nextsent>
<nextsent>nevertheless, these approaches still suffer from some crucial deficits, including limited coverage (see (horacek, 1995, horacek, 1996) for an improved algo- rithm), and too strong assumptions about adjacent processing components, namely: ? the instant availability of all descriptors for an object to be described, ? the adequate expressibility of chosen set of descriptors in terms of lexical items.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W790">
<title id=" P98-2129.xml">evaluating response strategies in a web based spoken dialogue agent </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using the paradise evaluation framework to estimate an overall performance function, we identify inter-dependencies that exist between speech recognition and response strategy.
</prevsent>
<prevsent>our results elaborate the con-ditions under which toot  cooperative rather than literal strategy contributes to greater performance.
</prevsent>
</prevsection>
<citsent citstr=" P84-1029 ">
the notion of cooperative response has been the focus of considerable research in natural anguage and spoken dialogue systems (allen and perrault, 1980; mays, 1980; kaplan, 1981; joshi et al, 1984; <papid> P84-1029 </papid>mccoy, 1989; pao and wilpon, 1992; <papid> H92-1008 </papid>moore, 1994; seneff et al, 1995; goddeau et al, 1996; pierac-cini et al, 1997).</citsent>
<aftsection>
<nextsent>however, despite the existence of many algorithms for generating cooperative re-sponses, there has been little empirical work ad-dressing the evaluation of such algorithms in the context of real-time natural anguage dialogue sys-tems with human users.
</nextsent>
<nextsent>thus it is unclear un-der what conditions cooperative responses result in more efficient or efficacious dialogues.
</nextsent>
<nextsent>this paper presents an empirical evaluation of two alternative algorithms for responding to database queries in toot, spoken dialogue agent for accessing online train schedules via telephone conversation.
</nextsent>
<nextsent>we conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of toot (literal and coopera-tive toot), resulting in corpus of 48 dialogues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W792">
<title id=" P98-2129.xml">evaluating response strategies in a web based spoken dialogue agent </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using the paradise evaluation framework to estimate an overall performance function, we identify inter-dependencies that exist between speech recognition and response strategy.
</prevsent>
<prevsent>our results elaborate the con-ditions under which toot  cooperative rather than literal strategy contributes to greater performance.
</prevsent>
</prevsection>
<citsent citstr=" H92-1008 ">
the notion of cooperative response has been the focus of considerable research in natural anguage and spoken dialogue systems (allen and perrault, 1980; mays, 1980; kaplan, 1981; joshi et al, 1984; <papid> P84-1029 </papid>mccoy, 1989; pao and wilpon, 1992; <papid> H92-1008 </papid>moore, 1994; seneff et al, 1995; goddeau et al, 1996; pierac-cini et al, 1997).</citsent>
<aftsection>
<nextsent>however, despite the existence of many algorithms for generating cooperative re-sponses, there has been little empirical work ad-dressing the evaluation of such algorithms in the context of real-time natural anguage dialogue sys-tems with human users.
</nextsent>
<nextsent>thus it is unclear un-der what conditions cooperative responses result in more efficient or efficacious dialogues.
</nextsent>
<nextsent>this paper presents an empirical evaluation of two alternative algorithms for responding to database queries in toot, spoken dialogue agent for accessing online train schedules via telephone conversation.
</nextsent>
<nextsent>we conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of toot (literal and coopera-tive toot), resulting in corpus of 48 dialogues.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W794">
<title id=" P98-2129.xml">evaluating response strategies in a web based spoken dialogue agent </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we conduct an experiment in which 12 users carry out 4 tasks of varying difficulty with one of two versions of toot (literal and coopera-tive toot), resulting in corpus of 48 dialogues.
</prevsent>
<prevsent>the values for wide range of evaluation measures are then extracted from this corpus.
</prevsent>
</prevsection>
<citsent citstr=" P97-1035 ">
we analyze our data using both traditional hypothesis testing methods and the paradise (walker et al, 1997; <papid> P97-1035 </papid>walker et al, 1998) methodology for estimating performance function.</citsent>
<aftsection>
<nextsent>hypothesis testing shows that while differences among some evaluation mea-sures depend on the response strategy (literal or co- operative), other differences are function of appli-cation task and task/strategy interactions.
</nextsent>
<nextsent>a par-adise assessment of the contribution of each eval-uation measure to overall performance shows that strategy-dependent dialogue phenomena well as phenomena associated with speech recognition sig-nificantly predict performance.
</nextsent>
<nextsent>our results identify the conditions under which toot  cooperative - sponse strategy leads to greater agent performance.
</nextsent>
<nextsent>toot allows users to access online amtrak train schedules via telephone dialogue, as in figure 1 .i (all examples are from the experiment in section 3.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W803">
<title id=" P98-2129.xml">evaluating response strategies in a web based spoken dialogue agent </title>
<section> measuring aspects of performance.  </section>
<citcontext>
<prevsection>
<prevsent>to measure dialogue efficiency., the number of system turns and user turns were extracted from the dialogue manager log, and the total elapsed time was determined from the recording.
</prevsent>
<prevsent>to measure user satisfaction 4, users responded to the web survey in figure 4, which assessed their subjective valuation of the agent performance.
</prevsent>
</prevsection>
<citsent citstr=" H92-1009 ">
each question was designed to measure partic- 4questionnaire-based user satisfaction ratings (shriberg et al., 1992; <papid> H92-1009 </papid>polifroni et al, 1992) <papid> H92-1005 </papid>have been frequently used in the literature as an external indicator of agent usability.</citsent>
<aftsection>
<nextsent>783 ? was the system easy to understand in this conver- sation?
</nextsent>
<nextsent>(tts performance) ? in this conversation, did the system understand what you said?
</nextsent>
<nextsent>(asr performance) ? in this conversation, was it easy to find the schedule you wanted?
</nextsent>
<nextsent>(task ease) ? was the pace of interaction with the system appro-priate in this conversation?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W804">
<title id=" P98-2129.xml">evaluating response strategies in a web based spoken dialogue agent </title>
<section> measuring aspects of performance.  </section>
<citcontext>
<prevsection>
<prevsent>to measure dialogue efficiency., the number of system turns and user turns were extracted from the dialogue manager log, and the total elapsed time was determined from the recording.
</prevsent>
<prevsent>to measure user satisfaction 4, users responded to the web survey in figure 4, which assessed their subjective valuation of the agent performance.
</prevsent>
</prevsection>
<citsent citstr=" H92-1005 ">
each question was designed to measure partic- 4questionnaire-based user satisfaction ratings (shriberg et al., 1992; <papid> H92-1009 </papid>polifroni et al, 1992) <papid> H92-1005 </papid>have been frequently used in the literature as an external indicator of agent usability.</citsent>
<aftsection>
<nextsent>783 ? was the system easy to understand in this conver- sation?
</nextsent>
<nextsent>(tts performance) ? in this conversation, did the system understand what you said?
</nextsent>
<nextsent>(asr performance) ? in this conversation, was it easy to find the schedule you wanted?
</nextsent>
<nextsent>(task ease) ? was the pace of interaction with the system appro-priate in this conversation?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W806">
<title id=" P98-1068.xml">japanese morphological analyzer using word cooccurrence  jtag </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>currently, there are two main methods for automatic part-of-speech tagging, namely, corpus- based and rule-based methods.
</prevsent>
<prevsent>the corpus-based method is popular for european languages.
</prevsent>
</prevsection>
<citsent citstr=" P97-1032 ">
samuelsson and voutilainen (1997), <papid> P97-1032 </papid>however, show significantly higher achievement of rule- based tagger than that of statistical taggers for english text.</citsent>
<aftsection>
<nextsent>on the other hand, most japanese taggers are rule-based.
</nextsent>
<nextsent>in previous japanese taggers, it was difficult increase the accuracy of the analysis.
</nextsent>
<nextsent>takeuchi and matsumoto (1995) combined rule-based and corpus-based method, in this paper, tagger is identical to morphological nalyzer.
</nextsent>
<nextsent>resulting in marginal increase in the accuracy of their taggers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W807">
<title id=" P98-1071.xml">automatic extraction of sub corpora based on subcategorization frames from a partofspeech tagged corpus </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a number of resources are available for obtaining subcategorization formation, i.e. information on the types of syntactic complements associated with valence-bearing predicators (which include verbs, nouns, and adjectives).
</prevsent>
<prevsent>this information, also referred to as valence information is available both in machine-readable form, as in the comlex database (macleod et al 1995), and in human- readable dictionaries (e.g. hornby 1989, procter 1978, sinclair 1987).
</prevsent>
</prevsection>
<citsent citstr=" P93-1032 ">
increasingly, tools are also becoming available for acquiring subcategorization formation from corpora, i.e. for inferring the subcategorization frames of given lemma (e.g. manning 1993).<papid> P93-1032 </papid></citsent>
<aftsection>
<nextsent>none of these resources provide immediate access to corpus evidence, nor do they provide information on the relative frequency of the patterns that are listed forgiven lemma.
</nextsent>
<nextsent>there is need for tool that can (1) find evidence for subcategorization patterns and (2) determine their frequencies in large corpora: 1.
</nextsent>
<nextsent>statistical approaches to nlp relyon.
</nextsent>
<nextsent>information not just on the range of combinatory possibilities of words, but also the relative frequencies of the expected patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W808">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the methods for recognition of proper nouns (pns) described in literature closely reflects this view of the problem.
</prevsent>
<prevsent>pn identification typically includes: ? gazetteer lookup, which locates simple and complex nominals identifying common pns, such as companies, person names, locations, etc. ? set of patterns or rules, stated in terms of part-of-speech, syntactic or lexical features (e.g. mr. as an indicator of person entity type), orthographic features (e.g. capitalization), etc. 286 proper nouns recognition has recently attracted much attention especially in the area of information extraction, where this problem is known as the named entity recognition task.
</prevsent>
</prevsection>
<citsent citstr=" M92-1032 ">
the highest performing systems include large numbers of hand- coded rules, or patterns, such as vie (humphreys et al 1996), the umass system (fisher et al 1997) and proteus (grishman et al 1992), <papid> M92-1032 </papid>but lately high performance has been obtained by the use of statistical methods.</citsent>
<aftsection>
<nextsent>for example, ny.mble (bikel et al 1997) <papid> A97-1029 </papid>learns names using trained approach based on variant of hidden markov models.</nextsent>
<nextsent>however, 90% success rate is reached at the price of tagging manually around half million words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W809">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>pn identification typically includes: ? gazetteer lookup, which locates simple and complex nominals identifying common pns, such as companies, person names, locations, etc. ? set of patterns or rules, stated in terms of part-of-speech, syntactic or lexical features (e.g. mr. as an indicator of person entity type), orthographic features (e.g. capitalization), etc. 286 proper nouns recognition has recently attracted much attention especially in the area of information extraction, where this problem is known as the named entity recognition task.
</prevsent>
<prevsent>the highest performing systems include large numbers of hand- coded rules, or patterns, such as vie (humphreys et al 1996), the umass system (fisher et al 1997) and proteus (grishman et al 1992), <papid> M92-1032 </papid>but lately high performance has been obtained by the use of statistical methods.</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
for example, ny.mble (bikel et al 1997) <papid> A97-1029 </papid>learns names using trained approach based on variant of hidden markov models.</citsent>
<aftsection>
<nextsent>however, 90% success rate is reached at the price of tagging manually around half million words.
</nextsent>
<nextsent>since pns are mostly domain-specific, presumably comparable effort is needed when shifting to different domains.
</nextsent>
<nextsent>high performances of the existing systems are by no means the result of many years of studies and research in the area of ie from newswire english texts, promoted and funded by the message understanding conferences (muc) organizers.
</nextsent>
<nextsent>yet, there is no evidence that similar performance could be obtained in other languages and domains, if not at the price of similar effort for rule writing (or manual training), and for the compilation of high- coverage gazetteer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W810">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>high performances of the existing systems are by no means the result of many years of studies and research in the area of ie from newswire english texts, promoted and funded by the message understanding conferences (muc) organizers.
</prevsent>
<prevsent>yet, there is no evidence that similar performance could be obtained in other languages and domains, if not at the price of similar effort for rule writing (or manual training), and for the compilation of high- coverage gazetteer.
</prevsent>
</prevsection>
<citsent citstr=" A97-1028 ">
a recent study (palmer and day, 1997) <papid> A97-1028 </papid>established that the baseline performances of the pn recognition task for several anguages and application domains vary between 34% and 71%.</citsent>
<aftsection>
<nextsent>the lower bound is calculated by considering simple algorithm that recognizes pns on the basis of list of frequent proper nouns seen in training set.
</nextsent>
<nextsent>the method we propose in this paper combines symbolic and statistical approaches to classify unknown pns using context evidence previously extracted from the application corpus.
</nextsent>
<nextsent>the method can be used to overcome the limitation of small gazette ers and poorly encoded rule bases.
</nextsent>
<nextsent>our method is untrained: what is needed is learning (raw) corpus, surface syntactic analyzer, dictionary of synonyms, list of category names for classifying pns (we used the categories proposed in the forthcoming muc-7), and  start-up  gazetteer and rule base, used to acquire an initial model of typical pns contexts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W811">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> the method.  </section>
<citcontext>
<prevsection>
<prevsent>for example the following rule is used to recognize street names: rule(tagged_location_np(s form: \[via,  , f2 ,   ,f3\],sem:a^b), \[nome(s..form:via, sem:_a_), organ_names_np( s_form: f2,sem:_^_), num(s_form:f3)\]) ex:   via giorgio marini 34   when running these first two modules on one million word corpus of economic news (extracted from the newspaper ii sole 24 ore), we obtained the following.
</prevsent>
<prevsent>performances: 84% precision, 85% recall, about 20% proper nouns correctly identified as such, but not classified.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
unknown proper nouns are identified initially by the brill part-of-speech tagger (brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>complex unknown nominals (e.g. quick take 200) are partly detected by simple heuristics.
</nextsent>
<nextsent>one of the motivations for such high percentage of unknowns and relatively low performance (as compared with state-of- art pn recognizers) is that at the present state of implementation the gazetteer has 287 imited coveragel ; yet, the problem of unknowns is generally recognized as crucial in real-world applications, because oroder nouns are an open class.
</nextsent>
<nextsent>we have therefore devised method to reinforce external evidence, using corpus-driven algorithm to incrementally update the gazetteer and classification of unknown pns in running texts.
</nextsent>
<nextsent>the algorithm to classify unknown proper nouns uses the fo lowing inguistic resources: (raw text) learning corpus in the same domain as the application, shallow corpus parser,  seed  gazetteer, and dictionary of synonyms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W812">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> the method.  </section>
<citcontext>
<prevsection>
<prevsent>we have therefore devised method to reinforce external evidence, using corpus-driven algorithm to incrementally update the gazetteer and classification of unknown pns in running texts.
</prevsent>
<prevsent>the algorithm to classify unknown proper nouns uses the fo lowing inguistic resources: (raw text) learning corpus in the same domain as the application, shallow corpus parser,  seed  gazetteer, and dictionary of synonyms.
</prevsent>
</prevsection>
<citsent citstr=" C94-1074 ">
the shallow parser (basili et al 1994<papid> C94-1074 </papid>extracts rom the learning corpus elementary syntactic relations such as subject-object, noun-preposition-noun, etc. syntactic ink (hereafter esl) is represented as: esli(wj, mod(typei, wk)) where i is the head word, wk is the modifier, ~-ad typei is the type of syntactic relation (e.g. pp(of), pp(for), sub j-verb, verb-directobject, etc.).</citsent>
<aftsection>
<nextsent>the learning corpus is previously morpho logica ly and syntact ica ly processed.
</nextsent>
<nextsent>step 1 and 2 described at the beginning of this section are used to detect pns.
</nextsent>
<nextsent>a database of esls including known pns 2 is then created and used by the algorithm to assign category to unknown pns.
</nextsent>
<nextsent>the algorithm works as follows: let pn_u be an unknown proper noun, i.e. single word or complex nominal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W819">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> the method.  </section>
<citcontext>
<prevsection>
<prevsent>finally, let: - esla be set of esls defined as follows: for each esli(x,pn_u) in esl put in esla the set of es j (x ,pnj) , in the corpus, with type=typei, in the same position of esli, and pni known proper noun, in the same position as pn_u in esli, eslb be the set of eslk defined as follows: for each esli(x,pn_u) in esl put in eslb the set of eslj(w,pnj), in the corpus, with type=typei, in the same position of in esli, sim(w,x)  8, and pnj known proper noun, in the same position as pnu in esl i. sim(w,x) is similarity measure between and w. in our first experiments, sim(w,x)  8 iff is synonym of x. for each semantic ategory con compute evidence(cpnj) as shown ih-figure 1, where: amb(esl(x, pni)) is measure of the ambiguity of and pnj in esli; - tx and 13 are experimentally determined weights (currently, t~=0.7 and 13=0.3).
</prevsent>
<prevsent>the selected category for pn_u is: c=argmax( evidence( cpnk) )=maxj( evidence( cpnj) the underlying hypothesis is that, in given application corpus, pn has unique sense.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
this is reasonable restrict ion supported by empir ica evidence (see also (gale et al 1992)).<papid> H92-1045 </papid></citsent>
<aftsection>
<nextsent>an alternative solution would be to select the  best performing  tags, and then apply 288 ?
</nextsent>
<nextsent>(1)ev idence (cpn j) = ~, (pl(esl (x, pnj)) * amb(esl (x, pn~))) (~ esll eesl ,c(pni)=ct,,, epl(esl i(x,pnj) esl ~esl ~ ,anypn + (pl (esl (w, pnj)) * arab (esl (x, pnj))) ~ eslj ,~esl .c(pn )=cp./ ep (es i (w,pnj) eslj eesl ,anypn figure 1 - the evidence(cpnj) computation formula some wsd algorithm to predict he precise sense?
</nextsent>
<nextsent>in running texts.
</nextsent>
<nextsent>in our experiment, we used corpus of one million words extracted from articles in the ii sole 24 ore economic newspaper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W820">
<title id=" P98-1045.xml">automatic semantic tagging of unknown proper names </title>
<section> conc lus ions  and future  work.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of the proposed algorithm is more than satisfactory.
</prevsent>
<prevsent>a comparison with existing systems is difficult because in the literature global pn recognition performances are reported, without considering the semantic classification of unknowns as subtask.
</prevsent>
</prevsection>
<citsent citstr=" A97-1030 ">
the only exception is in (wacholder et al 1997) <papid> A97-1030 </papid>where the reported performance for the sole semantic disambiguation task of pns is 79%.</citsent>
<aftsection>
<nextsent>in that paper, however, semantic disambiguation is performed among lower number of classes 5.
</nextsent>
<nextsent>the performance of our system is clearly affected by the dimension of the initial seed gazetteer and contextual rules.
</nextsent>
<nextsent>if the sets esla and eslb are large enough, obviously more examples of similar contexts are found, even for unknown pns with single occurrence.
</nextsent>
<nextsent>in our test experiment, we always managed to find at least one or two similar contexts of an unknown pn, but in some cases they were misleading and caused wrong classification, especially for products.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W821">
<title id=" P98-1024.xml">managing information at linguistic interfaces </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a key question in the design of an interface lan-guage is what information must be carried and to what purpose.
</prevsent>
<prevsent>the primary definition criterion within the linguistic modules has been the transla-tion task.
</prevsent>
</prevsection>
<citsent citstr=" C96-1054 ">
the actual translation operation is per-formed in the transfer module as mapping between semantic representations of the source and target languages, ee (dorna and emele, 1996).<papid> C96-1054 </papid></citsent>
<aftsection>
<nextsent>however, the information requirements of this process are flexible, since information from various levels of analysis are used in disambiguation within the trans-fer module, including prosody and dialogue struc-ture.
</nextsent>
<nextsent>to large extent he information requirements divide into two parts: ? the expressive adequacy of the semantic repre- sentation; ? representing other types of linguistic informa-tion so as to meet the disambiguation require-ments with the minimum of redundancy.
</nextsent>
<nextsent>the design of the semantic representations en-coded within vits has been guided by an ongoing movement in representational semantic formalisms which takes as starting point the fact that certain key features of purely logical semantics are not fully defined in natural language utterances and typ-ically play no part in translation operations.
</nextsent>
<nextsent>this has been most clearly demonstrated for cases where quantifier scope ambiguities are exactly preserved under translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W822">
<title id=" P98-1024.xml">managing information at linguistic interfaces </title>
<section> the verb mobil interface term.  </section>
<citcontext>
<prevsection>
<prevsent>vit semantics follows similar strategy but some- what extends the expressivity of the metalanguage.
</prevsent>
<prevsent>there are two constructs in the vit semantic meta language which provide for an extension in expressivity relative to udrss.
</prevsent>
</prevsection>
<citsent citstr=" C96-1024 ">
these have both been adopted from immediate precursors within the project, such as bos et al (1996), <papid> C96-1024 </papid>and further e- fined.</citsent>
<aftsection>
<nextsent>the first of these is the mechanism of holes and plugging which originates in the hole seman-tics of bos (1996).
</nextsent>
<nextsent>this requires distinction be-tween two types of meta variable employed: labels and holes.
</nextsent>
<nextsent>labels denote the instantiated structures, primarily the individual predicates.
</nextsent>
<nextsent>holes, on the other hand, mark the underspecified argument po-sitions of propositional arguments and scope do-mains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W823">
<title id=" P98-1024.xml">managing information at linguistic interfaces </title>
<section> vit processing.  </section>
<citcontext>
<prevsection>
<prevsent>this property has been presented as an independent motivation for minimally recursive representations from thema- chine translation point of view (copestake al., 1995), and has been most thoroughly explored in the context of the substitution operations required for transfer.
</prevsent>
<prevsent>we believe we have taken this argu-ment to its logical conclusion, in implementing non-recursive semantic meta language in an appro-priate data structure.
</prevsent>
</prevsection>
<citsent citstr=" P91-1021 ">
this, in itself, provides suf-ficient motivation for opting for such representa-tions rather than, say, feature structures or the recur-sive qlfs (alshawi et al, 1991) <papid> P91-1021 </papid>of cle (alshawi, 1992).</citsent>
<aftsection>
<nextsent>3.2 adt package.
</nextsent>
<nextsent>in general, linguistic analysis components are very sensitive to changes in input data caused by modifi- 4in typical ai languages, such as lisp and prolog, lists are built-in, and they can be ported easily to other programming languages.
</nextsent>
<nextsent>cations of analyses or by increasing coverage.
</nextsent>
<nextsent>ob-viously, there is need for some kind of robust-ness at the interface level, especially in large dis-tributed software projects like verb mobil with par-allel development of different components.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W824">
<title id=" P98-1024.xml">managing information at linguistic interfaces </title>
<section> vit processing.  </section>
<citcontext>
<prevsection>
<prevsent>it has been shown that this form of test-ing is well-suited for error detection in components with rapidly growing linguistic overage.
</prevsent>
<prevsent>it is worth noting that the source language lexical coverage in the verb mobil research prototype is around 2500 words, rising to 10k at the end of the second phase 6.
</prevsent>
</prevsection>
<citsent citstr=" C96-1008 ">
furthermore, the complex information produced by 5the kind of data structure used by the communication ar-chitecture (amtrup and benra, 1996) <papid> C96-1008 </papid>is, similarly, transparent to the modules.</citsent>
<aftsection>
<nextsent>6in the year 2000.
</nextsent>
<nextsent>165 linguistic omponents even makes automatic output control necessary.
</nextsent>
<nextsent>the same checking can be used to define quality rating, e.g. for correctness, interpret ability, etc. of the content of vit.
</nextsent>
<nextsent>such results are much better and more productive in improving system than common, purely quantitative, measures based on failure or success rates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W825">
<title id=" P98-1022.xml">a probabilistic corpus driven model for lexical functional analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting lfg-dop model triggers new, corpus-based notion of grammaticality, and its probability models exhibit interesting behavior with respect to specificity and the interpretation of ill-formed strings.
</prevsent>
<prevsent>data-oriented parsing (dop) models of natural language embody the assumption that human language perception and production works with representations of past language experiences, rather than with abstract grammar ules (cf.
</prevsent>
</prevsection>
<citsent citstr=" C92-3126 ">
bod 1992, <papid> C92-3126 </papid>95; scha 1992; sima an 1995; rajman 1995).</citsent>
<aftsection>
<nextsent>dop models therefore maintain large corpora of linguistic representations of previously occurring utterances.
</nextsent>
<nextsent>new utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one.
</nextsent>
<nextsent>in accordance with the general dop architecture outlined by bod (1995), particular dop model is described by specifying settings for the following four parameters: ? formal definition of well-formed represen-tation fo utterance analyses, ? set of deco mpos t ion operations that divide given utterance analysis into set of fragments, ? set of compos t ion operations by which such fragments may be recombined to derive an analysis of new utterance, and ? definition of probabi i ty model that indicates how the probability of new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up.
</nextsent>
<nextsent>previous instantiations of the dop architecture were based on utterance-analyses presented as surface phrase-structure trees ctree-dop , e.g. bod 1993; <papid> E93-1006 </papid>rajman 1995; sima an 1995; goodman 1996; <papid> W96-0214 </papid>bonnema et al 1997).<papid> P97-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W826">
<title id=" P98-1022.xml">a probabilistic corpus driven model for lexical functional analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one.
</prevsent>
<prevsent>in accordance with the general dop architecture outlined by bod (1995), particular dop model is described by specifying settings for the following four parameters: ? formal definition of well-formed represen-tation fo utterance analyses, ? set of deco mpos t ion operations that divide given utterance analysis into set of fragments, ? set of compos t ion operations by which such fragments may be recombined to derive an analysis of new utterance, and ? definition of probabi i ty model that indicates how the probability of new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up.
</prevsent>
</prevsection>
<citsent citstr=" E93-1006 ">
previous instantiations of the dop architecture were based on utterance-analyses presented as surface phrase-structure trees ctree-dop , e.g. bod 1993; <papid> E93-1006 </papid>rajman 1995; sima an 1995; goodman 1996; <papid> W96-0214 </papid>bonnema et al 1997).<papid> P97-1021 </papid></citsent>
<aftsection>
<nextsent>tree-dop uses two decomposition operations that produce connected subtrees of utterance representations: (1) the root operation selects any node of tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the front ie operation then chooses set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes.
</nextsent>
<nextsent>the only composition operation used by tree- dop is node-substitution per ation that replaces the left-most nonterminal frontier node in subtree with fragment whose root category matches the category of the frontier node.
</nextsent>
<nextsent>thus tree-dop provides tree- representations for new utterances by combining fragments from corpus of phrase structure trees.
</nextsent>
<nextsent>a tree-dop representation can typically be derived in many different ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W827">
<title id=" P98-1022.xml">a probabilistic corpus driven model for lexical functional analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one.
</prevsent>
<prevsent>in accordance with the general dop architecture outlined by bod (1995), particular dop model is described by specifying settings for the following four parameters: ? formal definition of well-formed represen-tation fo utterance analyses, ? set of deco mpos t ion operations that divide given utterance analysis into set of fragments, ? set of compos t ion operations by which such fragments may be recombined to derive an analysis of new utterance, and ? definition of probabi i ty model that indicates how the probability of new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up.
</prevsent>
</prevsection>
<citsent citstr=" W96-0214 ">
previous instantiations of the dop architecture were based on utterance-analyses presented as surface phrase-structure trees ctree-dop , e.g. bod 1993; <papid> E93-1006 </papid>rajman 1995; sima an 1995; goodman 1996; <papid> W96-0214 </papid>bonnema et al 1997).<papid> P97-1021 </papid></citsent>
<aftsection>
<nextsent>tree-dop uses two decomposition operations that produce connected subtrees of utterance representations: (1) the root operation selects any node of tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the front ie operation then chooses set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes.
</nextsent>
<nextsent>the only composition operation used by tree- dop is node-substitution per ation that replaces the left-most nonterminal frontier node in subtree with fragment whose root category matches the category of the frontier node.
</nextsent>
<nextsent>thus tree-dop provides tree- representations for new utterances by combining fragments from corpus of phrase structure trees.
</nextsent>
<nextsent>a tree-dop representation can typically be derived in many different ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W828">
<title id=" P98-1022.xml">a probabilistic corpus driven model for lexical functional analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>new utterances are analyzed by combining (arbitrarily large) fragments from the corpus; the occurrence-frequencies of the fragments are used to determine which analysis is the most probable one.
</prevsent>
<prevsent>in accordance with the general dop architecture outlined by bod (1995), particular dop model is described by specifying settings for the following four parameters: ? formal definition of well-formed represen-tation fo utterance analyses, ? set of deco mpos t ion operations that divide given utterance analysis into set of fragments, ? set of compos t ion operations by which such fragments may be recombined to derive an analysis of new utterance, and ? definition of probabi i ty model that indicates how the probability of new utterance analysis is computed on the basis of the probabilities of the fragments that combine to make it up.
</prevsent>
</prevsection>
<citsent citstr=" P97-1021 ">
previous instantiations of the dop architecture were based on utterance-analyses presented as surface phrase-structure trees ctree-dop , e.g. bod 1993; <papid> E93-1006 </papid>rajman 1995; sima an 1995; goodman 1996; <papid> W96-0214 </papid>bonnema et al 1997).<papid> P97-1021 </papid></citsent>
<aftsection>
<nextsent>tree-dop uses two decomposition operations that produce connected subtrees of utterance representations: (1) the root operation selects any node of tree to be the root of the new subtree and erases all nodes except the selected node and the nodes it dominates; (2) the front ie operation then chooses set (possibly empty) of nodes in the new subtree different from its root and erases all subtrees dominated by the chosen nodes.
</nextsent>
<nextsent>the only composition operation used by tree- dop is node-substitution per ation that replaces the left-most nonterminal frontier node in subtree with fragment whose root category matches the category of the frontier node.
</nextsent>
<nextsent>thus tree-dop provides tree- representations for new utterances by combining fragments from corpus of phrase structure trees.
</nextsent>
<nextsent>a tree-dop representation can typically be derived in many different ways.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W831">
<title id=" P98-1022.xml">a probabilistic corpus driven model for lexical functional analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, tree-dop is limited in that it cannot account for underlying syntactic (and semantic) dependencies that are not 145 reflected directly in surface tree.
</prevsent>
<prevsent>all modern linguistic theories propose more articulated represen-tations and mechanisms in order to characterize such linguistic phenomena.
</prevsent>
</prevsection>
<citsent citstr=" E95-1038 ">
dop models for number of richer representations have been explored (van den berg et al 1994; tugwell 1995), <papid> E95-1038 </papid>but these approaches have remained context-free in their generative power.</citsent>
<aftsection>
<nextsent>in contrast, lexical-functional grammar (kaplan &amp; bresnan 1982; kaplan 1989), which assigns representations consisting of surface constituent tree enriched with corresponding functional structure, is known to be beyond context-free.
</nextsent>
<nextsent>in the current work, we develop dop model based on representations defined by lfg theory clfg-dop ).
</nextsent>
<nextsent>that is, we provide new instantiation for the four parameters of the dop architecture.
</nextsent>
<nextsent>we will see that this basic lfg-dop model triggers new, corpus-based notion of grammaticality, and that it leads to different class of its probability models which exhibit interesting properties with respect specificity and the interpretation of ill-formed strings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W832">
<title id=" P98-1029.xml">classifier combination for improved lexical disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using contextual cues to guide tagger combination, we are able to derive new tagger that achieves performance significantly greater than any of the individual taggers.
</prevsent>
<prevsent>part of speech tagging has been central problem in natural language processing for many years.
</prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
since the advent of manually tagged corpora such as the brown corpus and the penn treebank (francis(1982), marcus(1993)), the efficacy of machine learning for training tagger has been demonstrated using wide array of techniques, including: markov models, decision trees, connection ist machines, transformations, nearest-neighbor algorithms, and maximum entropy (weischedel(1993), black(1992), schmid(1994), <papid> C94-1027 </papid>brill(1995),<papid> J95-4004 </papid>daelemans(1995),ratnaparkhi(1996 )).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>all of these methods seem to achieve roughly comparable accuracy.
</nextsent>
<nextsent>the fact that most machine-learning- based taggers achieve comparable sults could be attributed to number of causes.
</nextsent>
<nextsent>it is possible that the 80/20 rule of engineering is applying: certain number of tagging instances are relatively simple to disambiguate and are therefore being successfully tagged by all approaches, while another percentage is extremely difficult to disambiguate, requiring deep linguistic knowledge, thereby causing all taggers to err.
</nextsent>
<nextsent>another possibility could be that all of the different machine learning techniques are essentially doing the same thing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W833">
<title id=" P98-1029.xml">classifier combination for improved lexical disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using contextual cues to guide tagger combination, we are able to derive new tagger that achieves performance significantly greater than any of the individual taggers.
</prevsent>
<prevsent>part of speech tagging has been central problem in natural language processing for many years.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
since the advent of manually tagged corpora such as the brown corpus and the penn treebank (francis(1982), marcus(1993)), the efficacy of machine learning for training tagger has been demonstrated using wide array of techniques, including: markov models, decision trees, connection ist machines, transformations, nearest-neighbor algorithms, and maximum entropy (weischedel(1993), black(1992), schmid(1994), <papid> C94-1027 </papid>brill(1995),<papid> J95-4004 </papid>daelemans(1995),ratnaparkhi(1996 )).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>all of these methods seem to achieve roughly comparable accuracy.
</nextsent>
<nextsent>the fact that most machine-learning- based taggers achieve comparable sults could be attributed to number of causes.
</nextsent>
<nextsent>it is possible that the 80/20 rule of engineering is applying: certain number of tagging instances are relatively simple to disambiguate and are therefore being successfully tagged by all approaches, while another percentage is extremely difficult to disambiguate, requiring deep linguistic knowledge, thereby causing all taggers to err.
</nextsent>
<nextsent>another possibility could be that all of the different machine learning techniques are essentially doing the same thing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W834">
<title id=" P98-1029.xml">classifier combination for improved lexical disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by using contextual cues to guide tagger combination, we are able to derive new tagger that achieves performance significantly greater than any of the individual taggers.
</prevsent>
<prevsent>part of speech tagging has been central problem in natural language processing for many years.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
since the advent of manually tagged corpora such as the brown corpus and the penn treebank (francis(1982), marcus(1993)), the efficacy of machine learning for training tagger has been demonstrated using wide array of techniques, including: markov models, decision trees, connection ist machines, transformations, nearest-neighbor algorithms, and maximum entropy (weischedel(1993), black(1992), schmid(1994), <papid> C94-1027 </papid>brill(1995),<papid> J95-4004 </papid>daelemans(1995),ratnaparkhi(1996 )).<papid> W96-0213 </papid></citsent>
<aftsection>
<nextsent>all of these methods seem to achieve roughly comparable accuracy.
</nextsent>
<nextsent>the fact that most machine-learning- based taggers achieve comparable sults could be attributed to number of causes.
</nextsent>
<nextsent>it is possible that the 80/20 rule of engineering is applying: certain number of tagging instances are relatively simple to disambiguate and are therefore being successfully tagged by all approaches, while another percentage is extremely difficult to disambiguate, requiring deep linguistic knowledge, thereby causing all taggers to err.
</nextsent>
<nextsent>another possibility could be that all of the different machine learning techniques are essentially doing the same thing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W837">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> introduction: using massive lexicographic resources.  </section>
<citcontext>
<prevsection>
<prevsent>word-sense disambiguation is long-standing problem in computational linguistics (e.g., kaplan (1950), yngve (1955), bar-i-iillel (1960), masterson (1967)), with important implications for number of practical applications including text-to-speech (ti s), machine translation (mt), information retrieval (ir), and many others.
</prevsent>
<prevsent>the recent interest in computational lexicography has fueled large body of recent work on this 40-year-old problem, e.g., black (1988), brown et al.
</prevsent>
</prevsection>
<citsent citstr=" P91-1017 ">
(1991), choueka nd lusignan (1985), clear (1989), dagan et al (1991), <papid> P91-1017 </papid>gale et al (to appear), hearst (1991), lesk (1986), smadja and mckeown (1990), <papid> P90-1032 </papid>walker (1987), veronis and ide (1990), <papid> C90-2067 </papid>yarowsky (1992), <papid> C92-2070 </papid>zemik (1990),  zemik (1991).</citsent>
<aftsection>
<nextsent>much of this work offers the prospect that disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., church (1988)) <papid> A88-1019 </papid>can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</nextsent>
<nextsent>the availability of massive lexicographic databases offers promising route to overcoming the knowledge acquisition bottleneck.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W838">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> introduction: using massive lexicographic resources.  </section>
<citcontext>
<prevsection>
<prevsent>word-sense disambiguation is long-standing problem in computational linguistics (e.g., kaplan (1950), yngve (1955), bar-i-iillel (1960), masterson (1967)), with important implications for number of practical applications including text-to-speech (ti s), machine translation (mt), information retrieval (ir), and many others.
</prevsent>
<prevsent>the recent interest in computational lexicography has fueled large body of recent work on this 40-year-old problem, e.g., black (1988), brown et al.
</prevsent>
</prevsection>
<citsent citstr=" P90-1032 ">
(1991), choueka nd lusignan (1985), clear (1989), dagan et al (1991), <papid> P91-1017 </papid>gale et al (to appear), hearst (1991), lesk (1986), smadja and mckeown (1990), <papid> P90-1032 </papid>walker (1987), veronis and ide (1990), <papid> C90-2067 </papid>yarowsky (1992), <papid> C92-2070 </papid>zemik (1990),  zemik (1991).</citsent>
<aftsection>
<nextsent>much of this work offers the prospect that disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., church (1988)) <papid> A88-1019 </papid>can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</nextsent>
<nextsent>the availability of massive lexicographic databases offers promising route to overcoming the knowledge acquisition bottleneck.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W839">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> introduction: using massive lexicographic resources.  </section>
<citcontext>
<prevsection>
<prevsent>word-sense disambiguation is long-standing problem in computational linguistics (e.g., kaplan (1950), yngve (1955), bar-i-iillel (1960), masterson (1967)), with important implications for number of practical applications including text-to-speech (ti s), machine translation (mt), information retrieval (ir), and many others.
</prevsent>
<prevsent>the recent interest in computational lexicography has fueled large body of recent work on this 40-year-old problem, e.g., black (1988), brown et al.
</prevsent>
</prevsection>
<citsent citstr=" C90-2067 ">
(1991), choueka nd lusignan (1985), clear (1989), dagan et al (1991), <papid> P91-1017 </papid>gale et al (to appear), hearst (1991), lesk (1986), smadja and mckeown (1990), <papid> P90-1032 </papid>walker (1987), veronis and ide (1990), <papid> C90-2067 </papid>yarowsky (1992), <papid> C92-2070 </papid>zemik (1990),  zemik (1991).</citsent>
<aftsection>
<nextsent>much of this work offers the prospect that disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., church (1988)) <papid> A88-1019 </papid>can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</nextsent>
<nextsent>the availability of massive lexicographic databases offers promising route to overcoming the knowledge acquisition bottleneck.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W840">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> introduction: using massive lexicographic resources.  </section>
<citcontext>
<prevsection>
<prevsent>word-sense disambiguation is long-standing problem in computational linguistics (e.g., kaplan (1950), yngve (1955), bar-i-iillel (1960), masterson (1967)), with important implications for number of practical applications including text-to-speech (ti s), machine translation (mt), information retrieval (ir), and many others.
</prevsent>
<prevsent>the recent interest in computational lexicography has fueled large body of recent work on this 40-year-old problem, e.g., black (1988), brown et al.
</prevsent>
</prevsection>
<citsent citstr=" C92-2070 ">
(1991), choueka nd lusignan (1985), clear (1989), dagan et al (1991), <papid> P91-1017 </papid>gale et al (to appear), hearst (1991), lesk (1986), smadja and mckeown (1990), <papid> P90-1032 </papid>walker (1987), veronis and ide (1990), <papid> C90-2067 </papid>yarowsky (1992), <papid> C92-2070 </papid>zemik (1990),  zemik (1991).</citsent>
<aftsection>
<nextsent>much of this work offers the prospect that disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., church (1988)) <papid> A88-1019 </papid>can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</nextsent>
<nextsent>the availability of massive lexicographic databases offers promising route to overcoming the knowledge acquisition bottleneck.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W841">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> introduction: using massive lexicographic resources.  </section>
<citcontext>
<prevsection>
<prevsent>the recent interest in computational lexicography has fueled large body of recent work on this 40-year-old problem, e.g., black (1988), brown et al.
</prevsent>
<prevsent>(1991), choueka nd lusignan (1985), clear (1989), dagan et al (1991), <papid> P91-1017 </papid>gale et al (to appear), hearst (1991), lesk (1986), smadja and mckeown (1990), <papid> P90-1032 </papid>walker (1987), veronis and ide (1990), <papid> C90-2067 </papid>yarowsky (1992), <papid> C92-2070 </papid>zemik (1990),  zemik (1991).</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
much of this work offers the prospect that disambiguation system might be able to input unrestricted text and tag each word with the most likely sense with fairly reasonable accuracy and efficiency, just as part of speech taggers (e.g., church (1988)) <papid> A88-1019 </papid>can now input unrestricted text and assign each word with the most likely part of speech with fairly reasonable accuracy and efficiency.</citsent>
<aftsection>
<nextsent>the availability of massive lexicographic databases offers promising route to overcoming the knowledge acquisition bottleneck.
</nextsent>
<nextsent>more than thirty years ago, bar- i-iillel (1960) predicted that it would be  futile  to write expert-system-like rules by-hand (as they had been doing at georgetown at the time) because there would be no way to scale up such rules to cope with unrestricted input.
</nextsent>
<nextsent>indeed, it is now well-known that expert-system- like rules can be notoriously difficult to scale up, as small and reiger (1982) and many others have observed:  the expert for throw is currently six pages long.., but it should be 10 times that size.
</nextsent>
<nextsent>bar-hillel was very early in realizing the scope of the problem; he observed that people have large set of facts at their disposal, and it is not obvious how computer could ever hope to gain access to this wealth of knowledge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W891">
<title id=" P92-1032.xml">estimating upper and lower bounds on the performance of word sense disambiguation programs </title>
<section> the literature on evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, if we focus on easier tasks, then we might have enough dynamic range to show some interesting differences.
</prevsent>
<prevsent>therefore, unlike jorgensen who was interested in highlighting differences among judgments, we are much more interested in highlighting agreements.
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
fortunately, we have found in (gale et al, 1992) <papid> H92-1045 </papid>that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions.</citsent>
<aftsection>
<nextsent>of course, it is fairly major step to redefine the problem from classification task to discrimination one, as we are proposing.
</nextsent>
<nextsent>one might have preferred not to do so, but we simply don know how one could establish enough dynamic range in that case to show any interesting differences.
</nextsent>
<nextsent>it has been our experience that it is very hard to design an experiment of any kind which will produce the desired agreement among judges.
</nextsent>
<nextsent>we are very happy with the 96.8% agreement that we were able to show, even if it is limited to much easier task than the one that jorgensen was interested in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W893">
<title id=" P98-1039.xml">hybrid approaches to improvement of translation quality in web based english korean machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this paper describes the symbolic and statistical hybrid approaches to solutions of problems of the previous english-to-korean machine translation system in terms of the improvement of translation quality.
</prevsent>
<prevsent>the solutions are now successfully applied to the web-based english-korean machine translation system  fromto/ek  which has been developed from 1997.
</prevsent>
</prevsection>
<citsent citstr=" C94-1020 ">
the transfer-based english-to-korean machine translation system  mates/ek  that has been developed from 1988 to 1992 in kaist(korean advanced institute of science and technology) and serl(systems engineering research institute) enumerated following list that doesn seem to be easy to solve in the near future in terms of the problems for evolution of the system (choi et. al., 1994) : ? <papid> C94-1020 </papid>processing of non-continuous idiomatic expressions ? generation of korean sentence style ? reduction or ranking of too many ambiguities in english syntactic analysis ? robust processing for failed or ill-formed sentences ? selecting correct word correspondency between several alternatives the problems result in dropping translation assessment such as fidelity, intelligibility, and style (hutchins and somers, 1992).</citsent>
<aftsection>
<nextsent>they can be the problems with which mates/ek as well as other mt systems have faced.
</nextsent>
<nextsent>this paper describes the symbolic and statistical hybird approaches to solve the problems and to improve the translation quality of web-based english-to-korean machine translation.
</nextsent>
<nextsent>1 system overview.
</nextsent>
<nextsent>english-to-korean machine translation system  fromto/ek  has been developed from 1997, solving the problems of its predecessor  mates/ek  and expanding its coverage to www.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W894">
<title id=" P96-1016.xml">synchronous models off language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems have been borrowed from translation theory, sub field of formal language theory, or have been originally (and sometimes redundantly) developed.
</prevsent>
<prevsent>finite state transducers (for an overview, see, e.g., (aho and ullman, 1972)) provide translations between regular languages.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
these devices have been pop-ular in computational morphology and computa-tional phonology since the early eighties (kosken- niemi, 1983; kaplan and kay, 1994), <papid> J94-3001 </papid>and more re-cently in parsing as well (see, e.g., (gross, 1989; pereira, 1991; <papid> P91-1032 </papid>roche, 1993)).</citsent>
<aftsection>
<nextsent>push down transduc-ers and syntax directed translation schemata (sdts) (aho and ullman, 1969) translate between context- free languages and are therefore more powerful than finite state transducers.
</nextsent>
<nextsent>push down transducers are standard model for parsing, and have also been used (usually implicitly) in speech understanding.
</nextsent>
<nextsent>recently, variants of sdts have been proposed as models for simultaneously bracketing parallel cor-pora (wu, 1995).<papid> P95-1033 </papid></nextsent>
<nextsent>synchronization of tree adjoin-ing grammars (tags) (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 1994) are even more powerful than the pre-vious formalisms, and have been applied in machine translation (abeill6, schabes, and joshi, 1990; egedi and palmer, 1994; harbusch and poller, 1994; pri-gent, 1994), natural language generation (shieber and schabes, 1991), and theoretical syntax (abeilld, 1994).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W895">
<title id=" P96-1016.xml">synchronous models off language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems have been borrowed from translation theory, sub field of formal language theory, or have been originally (and sometimes redundantly) developed.
</prevsent>
<prevsent>finite state transducers (for an overview, see, e.g., (aho and ullman, 1972)) provide translations between regular languages.
</prevsent>
</prevsection>
<citsent citstr=" P91-1032 ">
these devices have been pop-ular in computational morphology and computa-tional phonology since the early eighties (kosken- niemi, 1983; kaplan and kay, 1994), <papid> J94-3001 </papid>and more re-cently in parsing as well (see, e.g., (gross, 1989; pereira, 1991; <papid> P91-1032 </papid>roche, 1993)).</citsent>
<aftsection>
<nextsent>push down transduc-ers and syntax directed translation schemata (sdts) (aho and ullman, 1969) translate between context- free languages and are therefore more powerful than finite state transducers.
</nextsent>
<nextsent>push down transducers are standard model for parsing, and have also been used (usually implicitly) in speech understanding.
</nextsent>
<nextsent>recently, variants of sdts have been proposed as models for simultaneously bracketing parallel cor-pora (wu, 1995).<papid> P95-1033 </papid></nextsent>
<nextsent>synchronization of tree adjoin-ing grammars (tags) (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 1994) are even more powerful than the pre-vious formalisms, and have been applied in machine translation (abeill6, schabes, and joshi, 1990; egedi and palmer, 1994; harbusch and poller, 1994; pri-gent, 1994), natural language generation (shieber and schabes, 1991), and theoretical syntax (abeilld, 1994).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W896">
<title id=" P96-1016.xml">synchronous models off language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>push down transduc-ers and syntax directed translation schemata (sdts) (aho and ullman, 1969) translate between context- free languages and are therefore more powerful than finite state transducers.
</prevsent>
<prevsent>push down transducers are standard model for parsing, and have also been used (usually implicitly) in speech understanding.
</prevsent>
</prevsection>
<citsent citstr=" P95-1033 ">
recently, variants of sdts have been proposed as models for simultaneously bracketing parallel cor-pora (wu, 1995).<papid> P95-1033 </papid></citsent>
<aftsection>
<nextsent>synchronization of tree adjoin-ing grammars (tags) (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 1994) are even more powerful than the pre-vious formalisms, and have been applied in machine translation (abeill6, schabes, and joshi, 1990; egedi and palmer, 1994; harbusch and poller, 1994; pri-gent, 1994), natural language generation (shieber and schabes, 1991), and theoretical syntax (abeilld, 1994).</nextsent>
<nextsent>the common underlying idea in all of these formalisms is to combine two generative devices through pairing of their productions (or, in the case of the corresponding automata, of their tran- sitions) in such way that right-hand side nonter-minal symbols in the paired productions are linked.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W897">
<title id=" P96-1016.xml">synchronous models off language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>push down transducers are standard model for parsing, and have also been used (usually implicitly) in speech understanding.
</prevsent>
<prevsent>recently, variants of sdts have been proposed as models for simultaneously bracketing parallel cor-pora (wu, 1995).<papid> P95-1033 </papid></prevsent>
</prevsection>
<citsent citstr=" C90-3045 ">
synchronization of tree adjoin-ing grammars (tags) (shieber and schabes, 1990; <papid> C90-3045 </papid>shieber, 1994) are even more powerful than the pre-vious formalisms, and have been applied in machine translation (abeill6, schabes, and joshi, 1990; egedi and palmer, 1994; harbusch and poller, 1994; pri-gent, 1994), natural language generation (shieber and schabes, 1991), and theoretical syntax (abeilld, 1994).</citsent>
<aftsection>
<nextsent>the common underlying idea in all of these formalisms is to combine two generative devices through pairing of their productions (or, in the case of the corresponding automata, of their tran- sitions) in such way that right-hand side nonter-minal symbols in the paired productions are linked.
</nextsent>
<nextsent>the processes of derivation proceed synchronously in the two devices by applying the paired grammar rules only to linked nonterminals introduced previ-ously in the derivation.
</nextsent>
<nextsent>the fact that the above sys-tems all reflect the same translation technique has not always been recognized in the computational lin-guistics literature.
</nextsent>
<nextsent>following (shieber and schabes, 1990) <papid> C90-3045 </papid>we will refer to the general approach as syn-chronous rewriting.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W900">
<title id=" P96-1016.xml">synchronous models off language </title>
<section> extended local synch ron ization.  </section>
<citcontext>
<prevsection>
<prevsent>this can be shown by adapting the proof technique in (becker, rambow, and niv, 1992).
</prevsent>
<prevsent>in the follow-ing section, we will present synchronous system which has local synchronization formal advantages, but handles the scoping data.
</prevsent>
</prevsection>
<citsent citstr=" P94-1036 ">
in this section, we propose new synchronous sys-tem, which is based on local synchronization of unordered vector grammars with dominance links (uvg-dl) (rambow, 1994).<papid> P94-1036 </papid></citsent>
<aftsection>
<nextsent>the presentations will be informal for reasons of space; we refer to (ram- bow and satta, 1996) for details.
</nextsent>
<nextsent>in uvg-dl, sev-eral context-free string rewriting rules are grouped into sets, called vectors.
</nextsent>
<nextsent>in derivation, all or no rules from given instance of vector must be used.
</nextsent>
<nextsent>put differently, all productions from given vector must be used the same number of times.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W901">
<title id=" P91-1031.xml">strategies for adding control information to declarative grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the information can be used by the linguistic processor for ordering the sequence in which conjuncts and disjuncts are processed, for mixing depth-first and breadth-first search, for cutting off undesired erivations, and for constraint-relaxation.
</prevsent>
<prevsent>feature term formalisms (ftf) have proven extremely useful for the declarative representation linguistic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" P84-1018 ">
the family of grammar models that are based on such formalisms include generalized phrase structure grammar (gpsg) \[gazdar et al 1985\], lexical functional grammar (lfg) \[bresnan 1982\], functional unification grammar (bug) \[kay 1984\], <papid> P84-1018 </papid>head-driven phrase structure grammar (i-ipsg) \[pollard and sag 1988\], and categorial unification grammar (cug) \[karttunen 1986, uszkoreit 1986, <papid> C86-1045 </papid>zeevat et al 1987\].</citsent>
<aftsection>
<nextsent>research for this paper was carried out in parts at dfki in the project disco which is funded by the german ministry for research and technology under grant-no.: 1tw 9002.
</nextsent>
<nextsent>partial funding was also provided by the german research association (dfg) through the project bild in the sfb 314: artificial intelligence and knowledge-based systems.
</nextsent>
<nextsent>for fruitful discussions we would like to thank our colleagues inthe projects disco, bild and liix)g as well as members of audiences at austin, texas, and kyoto, japan, where preliminary versions were presented.
</nextsent>
<nextsent>special thanks for valuable comment and suggestions go to gregor erbach, stanley peters, jim talley, and gertjan van noord.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W902">
<title id=" P91-1031.xml">strategies for adding control information to declarative grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the information can be used by the linguistic processor for ordering the sequence in which conjuncts and disjuncts are processed, for mixing depth-first and breadth-first search, for cutting off undesired erivations, and for constraint-relaxation.
</prevsent>
<prevsent>feature term formalisms (ftf) have proven extremely useful for the declarative representation linguistic knowledge.
</prevsent>
</prevsection>
<citsent citstr=" C86-1045 ">
the family of grammar models that are based on such formalisms include generalized phrase structure grammar (gpsg) \[gazdar et al 1985\], lexical functional grammar (lfg) \[bresnan 1982\], functional unification grammar (bug) \[kay 1984\], <papid> P84-1018 </papid>head-driven phrase structure grammar (i-ipsg) \[pollard and sag 1988\], and categorial unification grammar (cug) \[karttunen 1986, uszkoreit 1986, <papid> C86-1045 </papid>zeevat et al 1987\].</citsent>
<aftsection>
<nextsent>research for this paper was carried out in parts at dfki in the project disco which is funded by the german ministry for research and technology under grant-no.: 1tw 9002.
</nextsent>
<nextsent>partial funding was also provided by the german research association (dfg) through the project bild in the sfb 314: artificial intelligence and knowledge-based systems.
</nextsent>
<nextsent>for fruitful discussions we would like to thank our colleagues inthe projects disco, bild and liix)g as well as members of audiences at austin, texas, and kyoto, japan, where preliminary versions were presented.
</nextsent>
<nextsent>special thanks for valuable comment and suggestions go to gregor erbach, stanley peters, jim talley, and gertjan van noord.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W903">
<title id=" P91-1031.xml">strategies for adding control information to declarative grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>yet the conclusion should not be to abandon unification grammar but to search for better processing models.
</prevsent>
<prevsent>certain effective control strategies for linguistic deduction with unification grammars have been suggested in the recent literature.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
\[shieber et al 1990, <papid> J90-1004 </papid>gerdemarm and hinrichs 1990\] the strategies do not allow the grammar writer to attach control information to the constraints in the grammar.</citsent>
<aftsection>
<nextsent>neither can they be used for dynamic preference assignments.
</nextsent>
<nextsent>the model of control proposed in this paper can be used to implement these strategies in combination with others.
</nextsent>
<nextsent>however, the strategies are not encoded in the program but control information and parametrization of deduction.
</nextsent>
<nextsent>the claim is that unification grammar is much better suited for the experimental and inductive development of plausible processing models than previous grammar models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W904">
<title id=" P91-1031.xml">strategies for adding control information to declarative grammars </title>
<section> control information in conjunctions.  </section>
<citcontext>
<prevsection>
<prevsent>if conjunction is unified with some other feature term, every conjunct has to be unified.
</prevsent>
<prevsent>controlling the order in which ope rands are processed in conjunctions may save time if conjuncts can be processed first that are most likely to fail.
</prevsent>
</prevsection>
<citsent citstr=" C90-2039 ">
this observation is the basis for reordering method proposed by kogure \[1990\].<papid> C90-2039 </papid></citsent>
<aftsection>
<nextsent>if, e.g., in syntactic rule applications, the value of the attribute agreement in the representation nominal elements 238 leads to clashes more often than the value of the attribute definiteneness, it would in general be more efficient to unify agreement before definiteness.
</nextsent>
<nextsent>every unification failure in processing cuts off some unsuccessful branch in the search tree.
</nextsent>
<nextsent>for every piece of information in linguistic knowledge base we will call the probability at which it is directly involved in search tree pruning its failure potential.
</nextsent>
<nextsent>more exactly, the failure potential of piece of information is the average number of times, copies of this (sub)term turn to _1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W906">
<title id=" P98-2130.xml">formal aspects and parsing issues of dependency theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present an earley-style parser for the formalism and discuss the (polynomial) complexity results.
</prevsent>
<prevsent>many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (sgall et al 1986), to lexically-based descriptions (mel cuk 1988), to hierarchical organizations of linguistic knowledge (hudson 1990) (fraser, hudson 1992), to constrained categorial grammars (milward 1994).
</prevsent>
</prevsection>
<citsent citstr=" J90-4003 ">
also, number of parsers have been developed for some dependency frameworks (covington 1990) (<papid> J90-4003 </papid>kwon, yoon 1991) (sleator, temperley 1993) (hahn et al 1994) (lombardo, lesmo 1996), including stochastic treatment (eisner 1996) <papid> C96-1058 </papid>and an object-oriented parallel parsing method (neuhaus, hahn 1996).</citsent>
<aftsection>
<nextsent>however, dependency theories have never been explicitly linked to formal models.
</nextsent>
<nextsent>parsers and applications usually refer to grammars built around core of dependency oncepts, but there is great variety in the description of syntactic onstraints, from rules that are very similar to cfg productions (gaifman 1965) to individual binary relations on words or syntactic ategories (covington 1990) (<papid> J90-4003 </papid>sleator, temperley 1993).</nextsent>
<nextsent>know ub/ scomp ~ ikes sub?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W908">
<title id=" P98-2130.xml">formal aspects and parsing issues of dependency theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we present an earley-style parser for the formalism and discuss the (polynomial) complexity results.
</prevsent>
<prevsent>many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (sgall et al 1986), to lexically-based descriptions (mel cuk 1988), to hierarchical organizations of linguistic knowledge (hudson 1990) (fraser, hudson 1992), to constrained categorial grammars (milward 1994).
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
also, number of parsers have been developed for some dependency frameworks (covington 1990) (<papid> J90-4003 </papid>kwon, yoon 1991) (sleator, temperley 1993) (hahn et al 1994) (lombardo, lesmo 1996), including stochastic treatment (eisner 1996) <papid> C96-1058 </papid>and an object-oriented parallel parsing method (neuhaus, hahn 1996).</citsent>
<aftsection>
<nextsent>however, dependency theories have never been explicitly linked to formal models.
</nextsent>
<nextsent>parsers and applications usually refer to grammars built around core of dependency oncepts, but there is great variety in the description of syntactic onstraints, from rules that are very similar to cfg productions (gaifman 1965) to individual binary relations on words or syntactic ategories (covington 1990) (<papid> J90-4003 </papid>sleator, temperley 1993).</nextsent>
<nextsent>know ub/ scomp ~ ikes sub?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W912">
<title id=" P98-2130.xml">formal aspects and parsing issues of dependency theory </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the controlled relaxation of projective constraints, nasr (1995) has introduced the condition of pseudo?
</prevsent>
<prevsent>projectivity, which provides ome controlled looser constraints on arc crossing in dependency tree, and has developed polynomial parser based on graph-structured stack.
</prevsent>
</prevsection>
<citsent citstr=" P97-1043 ">
neuhaus and broker (1997) <papid> P97-1043 </papid>have recently showed that the general recognition problem for non-projective dependency grammars (what hey call discontinuous dg) is np-complete.</citsent>
<aftsection>
<nextsent>they have devised discontinuous dg with exclusively lexical categories (no traces, as most dependency theories do), and dealing with free word order constructs through looser subtree ordering.
</nextsent>
<nextsent>this formalism, considered as the most straightforward extension to projective formalism, permits the reduction of the vertex cover problem to the dependency recognition problem, thus yielding the np-completeness result.
</nextsent>
<nextsent>however, even if banned from the dependency literature, the use of non lexical categories only notational variant of some graph structures already present in some formalisms (see, e.g., word grammar (hudson 1990)).
</nextsent>
<nextsent>this paper introduces lexicalized dependency formalism, which deals 787 with long distance dependencies, and polynomial parsing algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W913">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound is course le- ments.
</prevsent>
<prevsent>this claim is backed up by an empir-ical evaluation of functional centering.
</prevsent>
</prevsection>
<citsent citstr=" P83-1007 ">
the centering model has evolved as methodology for the description and explanation of the local coherence of discourse (grosz et al, 1983; <papid> P83-1007 </papid>1995), with focus on pronominal and nominal anaphora.</citsent>
<aftsection>
<nextsent>though several cross-linguistic studies have been carded out (cf.
</nextsent>
<nextsent>the enumeration grosz et al (1995)), <papid> J95-2003 </papid>an almost canon-ical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known reg-ularities of fixed word order languages such as en- glish.</nextsent>
<nextsent>with the exception of walker et al (1990), walker et al (1994) <papid> J94-2003 </papid>for japanese, turan (1995) for turkish, ram- bow (1993) for german and cote (1996) for english, only grammatical roles are considered and the (par- tial) ordering in table 11 is taken for granted.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W914">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the centering model has evolved as methodology for the description and explanation of the local coherence of discourse (grosz et al, 1983; <papid> P83-1007 </papid>1995), with focus on pronominal and nominal anaphora.</prevsent>
<prevsent>though several cross-linguistic studies have been carded out (cf.</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
the enumeration grosz et al (1995)), <papid> J95-2003 </papid>an almost canon-ical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known reg-ularities of fixed word order languages such as en- glish.</citsent>
<aftsection>
<nextsent>with the exception of walker et al (1990), walker et al (1994) <papid> J94-2003 </papid>for japanese, turan (1995) for turkish, ram- bow (1993) for german and cote (1996) for english, only grammatical roles are considered and the (par- tial) ordering in table 11 is taken for granted.</nextsent>
<nextsent>i subject   dir-object   indir-object   complement(s)   adjunct(s) table 1: grammatical role based ranking on the c!</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W915">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>though several cross-linguistic studies have been carded out (cf.
</prevsent>
<prevsent>the enumeration grosz et al (1995)), <papid> J95-2003 </papid>an almost canon-ical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known reg-ularities of fixed word order languages such as en- glish.</prevsent>
</prevsection>
<citsent citstr=" J94-2003 ">
with the exception of walker et al (1990), walker et al (1994) <papid> J94-2003 </papid>for japanese, turan (1995) for turkish, ram- bow (1993) for german and cote (1996) for english, only grammatical roles are considered and the (par- tial) ordering in table 11 is taken for granted.</citsent>
<aftsection>
<nextsent>i subject   dir-object   indir-object   complement(s)   adjunct(s) table 1: grammatical role based ranking on the c!
</nextsent>
<nextsent>~table 1contains the most explicit ordering of grammat-ical roles we are aware of and has been taken from bren-nan et al (1987).<papid> P87-1022 </papid></nextsent>
<nextsent>often, the distinction between comple-ments and adjuncts collapsed into the category  others  (c.f., e.g., grosz et al (1995)).<papid> J95-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W916">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with the exception of walker et al (1990), walker et al (1994) <papid> J94-2003 </papid>for japanese, turan (1995) for turkish, ram- bow (1993) for german and cote (1996) for english, only grammatical roles are considered and the (par- tial) ordering in table 11 is taken for granted.</prevsent>
<prevsent>i subject   dir-object   indir-object   complement(s)   adjunct(s) table 1: grammatical role based ranking on the c!</prevsent>
</prevsection>
<citsent citstr=" P87-1022 ">
~table 1contains the most explicit ordering of grammat-ical roles we are aware of and has been taken from bren-nan et al (1987).<papid> P87-1022 </papid></citsent>
<aftsection>
<nextsent>often, the distinction between comple-ments and adjuncts collapsed into the category  others  (c.f., e.g., grosz et al (1995)).<papid> J95-2003 </papid></nextsent>
<nextsent>our work on the resolution of anaphora (strube &amp; hahn, 1995; <papid> E95-1033 </papid>hahn &amp; strube, 1996) <papid> P96-1057 </papid>and textual el-lipsis (hahn et al, 1996), <papid> C96-1084 </papid>however, is based on ger-man, free word order language, in which grammat-ical role information is far less predictive for the or-ganization of centers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W918">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>~table 1contains the most explicit ordering of grammat-ical roles we are aware of and has been taken from bren-nan et al (1987).<papid> P87-1022 </papid></prevsent>
<prevsent>often, the distinction between comple-ments and adjuncts collapsed into the category  others  (c.f., e.g., grosz et al (1995)).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" E95-1033 ">
our work on the resolution of anaphora (strube &amp; hahn, 1995; <papid> E95-1033 </papid>hahn &amp; strube, 1996) <papid> P96-1057 </papid>and textual el-lipsis (hahn et al, 1996), <papid> C96-1084 </papid>however, is based on ger-man, free word order language, in which grammat-ical role information is far less predictive for the or-ganization of centers.</citsent>
<aftsection>
<nextsent>rather, for establishing proper referential relations, the functional information struc-ture of the utterances becomes crucial (different per-spectives on functional analysis are brought forward in dane~ (1974b) and dahl (1974)).
</nextsent>
<nextsent>we share the no-tion of functional information structure as developed by dane~ (1974a).
</nextsent>
<nextsent>he distinguishes between two cru-cial dichotomies, viz.
</nextsent>
<nextsent>given information vs. new infor-mation (constituting the information structure of ut- terances) on the one hand, and theme vs. rheme on the other (constituting the thematic structure of utter- ances; cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W920">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>~table 1contains the most explicit ordering of grammat-ical roles we are aware of and has been taken from bren-nan et al (1987).<papid> P87-1022 </papid></prevsent>
<prevsent>often, the distinction between comple-ments and adjuncts collapsed into the category  others  (c.f., e.g., grosz et al (1995)).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1057 ">
our work on the resolution of anaphora (strube &amp; hahn, 1995; <papid> E95-1033 </papid>hahn &amp; strube, 1996) <papid> P96-1057 </papid>and textual el-lipsis (hahn et al, 1996), <papid> C96-1084 </papid>however, is based on ger-man, free word order language, in which grammat-ical role information is far less predictive for the or-ganization of centers.</citsent>
<aftsection>
<nextsent>rather, for establishing proper referential relations, the functional information struc-ture of the utterances becomes crucial (different per-spectives on functional analysis are brought forward in dane~ (1974b) and dahl (1974)).
</nextsent>
<nextsent>we share the no-tion of functional information structure as developed by dane~ (1974a).
</nextsent>
<nextsent>he distinguishes between two cru-cial dichotomies, viz.
</nextsent>
<nextsent>given information vs. new infor-mation (constituting the information structure of ut- terances) on the one hand, and theme vs. rheme on the other (constituting the thematic structure of utter- ances; cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W921">
<title id=" P96-1036.xml">functional centering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>~table 1contains the most explicit ordering of grammat-ical roles we are aware of and has been taken from bren-nan et al (1987).<papid> P87-1022 </papid></prevsent>
<prevsent>often, the distinction between comple-ments and adjuncts collapsed into the category  others  (c.f., e.g., grosz et al (1995)).<papid> J95-2003 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-1084 ">
our work on the resolution of anaphora (strube &amp; hahn, 1995; <papid> E95-1033 </papid>hahn &amp; strube, 1996) <papid> P96-1057 </papid>and textual el-lipsis (hahn et al, 1996), <papid> C96-1084 </papid>however, is based on ger-man, free word order language, in which grammat-ical role information is far less predictive for the or-ganization of centers.</citsent>
<aftsection>
<nextsent>rather, for establishing proper referential relations, the functional information struc-ture of the utterances becomes crucial (different per-spectives on functional analysis are brought forward in dane~ (1974b) and dahl (1974)).
</nextsent>
<nextsent>we share the no-tion of functional information structure as developed by dane~ (1974a).
</nextsent>
<nextsent>he distinguishes between two cru-cial dichotomies, viz.
</nextsent>
<nextsent>given information vs. new infor-mation (constituting the information structure of ut- terances) on the one hand, and theme vs. rheme on the other (constituting the thematic structure of utter- ances; cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W927">
<title id=" P96-1036.xml">functional centering </title>
<section> types of anaphora considered.  </section>
<citcontext>
<prevsection>
<prevsent>(5 minutes - before - it - itself- turns off- begins - the low-battery-led - to flash.)
</prevsent>
<prevsent>in the case of textual ellipsis, the missing concep-tual link between two discourse lements occurring in adjacent utterances must be inferred in order to estab-lish the local coherence of the discourse (for an early statement of that idea, cf.
</prevsent>
</prevsection>
<citsent citstr=" T75-2034 ">
clark (1975)).<papid> T75-2034 </papid></citsent>
<aftsection>
<nextsent>in the sur-face form of utterance (lb) the information is missing that  akkus   (accumulator) links up with  316lt .
</nextsent>
<nextsent>this relation can only be made explicit if conceptual knowledge about he domain, viz.
</nextsent>
<nextsent>the relation part-of between the concepts accumulator and 316lt, is available (see hahn et al (1996) <papid> C96-1084 </papid>for more detailed treatment of text ellipsis resolution).</nextsent>
<nextsent>within the framework of the centering model (grosz et al, 1995), <papid> J95-2003 </papid>we distinguish each utterance backward-looking center (gb(u,~)) and its forward- looking centers (g!</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="W932">
<title id=" P94-1001.xml">discourse obligations in dialogue processing </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>we show that in modeling social interaction, particularly di-alogue, the attitude of obligation can be useful adjunct the popularly considered attitudes of belief, goal, and inten-tion and their mutual and shared counterparts.
</prevsent>
<prevsent>in particular, we show how discourse obligations can be used to account in natural manner for the connection between question and its answer in dialogue and how obligations can be used along with other parts of the discourse context extend the coverage of dialogue system.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
most computational models of discourse are based pri-marily on an analysis of the intentions of the speakers (e.g., \[cohen and perrault, 1979; allen and perrault, 1980; grosz and sidner, 1986\]).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>an agent has certain goals, and communication results from planning process to achieve these goals.
</nextsent>
<nextsent>the speaker will form intentions based on the goals and then act on these intentions, producing utterances.
</nextsent>
<nextsent>the hearer will then reconstruct model of the speaker intentions upon hearing the utterance.
</nextsent>
<nextsent>this approach as many strongpoints, but does not provide very satisfac-tory account of the adherence to discourse conventions in dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>