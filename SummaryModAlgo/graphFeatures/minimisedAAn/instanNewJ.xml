<paper>
<cited id="J0">
<title id=" E91-1041.xml">a dialogue manager using initiative response units and distributed control </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the control is distributed and the dialogue is directed from action plans in the nodes in the dialogue tree.
</prevsent>
<prevsent>to achieve true cooperation natural language inter-face must be able to participate incoherent dialogue with the user.
</prevsent>
</prevsection>
<citsent citstr=" J89-2001 ">
a common, generally applicable approach is to use plan-inference asa basis for reasoning:about in-tentions of the user as proposed by, for instance, allen &amp; perrault (1980), litman (1986), carberry (1989) <papid> J89-2001 </papid>and pollack (1986).<papid> P86-1032 </papid></citsent>
<aftsection>
<nextsent>however, computationally these ap-proaches are not so efficient.
</nextsent>
<nextsent>reichman (1985) describes discourse grammar based on the assumption that conversation can be de-scribed using conventional ized discourse rules.
</nextsent>
<nextsent>gilbert, buckland, frolich, jirotka &amp; luff (1990) uses interac-tion rules in their menu-based advisory system.
</nextsent>
<nextsent>our ap-proach is similar to reichman and gilbert el al. in series of experiments (dahlb~lck &amp; jonsson, 1989, j0ns- son &amp; dahib/tck, 1988) we studied ialogue behaviour in an information-seeking ter action between human and computer using simulated natural language interface (nli).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1">
<title id=" E91-1041.xml">a dialogue manager using initiative response units and distributed control </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the control is distributed and the dialogue is directed from action plans in the nodes in the dialogue tree.
</prevsent>
<prevsent>to achieve true cooperation natural language inter-face must be able to participate incoherent dialogue with the user.
</prevsent>
</prevsection>
<citsent citstr=" P86-1032 ">
a common, generally applicable approach is to use plan-inference asa basis for reasoning:about in-tentions of the user as proposed by, for instance, allen &amp; perrault (1980), litman (1986), carberry (1989) <papid> J89-2001 </papid>and pollack (1986).<papid> P86-1032 </papid></citsent>
<aftsection>
<nextsent>however, computationally these ap-proaches are not so efficient.
</nextsent>
<nextsent>reichman (1985) describes discourse grammar based on the assumption that conversation can be de-scribed using conventional ized discourse rules.
</nextsent>
<nextsent>gilbert, buckland, frolich, jirotka &amp; luff (1990) uses interac-tion rules in their menu-based advisory system.
</nextsent>
<nextsent>our ap-proach is similar to reichman and gilbert el al. in series of experiments (dahlb~lck &amp; jonsson, 1989, j0ns- son &amp; dahib/tck, 1988) we studied ialogue behaviour in an information-seeking ter action between human and computer using simulated natural language interface (nli).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2">
<title id=" E91-1041.xml">a dialogue manager using initiative response units and distributed control </title>
<section> dialogue objects.  </section>
<citcontext>
<prevsection>
<prevsent>the communication is hierarchically structured using three different categories of dialogue objects.
</prevsent>
<prevsent>there are various proposals as to the number of levels needed.
</prevsent>
</prevsection>
<citsent citstr=" P84-1085 ">
the system developed by polanyi &amp; scha (1984) <papid> P84-1085 </papid>uses five different levels to hierarchically structure dialogue and loki (wachtel, 1986) <papid> C86-1007 </papid>uses four.</citsent>
<aftsection>
<nextsent>in loki the levels are: conversation, dialogue, exchange and move.
</nextsent>
<nextsent>when ana-lysing our dialogues we found no certain criteria con-cerning how to divide dialogue into set of exchanges.
</nextsent>
<nextsent>therefore we only use three different dialogue object types: dialogue, initiative-response-unit (ir) and move.
</nextsent>
<nextsent>dialogue, in our notation, is similar to conversation loki, while ir-units resemble xchanges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J3">
<title id=" E91-1041.xml">a dialogue manager using initiative response units and distributed control </title>
<section> dialogue objects.  </section>
<citcontext>
<prevsection>
<prevsent>the communication is hierarchically structured using three different categories of dialogue objects.
</prevsent>
<prevsent>there are various proposals as to the number of levels needed.
</prevsent>
</prevsection>
<citsent citstr=" C86-1007 ">
the system developed by polanyi &amp; scha (1984) <papid> P84-1085 </papid>uses five different levels to hierarchically structure dialogue and loki (wachtel, 1986) <papid> C86-1007 </papid>uses four.</citsent>
<aftsection>
<nextsent>in loki the levels are: conversation, dialogue, exchange and move.
</nextsent>
<nextsent>when ana-lysing our dialogues we found no certain criteria con-cerning how to divide dialogue into set of exchanges.
</nextsent>
<nextsent>therefore we only use three different dialogue object types: dialogue, initiative-response-unit (ir) and move.
</nextsent>
<nextsent>dialogue, in our notation, is similar to conversation loki, while ir-units resemble xchanges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J4">
<title id=" E99-1030.xml">the development of lexical resources for information extraction from text combining wordnet and dewey decimal classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>tion requirement.
</prevsent>
<prevsent>unfortunately one of the cur-rent trends in ie is the progressive reduction of the size of training corpora: e.g., from the 1,000 texts of the muc-5 (muc-5, 1993) to the 100 texts in muc-6 (muc-6, 1995).
</prevsent>
</prevsection>
<citsent citstr=" M95-1014 ">
when the cor-pus size is limited, the assumption of lexical rep-resentativeness of the sample corpus may not hold any longer, and the problem of producing repre-sentative lexicon starting from the corpus lexicon arises (grishman, 1995).<papid> M95-1014 </papid></citsent>
<aftsection>
<nextsent>generic resources are interesting as they con-tain (among others) most of the terms necessary for an ie application.
</nextsent>
<nextsent>nevertheless up to now the use of generic resources within ie system has been limited for two main reasons.
</nextsent>
<nextsent>first the in-formation associated to each term is often not de-tailed enough for describing the relations neces-sary for ie lexicon; secondly the presence of large amount of lexical polysemy.
</nextsent>
<nextsent>in this paper we propose methodology for semi-automatically developing the relevant part of lexicon (foreground lexicon) for ie applications by using both small corpus and wordnet.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J5">
<title id=" E99-1030.xml">the development of lexical resources for information extraction from text combining wordnet and dewey decimal classification </title>
<section> developing ie lexical resources.  </section>
<citcontext>
<prevsection>
<prevsent>as mentioned, there are two problems related to the use of generic dictionaries with respect the ie needs.
</prevsent>
<prevsent>first there is no clear way of extracting from them the mapping between the fl and the ontol- ogy; this is mainly due to lack of information and cannot in general be solved; generic lexica cannot then be used during the bootstrapping phase to generate the core lexicon.
</prevsent>
</prevsection>
<citsent citstr=" M95-1007 ">
secondly experience showed that the lexical am-biguity carried by generic dictionaries does not allow their direct use in computational systems (basili and pazienza, 1997; morgan et al, 1995).<papid> M95-1007 </papid></citsent>
<aftsection>
<nextsent>even when they are used off-line, lexical ambigu-ity can introduce so much noise (and then over- head) in the lexical development process that their use can be inconvenient from the point of view of efficiency and effectiveness.
</nextsent>
<nextsent>the next section explains how it is possible to cope with lexical ambiguity in wordnet by combining its information with another source of information: the dewey decimal classification (ddc) (dewey, 1989).
</nextsent>
<nextsent>in wordnet the main problem with the use of wordnet is lex-ical polysemy 1.
</nextsent>
<nextsent>lexical polysemy is present when word is associated to many senses (synsets).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J6">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translations are produced by means ofa beam-search decoder.
</prevsent>
<prevsent>experimental results are presented, that demonstrate howthe proposed method allows to better generalize from the training data.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (och et al, 1999; <papid> W99-0604 </papid>marcu and wong, 2002; <papid> W02-1018 </papid>yamada and knight, 2002; <papid> P02-1039 </papid>tillmann and xia, 2003).<papid> N03-2036 </papid></citsent>
<aftsection>
<nextsent>while in traditional word-based statistical models (brown et al, 1993) <papid> J93-2003 </papid>the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in statistical framework the insight behind example-based machine translation (somers, 1999).</nextsent>
<nextsent>however, phrase-based models proposed so far only deal with multi-word units that are sequences of contiguous words on both the source and the target side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J7">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translations are produced by means ofa beam-search decoder.
</prevsent>
<prevsent>experimental results are presented, that demonstrate howthe proposed method allows to better generalize from the training data.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (och et al, 1999; <papid> W99-0604 </papid>marcu and wong, 2002; <papid> W02-1018 </papid>yamada and knight, 2002; <papid> P02-1039 </papid>tillmann and xia, 2003).<papid> N03-2036 </papid></citsent>
<aftsection>
<nextsent>while in traditional word-based statistical models (brown et al, 1993) <papid> J93-2003 </papid>the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in statistical framework the insight behind example-based machine translation (somers, 1999).</nextsent>
<nextsent>however, phrase-based models proposed so far only deal with multi-word units that are sequences of contiguous words on both the source and the target side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J8">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translations are produced by means ofa beam-search decoder.
</prevsent>
<prevsent>experimental results are presented, that demonstrate howthe proposed method allows to better generalize from the training data.
</prevsent>
</prevsection>
<citsent citstr=" P02-1039 ">
possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (och et al, 1999; <papid> W99-0604 </papid>marcu and wong, 2002; <papid> W02-1018 </papid>yamada and knight, 2002; <papid> P02-1039 </papid>tillmann and xia, 2003).<papid> N03-2036 </papid></citsent>
<aftsection>
<nextsent>while in traditional word-based statistical models (brown et al, 1993) <papid> J93-2003 </papid>the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in statistical framework the insight behind example-based machine translation (somers, 1999).</nextsent>
<nextsent>however, phrase-based models proposed so far only deal with multi-word units that are sequences of contiguous words on both the source and the target side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J9">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>translations are produced by means ofa beam-search decoder.
</prevsent>
<prevsent>experimental results are presented, that demonstrate howthe proposed method allows to better generalize from the training data.
</prevsent>
</prevsection>
<citsent citstr=" N03-2036 ">
possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (och et al, 1999; <papid> W99-0604 </papid>marcu and wong, 2002; <papid> W02-1018 </papid>yamada and knight, 2002; <papid> P02-1039 </papid>tillmann and xia, 2003).<papid> N03-2036 </papid></citsent>
<aftsection>
<nextsent>while in traditional word-based statistical models (brown et al, 1993) <papid> J93-2003 </papid>the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in statistical framework the insight behind example-based machine translation (somers, 1999).</nextsent>
<nextsent>however, phrase-based models proposed so far only deal with multi-word units that are sequences of contiguous words on both the source and the target side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J11">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results are presented, that demonstrate howthe proposed method allows to better generalize from the training data.
</prevsent>
<prevsent>possibly the most remarkable evolution of recent years in statistical machine translation is the step from word-based models to phrase-based models (och et al, 1999; <papid> W99-0604 </papid>marcu and wong, 2002; <papid> W02-1018 </papid>yamada and knight, 2002; <papid> P02-1039 </papid>tillmann and xia, 2003).<papid> N03-2036 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
while in traditional word-based statistical models (brown et al, 1993) <papid> J93-2003 </papid>the atomic unit that translation operates on is the word, phrase-based methods acknowledge the significant role played in language by multiword expressions, thus incorporating in statistical framework the insight behind example-based machine translation (somers, 1999).</citsent>
<aftsection>
<nextsent>however, phrase-based models proposed so far only deal with multi-word units that are sequences of contiguous words on both the source and the target side.
</nextsent>
<nextsent>we propose here model designed to deal with multi-word expressions that need not be contiguous in either or both the source and the target side.the rest of this paper is organised as follows.
</nextsent>
<nextsent>section 2 provides motivations, definition and extraction procedure for non-contiguous phrases.
</nextsent>
<nextsent>the loglinear conditional translation model we adopted is the object of section 3; the method used to train its parameters is described in section 4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J12">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> non-contiguous phrases.  </section>
<citcontext>
<prevsection>
<prevsent>such library is constructed from corpus of existing translations, aligned at the word level.two strategies come to mind to produce noncontiguous bi-phrases for these libraries.
</prevsent>
<prevsent>the first isto align the words using standard?
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
word alignement technique, such as the refined method described in (och and ney, 2003) (<papid> J03-1002 </papid>the intersection of two ibm viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sen tences.</citsent>
<aftsection>
<nextsent>this is the strategy that is usually adopted in other phrase-based mt approaches (zens and ney, 2003; och and ney, 2004).<papid> J04-4002 </papid></nextsent>
<nextsent>here, the difference isthat we are not restricted to combinations that produce strictly contiguous bi-phrases.the second strategy is to relyon word alignment method that naturally produces many-to many alignments between non-contiguous words, such as the method described in (goutte et al, 2004).<papid> P04-1064 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J14">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> non-contiguous phrases.  </section>
<citcontext>
<prevsection>
<prevsent>the first isto align the words using standard?
</prevsent>
<prevsent>word alignement technique, such as the refined method described in (och and ney, 2003) (<papid> J03-1002 </papid>the intersection of two ibm viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sen tences.</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
this is the strategy that is usually adopted in other phrase-based mt approaches (zens and ney, 2003; och and ney, 2004).<papid> J04-4002 </papid></citsent>
<aftsection>
<nextsent>here, the difference isthat we are not restricted to combinations that produce strictly contiguous bi-phrases.the second strategy is to relyon word alignment method that naturally produces many-to many alignments between non-contiguous words, such as the method described in (goutte et al, 2004).<papid> P04-1064 </papid></nextsent>
<nextsent>by means of matrix factor ization, this method produces parallel partition of the two texts, seen assets of word tokens.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J15">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> non-contiguous phrases.  </section>
<citcontext>
<prevsection>
<prevsent>word alignement technique, such as the refined method described in (och and ney, 2003) (<papid> J03-1002 </papid>the intersection of two ibm viterbi alignments, forward and reverse, enriched with alignments from the union) and then generate bi-phrases by combining together individual alignments that co-occur in the same pair of sen tences.</prevsent>
<prevsent>this is the strategy that is usually adopted in other phrase-based mt approaches (zens and ney, 2003; och and ney, 2004).<papid> J04-4002 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1064 ">
here, the difference isthat we are not restricted to combinations that produce strictly contiguous bi-phrases.the second strategy is to relyon word alignment method that naturally produces many-to many alignments between non-contiguous words, such as the method described in (goutte et al, 2004).<papid> P04-1064 </papid></citsent>
<aftsection>
<nextsent>by means of matrix factor ization, this method produces parallel partition of the two texts, seen assets of word tokens.
</nextsent>
<nextsent>each token therefore belongs to one, and only one, subset within this partition, and corresponding subsets in the source and target make up what are called cepts.
</nextsent>
<nextsent>for example, in figure 1, these cepts are represented by the circles numbered 1, 2 and 3; each cept thus connects word tokens in the source and the target, regardless of position or contiguity.
</nextsent>
<nextsent>these cepts naturally constitutebi-phrases, and can be used directly to produce bi phrase library.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J17">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> the model.  </section>
<citcontext>
<prevsection>
<prevsent>the bi-phrase feature function hbp: it represents the probability of producing ti1 using some set of bi-phrases, under the assumption that each source phrase produces target phrase independently of the others: hbp(t 1, j 1 , k 1 ) = k?
</prevsent>
<prevsent>k=1 logpr(tk|sk) (2) individual bi-phrase probabilities pr(tk|sk) are estimated based on occurrence counts in the word-aligned training corpus.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
the compositional bi-phrase feature function hcomp: this is introduced to compensate for1recent work from chiang (chiang, 2005) <papid> P05-1033 </papid>addresses similar concerns to those motivating our work by introducing synchronous cfg for bi-phrases.</citsent>
<aftsection>
<nextsent>if on one hand scfgs allow to better control the order of the material inserted in the gaps, on the other gap size does not seem to be taken into account, and phrase dovetailing such as the one involving do want?
</nextsent>
<nextsent>and not anymore?
</nextsent>
<nextsent>in fig.
</nextsent>
<nextsent>2 is disallowed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J19">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>the values of the ? parameters of the log-linearmodel can be set so as to optimize given criterion.
</prevsent>
<prevsent>for instance, one can maximize the likely hood of some set of training sentences.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
instead, and as suggested by och (2003), <papid> P03-1021 </papid>we chose to maximize directly the quality of the translations produced by the system, as measured with machine translation evaluation metric.</citsent>
<aftsection>
<nextsent>say we have set of source-language sentences s. forgiven value of ?, we can compute the set of corresponding target-language translations . given set of reference (gold-standard?)
</nextsent>
<nextsent>translations for and function e(t,r) which measures the error?
</nextsent>
<nextsent>in relative to r, then we can formulate the parameter estimation problem as2: ??
</nextsent>
<nextsent>= argmine(t,r) as pointed out by och, one notable difficulty with this approach is that, because the computation of is based on an argmax operation (see eq.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J20">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> parameter estimation.  </section>
<citcontext>
<prevsection>
<prevsent>we have opted for this last approach.
</prevsent>
<prevsent>och shows how to implement it when the error function can be computed as the sum of errors on individual sentences.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
unfortunately, this is not the case for such widely used mt evaluation metrics as bleu (pa pineni et al, 2002) <papid> P02-1040 </papid>and nist (doddington, 2002).we show here how it can be done for nist; similar derivation is possible for bleu.</citsent>
<aftsection>
<nextsent>the nist evaluation metric computes weighted n-gram precision between and r, multiplied by factor b(s, t,r) that penalizes short translations.
</nextsent>
<nextsent>it can be formulated as: b(s, t,r) ? n? n=1 ? ss in(ts, rs) ? ss cn(ts) (3) where is the largest n-gram considered (usually = 4), in(ts, rs) is weighted count of common n-grams between the target (ts) and reference (rs) translations of sentence s, and cn(ts) is the total number of n-grams in ts.to derive version of this formula that is continuous function of ?, we will need multiple translations ts,1, ..., ts,k for each source sentence s. the general idea is to weight each of these translations 2for the sake of simplicity, we consider single reference translation per source sentence, but the argument can easily be extended to multiple references.
</nextsent>
<nextsent>758 by factor w(?, s, k), proportional to the score m?(ts,k|s) that ts,k is assigned by the log-linear model forgiven ?: w(?, s, k) = [ m?(ts,k|s) ? k? m?(ts,k? |s) ]?
</nextsent>
<nextsent>where ? is the smoothing factor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J24">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the parameters of the model were optimized inde pendantly for each bi-phrase library.
</prevsent>
<prevsent>in all cases,we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the nist and bleu scores against one reference translation.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
as point of comparison, we also trained an ibm-4 translation model with the giza++ toolkit (och and ney, 2000), <papid> P00-1056 </papid>using the combined bi-phrase building and training sets, and translated the test set using the rewrite decoder (germann et al, 2001)<papid> P01-1030 </papid>5.</citsent>
<aftsection>
<nextsent>table 2 describes the various libraries that were used for our experiments, and the results obtained for each.
</nextsent>
<nextsent>system/library bi-phrases nist bleu rewrite 6.6838 0.3324 a1 238 6.6695 0.3310 a2-g0 642 6.7675 0.3363 a2-g3 4.1 6.7068 0.3283 b1-g0 193 6.7898 0.3369 b1 267 6.9172 0.3407 b2-g0 499 6.7290 0.3391 b2-g3 3.3 6.9707 0.3552 b1-g1 206 6.8979 0.3441 b1-g2 213 6.9406 0.3454 b1-g3 218 6.9546 0.3518 b1-g4 222 6.9527 0.3423 table 2: bi-phrase libraries and results the top part of the table presents the results for the libraries.
</nextsent>
<nextsent>as can be seen, library a1 achieves approximately the same score as the baseline sys tem; this is expected, since this library is essentially 5both the rewrite and our own system relied on trigram language model trained on the english half of the bi-phrase building set.
</nextsent>
<nextsent>760 subset sentences source words target words bi-phrase-building set 931,000 17.2m 15.2m training set 800 11,667 10,601 test set 500 6726 6041 table 1: datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J25">
<title id=" H05-1095.xml">translating with noncontiguous phrases </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the parameters of the model were optimized inde pendantly for each bi-phrase library.
</prevsent>
<prevsent>in all cases,we performed only 2 iterations of the training procedure, then measured the performance of the system on the test set in terms of the nist and bleu scores against one reference translation.
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
as point of comparison, we also trained an ibm-4 translation model with the giza++ toolkit (och and ney, 2000), <papid> P00-1056 </papid>using the combined bi-phrase building and training sets, and translated the test set using the rewrite decoder (germann et al, 2001)<papid> P01-1030 </papid>5.</citsent>
<aftsection>
<nextsent>table 2 describes the various libraries that were used for our experiments, and the results obtained for each.
</nextsent>
<nextsent>system/library bi-phrases nist bleu rewrite 6.6838 0.3324 a1 238 6.6695 0.3310 a2-g0 642 6.7675 0.3363 a2-g3 4.1 6.7068 0.3283 b1-g0 193 6.7898 0.3369 b1 267 6.9172 0.3407 b2-g0 499 6.7290 0.3391 b2-g3 3.3 6.9707 0.3552 b1-g1 206 6.8979 0.3441 b1-g2 213 6.9406 0.3454 b1-g3 218 6.9546 0.3518 b1-g4 222 6.9527 0.3423 table 2: bi-phrase libraries and results the top part of the table presents the results for the libraries.
</nextsent>
<nextsent>as can be seen, library a1 achieves approximately the same score as the baseline sys tem; this is expected, since this library is essentially 5both the rewrite and our own system relied on trigram language model trained on the english half of the bi-phrase building set.
</nextsent>
<nextsent>760 subset sentences source words target words bi-phrase-building set 931,000 17.2m 15.2m training set 800 11,667 10,601 test set 500 6726 6041 table 1: datasets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J29">
<title id=" E95-1003.xml">criteria for measuring term recognition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>accounts of term-recognition systems ometimes consist of purely descriptive statement of the ad-vantages of particular approach and make no at-tempt measure the pay-off the proposed approach yields (david, 1990).
</prevsent>
<prevsent>others produce partial fig-ures without any clear statement of how they are derived (otman, 1991).
</prevsent>
</prevsection>
<citsent citstr=" J93-1007 ">
one of the best efforts to quantify the performance ofa term-recognition sys-tem (smadja, 1993) <papid> J93-1007 </papid>does so only for one processing stage, leaving unassessed the text-to-output perfor-mance of the system.</citsent>
<aftsection>
<nextsent>while most automatic term-recognition systems developed to date have been experimental or in- house ones, few systems like term cruncher (nor- mand, 1993) are now being marketed.
</nextsent>
<nextsent>both the developers and users of such systems would benefit greatly by clearly qualifying what each system aims to achieve, and precisely quantifying how closely the system comes to achieving its stated aim.
</nextsent>
<nextsent>before discussing what term-recognition system should be expected to recognize and how perfor-mance in recognition should be measured, two un-derlying premises hould be made clear.
</nextsent>
<nextsent>firstly, the automatic system is designed to recognize seg-ments of text that, conventionally, have been man-ually identified by terminologist, indexer, lexicog- rapher or other trained individual.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J30">
<title id=" E95-1003.xml">criteria for measuring term recognition </title>
<section> what is to be recognized?.  </section>
<citcontext>
<prevsection>
<prevsent>proper attention to capitalization can generally result in the correct recognition of compressed forms.
</prevsent>
<prevsent>part-of-speech tagging is required to detect new terms formed through conversion.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
this is quite feasible using statistical taggers like those of gar-side (1987), church (1988) <papid> A88-1019 </papid>or foster (1991) which achieve performance upwards of 97% on unrestricted text.</citsent>
<aftsection>
<nextsent>terms formed through semantic drift are the wolves in sheep clothing stealing through termino-logical pastures.
</nextsent>
<nextsent>they are well enough concemcd to allude at times even the human reader and no au-tomatic term-recognition system has attempted to distinguish such terms, despite the prevalence ofpol- ysemy in such fields as the social sciences (r.iggs, 1993) and the importance for purposes of termi-nological standardization that  deviant  usage be tracked.
</nextsent>
<nextsent>implementing system to distinguish new 1conversion occurs when term is formed by change in grammatical category.
</nextsent>
<nextsent>verb-to-noun conver-sion commonly occurs for commands in programming or wordprocessing (e.g. un delete works if you catch your mistake quickly).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J32">
<title id=" E95-1020.xml">distributional partofspeech tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the simplest part-of-speech taggers are bigram or trigram models (church, 1989; charniak et al., 1993).
</prevsent>
<prevsent>they require relatively large tagged training text.
</prevsent>
</prevsection>
<citsent citstr=" P93-1035 ">
transformation-based tagging as introduced by brill (1993) <papid> P93-1035 </papid>also requires hand- tagged text for training.</citsent>
<aftsection>
<nextsent>no pre tagged text is nec-essary for hidden markov models (jelinek, 1985; cutting et al, 1991; kupiec, 1992).
</nextsent>
<nextsent>still, lexi-con is needed that specifies the possible parts of speech for every word.
</nextsent>
<nextsent>brill and marcus (1992<papid> H92-1030 </papid>a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably re-duced by combining learning procedures and partial part-of-speech ategorization elicited from an informant.</nextsent>
<nextsent>the present paper is concerned with tagging languages and sublanguages for which no priori knowledge about grammatical categories avail-able, situation that occurs often in practice (brill and marcus, 1992<papid> H92-1030 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J33">
<title id=" E95-1020.xml">distributional partofspeech tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>no pre tagged text is nec-essary for hidden markov models (jelinek, 1985; cutting et al, 1991; kupiec, 1992).
</prevsent>
<prevsent>still, lexi-con is needed that specifies the possible parts of speech for every word.
</prevsent>
</prevsection>
<citsent citstr=" H92-1030 ">
brill and marcus (1992<papid> H92-1030 </papid>a) have shown that the effort necessary to construct the part-of-speech lexicon can be considerably re-duced by combining learning procedures and partial part-of-speech ategorization elicited from an informant.</citsent>
<aftsection>
<nextsent>the present paper is concerned with tagging languages and sublanguages for which no priori knowledge about grammatical categories avail-able, situation that occurs often in practice (brill and marcus, 1992<papid> H92-1030 </papid>a).</nextsent>
<nextsent>several researchers have worked on learning grammatical properties of words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J36">
<title id=" E95-1020.xml">distributional partofspeech tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several researchers have worked on learning grammatical properties of words.
</prevsent>
<prevsent>elman (1990) trains connection ist netto predict words, pro-cess that generates internal representations that reflect grammatical category.
</prevsent>
</prevsection>
<citsent citstr=" H90-1055 ">
brill et al (1990) <papid> H90-1055 </papid>try to infer grammatical category from bi-gram statistics.</citsent>
<aftsection>
<nextsent>finch and chater (1992) and finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in corpus.
</nextsent>
<nextsent>kneser and ney (1993) present probabilistic model for entropy maxi-mization that also relies on the immediate neigh-bors of words in corpus.
</nextsent>
<nextsent>biber (1993) <papid> J93-3004 </papid>ap-plies factor analysis to collocations of two target words ( certain  and  right ) with their immedi-ate neighbors.</nextsent>
<nextsent>what these approaches have in common is that they classify words instead of individual occur- rences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J37">
<title id=" E95-1020.xml">distributional partofspeech tagging </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finch and chater (1992) and finch (1993) use vector models in which words are clustered according to the similarity of their close neighbors in corpus.
</prevsent>
<prevsent>kneser and ney (1993) present probabilistic model for entropy maxi-mization that also relies on the immediate neigh-bors of words in corpus.
</prevsent>
</prevsection>
<citsent citstr=" J93-3004 ">
biber (1993) <papid> J93-3004 </papid>ap-plies factor analysis to collocations of two target words ( certain  and  right ) with their immedi-ate neighbors.</citsent>
<aftsection>
<nextsent>what these approaches have in common is that they classify words instead of individual occur-rences.
</nextsent>
<nextsent>given the widespread part-of-speech am-biguity of words this is problematicj how should word like  plant  be categorized if it has uses both as verb and as noun?
</nextsent>
<nextsent>how can cate-gorization be considered meaningful if the infini- tive marker  to  is not distinguished from the ho- mophonous preposition?
</nextsent>
<nextsent>in previous paper (schfitze, 1993), we trained neural network to disambiguate part-of-speech *although biber (1993) <papid> J93-3004 </papid>classifies collocations, these can also be ambiguous.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J41">
<title id=" E95-1020.xml">distributional partofspeech tagging </title>
<section> tag induction.  </section>
<citcontext>
<prevsection>
<prevsent>we formed such concate- nated vectors for all 47,025 words (surface forms) in the brown corpus.
</prevsent>
<prevsent>here, we use the raw 250- dimensional context vectors and apply the svd to the 47,025-by-500 matrix (47,025 words with two 250-dimensional context vectors each).
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
we obtained 47,025 50-dimensional reduced vectors from the svd and clustered them into 200 classes using the fast clustering algorithm buckshot (cut- ting et al, 1992) (<papid> A92-1018 </papid>group average agglomeration ap-plied to sample).</citsent>
<aftsection>
<nextsent>this classification constitutes the baseline performance for distributional part- of-speech tagging.
</nextsent>
<nextsent>all occurrences of word are assigned to one class.
</nextsent>
<nextsent>as pointed out above, such procedure is problematic for ambiguous words.
</nextsent>
<nextsent>3.2 induction based on word type and.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J44">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.3 previous work.
</prevsent>
<prevsent>there has been much interest recently on the discovery of semantic relations from open-text using symbolic and statistical techniques.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
this includes the seminal paper of (gildea and jurafsky, 2002), <papid> J02-3001 </papid>senseval 3 and conll competitions on automatic labeling of semantic roles detection of noun compound semantics (lapata, 2000), (rosario and hearst, 2001) <papid> W01-0511 </papid>and many others.</citsent>
<aftsection>
<nextsent>however, not much work has been done to automatically interpret the genitive constructions.
</nextsent>
<nextsent>in 1999, berland and charniak (berland and charniak, 1999) <papid> P99-1008 </papid>applied statistical methods on very large corpus to find part-whole relations.following hearsts method for the automatic acquisition of hypernymy relations (hearst, 1998),they used the genitive construction to detect part whole relations based on list of six seeds representing whole objects, (i.e. book, building, car, hospital, plant, and school).</nextsent>
<nextsent>their systems output was an ordered list of possible parts according to some statistical metrics (dunnings log-likelihood metric and johnsons significant-difference metric).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J45">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.3 previous work.
</prevsent>
<prevsent>there has been much interest recently on the discovery of semantic relations from open-text using symbolic and statistical techniques.
</prevsent>
</prevsection>
<citsent citstr=" W01-0511 ">
this includes the seminal paper of (gildea and jurafsky, 2002), <papid> J02-3001 </papid>senseval 3 and conll competitions on automatic labeling of semantic roles detection of noun compound semantics (lapata, 2000), (rosario and hearst, 2001) <papid> W01-0511 </papid>and many others.</citsent>
<aftsection>
<nextsent>however, not much work has been done to automatically interpret the genitive constructions.
</nextsent>
<nextsent>in 1999, berland and charniak (berland and charniak, 1999) <papid> P99-1008 </papid>applied statistical methods on very large corpus to find part-whole relations.following hearsts method for the automatic acquisition of hypernymy relations (hearst, 1998),they used the genitive construction to detect part whole relations based on list of six seeds representing whole objects, (i.e. book, building, car, hospital, plant, and school).</nextsent>
<nextsent>their systems output was an ordered list of possible parts according to some statistical metrics (dunnings log-likelihood metric and johnsons significant-difference metric).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J46">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this includes the seminal paper of (gildea and jurafsky, 2002), <papid> J02-3001 </papid>senseval 3 and conll competitions on automatic labeling of semantic roles detection of noun compound semantics (lapata, 2000), (rosario and hearst, 2001) <papid> W01-0511 </papid>and many others.</prevsent>
<prevsent>however, not much work has been done to automatically interpret the genitive constructions.</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
in 1999, berland and charniak (berland and charniak, 1999) <papid> P99-1008 </papid>applied statistical methods on very large corpus to find part-whole relations.following hearsts method for the automatic acquisition of hypernymy relations (hearst, 1998),they used the genitive construction to detect part whole relations based on list of six seeds representing whole objects, (i.e. book, building, car, hospital, plant, and school).</citsent>
<aftsection>
<nextsent>their systems output was an ordered list of possible parts according to some statistical metrics (dunnings log-likelihood metric and johnsons significant-difference metric).
</nextsent>
<nextsent>they presented the results for two specific patterns (nns nn?
</nextsent>
<nextsent>and nn of dt nn?).
</nextsent>
<nextsent>the accuracy obtained for the first 50 parts was 55% and for the first 20 parts was 70%.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J47">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> genitives? corpus analysis.  </section>
<citcontext>
<prevsection>
<prevsent>a set of 20,000 sentences were randomly selected from the la times collection.
</prevsent>
<prevsent>in these 20,000 sentences, there were 3,255 genitive instances (2,249 of-constructions and 1,006 s-constructions).
</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
from these, 80% were used for training and 20% for test ing.each genitive instance was tagged with the corresponding semantic relations by two annotators, based on list of 35 most frequently used semantic relations proposed by (moldovan et al, 2004) <papid> W04-2609 </papid>and shown in table 1.</citsent>
<aftsection>
<nextsent>the genitives?
</nextsent>
<nextsent>noun components were manually disambiguated with the corresponding wordnet 2.0 senses or the named entities if they are not in wordnet (e.g. names of persons, names of locations, etc).
</nextsent>
<nextsent>2.2 inter-annotator agreement.
</nextsent>
<nextsent>the annotators, two graduate students in computational semantics, were given the genitives and the sentences in which they occurred.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J48">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> 109 75 property johns coldness? </section>
<citcontext>
<prevsection>
<prevsent>in order to classify given set of examples (members of x), one needs some kind of measure of the similarity (or the difference) between any two given members of . 3.2 feature space.
</prevsent>
<prevsent>an essential aspect of our approach below is theword sense disambiguation (wsd) of the noun.
</prevsent>
</prevsection>
<citsent citstr=" W04-0848 ">
using state-of-the-art open-text wsd system with 70% accuracy for nouns (novischi et al, 2004), <papid> W04-0848 </papid>each word is mapped into its corresponding wordnet 2.0sense.</citsent>
<aftsection>
<nextsent>the disambiguation process takes into account surrounding words, and it is through this process that context gets to play role in labeling the genitives?
</nextsent>
<nextsent>semantics.
</nextsent>
<nextsent>so far, we have identified and experimented with the following np features: 1.
</nextsent>
<nextsent>semantic class of head noun specifies the word-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J49">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> 109 75 property johns coldness? </section>
<citcontext>
<prevsection>
<prevsent>however, we do not know how well it behaves on unseen examples and we are looking for boundary that classifies with high accuracy the unseen examples.
</prevsent>
<prevsent>we test the boundary on unseen examples.
</prevsent>
</prevsection>
<citsent citstr=" W97-0808 ">
forthat we used 10% of the annotated examples (differ ent from the 10% of the examples used for testing) and compute the accuracy (f-measure) of the new boundary on them.if the accuracy is larger than the previous bound arys accuracy, we are converging toward the best approximation of the boundary and thus we should repeat step 2 for the new boundary.if the accuracy is lower than the previous bound arys accuracy, the new boundary is too specific and the previous boundary is better approximation of the ideal boundary.for the automatic detection of the semantic relations encoded by genitives, the boundary constructed by the semantic scattering model is more apppropriate than tree cut?, like the ones used for verb disambiguation (mccarthy, 1997) (<papid> W97-0808 </papid>li and abe,1998) <papid> J98-2002 </papid>and constructed using the minimum description length model (rissanen, 1978).</citsent>
<aftsection>
<nextsent>the deve lope ment of tree cut?
</nextsent>
<nextsent>model for the detection of the semantic relations encoded by genitives involves the construction of different tree cut?
</nextsent>
<nextsent>for each head noun and threfore the usage of these cuts is restricted to those head nouns.
</nextsent>
<nextsent>on the other hand, semantic scattering constructs only one boundary that, unlike the tree cut?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J50">
<title id=" H05-1112.xml">a semantic scattering model for the automatic interpretation of genitives </title>
<section> 109 75 property johns coldness? </section>
<citcontext>
<prevsection>
<prevsent>however, we do not know how well it behaves on unseen examples and we are looking for boundary that classifies with high accuracy the unseen examples.
</prevsent>
<prevsent>we test the boundary on unseen examples.
</prevsent>
</prevsection>
<citsent citstr=" J98-2002 ">
forthat we used 10% of the annotated examples (differ ent from the 10% of the examples used for testing) and compute the accuracy (f-measure) of the new boundary on them.if the accuracy is larger than the previous bound arys accuracy, we are converging toward the best approximation of the boundary and thus we should repeat step 2 for the new boundary.if the accuracy is lower than the previous bound arys accuracy, the new boundary is too specific and the previous boundary is better approximation of the ideal boundary.for the automatic detection of the semantic relations encoded by genitives, the boundary constructed by the semantic scattering model is more apppropriate than tree cut?, like the ones used for verb disambiguation (mccarthy, 1997) (<papid> W97-0808 </papid>li and abe,1998) <papid> J98-2002 </papid>and constructed using the minimum description length model (rissanen, 1978).</citsent>
<aftsection>
<nextsent>the deve lope ment of tree cut?
</nextsent>
<nextsent>model for the detection of the semantic relations encoded by genitives involves the construction of different tree cut?
</nextsent>
<nextsent>for each head noun and threfore the usage of these cuts is restricted to those head nouns.
</nextsent>
<nextsent>on the other hand, semantic scattering constructs only one boundary that, unlike the tree cut?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J51">
<title id=" H05-1119.xml">searching the audio notebook keyword search in recorded conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a successful way to deal with high word error rates is the use of recognition alternates (lattices).
</prevsent>
<prevsent>for example,(seide and yu, 2004; yu and seide, 2004) reports substantial 50% improvement of fom (figure of merit) for word-spotting task in voicemails.
</prevsent>
</prevsection>
<citsent citstr=" P05-1055 ">
improvements from using lattices were also reported by (saraclar and sproat, 2004) and (chelba and acero, 2005).<papid> P05-1055 </papid></citsent>
<aftsection>
<nextsent>to address the problem of domain independence, subword-based approach is needed.
</nextsent>
<nextsent>in (logan, 2002) the authors address the problem by indexing phonetic or word-fragment based transcriptions.
</nextsent>
<nextsent>similar approaches,e.g. using overlapping -grams of phonemes, are discussed in (schauble, 1995) and (ng, 2000).
</nextsent>
<nextsent>(james and young, 1994) introduces the approach of searching phoneme lattices.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J52">
<title id=" H05-1119.xml">searching the audio notebook keyword search in recorded conversation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>becomes non-trivial problem, because due to the unknown-word nature, we need to deal with an open set of index keys.
</prevsent>
<prevsent>(saraclar and sproat, 2004) proposes to store the individual lattice arcs (inverting the lattice).
</prevsent>
</prevsection>
<citsent citstr=" W04-2907 ">
(allauzen et al, 2004) <papid> W04-2907 </papid>introduces general indexation framework by indexing expected term frequencies (expected counts?)</citsent>
<aftsection>
<nextsent>instead of each individual keyword occurrence or lattice arcs.
</nextsent>
<nextsent>in (yu et al, 2005), similar ideaof indexing expected term frequencies is proposed, suggesting to approximate expected term frequencies by gram phoneme language models estimated on segments of audio.in this paper, we combine previous work on phonetic lattice search, hybrid search and lattice indexing into real system for searching recorded conversations that achieves high accuracy and can handle hundreds of hours of audio.
</nextsent>
<nextsent>the main contributions of this paper are: real system for searching conversational speech, novel method for combining phoneme and word lattices,and experimental results for searching recorded conversations.
</nextsent>
<nextsent>the paper is organized as follows.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J55">
<title id=" E93-1042.xml">new frontiers beyond context freeness digrammars and diautomata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it is common practice to parse nested wh-dependen- cies, like the classical example of rizzi (1982) in (1), (1) tuo fra tello, \[a cui\]l mi domando \[che storie\]2 abbiano raccontato 2 1, era molto preoccupato (your brother, \[to whom\] 1 wonder \[which sto- ries\] 2 they told 2 1 was very troubled) using stack mechanism.
</prevsent>
<prevsent>under the binary branching hypothesis the relevant structure of (1) augmented by wh-stacks is as follows: (2) \[a cui\] 1 mi dor nando lpush ---tit 11--~ i--push ---~\[t2,tll--- 1 \[che storie\]2abbiano v2\[t2,tl\] / \ vlit21 ppitll / \ v~i\] np.it2i pop ipp \] raccontato 2 1 up to now it is unclear, how far beyond context- freeness the generative power of type 2 grammar formalism is being extended if such stack mechanism is grafted on it (assuming, of course, that an upper bound for the size of the stack can not be motivated).
</prevsent>
</prevsection>
<citsent citstr=" J81-4003 ">
fernando pereira concept of extra position gram-mar (xg), introduced in his influential paper (pereira, 1981; <papid> J81-4003 </papid>1983; cf.</citsent>
<aftsection>
<nextsent>stabler, 1987) <papid> J87-1001 </papid>in order to delimit the new territory, can be shown to be inadequate for this purpose, since it is provable that the class of languages genera ble by xgs coincides with type 0 (i,e. xgs have the power of turing machines), whereas the increase of power by the stack mechanism is not even enough to generate all type 1 languages (see below).</nextsent>
<nextsent>in (2) an additional point is illustrated: the stack \[t2,tl\] belonging to 2 has to be divided into the sub stacks \[t2\] and \[tl\], which are then inherited by the daughters l and pp.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J56">
<title id=" E93-1042.xml">new frontiers beyond context freeness digrammars and diautomata </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>under the binary branching hypothesis the relevant structure of (1) augmented by wh-stacks is as follows: (2) \[a cui\] 1 mi dor nando lpush ---tit 11--~ i--push ---~\[t2,tll--- 1 \[che storie\]2abbiano v2\[t2,tl\] / \ vlit21 ppitll / \ v~i\] np.it2i pop ipp \] raccontato 2 1 up to now it is unclear, how far beyond context- freeness the generative power of type 2 grammar formalism is being extended if such stack mechanism is grafted on it (assuming, of course, that an upper bound for the size of the stack can not be motivated).
</prevsent>
<prevsent>fernando pereira concept of extra position gram-mar (xg), introduced in his influential paper (pereira, 1981; <papid> J81-4003 </papid>1983; cf.</prevsent>
</prevsection>
<citsent citstr=" J87-1001 ">
stabler, 1987) <papid> J87-1001 </papid>in order to delimit the new territory, can be shown to be inadequate for this purpose, since it is provable that the class of languages genera ble by xgs coincides with type 0 (i,e. xgs have the power of turing machines), whereas the increase of power by the stack mechanism is not even enough to generate all type 1 languages (see below).</citsent>
<aftsection>
<nextsent>in (2) an additional point is illustrated: the stack \[t2,tl\] belonging to 2 has to be divided into the sub stacks \[t2\] and \[tl\], which are then inherited by the daughters l and pp.
</nextsent>
<nextsent>for the pp-index tlis not dis-charged from the top of the v2-stack \[t2,tl\].
</nextsent>
<nextsent>generaliz-ing to stacks of unlimited size, the partition of stack among the inheriting sub constituents 1 and 2 of constituent 0 is as in (3) (3) k0 it 1,...,tj,tj+l,...,tk\] / \ klltl,...,tjl k2ltj+l,...,tkl 358 if the generalization (3) is tenable, the extension of context-free grmnmars (vijay-shanker and weir, 1991, call the resulting formalism  linear indexed granunar  (lig)) discussed by gazdar in (gazdar, 1988), in which stacks are exclusively passed over to single daughter (as in (3.1)), is too weak.
</nextsent>
<nextsent>(3.1) a)k0\[tl,..,,tk\] b) koitl,....,tk\] / \ / \ kl\[tl,...,t k\] 2 k1 k2\[tl,...,tk\] stack-transmission by distribution, however, as in (3) suggests the definition of new class of grammars properly containing the context-free class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J57">
<title id=" E93-1042.xml">new frontiers beyond context freeness digrammars and diautomata </title>
<section> dl=grammars and di- languages.  </section>
<citcontext>
<prevsection>
<prevsent>i.e. i generates words  =xl \[xm \]xr , the central part of which contain duplication (of   in \[xm \]=ylzy2zy 3) without correspondence in xl  or xr , thus contradicting the general form of words of 3.
</prevsent>
<prevsent>hence 3 is not indexed.
</prevsent>
</prevsection>
<citsent citstr=" P88-1034 ">
2.2 di-grammars and linear indexed grammars i. as already mentioned above, gazdar in (gazdar, 1988) introduced and discussed grammar formalism, after-wards (e.g. in (weir and joshi, 1988)) <papid> P88-1034 </papid>called linear in-dexed granunars (lig s), using index stacks in which only one nonterminai on the right-hand-side of rule can inherit the stack from the left-hand-side, i.e. the rules of lig g=(n,t, f, p, s) with n,f,t,s as above, are of the form i. a\[..\] ~a1u...ai\[..\]..~i ii.</citsent>
<aftsection>
<nextsent>a\[..\] -~ai\[\]...ai\[f..\]...a iii.
</nextsent>
<nextsent>a \[f..\]-~a 1 \[\]...ai\[..\]...an iv.
</nextsent>
<nextsent>all ~a wherea 1,...,anen, fef, and aet~{e}.
</nextsent>
<nextsent>the  derives - relation =  is defined as follows o(a \[fl..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J63">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> iterative graphical algorithms for.  </section>
<citcontext>
<prevsection>
<prevsent>if an annotated corpus is available, dependencies can be defined as label co-occurrence probabilities approximated with frequency counts (ltwi , lswj ), or as conditional probabilities (ltwi |lswj ).
</prevsent>
<prevsent>optionally, these dependencies can be lexicalized by taking into account the corresponding words in the sequence, e.g. (ltwi |lswj ) (wi|ltwi).
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
in the absence of an annotated corpus, dependencies can be derived by other means, e.g. part 413 of-speech probabilities can be approximated from araw corpus as in (cutting et al, 1992), <papid> A92-1018 </papid>word-sense dependencies can be derived as definition-based similarities, etc. label dependencies are set as weights on the arcs drawn between corresponding labels.</citsent>
<aftsection>
<nextsent>arcs can be directed or undirected for joint probabilities or similarity measures, and are usually directed for conditional probabilities.
</nextsent>
<nextsent>2.5 labeling example.
</nextsent>
<nextsent>consider again the example from figure 1, consisting of sequence of four words, and their possible corresponding labels.
</nextsent>
<nextsent>in the first step of the algorithm, label dependencies are determined, and let us assume that the values for these dependencies are as indicated through the edge weights in figure 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J64">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, none of the algorithms take into account the dictionary sense order (e.g. the most frequent sense provided by wordnet), and therefore they are both fully unsupervised.
</prevsent>
<prevsent>table 1 shows precision and recall figures4 for a3given sequence of words, the original lesk algorithm attempts to identify the combination of word senses that maximizes the redundancy (overlap) across all corresponding definitions.
</prevsent>
</prevsection>
<citsent citstr=" C92-1056 ">
the algorithm was later improved through method for simulated annealing (cowie et al, 1992), <papid> C92-1056 </papid>which solved the combinatorial explosion of word senses, while still finding an optimal solution.</citsent>
<aftsection>
<nextsent>however, recent comparative evaluations of different variants of the lesk algorithm have shown that the performance of the original algorithm is significantly exceeded by an algorithm variation that relies on the overlap between word senses and current context (vasilescu et al, 2004).
</nextsent>
<nextsent>we are thus using this latter lesk variant in our implementation.
</nextsent>
<nextsent>4recall is particularly low for each individual part-of-speech because it is calculated with respect to the entire dataset.
</nextsent>
<nextsent>the overall precision and recall figures coincide, reflecting the 100% coverage of the algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J66">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>as seen in the figure, both algorithms benefit from larger contexts, with steady increase in performance observed for increasingly larger windowsizes.
</prevsent>
<prevsent>although the initial growth observed for these quence data labeling algorithm is somewhat sharper, the gap between the two curves stabilizes for window sizes larger than five words, which suggests that the improvement in performance achieved with sequence data labeling over individual data labeling does not depend on the size of available context.
</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
the algorithm was also evaluated on two other datasets, senseval-3 english all-words data (snyder and palmer, 2004) <papid> W04-0811 </papid>and subset of semcor (miller et al, 1993), <papid> H93-1061 </papid>although only fine-grained sense evaluations could be conducted on these test sets.</citsent>
<aftsection>
<nextsent>the disambiguation precision on the senseval-3 data was measured at 52.2% using sequence data labeling, compared to 48.1% obtained with individual 416 fine-grained sense distinctions coarse-grained sense distinctions random individual sequence random individual sequence part-of baseline (lesk) (graph-based) baseline (lesk) (graph-based) speech r r r r r r noun 41.4% 19.4% 50.3% 23.6% 57.5% 27.0% 42.7% 20.0% 51.4% 24.1% 58.8% 27.5% verb 20.7% 3.9% 30.5% 5.7% 36.5% 6.9% 22.8% 4.3% 31.9% 6.0% 37.9% 7.1% adjective 41.3% 9.3% 49.1% 11.0% 56.7% 12.7% 42.6% 42.6% 49.8% 11.2% 57.6% 12.9% adverb 44.6% 5.2% 64.6% 7.6% 70.9% 8.3% 40.7% 4.8% 65.3% 7.7% 71.9% 8.5% all 37.9% 37.9% 48.7% 48.7% 54.2% 54.2% 38.7% 38.7% 49.8% 49.8% 55.3% 55.3% table 1: precision and recall for graph-based sequence data labeling, individual data labeling, and random baseline, for fine-grained and coarse-grained sense distinctions.
</nextsent>
<nextsent>35 40 45 50 55 60 0 5 10 15 20 25 30 is am bi gu at io pr ec isi on (% ) window size sequence individual random figure 3: disambiguation results using sequence data labeling, individual labeling, and random baseline, for various context sizes.
</nextsent>
<nextsent>data labeling, and 34.3% achieved through random sense assignment.
</nextsent>
<nextsent>the average disambiguation figure obtained on all the words in random subset of 10 semcor documents, covering different domains, was56.5% for sequence data labeling, 47.4% for individual labeling, and 35.3% for the random baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J67">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>as seen in the figure, both algorithms benefit from larger contexts, with steady increase in performance observed for increasingly larger windowsizes.
</prevsent>
<prevsent>although the initial growth observed for these quence data labeling algorithm is somewhat sharper, the gap between the two curves stabilizes for window sizes larger than five words, which suggests that the improvement in performance achieved with sequence data labeling over individual data labeling does not depend on the size of available context.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
the algorithm was also evaluated on two other datasets, senseval-3 english all-words data (snyder and palmer, 2004) <papid> W04-0811 </papid>and subset of semcor (miller et al, 1993), <papid> H93-1061 </papid>although only fine-grained sense evaluations could be conducted on these test sets.</citsent>
<aftsection>
<nextsent>the disambiguation precision on the senseval-3 data was measured at 52.2% using sequence data labeling, compared to 48.1% obtained with individual 416 fine-grained sense distinctions coarse-grained sense distinctions random individual sequence random individual sequence part-of baseline (lesk) (graph-based) baseline (lesk) (graph-based) speech r r r r r r noun 41.4% 19.4% 50.3% 23.6% 57.5% 27.0% 42.7% 20.0% 51.4% 24.1% 58.8% 27.5% verb 20.7% 3.9% 30.5% 5.7% 36.5% 6.9% 22.8% 4.3% 31.9% 6.0% 37.9% 7.1% adjective 41.3% 9.3% 49.1% 11.0% 56.7% 12.7% 42.6% 42.6% 49.8% 11.2% 57.6% 12.9% adverb 44.6% 5.2% 64.6% 7.6% 70.9% 8.3% 40.7% 4.8% 65.3% 7.7% 71.9% 8.5% all 37.9% 37.9% 48.7% 48.7% 54.2% 54.2% 38.7% 38.7% 49.8% 49.8% 55.3% 55.3% table 1: precision and recall for graph-based sequence data labeling, individual data labeling, and random baseline, for fine-grained and coarse-grained sense distinctions.
</nextsent>
<nextsent>35 40 45 50 55 60 0 5 10 15 20 25 30 is am bi gu at io pr ec isi on (% ) window size sequence individual random figure 3: disambiguation results using sequence data labeling, individual labeling, and random baseline, for various context sizes.
</nextsent>
<nextsent>data labeling, and 34.3% achieved through random sense assignment.
</nextsent>
<nextsent>the average disambiguation figure obtained on all the words in random subset of 10 semcor documents, covering different domains, was56.5% for sequence data labeling, 47.4% for individual labeling, and 35.3% for the random baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J70">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>this precision is exceeded by the lesk algorithm variation used in the experiments reported in this paper, which measures the overlap between sense definitions and the current context, for precision of 48.7% on the same dataset (see table 1).
</prevsent>
<prevsent>in the senseval-2 evaluations, the best performing fully unsupervised algorithm5 was developed by (litkowski, 2001), who combines analysis of multiword units and contextual clues based on collocations and content words from dictionary definitions and examples, for an overall precision and recall of 45.1%.
</prevsent>
</prevsection>
<citsent citstr=" W04-0837 ">
more recently, (mccarthy et al, 2004) <papid> W04-0837 </papid>reports one of the best results on the senseval-2 dataset, using an algorithm that automatically derives the most frequent sense for word using distributional similarities learned from large raw corpus, for disambiguation precision of 53.0% and recall of 49.0%.another related line of work consists of the disambiguation algorithms based on lexical chains (morris and hirst, 1991), <papid> J91-1002 </papid>and the more recent improvements reported in (galley and mckeown, 2003) ? where threads of meaning are identified throughout text.lexical chains however only take into account connections between concepts identified in static way, without considering the importance of the concepts that participate in relation, which is recursively determined in our algorithm.</citsent>
<aftsection>
<nextsent>moreover, the construction of lexical chains requires structured dictionaries such as wordnet, with explicitly defined semantic relations between word senses, whereas our algorithm can also work with simple unstructured dictionaries that provide only word sense definitions.
</nextsent>
<nextsent>(galley and mckeown, 2003) evaluated their algorithm on the nouns from subset of semcor, reporting 62.09% disambiguation precision.
</nextsent>
<nextsent>the performance of our algorithm on the same subset of semcor nouns was measured at 64.2%6.
</nextsent>
<nextsent>finally, another disambiguation method relying on graph algorithms that exploit the5algorithms that integrate the most frequent sense in wordnet are not considered here, since this represents supervised knowledge source (wordnet sense frequencies are derived from sense-annotated corpus).6note that the results are not directly comparable, since (gal ley and mckeown, 2003) used the wordnet sense order to break the ties, whereas we assume that such sense order frequency is not available, and thus we break the ties through random choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J71">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>this precision is exceeded by the lesk algorithm variation used in the experiments reported in this paper, which measures the overlap between sense definitions and the current context, for precision of 48.7% on the same dataset (see table 1).
</prevsent>
<prevsent>in the senseval-2 evaluations, the best performing fully unsupervised algorithm5 was developed by (litkowski, 2001), who combines analysis of multiword units and contextual clues based on collocations and content words from dictionary definitions and examples, for an overall precision and recall of 45.1%.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
more recently, (mccarthy et al, 2004) <papid> W04-0837 </papid>reports one of the best results on the senseval-2 dataset, using an algorithm that automatically derives the most frequent sense for word using distributional similarities learned from large raw corpus, for disambiguation precision of 53.0% and recall of 49.0%.another related line of work consists of the disambiguation algorithms based on lexical chains (morris and hirst, 1991), <papid> J91-1002 </papid>and the more recent improvements reported in (galley and mckeown, 2003) ? where threads of meaning are identified throughout text.lexical chains however only take into account connections between concepts identified in static way, without considering the importance of the concepts that participate in relation, which is recursively determined in our algorithm.</citsent>
<aftsection>
<nextsent>moreover, the construction of lexical chains requires structured dictionaries such as wordnet, with explicitly defined semantic relations between word senses, whereas our algorithm can also work with simple unstructured dictionaries that provide only word sense definitions.
</nextsent>
<nextsent>(galley and mckeown, 2003) evaluated their algorithm on the nouns from subset of semcor, reporting 62.09% disambiguation precision.
</nextsent>
<nextsent>the performance of our algorithm on the same subset of semcor nouns was measured at 64.2%6.
</nextsent>
<nextsent>finally, another disambiguation method relying on graph algorithms that exploit the5algorithms that integrate the most frequent sense in wordnet are not considered here, since this represents supervised knowledge source (wordnet sense frequencies are derived from sense-annotated corpus).6note that the results are not directly comparable, since (gal ley and mckeown, 2003) used the wordnet sense order to break the ties, whereas we assume that such sense order frequency is not available, and thus we break the ties through random choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J72">
<title id=" H05-1052.xml">unsupervised large vocabulary word sense disambiguation with graph based algorithms for sequence data labeling </title>
<section> experiments in word sense.  </section>
<citcontext>
<prevsection>
<prevsent>the performance of our algorithm on the same subset of semcor nouns was measured at 64.2%6.
</prevsent>
<prevsent>finally, another disambiguation method relying on graph algorithms that exploit the5algorithms that integrate the most frequent sense in wordnet are not considered here, since this represents supervised knowledge source (wordnet sense frequencies are derived from sense-annotated corpus).6note that the results are not directly comparable, since (gal ley and mckeown, 2003) used the wordnet sense order to break the ties, whereas we assume that such sense order frequency is not available, and thus we break the ties through random choice.
</prevsent>
</prevsection>
<citsent citstr=" C04-1162 ">
417structure of semantic networks was proposed in (mi halcea et al, 2004), <papid> C04-1162 </papid>with disambiguation accuracy of 50.9% measured on all the words in the senseval-2 data set.although it relies exclusively on dictionary definitions, the graph-based sequence data labeling algorithm proposed in this paper, with its overall performance of 54.2%, exceeds significantly the accuracy of all these previously proposed unsupervised word sense disambiguation methods, proving the benefits of taking into account label dependencies when annotating sequence data.</citsent>
<aftsection>
<nextsent>an additional interesting benefit of the algorithm is that it provides ranking over word senses, and thus the selection of two or more most probable senses for each word is also possible.
</nextsent>
<nextsent>we proposed graphical algorithm for sequence data labeling that relies on random walks on graphs encoding label dependencies.
</nextsent>
<nextsent>through the label graphs it builds forgiven sequence of words, the algorithm exploits relations between word labels, and implements concept of recommendation.
</nextsent>
<nextsent>a label recommends other related labels, and the strength of the recommendation is recursively computed based on the importance of the labels making the recommendation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J73">
<title id=" H05-1061.xml">mining key phrase translations from web corpora </title>
<section> extracting key phrase translation.  </section>
<citcontext>
<prevsection>
<prevsent>maps each chinese character into latin letter strings.
</prevsent>
<prevsent>this normalization makes the string alignment possible.
</prevsent>
</prevsection>
<citsent citstr=" W03-1502 ">
we adopt the transliteration model proposed in (huang, et al 2003).<papid> W03-1502 </papid></citsent>
<aftsection>
<nextsent>this model calculates the probabilistic levin stein distance between roman ized source string and target string.
</nextsent>
<nextsent>unlike the traditional levin stein distance calculation, the character alignment cost is not binary (0/1); rather it is the logarithm of character alignment probability, which ensures that characters with similar pronunciations (e.g. `p` and `b`) have higher alignment probabilities and lower cost.
</nextsent>
<nextsent>these probabilities are automatically learned from bilingual name lists using em.
</nextsent>
<nextsent>assume the chinese phrase has chinese characters, , and the english candidate phrase has english words, . the transliteration cost between chinese query and an english translation candidate is calculated as: jfff ,..., 21 leee ,...,, 21 e where is the pinyin of chinese character , is the th letter in , and and are their aligned english letters, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J74">
<title id=" H05-1061.xml">mining key phrase translations from web corpora </title>
<section> extracting key phrase translation.  </section>
<citcontext>
<prevsection>
<prevsent>jy jf ijy , jy jae ),( ijae )|( ,),( jiji yep 3.2 translation model.
</prevsent>
<prevsent>the translation model measures the semantic equivalence between chinese phrase and an english candidate.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
one widely used model is the ibm model (brown et al 1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>the phrase translation probability is computed using the ibm model-1 as: where is the lexical translation probabilities, which can be calculated according to the ibm models.
</nextsent>
<nextsent>this alignment model is asymmetric, as one source word can only be aligned to one target word, while one target word can be aligned to multiple source words.
</nextsent>
<nextsent>we estimate both and , and define the ne translation cost as: )|( lj efp )|( efp trans )|( feptrans ).|(log)|(log),( efpfepfec transtranstrans += 3.3 frequency-distance model.
</nextsent>
<nextsent>the more often bilingual phrase pair co-occurs, or the closer bilingual phrase pair is within snippet, the more likely they are translations of each other.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J75">
<title id=" H05-1079.xml">recognising textual entailment with logical inference </title>
<section> deep semantic analysis.  </section>
<citcontext>
<prevsection>
<prevsent>this is covered by three numerical features measuring the length of the text, of the hypothesis and the relative length of hypothesis with regard to the text.
</prevsent>
<prevsent>3.1 semantic interpretation.
</prevsent>
</prevsection>
<citsent citstr=" C04-1180 ">
we use robust wide-coverage ccg-parser (bos etal., 2004) <papid> C04-1180 </papid>to generate fine-grained semantic representations for each t/h-pair.</citsent>
<aftsection>
<nextsent>the semantic representation language is first-order fragment of the drs language used in discourse representation theory(kamp and reyle, 1993), conveying argument structure with neo-davidsonian analysis and including the recursive drs structure to cover negation, disjunction, and implication.
</nextsent>
<nextsent>consider for example: example: 78 (false) t: clintons new book is not big seller here.
</nextsent>
<nextsent>h: clintons book is big seller.
</nextsent>
<nextsent>drs(t): x1 x2 x3 book(x1) book(x2) ? x1=x2 clinton(x3) of(x1,x3) ? e4 x5 big(x5) seller(x5) be(e4) agent(e4,x1) patient(e4,x5) loc(e4,here) drs(h): x1 x2 e3 x4 book(x1) clinton(x2) of(x1,x2) big(x4) seller(x4) be(e3) agent(e3,x1) patient(e3,x4) 629 proper names and definite descriptions are treated as anaphoric, and bound to previously introduced discourse referents if possible, otherwise accommodated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J76">
<title id=" H05-1080.xml">a self learning context aware lemmatizer for german </title>
<section> lemmatization algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 inflection of german nouns.
</prevsent>
<prevsent>in german there are seven declensional suffixes for nouns: -s, -es, -e, -n, -er, and -ern (caumanns, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P98-2123 ">
these suffixes are due to the morphological 2http://lucene.apache.org/ 3the morphy system (lezius et al, 1998) <papid> P98-2123 </papid>is described as freely available,?</citsent>
<aftsection>
<nextsent>but in fact is closed-source, binary-only, non changeable software.
</nextsent>
<nextsent>it is also no longer being maintained.
</nextsent>
<nextsent>636 class features remove suffix {sg}?
</nextsent>
<nextsent>{gen} ?{mascfemneut} none ii {sg}?{gen} ?{mascneut} -es or -s iii {pl}?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J78">
<title id=" H05-1080.xml">a self learning context aware lemmatizer for german </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>as shown in the example below, the lexicon can have wrong entries and entering word with more than one lemma, which is an inflectional form of word that has wrong entry, will not be assigned with the correct lemma because the procedure that updates the lemma will assign possible lemma candidates to this word.
</prevsent>
<prevsent>if word that has wrong entry in the lexicon will be entered again with the correct lemma, the word itself and all its inflectional forms will be updated with the correct lemma: 640 current state of the lexicon (lemma only) jahr jahr jahre jahre (wrong) new entry jahren jahre.jahr state of the lexicon after update jahr jahr jahre jahre (wrong) jahren jahre.jahr (two possibilities) new entry jahre jahr (correct lemmatization) state of the lexicon after update jahr jahr jahre jahr jahren jahr
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
the lemmatization algorithm and the lexicon have been implemented based on the gate architecture(cunningham et al, 2002).<papid> P02-1022 </papid></citsent>
<aftsection>
<nextsent>gate provides an infrastructure for developing and deploying software components that process human language.
</nextsent>
<nextsent>for the german pos tagger we currently use the treetag ger (schmid, 1995).
</nextsent>
<nextsent>the other main resource is multi-lingual base np chunker implemented within the jape language.
</nextsent>
<nextsent>the negra corpus version 2 (skut et al, 1998) based on approximately 70 000 tokens tagged with morphological features has been used to train the case tagger.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J79">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous cohesion based segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance.
</prevsent>
<prevsent>a novel moving window technique using ortho normal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
ever since morris and hirst (1991)<papid> J91-1002 </papid>s groundbreaking paper, topic segmentation has been steadily growing research area in computational linguistics, with applications in summarization (barzilay and elhadad, 1997), <papid> W97-0703 </papid>information retrieval (salton and allan, 1994), and text understanding (kozima, 1993).<papid> P93-1041 </papid></citsent>
<aftsection>
<nextsent>topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.
</nextsent>
<nextsent>there have been essentially two approaches to topic segmentation in the past.
</nextsent>
<nextsent>the first of these, lexical cohesion, may be used for either linear segmentation (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997) <papid> J97-1003 </papid>or hierarchical segmentation (yarri, 1997; choi, 2000).<papid> A00-2004 </papid></nextsent>
<nextsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J80">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous cohesion based segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance.
</prevsent>
<prevsent>a novel moving window technique using ortho normal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task.
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
ever since morris and hirst (1991)<papid> J91-1002 </papid>s groundbreaking paper, topic segmentation has been steadily growing research area in computational linguistics, with applications in summarization (barzilay and elhadad, 1997), <papid> W97-0703 </papid>information retrieval (salton and allan, 1994), and text understanding (kozima, 1993).<papid> P93-1041 </papid></citsent>
<aftsection>
<nextsent>topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.
</nextsent>
<nextsent>there have been essentially two approaches to topic segmentation in the past.
</nextsent>
<nextsent>the first of these, lexical cohesion, may be used for either linear segmentation (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997) <papid> J97-1003 </papid>or hierarchical segmentation (yarri, 1997; choi, 2000).<papid> A00-2004 </papid></nextsent>
<nextsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J81">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous cohesion based segmentation methods that focus on expository monologue are reapplied to these dialogues to create benchmarks for performance.
</prevsent>
<prevsent>a novel moving window technique using ortho normal bases of semantic vectors significantly outperforms these benchmarks on this dialogue segmentation task.
</prevsent>
</prevsection>
<citsent citstr=" P93-1041 ">
ever since morris and hirst (1991)<papid> J91-1002 </papid>s groundbreaking paper, topic segmentation has been steadily growing research area in computational linguistics, with applications in summarization (barzilay and elhadad, 1997), <papid> W97-0703 </papid>information retrieval (salton and allan, 1994), and text understanding (kozima, 1993).<papid> P93-1041 </papid></citsent>
<aftsection>
<nextsent>topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.
</nextsent>
<nextsent>there have been essentially two approaches to topic segmentation in the past.
</nextsent>
<nextsent>the first of these, lexical cohesion, may be used for either linear segmentation (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997) <papid> J97-1003 </papid>or hierarchical segmentation (yarri, 1997; choi, 2000).<papid> A00-2004 </papid></nextsent>
<nextsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J83">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.
</prevsent>
<prevsent>there have been essentially two approaches to topic segmentation in the past.
</prevsent>
</prevsection>
<citsent citstr=" J97-1003 ">
the first of these, lexical cohesion, may be used for either linear segmentation (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997) <papid> J97-1003 </papid>or hierarchical segmentation (yarri, 1997; choi, 2000).<papid> A00-2004 </papid></citsent>
<aftsection>
<nextsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.
</nextsent>
<nextsent>therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.
</nextsent>
<nextsent>the second major approach to topic segmentation looks for distinctive textual or acoustic markers of topic boundaries, e.g. referential noun phrases or pauses (passonneau and litman, 1993; <papid> P93-1020 </papid>passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
<nextsent>by using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have higher accuracy than methods using single marker alone (passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J84">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>topic segmentation likewise has multiple educational applications, such as question answering, detecting student initiative, and assessing student answers.
</prevsent>
<prevsent>there have been essentially two approaches to topic segmentation in the past.
</prevsent>
</prevsection>
<citsent citstr=" A00-2004 ">
the first of these, lexical cohesion, may be used for either linear segmentation (morris and hirst, 1991; <papid> J91-1002 </papid>hearst, 1997) <papid> J97-1003 </papid>or hierarchical segmentation (yarri, 1997; choi, 2000).<papid> A00-2004 </papid></citsent>
<aftsection>
<nextsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.
</nextsent>
<nextsent>therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.
</nextsent>
<nextsent>the second major approach to topic segmentation looks for distinctive textual or acoustic markers of topic boundaries, e.g. referential noun phrases or pauses (passonneau and litman, 1993; <papid> P93-1020 </papid>passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
<nextsent>by using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have higher accuracy than methods using single marker alone (passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J85">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.
</prevsent>
<prevsent>therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.
</prevsent>
</prevsection>
<citsent citstr=" P93-1020 ">
the second major approach to topic segmentation looks for distinctive textual or acoustic markers of topic boundaries, e.g. referential noun phrases or pauses (passonneau and litman, 1993; <papid> P93-1020 </papid>passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>by using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have higher accuracy than methods using single marker alone (passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
<nextsent>the primary technique used in previous studies, lexical cohesion, is no stranger to the educational nlp community.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J86">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the essential idea behind the lexical cohesion approaches is that different topics will have different vocabularies.
</prevsent>
<prevsent>therefore the lexical cohesion within topics will be higher than the lexical cohesion between topics, and gaps in cohesion may mark topic boundaries.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
the second major approach to topic segmentation looks for distinctive textual or acoustic markers of topic boundaries, e.g. referential noun phrases or pauses (passonneau and litman, 1993; <papid> P93-1020 </papid>passonneau and litman, 1997).<papid> J97-1005 </papid></citsent>
<aftsection>
<nextsent>by using multiple markers and machine learning methods, topic segmentation algorithms may be developed using this second approach that have higher accuracy than methods using single marker alone (passonneau and litman, 1997).<papid> J97-1005 </papid></nextsent>
<nextsent>the primary technique used in previous studies, lexical cohesion, is no stranger to the educational nlp community.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J90">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the focus on dialogue distinguishes our work from virtually all previous work on topic segmen tation: prior studies have focused on monologue rather than dialogue.
</prevsent>
<prevsent>without dialogue, previous approaches have only limited relevance to interactive educational applications such as intelligent tutoring systems (its).
</prevsent>
</prevsection>
<citsent citstr=" P03-1071 ">
the only existing work on topic segmentation in dialogue, galley et al  (2003), <papid> P03-1071 </papid>segments recorded speech between multiple persons using both lexical cohesion and dis 971 tinctive textual and acoustic markers.</citsent>
<aftsection>
<nextsent>the present work differs from galley et al  (2003) <papid> P03-1071 </papid>in two respects, viz.</nextsent>
<nextsent>we focus solely on textual information and we directly address the problem of tutorial dia logue.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J92">
<title id=" H05-1122.xml">an ortho normal basis for topic segmentation in tutorial dialogue </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the present work differs from galley et al  (2003) <papid> P03-1071 </papid>in two respects, viz.</prevsent>
<prevsent>we focus solely on textual information and we directly address the problem of tutorial dia logue.</prevsent>
</prevsection>
<citsent citstr=" P94-1002 ">
in this study we apply the methods of foltz et al  (1998), hearst (1994), <papid> P94-1002 </papid>hearst (1997)<papid> J97-1003 </papid>and new technique utilizing an ortho normal basis to topic segmentation of tutorial dialogue.</citsent>
<aftsection>
<nextsent>all three are vector space methods that measure lexical cohesion to determine topic shifts.
</nextsent>
<nextsent>our results show that the new using an ortho normal basis significantly outperforms the other methods.
</nextsent>
<nextsent>section 2 reviews previous work, and section 3 reviews the vector space model.
</nextsent>
<nextsent>section 4 introduces an extension of the vector space model which uses an ortho normal basis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J245">
<title id=" E93-1028.xml">similarity between words computed by spreading activation on an english dictionary </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>a text is not just sequence of words, but it also has coherent structure.
</prevsent>
<prevsent>the meaning of each word in text depends on the structure of the text.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
recogniz-ing the structure of text is an essential task in text understanding.\[grosz andsidner, 1986\] <papid> J86-3001 </papid>one of the valuable indicators of the structure of text is lexical cohesion.\[halliday nd hasan, 1976\] lexical cohesion is the relationship between words, classified as follows: 1.</citsent>
<aftsection>
<nextsent>reiteration:.
</nextsent>
<nextsent>molly likes cats.
</nextsent>
<nextsent>she keeps cat.
</nextsent>
<nextsent>a.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J246">
<title id=" E93-1028.xml">similarity between words computed by spreading activation on an english dictionary </title>
<section> semantic relation:.  </section>
<citcontext>
<prevsection>
<prevsent>similarity between words can be defined by either syntagmatic or paradigmatic relation.
</prevsent>
<prevsent>syntagmatic similarity is based on co-occurrence data extracted from corpora \[church and hanks, 1990\], definitions in dictionaries \[wilks etal., 1989\], and so on.
</prevsent>
</prevsection>
<citsent citstr=" J91-1002 ">
paradigmatic similarity is based on association data extracted from thesauri \[morris and hirst, 1991\], <papid> J91-1002 </papid>psychological experiments \[osgood, 1952\], and so on.</citsent>
<aftsection>
<nextsent>this paper concentrates on paradigmatic similar-ity, because paradigmatic relation can be estab-lished both inside sentence and across sentence boundaries, while syntagmatic relations can be seen mainly inside sentence -- like syntax deals with sentence structure.
</nextsent>
<nextsent>the rest of this section fo-cuses on two related works on measuring paradig- matic similarity -- psycho linguistic approach and thesaurus-based approach.
</nextsent>
<nextsent>2.1 psycho linguistic approach psycho linguists have been proposed methods for measuring similarity.
</nextsent>
<nextsent>one of the pioneering works is  semantic differential  \[osgood, 1952\] which anal-yses meaning of words into range of different di-mensions with the opposed adjectives at both ends (see figure 1), and locates the words in the semantic space.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J249">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by adapting transformation based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as pos tags.
</prevsent>
<prevsent>we show that our alignment link projection approach yields significantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on english spanish data and 23.2% relative reduction on english-chinese data).
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
word-level alignment is critical component of awide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when thereis enough training data, the quality of word alignment decreases as the size of training data decreases.
</nextsent>
<nextsent>moreover, word-alignment systems are of ten tripped up by many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).
</nextsent>
<nextsent>at the heart of the matter is set of assumptions that word-alignment algorithms must make in orderto reduce the hypothesis space, since word alignment is an exponential problem.
</nextsent>
<nextsent>because of these assumptions, learning algorithms tend to make similar errors throughout the entire data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J250">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by adapting transformation based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as pos tags.
</prevsent>
<prevsent>we show that our alignment link projection approach yields significantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on english spanish data and 23.2% relative reduction on english-chinese data).
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
word-level alignment is critical component of awide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when thereis enough training data, the quality of word alignment decreases as the size of training data decreases.
</nextsent>
<nextsent>moreover, word-alignment systems are of ten tripped up by many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).
</nextsent>
<nextsent>at the heart of the matter is set of assumptions that word-alignment algorithms must make in orderto reduce the hypothesis space, since word alignment is an exponential problem.
</nextsent>
<nextsent>because of these assumptions, learning algorithms tend to make similar errors throughout the entire data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J251">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>by adapting transformation based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as pos tags.
</prevsent>
<prevsent>we show that our alignment link projection approach yields significantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on english spanish data and 23.2% relative reduction on english-chinese data).
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
word-level alignment is critical component of awide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when thereis enough training data, the quality of word alignment decreases as the size of training data decreases.
</nextsent>
<nextsent>moreover, word-alignment systems are of ten tripped up by many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).
</nextsent>
<nextsent>at the heart of the matter is set of assumptions that word-alignment algorithms must make in orderto reduce the hypothesis space, since word alignment is an exponential problem.
</nextsent>
<nextsent>because of these assumptions, learning algorithms tend to make similar errors throughout the entire data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J252">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alp starts with an initial alignment and then fills out (i.e., projects) new word-level alignment relations (i.e., links) from existing alignment relations.
</prevsent>
<prevsent>alp then deletes certain alignment links associated with common errors, thus improving precision and recall.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
in our approach, we adapt transformation-basedlearning (tbl) (brill, 1995; <papid> J95-4004 </papid>brill, 1996) to the problem of word alignment.</citsent>
<aftsection>
<nextsent>alp attempts to find an ordered list of transformation rules (within pre specified search space) to improve baseline annotation.
</nextsent>
<nextsent>the rules decompose the search space into set of consecutive words (windows) within which alignment links are added, to or deleted from, the initial alignment.
</nextsent>
<nextsent>this window-based approach exploits the clustering tendency of alignment links, i.e., when there is link between two words, there is frequently another link in close proximity.
</nextsent>
<nextsent>tbl is an appropriate choice for this problem for the following reasons: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J253">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 compares alp to various alignments and presents results on english-spanish and englishchinese.
</prevsent>
<prevsent>we show that alp yields significant reductions in alignment error rate over that of the best performing alignment system.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
one of the major problems with the ibm models (brown et al, 1993) <papid> J93-2003 </papid>and the hmm models (vogel et al., 1996) <papid> C96-2141 </papid>is that they are restricted to the alignment of each source-language word to at most one target language word.</citsent>
<aftsection>
<nextsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</nextsent>
<nextsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J254">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>section 5 compares alp to various alignments and presents results on english-spanish and englishchinese.
</prevsent>
<prevsent>we show that alp yields significant reductions in alignment error rate over that of the best performing alignment system.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
one of the major problems with the ibm models (brown et al, 1993) <papid> J93-2003 </papid>and the hmm models (vogel et al., 1996) <papid> C96-2141 </papid>is that they are restricted to the alignment of each source-language word to at most one target language word.</citsent>
<aftsection>
<nextsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</nextsent>
<nextsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J255">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we show that alp yields significant reductions in alignment error rate over that of the best performing alignment system.
</prevsent>
<prevsent>one of the major problems with the ibm models (brown et al, 1993) <papid> J93-2003 </papid>and the hmm models (vogel et al., 1996) <papid> C96-2141 </papid>is that they are restricted to the alignment of each source-language word to at most one target language word.</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</citsent>
<aftsection>
<nextsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.
</nextsent>
<nextsent>these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></nextsent>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J256">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we show that alp yields significant reductions in alignment error rate over that of the best performing alignment system.
</prevsent>
<prevsent>one of the major problems with the ibm models (brown et al, 1993) <papid> J93-2003 </papid>and the hmm models (vogel et al., 1996) <papid> C96-2141 </papid>is that they are restricted to the alignment of each source-language word to at most one target language word.</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</citsent>
<aftsection>
<nextsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.
</nextsent>
<nextsent>these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></nextsent>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J257">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</prevsent>
<prevsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.
</nextsent>
<nextsent>these methods perform better than the ibm models and their variants but still tend to make similar errors because of the bias in their alignment modeling.
</nextsent>
<nextsent>we adopt an approach that post-processes given alignment using linguistically-oriented rules.
</nextsent>
<nextsent>the idea is similar to that of ayan et al (2004), wheremanually-crafted rules are used to correct alignment links related to language divergences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J258">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</prevsent>
<prevsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.
</nextsent>
<nextsent>these methods perform better than the ibm models and their variants but still tend to make similar errors because of the bias in their alignment modeling.
</nextsent>
<nextsent>we adopt an approach that post-processes given alignment using linguistically-oriented rules.
</nextsent>
<nextsent>the idea is similar to that of ayan et al (2004), wheremanually-crafted rules are used to correct alignment links related to language divergences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J259">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</prevsent>
<prevsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.
</nextsent>
<nextsent>these methods perform better than the ibm models and their variants but still tend to make similar errors because of the bias in their alignment modeling.
</nextsent>
<nextsent>we adopt an approach that post-processes given alignment using linguistically-oriented rules.
</nextsent>
<nextsent>the idea is similar to that of ayan et al (2004), wheremanually-crafted rules are used to correct alignment links related to language divergences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J260">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the standard method to overcome this problem to use the model in both directions (interchanging the source and target languages) and applying heuristic-based combination techniques to produce refined alignment (och and ney, 2000; <papid> P00-1056 </papid>koehn et al, 2003)<papid> N03-1017 </papid>henceforth referred to as ra.?</prevsent>
<prevsent>several researchers have proposed algorithms for improving word alignment systems by injecting additional knowledge or combining different alignment models.</prevsent>
</prevsection>
<citsent citstr=" P05-1057 ">
these approaches include an enhanced hmm alignment model that uses part-of speech tags (toutanova et al, 2002), <papid> W02-1012 </papid>log-linear combination of ibm translation models and hmm models (och and ney, 2003), <papid> J03-1002 </papid>techniques that relyon dependency relations (cherry and lin, 2003),<papid> P03-1012 </papid>and log-linear combination of ibm model 3 alignment probabilities, pos tags, and bilingual dictionary coverage (liu et al, 2005).<papid> P05-1057 </papid></citsent>
<aftsection>
<nextsent>a common theme for these methods is the use of additional features for enriching the alignment process.
</nextsent>
<nextsent>these methods perform better than the ibm models and their variants but still tend to make similar errors because of the bias in their alignment modeling.
</nextsent>
<nextsent>we adopt an approach that post-processes given alignment using linguistically-oriented rules.
</nextsent>
<nextsent>the idea is similar to that of ayan et al (2004), wheremanually-crafted rules are used to correct alignment links related to language divergences.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J263">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> alignment link projection (alp).  </section>
<citcontext>
<prevsection>
<prevsent>thus, alp adds new links based on linguistic features of words, rather than the words themselves.
</prevsent>
<prevsent>using these features is what sets alp apart from systems like the ra approach.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
specifically, three features are used to instantiate the tem plates: ? pos tags on both sides: we assign pos tags using the mxpost tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>for english and chinese, and conn exor for spanish.?</citsent>
<aftsection>
<nextsent>dependency relations: alp utilizes dependencies for better generalization if dependency parser is available in either language.
</nextsent>
<nextsent>in our experiments, we used dependency parser only in english (a version of the collins parser (collins, 1997) <papid> P97-1003 </papid>that has been adapted for building dependencies) but not in the other language.?</nextsent>
<nextsent>a set of closed-class words: we use 16 different classes, 9 of which are different semantic verb classes while the other 7 are function words, prepositions, and complementizers.5 if both pos tags and dependency relations are available, they can be used together to instantiate the templates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J264">
<title id=" H05-1024.xml">alignment link projection using transformation based learning </title>
<section> alignment link projection (alp).  </section>
<citcontext>
<prevsection>
<prevsent>specifically, three features are used to instantiate the tem plates: ? pos tags on both sides: we assign pos tags using the mxpost tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>for english and chinese, and conn exor for spanish.?</prevsent>
<prevsent>dependency relations: alp utilizes dependencies for better generalization if dependency parser is available in either language.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
in our experiments, we used dependency parser only in english (a version of the collins parser (collins, 1997) <papid> P97-1003 </papid>that has been adapted for building dependencies) but not in the other language.?</citsent>
<aftsection>
<nextsent>a set of closed-class words: we use 16 different classes, 9 of which are different semantic verb classes while the other 7 are function words, prepositions, and complementizers.5 if both pos tags and dependency relations are available, they can be used together to instantiate the templates.
</nextsent>
<nextsent>that is, word can be instantiated in tbl template with: (1) pos tag (e.g., noun, adj); (2) relation (e.g., subj, obj); (3) parameter class (e.g., change of state); or (4) different subsets of (1)?(3).
</nextsent>
<nextsent>we also employ more generalized form of instantiation, where words in the templates may match the keyword anything.
</nextsent>
<nextsent>4.4 best rule selection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J266">
<title id=" E99-1037.xml">dialogue processing in a call system </title>
<section> discourse grammars.  </section>
<citcontext>
<prevsection>
<prevsent>the following figure shows simplified part of discourse grammar, which models an information gathering dialogue such as is necessary in the case of collecting information about an accident.
</prevsent>
<prevsent>ad-ditional items of discourse grammars are of course needed, for example, to start and end telephone call, etc. the same type of structures is also used in the analysis of dialogues, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
(carletta et al, 1997).<papid> J97-1002 </papid></citsent>
<aftsection>
<nextsent>here dialogues are analysed with the help of  dialogue structure coding scheme , which in particular contains only limited number of pos-sible moves between dialogue partners.
</nextsent>
<nextsent>a similar analysis was done in the preparational phase of the verb mobil project (alexandersson etal., 1994).
</nextsent>
<nextsent>in dialogue system where the intentions of the dia-logue partners are known and the fixed structures serve to assess the performance of the language learner, the restrictions will probably not make the overall behaviour of the system worse than more flexible dialogue systems.
</nextsent>
<nextsent>in o_gather question open questions / ~ interpret able uninterpretable answer answer confirms_.__.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J267">
<title id=" E99-1037.xml">dialogue processing in a call system </title>
<section> system overview.  </section>
<citcontext>
<prevsection>
<prevsent>it organizes the flow of the input data be-tween the user-interface and the various process-ing modules.
</prevsent>
<prevsent>every input sentence is first passed to the linguistic module, which checks it for ortho-graphic and syntactic errors.
</prevsent>
</prevsection>
<citsent citstr=" J96-1003 ">
the orthographic check is done in the spirit of oflazer (1996).<papid> J96-1003 </papid></citsent>
<aftsection>
<nextsent>with 255 proceedings of eacl  99 the help of finite state recognizer mildly devi-ating strings are identified and correct versions are presented to the learner if necessary.
</nextsent>
<nextsent>the syntactic heck follows rather traditional path.
</nextsent>
<nextsent>the main work is done by lfg-parser (bresnan and kaplan, 1982), originally implemented by av-ery andrews (australian national university) and now modified to suit the needs of error detection with the help of modified grammar processing in-cluding error rules (kriiger et al, 1997).
</nextsent>
<nextsent>as next step the analysis of the sentence is checked against world knowledge base, from which feed- back follows to the learner if the sentence does not match the internal model of the world.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J268">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe method for identifying systematic patterns in translation data using part-of speech tag sequences.
</prevsent>
<prevsent>we incorporate this analysis into diagnostic tool intended forde velopers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
over the last few years, several automatic metrics forma chine translation (mt) evaluation have been introduced,largely to reduce the human cost of iterative system evaluation during the development cycle (papineni et al, 2002;<papid> P02-1040 </papid>melamed et al, 2003).<papid> N03-2021 </papid></citsent>
<aftsection>
<nextsent>all are predicated on the concept of n-gram matching between the sentence hypothesized by the translation system and one or more reference translations that is, human translations for the test sentence.
</nextsent>
<nextsent>although the formulae underlying these metrics vary, each produces single number representing the goodness?
</nextsent>
<nextsent>of the mt system output over set of reference documents.
</nextsent>
<nextsent>we can compare the numbers of competing systems to get coarse estimate of their relative performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J269">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we describe method for identifying systematic patterns in translation data using part-of speech tag sequences.
</prevsent>
<prevsent>we incorporate this analysis into diagnostic tool intended forde velopers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
over the last few years, several automatic metrics forma chine translation (mt) evaluation have been introduced,largely to reduce the human cost of iterative system evaluation during the development cycle (papineni et al, 2002;<papid> P02-1040 </papid>melamed et al, 2003).<papid> N03-2021 </papid></citsent>
<aftsection>
<nextsent>all are predicated on the concept of n-gram matching between the sentence hypothesized by the translation system and one or more reference translations that is, human translations for the test sentence.
</nextsent>
<nextsent>although the formulae underlying these metrics vary, each produces single number representing the goodness?
</nextsent>
<nextsent>of the mt system output over set of reference documents.
</nextsent>
<nextsent>we can compare the numbers of competing systems to get coarse estimate of their relative performance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J272">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> part-of-speech sequence recall.  </section>
<citcontext>
<prevsection>
<prevsent>what we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with.
</prevsent>
<prevsent>word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
in developing new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains high degree of flexibility (fox,2002; <papid> W02-1039 </papid>hwa et al, 2002).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>this suggests that in comparative analysis of two mt systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using syntax based, recall-oriented metric.as an initial step, we would like to summarize reordering patterns using part-of-speech sequences.
</nextsent>
<nextsent>unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text tothe noisy output of mt systems produces un useable results (e.g.
</nextsent>
<nextsent>(och et al, 2004)).<papid> N04-1021 </papid></nextsent>
<nextsent>therefore, we make the conservative choice to apply annotation only to the reference corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J273">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> part-of-speech sequence recall.  </section>
<citcontext>
<prevsection>
<prevsent>what we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with.
</prevsent>
<prevsent>word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
in developing new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains high degree of flexibility (fox,2002; <papid> W02-1039 </papid>hwa et al, 2002).<papid> P02-1050 </papid></citsent>
<aftsection>
<nextsent>this suggests that in comparative analysis of two mt systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using syntax based, recall-oriented metric.as an initial step, we would like to summarize reordering patterns using part-of-speech sequences.
</nextsent>
<nextsent>unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text tothe noisy output of mt systems produces un useable results (e.g.
</nextsent>
<nextsent>(och et al, 2004)).<papid> N04-1021 </papid></nextsent>
<nextsent>therefore, we make the conservative choice to apply annotation only to the reference corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J274">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> part-of-speech sequence recall.  </section>
<citcontext>
<prevsection>
<prevsent>this suggests that in comparative analysis of two mt systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using syntax based, recall-oriented metric.as an initial step, we would like to summarize reordering patterns using part-of-speech sequences.
</prevsent>
<prevsent>unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text tothe noisy output of mt systems produces un useable results (e.g.
</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
(och et al, 2004)).<papid> N04-1021 </papid></citsent>
<aftsection>
<nextsent>therefore, we make the conservative choice to apply annotation only to the reference corpus.
</nextsent>
<nextsent>word n-gram correspondences with reference translation are used to infer the part-of-speech tags for words in the system output.
</nextsent>
<nextsent>the method: 1.
</nextsent>
<nextsent>part-of-speech tag the reference corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J275">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> part-of-speech sequence recall.  </section>
<citcontext>
<prevsection>
<prevsent>we used.
</prevsent>
<prevsent>12 figure 1: comparing two systems that differ significantly in their recall for pos n-gram jj nn in dt nn.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
the interface uses color to make examples easy to find.mxpost (ratnaparkhi, 1996), <papid> W96-0213 </papid>and in order to discover more general patterns, we map the tag set down after tagging, e.g. nn, nnp, nnps and nns all map to nn.</citsent>
<aftsection>
<nextsent>2.
</nextsent>
<nextsent>compute the frequency freq(ti . . .
</nextsent>
<nextsent>t j) of every possi-.
</nextsent>
<nextsent>ble tag sequence ti . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J278">
<title id=" H05-2007.xml">pattern visualization for machine translation output </title>
<section> part-of-speech sequence recall.  </section>
<citcontext>
<prevsection>
<prevsent>t j)/freq(ti . . .
</prevsent>
<prevsent>t j), for all patterns in the corpus.to compare two systems (which could include two versions of the same system), we identify pos n-grams that are recalled significantly more frequently by one system than the other, using difference-of-proportions test to assess statistical significance.
</prevsent>
</prevsection>
<citsent citstr=" H05-1098 ">
we have used this method to analyze the output of two different statistical machine translation models (chiang et al, 2005).<papid> H05-1098 </papid></citsent>
<aftsection>
<nextsent>our demonstration system uses an html interface to summarize the observed pattern recall.
</nextsent>
<nextsent>based on frequent or significantly-different recall, the user can select and visually inspect color-coded examples of each pattern of interest in context with both source and reference sentences.
</nextsent>
<nextsent>an example visualization is shown in figure 1.
</nextsent>
<nextsent>the authors would like to thank david chiang, christof monz, and michael subotin for helpful commentary on this work.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J279">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recent interest in question answering has shifted away from factoid questions such as what city is the home to the rock and roll hall of fame??, which can typically be answered by short noun phrase, to more complex and difficult questions.one interesting class of information needs concerns so-called definition questions such as who is vlad the impaler??, whose answers would include nuggets?
</prevsent>
<prevsent>of information about the 16th century warrior princes life, accomplishments, and legacy.
</prevsent>
</prevsection>
<citsent citstr=" P04-1073 ">
actually misnomer, definition questions can be better paraphrased as tell me interesting things about x.?, where can be person, an organization, common noun, etc. taken another way, definition questions might be viewed as simultaneously asking whole series of factoid questions about the same entity (e.g., when was he born??, what was his occupation??, where did he live??, etc.), except that these questions are not known in advance; see prager et al (2004) <papid> P04-1073 </papid>for an implementation based on this view of definition questions.</citsent>
<aftsection>
<nextsent>much progress in natural language processing and information retrieval has been driven by the creation of reusable test collections.
</nextsent>
<nextsent>a test collection consists of corpus, series of well-defined tasks, and set of judgments indicating the correct answers?.to complete the picture, there must exist meaningful metrics to evaluate progress, and ideally, machine should be able to compute these values automatically.
</nextsent>
<nextsent>although answers?
</nextsent>
<nextsent>to definition questions are known, there is no way to automatically and objectively determine if they are present in given systems response (we will discuss why in section 2).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J280">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> evaluating definition questions.  </section>
<citcontext>
<prevsection>
<prevsent>vital nuggets represent concepts that must be present in good?
</prevsent>
<prevsent>definition;on the other hand, okay nuggets contribute worthwhile information about the target but are not essen tial; cf.
</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
(hildebrandt et al, 2004).<papid> N04-1007 </papid></citsent>
<aftsection>
<nextsent>as an example, nuggets for the question what is the cassini space probe??
</nextsent>
<nextsent>are shown in table 1.once this answer key of vital/okay nuggets is created, the assessor then manually scores each run.
</nextsent>
<nextsent>for each system response, he or she decides whether ornot each nugget is present.
</nextsent>
<nextsent>assessors do not simply perform string matches in this decision process; rather, this matching occurs at the conceptual level, abstracting away from issues such as vocabulary differences, syntactic divergences, paraphrases, etc. two examples of this matching process are shown in figure 1: nuggets 1 and 2 were found in the top passage, while nuggets 4, 5, and 6 were found in the bottom passage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J282">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>in system response, given that they were usually extracted text fragments from documents (voorhees,2003).
</prevsent>
<prevsent>thus, penalty for verbosity serves assur rogate for precision.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the idea of employing n-gram co-occurrence statistics to score the output of computer system again stone or more desired reference outputs was first successfully implemented in the bleu metric forma chine translation (papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>since then, the basic method for scoring translation quality has been improved upon by others, e.g., (babych and hartley, 2004; <papid> P04-1079 </papid>lin and och, 2004).<papid> C04-1072 </papid></nextsent>
<nextsent>the basic ideahas been extended to evaluating document summarization with rouge (lin and hovy, 2003).<papid> N03-1020 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J283">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, penalty for verbosity serves assur rogate for precision.
</prevsent>
<prevsent>the idea of employing n-gram co-occurrence statistics to score the output of computer system again stone or more desired reference outputs was first successfully implemented in the bleu metric forma chine translation (papineni et al, 2002).<papid> P02-1040 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1079 ">
since then, the basic method for scoring translation quality has been improved upon by others, e.g., (babych and hartley, 2004; <papid> P04-1079 </papid>lin and och, 2004).<papid> C04-1072 </papid></citsent>
<aftsection>
<nextsent>the basic ideahas been extended to evaluating document summarization with rouge (lin and hovy, 2003).<papid> N03-1020 </papid></nextsent>
<nextsent>933recently, soricut and brill (2004) <papid> P04-1078 </papid>employed ngram co-occurrences to evaluate question answering in faq domain; unfortunately, the task differs from definition question answering, making their results not directly applicable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J284">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>thus, penalty for verbosity serves assur rogate for precision.
</prevsent>
<prevsent>the idea of employing n-gram co-occurrence statistics to score the output of computer system again stone or more desired reference outputs was first successfully implemented in the bleu metric forma chine translation (papineni et al, 2002).<papid> P02-1040 </papid></prevsent>
</prevsection>
<citsent citstr=" C04-1072 ">
since then, the basic method for scoring translation quality has been improved upon by others, e.g., (babych and hartley, 2004; <papid> P04-1079 </papid>lin and och, 2004).<papid> C04-1072 </papid></citsent>
<aftsection>
<nextsent>the basic ideahas been extended to evaluating document summarization with rouge (lin and hovy, 2003).<papid> N03-1020 </papid></nextsent>
<nextsent>933recently, soricut and brill (2004) <papid> P04-1078 </papid>employed ngram co-occurrences to evaluate question answering in faq domain; unfortunately, the task differs from definition question answering, making their results not directly applicable.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J285">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the idea of employing n-gram co-occurrence statistics to score the output of computer system again stone or more desired reference outputs was first successfully implemented in the bleu metric forma chine translation (papineni et al, 2002).<papid> P02-1040 </papid></prevsent>
<prevsent>since then, the basic method for scoring translation quality has been improved upon by others, e.g., (babych and hartley, 2004; <papid> P04-1079 </papid>lin and och, 2004).<papid> C04-1072 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the basic ideahas been extended to evaluating document summarization with rouge (lin and hovy, 2003).<papid> N03-1020 </papid></citsent>
<aftsection>
<nextsent>933recently, soricut and brill (2004) <papid> P04-1078 </papid>employed ngram co-occurrences to evaluate question answering in faq domain; unfortunately, the task differs from definition question answering, making their results not directly applicable.</nextsent>
<nextsent>xu et al (2004) applied rouge to automatically evaluate answers to definition questions, viewing the task as variation of document summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J286">
<title id=" H05-1117.xml">automatically evaluating answers to definition questions </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>since then, the basic method for scoring translation quality has been improved upon by others, e.g., (babych and hartley, 2004; <papid> P04-1079 </papid>lin and och, 2004).<papid> C04-1072 </papid></prevsent>
<prevsent>the basic ideahas been extended to evaluating document summarization with rouge (lin and hovy, 2003).<papid> N03-1020 </papid></prevsent>
</prevsection>
<citsent citstr=" P04-1078 ">
933recently, soricut and brill (2004) <papid> P04-1078 </papid>employed ngram co-occurrences to evaluate question answering in faq domain; unfortunately, the task differs from definition question answering, making their results not directly applicable.</citsent>
<aftsection>
<nextsent>xu et al (2004) applied rouge to automatically evaluate answers to definition questions, viewing the task as variation of document summarization.
</nextsent>
<nextsent>because trec answer nuggets were terse phrases, the authors found it necessary to rephrase themtwo humans were asked to manually create reference answers?
</nextsent>
<nextsent>based on theassessors?
</nextsent>
<nextsent>nuggets and ir results, which was labor intensive process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J289">
<title id=" E95-1037.xml">topic identification in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the postulation (2) could be also observed from the above example.
</prevsent>
<prevsent>these relationships may be represented implicitly by collocational semantics.
</prevsent>
</prevsection>
<citsent citstr=" H89-2012 ">
collocation has been applied successfully to many possible applications (church et al , 1989), <papid> H89-2012 </papid>e.g, lexicography (church and hanks, 1990), <papid> J90-1003 </papid>information retrieval (salton, 1986<papid> C86-1090 </papid>a), text input (yamashina nd obashi, 1988), etc. this paper will touch on its feasibility in topic identification.</citsent>
<aftsection>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2 presents corpus-based language model and discuss how to train this model.
</nextsent>
<nextsent>section 3 touches on topic identification discourse.
</nextsent>
<nextsent>section 4 shows series of experiments based on the proposed model and discusses the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J290">
<title id=" E95-1037.xml">topic identification in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the postulation (2) could be also observed from the above example.
</prevsent>
<prevsent>these relationships may be represented implicitly by collocational semantics.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
collocation has been applied successfully to many possible applications (church et al , 1989), <papid> H89-2012 </papid>e.g, lexicography (church and hanks, 1990), <papid> J90-1003 </papid>information retrieval (salton, 1986<papid> C86-1090 </papid>a), text input (yamashina nd obashi, 1988), etc. this paper will touch on its feasibility in topic identification.</citsent>
<aftsection>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2 presents corpus-based language model and discuss how to train this model.
</nextsent>
<nextsent>section 3 touches on topic identification discourse.
</nextsent>
<nextsent>section 4 shows series of experiments based on the proposed model and discusses the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J291">
<title id=" E95-1037.xml">topic identification in discourse </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the postulation (2) could be also observed from the above example.
</prevsent>
<prevsent>these relationships may be represented implicitly by collocational semantics.
</prevsent>
</prevsection>
<citsent citstr=" C86-1090 ">
collocation has been applied successfully to many possible applications (church et al , 1989), <papid> H89-2012 </papid>e.g, lexicography (church and hanks, 1990), <papid> J90-1003 </papid>information retrieval (salton, 1986<papid> C86-1090 </papid>a), text input (yamashina nd obashi, 1988), etc. this paper will touch on its feasibility in topic identification.</citsent>
<aftsection>
<nextsent>this paper is organized as follows.
</nextsent>
<nextsent>section 2 presents corpus-based language model and discuss how to train this model.
</nextsent>
<nextsent>section 3 touches on topic identification discourse.
</nextsent>
<nextsent>section 4 shows series of experiments based on the proposed model and discusses the results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J295">
<title id=" H05-1065.xml">disambiguation of morphological structure using a pcfg </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>new methods are often first developed for english and later adapted to other languages.
</prevsent>
<prevsent>this might explain why morphological disambiguation has been 521 so rarely addressed in the past: english morphology is seldom ambiguous except for noun compounds.we are not aware of any work on the disambiguation of morphological analyses which is directly comparable to ours.
</prevsent>
</prevsection>
<citsent citstr=" P95-1007 ">
mark lauer (1995) <papid> P95-1007 </papid>only considered english noun compounds and applied adifferent disambiguation strategy based on word association scores.</citsent>
<aftsection>
<nextsent>koehn and knight (2003) <papid> E03-1076 </papid>proposed splitting method for german compounds and showed thatit improves statistical machine translation.</nextsent>
<nextsent>compounds are split into smaller pieces (which have to be words themselves) if the geometric mean of theword frequencies of the pieces is higher than the frequency of the compound.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J296">
<title id=" H05-1065.xml">disambiguation of morphological structure using a pcfg </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this might explain why morphological disambiguation has been 521 so rarely addressed in the past: english morphology is seldom ambiguous except for noun compounds.we are not aware of any work on the disambiguation of morphological analyses which is directly comparable to ours.
</prevsent>
<prevsent>mark lauer (1995) <papid> P95-1007 </papid>only considered english noun compounds and applied adifferent disambiguation strategy based on word association scores.</prevsent>
</prevsection>
<citsent citstr=" E03-1076 ">
koehn and knight (2003) <papid> E03-1076 </papid>proposed splitting method for german compounds and showed thatit improves statistical machine translation.</citsent>
<aftsection>
<nextsent>compounds are split into smaller pieces (which have to be words themselves) if the geometric mean of theword frequencies of the pieces is higher than the frequency of the compound.
</nextsent>
<nextsent>information from bilingual corpus is used to improve the splitting accuracy.
</nextsent>
<nextsent>andreas eisele (unpublished work) implemented statistical disambiguator for german based on weighted finite-state transducers as described in the introduction.
</nextsent>
<nextsent>however, his system fails to represent and disambiguate the ambiguities observed in compounds with three or more elements and similar constructions with structural ambiguities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J297">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>1.1 dynamic programming as deduction.
</prevsent>
<prevsent>the parsing as deduction?
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
framework (pereira and warren, 1983) <papid> P83-1021 </papid>is now over 20 years old.</citsent>
<aftsection>
<nextsent>it provide san elegant notation for specifying variety of parsing algorithms (shieber et al, 1995), including algorithms for probabilistic or other semiring-weightedparsing (goodman, 1999)<papid> J99-4004 </papid></nextsent>
<nextsent>in the parsing community, new algorithms are often stated simply as set of deductive inference rules (sikkel, 1997; eisner and satta, 1999).<papid> P99-1059 </papid>it is also straightforward to specify other nlp algorithms this way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J298">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the parsing as deduction?
</prevsent>
<prevsent>framework (pereira and warren, 1983) <papid> P83-1021 </papid>is now over 20 years old.</prevsent>
</prevsection>
<citsent citstr=" J99-4004 ">
it provide san elegant notation for specifying variety of parsing algorithms (shieber et al, 1995), including algorithms for probabilistic or other semiring-weightedparsing (goodman, 1999)<papid> J99-4004 </papid></citsent>
<aftsection>
<nextsent>in the parsing community, new algorithms are often stated simply as set of deductive inference rules (sikkel, 1997; eisner and satta, 1999).<papid> P99-1059 </papid>it is also straightforward to specify other nlp algorithms this way.</nextsent>
<nextsent>syntactic mt models, language models, and stack decoders can be easily described using deductive rules.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J300">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>framework (pereira and warren, 1983) <papid> P83-1021 </papid>is now over 20 years old.</prevsent>
<prevsent>it provide san elegant notation for specifying variety of parsing algorithms (shieber et al, 1995), including algorithms for probabilistic or other semiring-weightedparsing (goodman, 1999)<papid> J99-4004 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1059 ">
in the parsing community, new algorithms are often stated simply as set of deductive inference rules (sikkel, 1997; eisner and satta, 1999).<papid> P99-1059 </papid>it is also straightforward to specify other nlp algorithms this way.</citsent>
<aftsection>
<nextsent>syntactic mt models, language models, and stack decoders can be easily described using deductive rules.
</nextsent>
<nextsent>so can operations on finite state and infinite-state machines.
</nextsent>
<nextsent>we thank joshua goodman, david mcallester, and paul ruhlen for useful early discussions; pioneer users markus dreyer, david smith, and roy tromble for their feedback and input; john blatz for discussion of program transformations; and several reviewers for useful criticism.
</nextsent>
<nextsent>this work was supported by nsf itr grant iis-0313193, onr muri grant n00014-01-1-0685, and hertz foundation fellowship to the third author.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J303">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>% we want complete constituent covering the sentence 6.
</prevsent>
<prevsent>need(nonterm,j) += constit( /cons(nonterm, ), ,j).
</prevsent>
</prevsection>
<citsent citstr=" J95-2002 ">
% note: underscore matches anything (anonymous wildcard) figure 2: an earley parser that recovers inside probabilities (earley, 1970; stolcke, 1995).<papid> J95-2002 </papid></citsent>
<aftsection>
<nextsent>the rule np ? det should be encoded as the axiom rewrite(np?,cons(det?,cons(n?,nil))), nested term.
</nextsent>
<nextsent>np?/needed is the label of partial np constituent that is still missing the list of sub constituents in needed.
</nextsent>
<nextsent>need(np?,3) is derived if some partial constituent seeks an np sub constituent starting at position 3.
</nextsent>
<nextsent>as in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J305">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>agenda algorithm to compute gradients or outside probabilities for parameter estimation.
</prevsent>
<prevsent>third, regarding weights, the dyna language is designed to express systems of arbitrary, heterogeneous equations over item values.
</prevsent>
</prevsection>
<citsent citstr=" J03-1006 ">
in previous work such as (goodman, 1999; <papid> J99-4004 </papid>nederhof, 2003), <papid> J03-1006 </papid>one only specifies the inference rules as unweighted horn clauses, and then weights are added automatically in standard way: all values have the same type w, and all rules transform to equations of the form ?= a1 ? a2 ? ?</citsent>
<aftsection>
<nextsent>ak, where ? and ? give the structure of semiring.4 in dyna one writes these equations explicitly in place of horn clauses (fig.
</nextsent>
<nextsent>1).
</nextsent>
<nextsent>accordingly, heterogeneous dyna programs, to be supported soon by our compiler, will allow items of different types to have values of different types, computed by different aggregation operations over arbitrary right-hand-side ex 3previous treatments of weighted deduction have used an agenda only for an unweighted parsing phase (goodman, 1999)<papid> J99-4004 </papid>or for finding the single best parse (nederhof, 2003).<papid> J03-1006 </papid></nextsent>
<nextsent>our algorithm works in arbitrary semi rings, including non-idempotent ones, taking care to avoid double-counting of weights and to handle side conditions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J308">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> relation to previous work.  </section>
<citcontext>
<prevsection>
<prevsent>pressions.
</prevsent>
<prevsent>this allows specification of wider classof algorithms from nlp and elsewhere (e.g., minimum expected loss decoding, smoothing formulas, neural networks, game tree analysis, and constraint programming).
</prevsent>
</prevsection>
<citsent citstr=" P96-1033 ">
although 4 and 5 have space to present only techniques for the semi ring case, these can be generalized.our approach may be most closely related to deductive databases, which even in their heyday were apparently ignored by the cl community (except forminnen, 1996).<papid> P96-1033 </papid></citsent>
<aftsection>
<nextsent>deductive database systems permit inference rules that can derive new database facts from old ones.5 they are essentially declarative logic programming languages (with restrictions or extensions) that areor could beimplementedusing efficient database techniques.
</nextsent>
<nextsent>some implemented deductive databases such as coral (ra makrishnan et al, 1994) and lola (zukowski and freitag, 1997) support aggregation (as in dynas +=, log+=, max=, . . .
</nextsent>
<nextsent>), although only stratified?
</nextsent>
<nextsent>forms of it that exclude unary cfg rule cycles.6 ross and sagiv (1992) (and in more restricted way, kifer and subrahmanian, 1992) come closest to our notion of attaching aggregable values to terms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J316">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing theorem values.  </section>
<citcontext>
<prevsection>
<prevsent>here the agenda algorithm can end up flipping values forever between false and true; more general solver would have to be called in order to find stable model of sat problems equations.
</prevsent>
<prevsent>14still assuming the number of items is finite, one could in principle materialize the system of equations and call dedicated numerical solver.
</prevsent>
</prevsection>
<citsent citstr=" P02-1001 ">
in some special cases only linear solver is needed: e.g., for unary rule cycles (stolcke, 1995), <papid> J95-2002 </papid>or ?-cycles in fsms (eisner, 2002).<papid> P02-1001 </papid></citsent>
<aftsection>
<nextsent>285 one can declare the conditions under which items of particular type (constit or goal) should be treated as having converged.
</nextsent>
<nextsent>then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion.
</nextsent>
<nextsent>4.5 prioritization.
</nextsent>
<nextsent>the order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J317">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing theorem values.  </section>
<citcontext>
<prevsection>
<prevsent>4.5 prioritization.
</prevsent>
<prevsent>the order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed.
</prevsent>
</prevsection>
<citsent citstr=" W98-1115 ">
we implement the agenda as priority queue whose priority function may be specified by the user.15charniak et al (1998) <papid> W98-1115 </papid>and caraballo and charniak (1998) <papid> J98-2004 </papid>showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective.</citsent>
<aftsection>
<nextsent>klein and manning (2003<papid> P03-1054 </papid>a) went on to describe admissible heuristics and an a*framework for parsing.</nextsent>
<nextsent>for a* in our general framework, the priority of item should be an estimate of the value of the best proof of goal that uses a.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J318">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing theorem values.  </section>
<citcontext>
<prevsection>
<prevsent>4.5 prioritization.
</prevsent>
<prevsent>the order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed.
</prevsent>
</prevsection>
<citsent citstr=" J98-2004 ">
we implement the agenda as priority queue whose priority function may be specified by the user.15charniak et al (1998) <papid> W98-1115 </papid>and caraballo and charniak (1998) <papid> J98-2004 </papid>showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective.</citsent>
<aftsection>
<nextsent>klein and manning (2003<papid> P03-1054 </papid>a) went on to describe admissible heuristics and an a*framework for parsing.</nextsent>
<nextsent>for a* in our general framework, the priority of item should be an estimate of the value of the best proof of goal that uses a.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J319">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing theorem values.  </section>
<citcontext>
<prevsection>
<prevsent>the order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed.
</prevsent>
<prevsent>we implement the agenda as priority queue whose priority function may be specified by the user.15charniak et al (1998) <papid> W98-1115 </papid>and caraballo and charniak (1998) <papid> J98-2004 </papid>showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
klein and manning (2003<papid> P03-1054 </papid>a) went on to describe admissible heuristics and an a*framework for parsing.</citsent>
<aftsection>
<nextsent>for a* in our general framework, the priority of item should be an estimate of the value of the best proof of goal that uses a.
</nextsent>
<nextsent>(this non-standard formulation is carefully chosen.16) if so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda.
</nextsent>
<nextsent>prioritizing good?
</nextsent>
<nextsent>items first can also be useful in other circumstances.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J329">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing gradients.  </section>
<citcontext>
<prevsection>
<prevsent>dynamite supports the em algorithm (and many variants), supervised and unsupervised training of log-linear (maximum en tropy?)
</prevsent>
<prevsent>models using quasi-newton methods, and smoothing-parameter tuning on development data.
</prevsent>
</prevsection>
<citsent citstr=" P04-1062 ">
as an object-oriented c++ library, it also facilitates rapid implementation of new estimation techniques (smith and eisner, 2004; <papid> P04-1062 </papid>smith and eisner, 2005).<papid> P05-1044 </papid></citsent>
<aftsection>
<nextsent>another interest of dyna is that its high-level specifications can be manipulated by mechanical source to-source program transformations.
</nextsent>
<nextsent>this makes it possible to derive new algorithms from old ones.
</nextsent>
<nextsent>5.1 already sketched the gradient transformation for finding goal.
</nextsent>
<nextsent>we note few other examples.bounding transformations generate new program that computes upper or lower bounds on goal, via generic bounding techniques (prieditis, 1993; cul berson and schaeffer, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J331">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> computing gradients.  </section>
<citcontext>
<prevsection>
<prevsent>dynamite supports the em algorithm (and many variants), supervised and unsupervised training of log-linear (maximum en tropy?)
</prevsent>
<prevsent>models using quasi-newton methods, and smoothing-parameter tuning on development data.
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
as an object-oriented c++ library, it also facilitates rapid implementation of new estimation techniques (smith and eisner, 2004; <papid> P04-1062 </papid>smith and eisner, 2005).<papid> P05-1044 </papid></citsent>
<aftsection>
<nextsent>another interest of dyna is that its high-level specifications can be manipulated by mechanical source to-source program transformations.
</nextsent>
<nextsent>this makes it possible to derive new algorithms from old ones.
</nextsent>
<nextsent>5.1 already sketched the gradient transformation for finding goal.
</nextsent>
<nextsent>we note few other examples.bounding transformations generate new program that computes upper or lower bounds on goal, via generic bounding techniques (prieditis, 1993; cul berson and schaeffer, 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J343">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>computation (mohri et al, 1998) and asymptotic space-saving tricks (binder et al, 1997).
</prevsent>
<prevsent>7.1 applications.
</prevsent>
</prevsection>
<citsent citstr=" W05-1504 ">
the current dyna compiler has proved indispensable in our own recent projects, in the sense that we would not have attempted many of them without it.in some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-length limited parsing (eisner and smith, 2005<papid> W05-1504 </papid>b) and loosely syntax-based machine translation (eisner and d. smith, 2005).</citsent>
<aftsection>
<nextsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)
</nextsent>
<nextsent>in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</nextsent>
<nextsent>these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J346">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the current dyna compiler has proved indispensable in our own recent projects, in the sense that we would not have attempted many of them without it.in some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-length limited parsing (eisner and smith, 2005<papid> W05-1504 </papid>b) and loosely syntax-based machine translation (eisner and d. smith, 2005).</prevsent>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</citsent>
<aftsection>
<nextsent>these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></nextsent>
<nextsent>these replications were easy to write and extend, and to train via 5.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J347">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the current dyna compiler has proved indispensable in our own recent projects, in the sense that we would not have attempted many of them without it.in some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-length limited parsing (eisner and smith, 2005<papid> W05-1504 </papid>b) and loosely syntax-based machine translation (eisner and d. smith, 2005).</prevsent>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)</prevsent>
</prevsection>
<citsent citstr=" H05-1060 ">
in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</citsent>
<aftsection>
<nextsent>these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></nextsent>
<nextsent>these replications were easy to write and extend, and to train via 5.2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J348">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)
</prevsent>
<prevsent>in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></citsent>
<aftsection>
<nextsent>these replications were easy to write and extend, and to train via 5.2.
</nextsent>
<nextsent>7.2 experiments.
</nextsent>
<nextsent>we compared the current dyna compiler to hand built systems on variety of parsing tasks.
</nextsent>
<nextsent>these problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J349">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)
</prevsent>
<prevsent>in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</prevsent>
</prevsection>
<citsent citstr=" N03-1021 ">
these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></citsent>
<aftsection>
<nextsent>these replications were easy to write and extend, and to train via 5.2.
</nextsent>
<nextsent>7.2 experiments.
</nextsent>
<nextsent>we compared the current dyna compiler to hand built systems on variety of parsing tasks.
</nextsent>
<nextsent>these problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J351">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)
</prevsent>
<prevsent>in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></citsent>
<aftsection>
<nextsent>these replications were easy to write and extend, and to train via 5.2.
</nextsent>
<nextsent>7.2 experiments.
</nextsent>
<nextsent>we compared the current dyna compiler to hand built systems on variety of parsing tasks.
</nextsent>
<nextsent>these problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J352">
<title id=" H05-1036.xml">compiling comp ling weighted dynamic programming and the dyna language </title>
<section> usefulness of the implementation.  </section>
<citcontext>
<prevsection>
<prevsent>(dyna would have been equally helpful in the first authors earlier work on new algorithms for lexicalized and ccg parsing,syntactic mt, transformational syntax, trainable parameterized fsms, and finite-state phonology.)
</prevsent>
<prevsent>in other cases (smith and eisner, 2004; <papid> P04-1062 </papid>smith and smith, 2004; <papid> W04-3207 </papid>smith et al, 2005), <papid> H05-1060 </papid>dyna let us quickly replicate, tweak, and combine useful techniques from the literature.</prevsent>
</prevsection>
<citsent citstr=" P02-1017 ">
these techniques included unweighted fs morphology, conditional random fields (lafferty et al, 2001), synchronous parsers (wu, 1997; <papid> J97-3002 </papid>melamed, 2003), <papid> N03-1021 </papid>lexicalized parsers (eisner and satta, 1999),<papid> P99-1059 </papid>22 partially supervised training a` la(pereira and schabes, 1992),<papid> P92-1017 </papid>23 and grammar induction (klein and manning, 2002).<papid> P02-1017 </papid></citsent>
<aftsection>
<nextsent>these replications were easy to write and extend, and to train via 5.2.
</nextsent>
<nextsent>7.2 experiments.
</nextsent>
<nextsent>we compared the current dyna compiler to hand built systems on variety of parsing tasks.
</nextsent>
<nextsent>these problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J373">
<title id=" H05-1040.xml">enhanced answer type inference from questions using sequential models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or answer type identification?.the answer type is picked from hand-built taxonomy having dozens to hundreds of answer types (harabagiu et al , 2000; hovy et al , 2001; kwok et al ., 2001; zheng, 2002; dumais et al , 2002).
</prevsent>
<prevsent>qa ? soumen@cse.iitb.ac.in systems can use the answer type to short-list answer tokens from passages retrieved by an information retrieval (ir) subsystem, or use the type together with other question words to inject ir queries.early successful qa systems used manually constructed sets of rules to map question to type, exploiting clues such as the wh-word (who, where, when, how many) and the head of noun phrases associated with the main verb (what is the tallest mountain in . . .).
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
with the increasing popularity of statistical nlp, li and roth (2002), <papid> C02-1150 </papid>hacioglu and ward (2003) <papid> N03-2010 </papid>and zhang and lee (2003) used supervised learning for question classification on dataset from uiuc that is now standard1.</citsent>
<aftsection>
<nextsent>it has 6 coarse and 50 fine answer types in two-level taxonomy, together with 5500 training and 500 test questions.
</nextsent>
<nextsent>webclopedia (hovy et al , 2001) has also published its taxonomy with over 140 types.
</nextsent>
<nextsent>the promise of machine learning approach isthat the qa system builder can now focus on designing features and providing labeled data, rather than coding and maintaining complex heuristic rule bases.
</nextsent>
<nextsent>the datasets and learning systems quoted above have made question classification well defined and non-trivial subtask of qa for which algorithms can be evaluated precisely, isolating more complex factors at work in complete qa system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J375">
<title id=" H05-1040.xml">enhanced answer type inference from questions using sequential models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>or answer type identification?.the answer type is picked from hand-built taxonomy having dozens to hundreds of answer types (harabagiu et al , 2000; hovy et al , 2001; kwok et al ., 2001; zheng, 2002; dumais et al , 2002).
</prevsent>
<prevsent>qa ? soumen@cse.iitb.ac.in systems can use the answer type to short-list answer tokens from passages retrieved by an information retrieval (ir) subsystem, or use the type together with other question words to inject ir queries.early successful qa systems used manually constructed sets of rules to map question to type, exploiting clues such as the wh-word (who, where, when, how many) and the head of noun phrases associated with the main verb (what is the tallest mountain in . . .).
</prevsent>
</prevsection>
<citsent citstr=" N03-2010 ">
with the increasing popularity of statistical nlp, li and roth (2002), <papid> C02-1150 </papid>hacioglu and ward (2003) <papid> N03-2010 </papid>and zhang and lee (2003) used supervised learning for question classification on dataset from uiuc that is now standard1.</citsent>
<aftsection>
<nextsent>it has 6 coarse and 50 fine answer types in two-level taxonomy, together with 5500 training and 500 test questions.
</nextsent>
<nextsent>webclopedia (hovy et al , 2001) has also published its taxonomy with over 140 types.
</nextsent>
<nextsent>the promise of machine learning approach isthat the qa system builder can now focus on designing features and providing labeled data, rather than coding and maintaining complex heuristic rule bases.
</nextsent>
<nextsent>the datasets and learning systems quoted above have made question classification well defined and non-trivial subtask of qa for which algorithms can be evaluated precisely, isolating more complex factors at work in complete qa system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J380">
<title id=" H05-1040.xml">enhanced answer type inference from questions using sequential models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>surprisingly, syntactic pattern-matching and heuristics widely used in qa systems are not very good at capturing informer spans (3.3).
</prevsent>
<prevsent>therefore, the notion of an informer is non-trivial.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
using parse of the question sentence, we derive novel set of multi-resolution features suitable for training conditional random field (crf) (laffertyet al , 2001; sha and pereira, 2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>our feature design paradigm may be of independent interest (4).
</nextsent>
<nextsent>our informer tagger is about 8587% accurate.
</nextsent>
<nextsent>we use meta-learning framework (chan andstolfo, 1993) in which linear svm predicts the answer type based on features derived from the original question as well as the output of the crf.
</nextsent>
<nextsent>thismeta-classifier beats all published numbers on standard question classification benchmarks (4.4).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J385">
<title id=" H05-1040.xml">enhanced answer type inference from questions using sequential models </title>
<section> using crfs to label informers.  </section>
<citcontext>
<prevsection>
<prevsent>sentences with similar parse trees are likely to havethe informer in similar positions.
</prevsent>
<prevsent>this was the intuition behind zhang et al tree kernel, and is also our starting point.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
we used the stanford lexicalized parser (klein and manning, 2003) <papid> P03-1054 </papid>to parse the ques tion.</citsent>
<aftsection>
<nextsent>(we assume familiarity with parse tree notation for lack of space.)
</nextsent>
<nextsent>figure 3 shows sample parse tree organized in levels.
</nextsent>
<nextsent>our first step was to trans 1 2 3 4 5 6 7 yi 0 0 0 1 1 2 2 xi what is the capital city of japan ` ? features for xis 1 wp,1 vbz,1 dt,1 nn,1 nn,1 in,1 nnp,1.
</nextsent>
<nextsent>2 whnp,1 vp,1 np,1 np,1 np,1 null,1 np,2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J386">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main goal of the work presented in this paperis to examine the relative compositionality of col1part of the work was done at institute for research in cognitive science (ircs), university of pennsylvania, philadelphia, pa 19104, usa, when he was visiting ircs as visiting scholar, february to december, 2004.
</prevsent>
<prevsent>locations of v-n type using svm based ranking function.
</prevsent>
</prevsection>
<citsent citstr=" P05-1068 ">
measuring the relative compositionality of v-n collocations is extremely helpful in applications such as machine translation where the collocations that are highly non-compositional can be handled in special way (schuler and joshi, 2004) (hwang and sasaki, 2005).<papid> P05-1068 </papid></citsent>
<aftsection>
<nextsent>multi-word expressions (mwes) are those whose structure and meaning cannot be derived from their component words, as they occur independently.
</nextsent>
<nextsent>examples include conjunctions like as well as?
</nextsent>
<nextsent>(meaning including?), idioms like kick the bucket?(meaning die?), phrasal verbs like find out?
</nextsent>
<nextsent>(meaning search?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J387">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(meaning search?)
</prevsent>
<prevsent>and compounds like village commu nity?.
</prevsent>
</prevsection>
<citsent citstr=" T75-2013 ">
a typical natural language system assumes each word to be lexical unit, but this assumption does not hold in case of mwes (becker, 1975)(<papid> T75-2013 </papid>fillmore, 2003).</citsent>
<aftsection>
<nextsent>they have idiosyncratic interpretations which cross word boundaries and hence are pain in the neck?
</nextsent>
<nextsent>(sag et al, 2002).
</nextsent>
<nextsent>they account for large portion of the language used in day-to day interactions (schuler and joshi, 2004) and so, handling them becomes an important task.a large number of mwes have standard syntactic structure but are non-compositional semantically.
</nextsent>
<nextsent>an example of such subset is the class ofnon-compositional verb-noun collocations (v-n collocations).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J388">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>is not mwe.
</prevsent>
<prevsent>899 it is well known that one cannot really make abinary distinction between compositional and non compositional mwes.
</prevsent>
</prevsection>
<citsent citstr=" W03-1809 ">
they do not fall cleanly into mutually exclusive classes, but populate the continuum between the two extremes (bannard et al, 2003).<papid> W03-1809 </papid></citsent>
<aftsection>
<nextsent>so, we rate the mwes (v-n collocations in this paper) on scale from 1 to 6 where 6 denot esa completely compositional expression, while 1 denotes completely opaque expression.
</nextsent>
<nextsent>various statistical measures have been suggested for ranking expressions based on their compositionality.
</nextsent>
<nextsent>some of these are frequency, mutual information (church and hanks, 1989) <papid> P89-1010 </papid>distributed frequency of object (tapanainen et al, 1998) <papid> P98-2210 </papid>and lsa model (baldwin et al, 2003) (<papid> W03-1812 </papid>schutze, 1998).</nextsent>
<nextsent>inthis paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of mwes of v-ntype (see section 6 for details).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J389">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, we rate the mwes (v-n collocations in this paper) on scale from 1 to 6 where 6 denot esa completely compositional expression, while 1 denotes completely opaque expression.
</prevsent>
<prevsent>various statistical measures have been suggested for ranking expressions based on their compositionality.
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
some of these are frequency, mutual information (church and hanks, 1989) <papid> P89-1010 </papid>distributed frequency of object (tapanainen et al, 1998) <papid> P98-2210 </papid>and lsa model (baldwin et al, 2003) (<papid> W03-1812 </papid>schutze, 1998).</citsent>
<aftsection>
<nextsent>inthis paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of mwes of v-ntype (see section 6 for details).
</nextsent>
<nextsent>integrating these statistical measures should provide better evidence for ranking the expressions.
</nextsent>
<nextsent>we use svm based ranking function to integrate the features and rank thev-n collocations according to their compositional ity.
</nextsent>
<nextsent>we then compare these ranks with the ranks provided by the human judge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J391">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, we rate the mwes (v-n collocations in this paper) on scale from 1 to 6 where 6 denot esa completely compositional expression, while 1 denotes completely opaque expression.
</prevsent>
<prevsent>various statistical measures have been suggested for ranking expressions based on their compositionality.
</prevsent>
</prevsection>
<citsent citstr=" P98-2210 ">
some of these are frequency, mutual information (church and hanks, 1989) <papid> P89-1010 </papid>distributed frequency of object (tapanainen et al, 1998) <papid> P98-2210 </papid>and lsa model (baldwin et al, 2003) (<papid> W03-1812 </papid>schutze, 1998).</citsent>
<aftsection>
<nextsent>inthis paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of mwes of v-ntype (see section 6 for details).
</nextsent>
<nextsent>integrating these statistical measures should provide better evidence for ranking the expressions.
</nextsent>
<nextsent>we use svm based ranking function to integrate the features and rank thev-n collocations according to their compositional ity.
</nextsent>
<nextsent>we then compare these ranks with the ranks provided by the human judge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J392">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>so, we rate the mwes (v-n collocations in this paper) on scale from 1 to 6 where 6 denot esa completely compositional expression, while 1 denotes completely opaque expression.
</prevsent>
<prevsent>various statistical measures have been suggested for ranking expressions based on their compositionality.
</prevsent>
</prevsection>
<citsent citstr=" W03-1812 ">
some of these are frequency, mutual information (church and hanks, 1989) <papid> P89-1010 </papid>distributed frequency of object (tapanainen et al, 1998) <papid> P98-2210 </papid>and lsa model (baldwin et al, 2003) (<papid> W03-1812 </papid>schutze, 1998).</citsent>
<aftsection>
<nextsent>inthis paper, we define novel measures (both collocation based and context based measures) to measure the relative compositionality of mwes of v-ntype (see section 6 for details).
</nextsent>
<nextsent>integrating these statistical measures should provide better evidence for ranking the expressions.
</nextsent>
<nextsent>we use svm based ranking function to integrate the features and rank thev-n collocations according to their compositional ity.
</nextsent>
<nextsent>we then compare these ranks with the ranks provided by the human judge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J393">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use svm based ranking function to integrate the features and rank thev-n collocations according to their compositional ity.
</prevsent>
<prevsent>we then compare these ranks with the ranks provided by the human judge.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
a similar comparison between the ranks according to latent-semanticanalysis (lsa) based features and the ranks of human judges has been made by mccarthy, keller and caroll (mccarthy et al, 2003) <papid> W03-1810 </papid>for verb-particle con structions.</citsent>
<aftsection>
<nextsent>(see section 3 for more details).
</nextsent>
<nextsent>some preliminary work on recognition of v-n collocations was presented in (venkatapathy and joshi, 2004).
</nextsent>
<nextsent>we show that the measures which we have defined contribute greatly to measuring the relative compo sitionality of v-n collocations when compared to the traditional features.
</nextsent>
<nextsent>we also show that the ranks assigned by the svm based ranking function correlated much better with the human judgement that the ranks assigned by individual statistical measures.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J395">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>these ranks are then compared with the human ranking.
</prevsent>
<prevsent>(breidt, 1995) has evaluated the usefulness of thepoint-wise mutual information measure (as suggested by (church and hanks, 1989)) <papid> P89-1010 </papid>for the extraction of v-n collocations from german text cor pora.</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
several other measures like log-likelihood (dunning, 1993), <papid> J93-1003 </papid>pearsons  (church et al,1991), z-score (church et al, 1991) , cubic association ratio (mi3), etc., have been also proposed.</citsent>
<aftsection>
<nextsent>these measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of mwes.
</nextsent>
<nextsent>dekang lin proposes way to automatically identify the noncompositionality of mwes (lin, 1999).<papid> P99-1041 </papid></nextsent>
<nextsent>he suggests that possible way to separate compositional phrases from non-compositional ones is to check the existence and mutual-information values of phrases obtained by replacing one of the words with similar word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J396">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several other measures like log-likelihood (dunning, 1993), <papid> J93-1003 </papid>pearsons  (church et al,1991), z-score (church et al, 1991) , cubic association ratio (mi3), etc., have been also proposed.</prevsent>
<prevsent>these measures try to quantify the association of two words but do not talk about quantifying the non-compositionality of mwes.</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
dekang lin proposes way to automatically identify the noncompositionality of mwes (lin, 1999).<papid> P99-1041 </papid></citsent>
<aftsection>
<nextsent>he suggests that possible way to separate compositional phrases from non-compositional ones is to check the existence and mutual-information values of phrases obtained by replacing one of the words with similar word.
</nextsent>
<nextsent>according to lin, phrase is probably non-compositional if such substitutions are not found in the collocations database or their mutual information values are significantly different from that of the phrase.
</nextsent>
<nextsent>another way of determining thenon-compositionality of v-n collocations is by using distributed frequency of object?
</nextsent>
<nextsent>(dfo) in v-n collocations (tapanainen et al, 1998).<papid> P98-2210 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J402">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the basic idea in there is that if an object appears only with one verb (or few verbs) in large corpus we expect that it has an idiomatic nature?
</prevsent>
<prevsent>(tapanainen et al, 1998).<papid> P98-2210 </papid></prevsent>
</prevsection>
<citsent citstr=" W01-0513 ">
schone and jurafsky (schone and jurafsky, 2001)<papid> W01-0513 </papid>applied latent-semantic analysis (lsa) to the analysis of mwes in the task of mwe discovery, by way 900 of rescoring mwes extracted from the corpus.</citsent>
<aftsection>
<nextsent>an interesting way of quantifying the relative compositionality of mwe is proposed by baldwin, bannard, tanaka and widdows (baldwin et al, 2003).<papid> W03-1812 </papid></nextsent>
<nextsent>they use lsa to determine the similarity between an mwe and its constituent words, and claim that higher similarity indicates great decomposability.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J410">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> data used for the experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we show that the ranks given by the svm based ranking function which integrates all the features provides significantly better correlation than the individual features.
</prevsent>
<prevsent>the data used for the experiments is british national corpus of 81 million words.
</prevsent>
</prevsection>
<citsent citstr=" W04-3224 ">
the corpus is parsed using bikels parser (bikel, 2004) <papid> W04-3224 </papid>and the verb-object collocations are extracted.</citsent>
<aftsection>
<nextsent>there are 4,775,697 v-n collocations of which 1.2 million areunique.
</nextsent>
<nextsent>all the v-n collocations above the frequency of 100 (n=4405) are taken to conduct the experiments so that the evaluation of the system is feasible.
</nextsent>
<nextsent>these 4405 v-n collocations were searched in wordnet, american heritage dictionary and said dictionary (ldc,2003).
</nextsent>
<nextsent>around 400 were found in at least one of the dictionaries.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J414">
<title id=" H05-1113.xml">measuring the relative compositionality of verb noun vn collocations by integrating features </title>
<section> features.  </section>
<citcontext>
<prevsection>
<prevsent>where  and  are the verb and object of collocations for which similar collocations do not exist.
</prevsent>
<prevsent>the higher the value of &amp; , the more is the likelihood of the collocation to be mwe.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
2obtained from lins (lin, 1998) <papid> P98-2127 </papid>automatically generated thesaurus (http://www.cs.ualberta.ca/ lindek/downloads.htm).</citsent>
<aftsection>
<nextsent>we obtained the best results (section 8) when we substitutedtop-5 similar words for both the verb and the object.
</nextsent>
<nextsent>to measure the compositionality, semantically similar words are more suitable than synomys.
</nextsent>
<nextsent>hence, we choose to use lins thesaurus (lin, 1998) <papid> P98-2127 </papid>instead of wordnet (miller et al, 1990).</nextsent>
<nextsent>902 6.1.4 distributed frequency of object (   ) the distributed frequency of object is based on the idea that if an object appears only with one verb (or few verbs) in large corpus, the collocation is expected to have idiomatic nature?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J422">
<title id=" H05-1124.xml">flexible text segmentation with structured multi label classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many parsers, translation systems, and extraction systems relyon such segment ations to accurately process the data.
</prevsent>
<prevsent>depending on the application, segments may be tokens, phrases, or sentences.however, in this paper we primarily focus on segmenting sentences into tokens.the most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with tag representing its role in segmentation.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
models of that form include hidden markov models (rabiner, 1989; bikel etal., 1999) as well as discriminative tagging models based on maximum entropy classification (rat naparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000), conditional random fields (lafferty et al, 2001; sha and pereira,2003), <papid> N03-1028 </papid>and large-margin techniques (kudo and matsumoto, 2001; <papid> N01-1025 </papid>taskar et al, 2003).</citsent>
<aftsection>
<nextsent>tagging models are the best previous methods for text segmentation.
</nextsent>
<nextsent>however, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments.we present here an alternative view of segmentation as structured multi label classification.
</nextsent>
<nextsent>in this view, segmentation of text is set of segments, each of which is defined by the set of text positions that belong to the segment.
</nextsent>
<nextsent>thus, particular segment may not be set of consecutive positions in the text, and segments may overlap.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J423">
<title id=" H05-1124.xml">flexible text segmentation with structured multi label classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many parsers, translation systems, and extraction systems relyon such segment ations to accurately process the data.
</prevsent>
<prevsent>depending on the application, segments may be tokens, phrases, or sentences.however, in this paper we primarily focus on segmenting sentences into tokens.the most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with tag representing its role in segmentation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
models of that form include hidden markov models (rabiner, 1989; bikel etal., 1999) as well as discriminative tagging models based on maximum entropy classification (rat naparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000), conditional random fields (lafferty et al, 2001; sha and pereira,2003), <papid> N03-1028 </papid>and large-margin techniques (kudo and matsumoto, 2001; <papid> N01-1025 </papid>taskar et al, 2003).</citsent>
<aftsection>
<nextsent>tagging models are the best previous methods for text segmentation.
</nextsent>
<nextsent>however, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments.we present here an alternative view of segmentation as structured multi label classification.
</nextsent>
<nextsent>in this view, segmentation of text is set of segments, each of which is defined by the set of text positions that belong to the segment.
</nextsent>
<nextsent>thus, particular segment may not be set of consecutive positions in the text, and segments may overlap.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J424">
<title id=" H05-1124.xml">flexible text segmentation with structured multi label classification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many parsers, translation systems, and extraction systems relyon such segment ations to accurately process the data.
</prevsent>
<prevsent>depending on the application, segments may be tokens, phrases, or sentences.however, in this paper we primarily focus on segmenting sentences into tokens.the most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with tag representing its role in segmentation.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
models of that form include hidden markov models (rabiner, 1989; bikel etal., 1999) as well as discriminative tagging models based on maximum entropy classification (rat naparkhi, 1996; <papid> W96-0213 </papid>mccallum et al, 2000), conditional random fields (lafferty et al, 2001; sha and pereira,2003), <papid> N03-1028 </papid>and large-margin techniques (kudo and matsumoto, 2001; <papid> N01-1025 </papid>taskar et al, 2003).</citsent>
<aftsection>
<nextsent>tagging models are the best previous methods for text segmentation.
</nextsent>
<nextsent>however, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments.we present here an alternative view of segmentation as structured multi label classification.
</nextsent>
<nextsent>in this view, segmentation of text is set of segments, each of which is defined by the set of text positions that belong to the segment.
</nextsent>
<nextsent>thus, particular segment may not be set of consecutive positions in the text, and segments may overlap.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J431">
<title id=" H05-1124.xml">flexible text segmentation with structured multi label classification </title>
<section> segmentation as tagging.  </section>
<citcontext>
<prevsection>
<prevsent>in both cases, it is difficult to see how asequential tagger could extract the segments correctly.
</prevsent>
<prevsent>it would be possible to grow the tag set to represent bounded number of overlapping, noncontiguous segments by representing all possible combinations of segment membership over overlapping segments, but this would require an arbitrary upper bound on and would lead to models that generalize poorly and are expensive to train.
</prevsent>
</prevsection>
<citsent citstr=" P05-1040 ">
dickinson and meurers (2005) <papid> P05-1040 </papid>point out that, as language processing begins to tackle problems in free-word order languages and discourse analysis,annotating and extracting non-contiguous segmen tations of text will become increasingly important.</citsent>
<aftsection>
<nextsent>though we focus primarily on entity extraction and np chunking in this paper, there is no reason why ideas presented here could not be extended to managing other non-contiguous phenomena.
</nextsent>
<nextsent>as outlined in section 1, we represent segmentation as multi label classification, assigning to each text the set of segments it contains.
</nextsent>
<nextsent>figure 3 shows the segments for the examples of figure 2.
</nextsent>
<nextsent>each segment is given by o/i assignment to its words, indicating which words belong to the segment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J432">
<title id=" H05-1124.xml">flexible text segmentation with structured multi label classification </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for these experiments, we set = n, where is the length of the sentence.
</prevsent>
<prevsent>this represents reasonable upper bound on the number of entities or chunks in sentence and results in time complexity of o(n3t ).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we compare our methods with both the averaged perceptron (collins, 2002) <papid> W02-1001 </papid>and conditional random fields (lafferty et al, 2001) using identical predicate sets.</citsent>
<aftsection>
<nextsent>though all systems use identical predicates, the actual features of the systems are different due to the fundamental differences between the multi label classification and sequential tagging models.
</nextsent>
<nextsent>4.1 standard datasets.
</nextsent>
<nextsent>our first experiments are standard named entity and base np datasets with no overlapping, embedded or non-contiguous segments.
</nextsent>
<nextsent>these experiments will show that, for simple segment ations, our model is competitive with sequential tagging models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J438">
<title id=" H05-1090.xml">context and learning in novelty detection </title>
<section> novelty track.  </section>
<citcontext>
<prevsection>
<prevsent>in 2003, group from the university of maryland and the center for computing sciences (conroy et al, 2003) used three techniques that used qr decomposition and singular value decomposition.
</prevsent>
<prevsent>the university of maryland, baltimore county, worked with clustering algorithms and singular value decomposition in sentence-sentence similarity matrices (kallurkar et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" W00-0405 ">
in 2004, conroy (conroy, 2004) tested maximal marginal relevance (goldstein et al., 2000) <papid> W00-0405 </papid>as well as qr decomposition.the information retrieval group at tsing hua university used pooling technique, grouping similar sentences into clusters in order to capture sentences that partially match two or more other sentences(ru et al, 2004).</citsent>
<aftsection>
<nextsent>they said they had found difficulties 717 with sentence-by-sentence comparisons.
</nextsent>
<nextsent>2.2 precision.
</nextsent>
<nextsent>at all three novelty track evaluations, from 2002 to 2004, it is clear that high precision is much harder to obtain than high recall.
</nextsent>
<nextsent>trivial baselines ? such as accept all sentences as novel ? have proved to be difficult to beat by very much.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J439">
<title id=" H05-1090.xml">context and learning in novelty detection </title>
<section> system.  </section>
<citcontext>
<prevsection>
<prevsent>this adds the sums of both new and old content words and compares that to threshold, tkeep.
</prevsent>
<prevsent>(b) if the first noun phrase is third person personal pronoun, use the classification in the focus variable.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
pronouns are known to signal that the same focus continues (grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>(c) if the sentence has not met any of the above tests but has minimum number of content words, shift the focus.
</nextsent>
<nextsent>if all tests above fail and there are minimum number of content words, with sum of tshift shift the focus.
</nextsent>
<nextsent>continue the focus, whether novel or old.
</nextsent>
<nextsent>we examined the 2003 novelty track data and found that more than half the novel sentences appear in sequences of consecutive sentences (see table 1).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J440">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the presented models are extensions of the part-of-speech tag-ging technique and are capable of emitting struc-ture.
</prevsent>
<prevsent>they utilize context-free grammar rules and add left-to-right transitional context information.
</prevsent>
</prevsection>
<citsent citstr=" A97-1014 ">
this type of model is used to facilitate the syntac-tic annotation of the negra corpus of german newspaper texts (skut et al, 1997).<papid> A97-1014 </papid></citsent>
<aftsection>
<nextsent>part-of-speech tagging is the assignment ofsyn- tactic categories (tags) to words that occur in the processed text.
</nextsent>
<nextsent>among others, this task is ef-ficiently solved with markov models.
</nextsent>
<nextsent>states of markov model represent syntactic ategories (or tuples of syntactic ategories), and outputs represent words and punctuation (church, 1988; <papid> A88-1019 </papid>derose, 1988, <papid> J88-1003 </papid>and others).</nextsent>
<nextsent>this technique of sta-tistical part-of-speech tagging operates very suc-cessfully, and usually accuracy rates between 96 and 97% are reported for new, unseen text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J442">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tagging is the assignment ofsyn- tactic categories (tags) to words that occur in the processed text.
</prevsent>
<prevsent>among others, this task is ef-ficiently solved with markov models.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
states of markov model represent syntactic ategories (or tuples of syntactic ategories), and outputs represent words and punctuation (church, 1988; <papid> A88-1019 </papid>derose, 1988, <papid> J88-1003 </papid>and others).</citsent>
<aftsection>
<nextsent>this technique of sta-tistical part-of-speech tagging operates very suc-cessfully, and usually accuracy rates between 96 and 97% are reported for new, unseen text.
</nextsent>
<nextsent>brants et al (1997) <papid> W97-0307 </papid>showed that the technique of statistical tagging can be shifted to the next level of syntactic processing and is capable of as-signing grammatical functions.</nextsent>
<nextsent>these are func-tions like subject, direct object, head, etc. they mark the function of child node within its par-ent phrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J443">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>part-of-speech tagging is the assignment ofsyn- tactic categories (tags) to words that occur in the processed text.
</prevsent>
<prevsent>among others, this task is ef-ficiently solved with markov models.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
states of markov model represent syntactic ategories (or tuples of syntactic ategories), and outputs represent words and punctuation (church, 1988; <papid> A88-1019 </papid>derose, 1988, <papid> J88-1003 </papid>and others).</citsent>
<aftsection>
<nextsent>this technique of sta-tistical part-of-speech tagging operates very suc-cessfully, and usually accuracy rates between 96 and 97% are reported for new, unseen text.
</nextsent>
<nextsent>brants et al (1997) <papid> W97-0307 </papid>showed that the technique of statistical tagging can be shifted to the next level of syntactic processing and is capable of as-signing grammatical functions.</nextsent>
<nextsent>these are func-tions like subject, direct object, head, etc. they mark the function of child node within its par-ent phrase.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J444">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>states of markov model represent syntactic ategories (or tuples of syntactic ategories), and outputs represent words and punctuation (church, 1988; <papid> A88-1019 </papid>derose, 1988, <papid> J88-1003 </papid>and others).</prevsent>
<prevsent>this technique of sta-tistical part-of-speech tagging operates very suc-cessfully, and usually accuracy rates between 96 and 97% are reported for new, unseen text.</prevsent>
</prevsection>
<citsent citstr=" W97-0307 ">
brants et al (1997) <papid> W97-0307 </papid>showed that the technique of statistical tagging can be shifted to the next level of syntactic processing and is capable of as-signing grammatical functions.</citsent>
<aftsection>
<nextsent>these are func-tions like subject, direct object, head, etc. they mark the function of child node within its par-ent phrase.
</nextsent>
<nextsent>figure 1 shows an example sentence and its structure.
</nextsent>
<nextsent>the terminal sequence is complemen-ted by tags (stuttgart-tiibingen-tagset, thielen and schiller, 1995).
</nextsent>
<nextsent>non-terminal nodes are la-beled with phrase categories, edges are labeled with grammatical functions (negra tagset).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J445">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the structure consists of terminal nodes (words and their parts-of-speech), non-terminal nodes (phrases) and edges (labeled with grammatical functions).
</prevsent>
<prevsent>as input to the next transducer.
</prevsent>
</prevsection>
<citsent citstr=" C94-1071 ">
(roche, 1994) <papid> C94-1071 </papid>uses the fix point of finite-state transducer.</citsent>
<aftsection>
<nextsent>the transducer is iteratively applied to its own out- put until it remains identical to the input.
</nextsent>
<nextsent>the method is successfully used for efficient processing with large grammars.
</nextsent>
<nextsent>(cardie and pierce, 1998) <papid> P98-1034 </papid>present an approach to chunking based on mix-ture of finite state and context-free techniques.</nextsent>
<nextsent>they use p rules of pruned treebank grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J446">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the transducer is iteratively applied to its own out- put until it remains identical to the input.
</prevsent>
<prevsent>the method is successfully used for efficient processing with large grammars.
</prevsent>
</prevsection>
<citsent citstr=" P98-1034 ">
(cardie and pierce, 1998) <papid> P98-1034 </papid>present an approach to chunking based on mix-ture of finite state and context-free techniques.</citsent>
<aftsection>
<nextsent>they use p rules of pruned treebank grammar.
</nextsent>
<nextsent>for processing, each point of text is matched against he treebank rules and the longest match is chosen.
</nextsent>
<nextsent>cascades of automata nd transducers can also be found in speech processing, see e.g.
</nextsent>
<nextsent>(pereira et al, 1994; <papid> H94-1050 </papid>mohri, 1997).<papid> J97-2003 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J447">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for processing, each point of text is matched against he treebank rules and the longest match is chosen.
</prevsent>
<prevsent>cascades of automata nd transducers can also be found in speech processing, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" H94-1050 ">
(pereira et al, 1994; <papid> H94-1050 </papid>mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>contrary to finite-state transducers, cascaded markov models exploit probabilities when pro-cessing layers of syntactic structure.
</nextsent>
<nextsent>they do not generate longest matches but most-probable sequences.
</nextsent>
<nextsent>furthermore, higher layer sees dif-ferent alternatives and their probabilities for the same span.
</nextsent>
<nextsent>it can choose lower ranked alterna-tive if it fits better into the context of the higher layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J448">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for processing, each point of text is matched against he treebank rules and the longest match is chosen.
</prevsent>
<prevsent>cascades of automata nd transducers can also be found in speech processing, see e.g.
</prevsent>
</prevsection>
<citsent citstr=" J97-2003 ">
(pereira et al, 1994; <papid> H94-1050 </papid>mohri, 1997).<papid> J97-2003 </papid></citsent>
<aftsection>
<nextsent>contrary to finite-state transducers, cascaded markov models exploit probabilities when pro-cessing layers of syntactic structure.
</nextsent>
<nextsent>they do not generate longest matches but most-probable sequences.
</nextsent>
<nextsent>furthermore, higher layer sees dif-ferent alternatives and their probabilities for the same span.
</nextsent>
<nextsent>it can choose lower ranked alterna-tive if it fits better into the context of the higher layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J451">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>input of the process is sequence of words (divided into sentences), output are part-of-speech tags and structures like the one indicated in figure 6.
</prevsent>
<prevsent>figure 7 presents results of the chunking task using cascaded markov models for different num-bers of layers.
</prevsent>
</prevsection>
<citsent citstr=" W98-1117 ">
2 percentages are slightly below those presented by (skut and brants, 1998).<papid> W98-1117 </papid></citsent>
<aftsection>
<nextsent>but 2the figure indicates unlabeled recall and preci-sion.
</nextsent>
<nextsent>differences to labeled recall/precision are small, since the number of different non-terminal categories is very restricted.
</nextsent>
<nextsent>they started with correctly tagged data, so our task is harder since it includes the process of part- of-speech tagging.
</nextsent>
<nextsent>recall increases with the number of layers.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J452">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>thereby some errors made at lower layers can be corrected.
</prevsent>
<prevsent>this leads to the increase of up to 0.3% inaccuracy.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
results for chunking penn treebank data were previously presented by several authors (ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al, 1998; <papid> P98-1010 </papid>veenstra, 1998; cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>these are not directly comparable to our results, 123 proceedings of eacl  99 die von der bundesregierung angestrebte entlassung des bundes aus einzelnen bereichen art appr art nn adja nn art nn appr adja nn the by the government intended dismissal (of) the federation rom several areas  the dismissal of the federation from several areas that was intended by the government  figure 6: complex german np and chunker output (postnominal genitive and pp are not attached).
</nextsent>
<nextsent>.2 ~9 ~9 negka corpus: chunking results 100 90 80 7o 6o 1 96.2 topline recall rain = 72.6% max= 100.0% ? recall ~ 0 0 0 0 = = = - rain = 54.0% / ~ max= 84.8% precision rain = 88.3% max= 91.4% i i i 2 3 4 5 6 7 8 9 # layers 96.3 96.4 96.4 96.5 96.5 96.5 96.5 96.5 % pos accuracy figure 7: np/pp chunking results for the negi~a corpus.
</nextsent>
<nextsent>the diagram shows recall and precision depending on the number of layers that are used for parsing.
</nextsent>
<nextsent>layer 0 is used for part-of-speech tagging, for which tagging accuracies are given at the bottom line.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J453">
<title id=" E99-1016.xml">cascaded markov models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>thereby some errors made at lower layers can be corrected.
</prevsent>
<prevsent>this leads to the increase of up to 0.3% inaccuracy.
</prevsent>
</prevsection>
<citsent citstr=" P98-1010 ">
results for chunking penn treebank data were previously presented by several authors (ramshaw and marcus, 1995; <papid> W95-0107 </papid>argamon et al, 1998; <papid> P98-1010 </papid>veenstra, 1998; cardie and pierce, 1998).<papid> P98-1034 </papid></citsent>
<aftsection>
<nextsent>these are not directly comparable to our results, 123 proceedings of eacl  99 die von der bundesregierung angestrebte entlassung des bundes aus einzelnen bereichen art appr art nn adja nn art nn appr adja nn the by the government intended dismissal (of) the federation rom several areas  the dismissal of the federation from several areas that was intended by the government  figure 6: complex german np and chunker output (postnominal genitive and pp are not attached).
</nextsent>
<nextsent>.2 ~9 ~9 negka corpus: chunking results 100 90 80 7o 6o 1 96.2 topline recall rain = 72.6% max= 100.0% ? recall ~ 0 0 0 0 = = = - rain = 54.0% / ~ max= 84.8% precision rain = 88.3% max= 91.4% i i i 2 3 4 5 6 7 8 9 # layers 96.3 96.4 96.4 96.5 96.5 96.5 96.5 96.5 % pos accuracy figure 7: np/pp chunking results for the negi~a corpus.
</nextsent>
<nextsent>the diagram shows recall and precision depending on the number of layers that are used for parsing.
</nextsent>
<nextsent>layer 0 is used for part-of-speech tagging, for which tagging accuracies are given at the bottom line.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J455">
<title id=" E95-1023.xml">deterministic consistency checking of lp constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these constraint solving rules can be employed for building an efficient implementa-tion.
</prevsent>
<prevsent>this is an important requirement for prac-tical systems.
</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
indeed we have successfully ex-tended the profit typed feature formalism (er- bach, 1995) <papid> E95-1025 </papid>with the constructs described in this paper.</citsent>
<aftsection>
<nextsent>approach to motivate our approach we start with an exam-ple on scrambling in german subordinate clauses.
</nextsent>
<nextsent>(4) dab er einen mann inder strafle lanfen that he man in the street walking sah.
</nextsent>
<nextsent>saw.
</nextsent>
<nextsent>that he saw man walking in the street.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J456">
<title id=" E95-1023.xml">deterministic consistency checking of lp constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, if cfg back- bone is employed then we assume that the value of the subcat attribute is treated as an unordered sequence (i.e. set) as defined in (11).
</prevsent>
<prevsent>(12) npdom vidom / v~ the essential idea is to use set-valued descrip-tions to model word-order domains.
</prevsent>
</prevsection>
<citsent citstr=" P94-1035 ">
in paxticu- lax subset constraints (manandhar, 1994) <papid> P94-1035 </papid>are em-ployed to construct larger domains from smaller ones.</citsent>
<aftsection>
<nextsent>thus in example (11) the domain of the verb is constructed by including the domains of the sub categorised arguments (enforced by the con-straints dora :d npdomf3dom :d idom) . note that in this example the verb itself is not part of its own domain.
</nextsent>
<nextsent>the binary constraint vi   en-forces precedence ordering between the signs i and v. the constraint v~dom   do,~ {v} en-sures that every element of the set vidom pre-cedes the sign v. in other words, the set vidom is in the domain precedence relation with the sin-gleton {v}.
</nextsent>
<nextsent>however there are strong constraints on order-ing in the middle field.
</nextsent>
<nextsent>for instance, when prono- mial complements are involved then not all per-mutations are acceptable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J457">
<title id=" E95-1023.xml">deterministic consistency checking of lp constraints </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>according to uszkoreit (uszkoreit, 1985), order-ing of arguments in the middle field is governed by the following set of lp constraints given in (14) which axe to be interpreted isjunctively.
</prevsent>
<prevsent>(14) pprn : +   pprn : - tr : agent   tr : theme tr : agent   tr : goal tr : goal   tr : theme focus: -   focus:+ the lp constraint in (14) states that for every pair of constituents in the middle field at least one of the conditions hould apply otherwise the sen-tence is considered ungrammatical.
</prevsent>
</prevsection>
<citsent citstr=" C94-1008 ">
a related but more elaborate lp rule mechanism is considered in (steinberger, 1994).<papid> C94-1008 </papid></citsent>
<aftsection>
<nextsent>166 to approximate this complex lp constraint em-ploying the kind of logical machinery described in this paper, we can use description such as the one given in (15).
</nextsent>
<nextsent>the definition given in (15) extends the description given in (11).
</nextsent>
<nextsent>(15) syn : dom : mf f3 3x3y f e mf y mf x   then f x=pprn : +ay=pprn : - - then else f ---- r : agent y = tr : theme then else f = r : agent y = tr : goal then else f = tr : goal y = tr : theme then else = focus : - y -= focus : + the definition in (15) can be understood as fol-lows.
</nextsent>
<nextsent>the feature constraint syn : dora : mf co- instantiates the middle field domain to the vari-able mf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J458">
<title id=" E95-1023.xml">deterministic consistency checking of lp constraints </title>
<section> doml is included in dom~.  </section>
<citcontext>
<prevsection>
<prevsent>the consequent is executed if the current set of constraints entail the guard g. the consequent is executed if the current set of constraints dis entail the guard g. if the cur-rent set of constraints neither entail nor dis entail then the execution of the whole guarded con-straint is blocked until more information is avail-able.
</prevsent>
<prevsent>the application of guarded constraints within computational linguistics has not been well ex-plored.
</prevsent>
</prevsection>
<citsent citstr=" E91-1007 ">
however, the horn extended feature struc-tures described in (hegner, 1991) <papid> E91-1007 </papid>can be thought of as adding guards to feature structures.</citsent>
<aftsection>
<nextsent>on the other hand, within logic programming uarded logic programming languages have longer his-tory originating with committed-choice languages (ueda, 1985) and popularised by the concurrent constraint programming paradigm due to saraswat (saraswat and rinard, 1990) (saraswat, 1993).
</nextsent>
<nextsent>for space reasons, we do not cover the logic of guarded feature constraints, guards on set mem-bership constraints and guards o.n precedence con-straints.
</nextsent>
<nextsent>guarded feature constraints have been extensively studied in (ait-kaci et al, 1992) (smolka and treinen, 1994) (ait-kaci and podel- ski, 1994).
</nextsent>
<nextsent>constraints in this section we provide formal definitions for the syntax and semantics of an extended feature logic that directly supports linear precedence con-straints as logical primitives.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J459">
<title id=" E95-1029.xml">specifying a shallow grammatical representation for parsing purposes </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the higher this percentage, the more susceptible seems the possibility of specify-ing workable grammatical representation.
</prevsent>
<prevsent>according to pessimistic view (e.g. church 1992), the part of speech of several percentage points of words in running text is impossible to agree on by different judges, even after negotia-tions.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
a more optimistic view can be found in (leech and eyes 1993, p. 39; marcus et al 1993, <papid> J93-2004 </papid>p. 328); they argue that near-100% inter judge agreement is possible, provided the part-of-speech annotation is done carefully by experts.</citsent>
<aftsection>
<nextsent>unfortu-nately, they give very little empirical evidence for their position e.g. in terms of double-blind exper-iments.
</nextsent>
<nextsent>supposing defining these lower levels of gram-matical representation is so problematic, the more distinctive levels should be even more difficult.
</nextsent>
<nextsent>if specifying the task of the parser - what the parser is supposed to do - turns out to be so problematic, one could even question the rationality of natu-ral language parser design as whole.
</nextsent>
<nextsent>in other words, the controversy regarding the specifiability of grammatical representation is fundamental issue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J461">
<title id=" H05-1087.xml">maximum expected fmeasure training of logistic regression models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop training procedure based on empirical risk minimization / utility maximization and evaluate it on simple extraction task.
</prevsent>
<prevsent>log-linear models have been used in many areas of natural language processing (nlp) and information retrieval (ir).
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
scenarios in which log-linear model shave been applied often involve simple binary classification decisions or probability assignments, as in the following three examples: ratnaparkhi et al (1994) <papid> H94-1048 </papid>consider restricted form of the prepositional phrase attachment problem where attachment decisions are binary; ittycheriah et al (2003) <papid> N03-2014 </papid>reduce entity mention tracking to the problem of modeling the probability of two mentions being linked; and greiff and ponte (2000) develop models of probabilistic information retrieval that involve binary decisions of relevance.</citsent>
<aftsection>
<nextsent>what is common to all three approaches is the application of log-linear models to binary classification tasks.1 as ratnaparkhi (1998, 1these kinds of log-linear models are also known among the nlp community as maximum entropy models?
</nextsent>
<nextsent>(berger et al,p. 27f.)
</nextsent>
<nextsent>points out, log-linear models of binary response variables are equivalent to, and in fact mere notational variants of, logistic regression models.
</nextsent>
<nextsent>in this paper we focus on binary classification tasks, and in particular on the loss or utility associated with classification decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J462">
<title id=" H05-1087.xml">maximum expected fmeasure training of logistic regression models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we develop training procedure based on empirical risk minimization / utility maximization and evaluate it on simple extraction task.
</prevsent>
<prevsent>log-linear models have been used in many areas of natural language processing (nlp) and information retrieval (ir).
</prevsent>
</prevsection>
<citsent citstr=" N03-2014 ">
scenarios in which log-linear model shave been applied often involve simple binary classification decisions or probability assignments, as in the following three examples: ratnaparkhi et al (1994) <papid> H94-1048 </papid>consider restricted form of the prepositional phrase attachment problem where attachment decisions are binary; ittycheriah et al (2003) <papid> N03-2014 </papid>reduce entity mention tracking to the problem of modeling the probability of two mentions being linked; and greiff and ponte (2000) develop models of probabilistic information retrieval that involve binary decisions of relevance.</citsent>
<aftsection>
<nextsent>what is common to all three approaches is the application of log-linear models to binary classification tasks.1 as ratnaparkhi (1998, 1these kinds of log-linear models are also known among the nlp community as maximum entropy models?
</nextsent>
<nextsent>(berger et al,p. 27f.)
</nextsent>
<nextsent>points out, log-linear models of binary response variables are equivalent to, and in fact mere notational variants of, logistic regression models.
</nextsent>
<nextsent>in this paper we focus on binary classification tasks, and in particular on the loss or utility associated with classification decisions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J463">
<title id=" H05-1087.xml">maximum expected fmeasure training of logistic regression models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>recall and precision are combined into single overall utility function, the well-known f-measure.
</prevsent>
<prevsent>it may be desirable to estimate the parameters of logistic regression model by maximizing f-measureduring training.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
this is analogous, and in certain sense equivalent, to empirical risk minimization, which has been used successfully in related areas, such as speech recognition (rahim and lee, 1997), language modeling (paciorek and rosenfeld, 2000), and machine translation (och, 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the novel contribution of this paper is training procedure for (approximately) maximizing the expected f-measure of probabilistic classifier based on logistic regression model.
</nextsent>
<nextsent>we formulate avector-valued utility function which has well defined expected value; f-measure is then rational function of this expectation and can be maximized numerically under certain conventional regularizing assumptions.
</nextsent>
<nextsent>1996; ratnaparkhi, 1998).
</nextsent>
<nextsent>this is an unfortunate choice of terminology, because the term maximum entropy?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J464">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>optical character recognition (ocr) is often the only feasible method to perform this conversion,owing to its speed and cost-effectiveness.
</prevsent>
<prevsent>unfortunately, the performance of ocr systems is farfrom perfect and recognition errors significantly degrade the performance of nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" N03-1018 ">
this is true both in resource acquisition, such as automated bilingual lexicon generation (kolak et al, 2003), <papid> N03-1018 </papid>and for end-user applications such as rapid machine translation (mt) in the battlefield for document filtering (voss and ess-dykema, 2000).</citsent>
<aftsection>
<nextsent>moreover, for low density languages, there simply may not be an ocr system available.
</nextsent>
<nextsent>in this paper, we demonstrate that via statistical post-processing of existing systems, it is possible to achieve reasonable recognition accuracy for low density languages altogether lacking an ocr system, to significantly improve on the performance of trainable commercial ocr system, and even to improve significantly on native commercial ocr system.1 by taking post-processing approach, we require minimal assumptions about the ocr system used as starting point.
</nextsent>
<nextsent>the proper role of our post-processing approach depends on the language.
</nextsent>
<nextsent>for languages with little commercial potential for ocr, it may well provide the most practical path for language-specific ocr development, given the expensive and time consuming nature of ocr development for new language sand the black box?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J466">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>we implemented our post-processing system using the framework of weighted finite state machines(wfsm), which provides strong theoretical foundation and reduces implementation time, thanks to freely available tool kits, such as the at&t; fsm toolkit (mohri et al, 1998).
</prevsent>
<prevsent>it also allows easy integration of our post-processor with numerous nlp applications that are implemented using fsms (e.g.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
(knight and graehl, 1997; <papid> P97-1017 </papid>kumar and byrne, 2003)).<papid> N03-1019 </papid></citsent>
<aftsection>
<nextsent>3.1 source model.
</nextsent>
<nextsent>the source model assigns probability (c) to original character sequences, c. we use character level n-gram language models as the source model, since n-gram models are simple, easy to train, and usually achieve good performance.
</nextsent>
<nextsent>more complicated models that make use of constraints imposed by particular language, such as vowel harmony, can be utilized if desired.
</nextsent>
<nextsent>we used the cmu-cambridge language modeling toolkit v2 (clarkson and rosenfeld, 1997) for training, using witten-bell smoothing and vocabulary type 1; all other parameters were left at their default values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J467">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>we implemented our post-processing system using the framework of weighted finite state machines(wfsm), which provides strong theoretical foundation and reduces implementation time, thanks to freely available tool kits, such as the at&t; fsm toolkit (mohri et al, 1998).
</prevsent>
<prevsent>it also allows easy integration of our post-processor with numerous nlp applications that are implemented using fsms (e.g.
</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
(knight and graehl, 1997; <papid> P97-1017 </papid>kumar and byrne, 2003)).<papid> N03-1019 </papid></citsent>
<aftsection>
<nextsent>3.1 source model.
</nextsent>
<nextsent>the source model assigns probability (c) to original character sequences, c. we use character level n-gram language models as the source model, since n-gram models are simple, easy to train, and usually achieve good performance.
</nextsent>
<nextsent>more complicated models that make use of constraints imposed by particular language, such as vowel harmony, can be utilized if desired.
</nextsent>
<nextsent>we used the cmu-cambridge language modeling toolkit v2 (clarkson and rosenfeld, 1997) for training, using witten-bell smoothing and vocabulary type 1; all other parameters were left at their default values.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J468">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>for example, (ajar|a car) ? (a7a)p ( 7??)p (c7j)p (a7a)p (r 7r).
</prevsent>
<prevsent>note that we are only considering the most likely edit sequence here, as opposed to summing over all possible ways to convert car to ajar.
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
the second is slightly modified version of the spelling correction model of brill and moore (2000).<papid> P00-1037 </papid>3 this model allows many-to-many edit operations, which makesp (liter|litre) ? (l 7l)p (i7i)p (tre7ter) possible.</citsent>
<aftsection>
<nextsent>we will refer to the these as the single character (sc) and multi-character (mc) error models, respectively.
</nextsent>
<nextsent>we train both error models over set of corresponding ground truth and ocr sequences,c,o?.
</nextsent>
<nextsent>training is performed using expectation maximization: we first find the most likely edit sequence for each training pair to update the edit counts, and then use the updated counts to re estimate edit probabilities.
</nextsent>
<nextsent>for mc, after finding the most likely edit sequence, extended versions of eachnon-copy operation that include neighboring characters are also considered, which allows learning any common multi-character mappings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J469">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the languages studied include igbo, ce buano, arabic, and spanish.
</prevsent>
<prevsent>for intrinsic evaluation, we use the conventional word error rate (wer) metric, which is defined as wer(c,o) = wordeditdistance(c,o) wordcount(c) 4ignoring errors that result invalid words, lexicon-based chunking is always error-free.5inversion reverses the direction of the error model, mapping observed sequences to possible ground truth sequences.we do not use the character error rate (cer) metric, since for almost all nlp applications the unit of information is the words.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
for extrinsic evaluation of machine translation, we use the bleu metric (pap ineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>4.1 igbo: creating an ocr system.
</nextsent>
<nextsent>igbo is an african language spoken mainly in nigeria by an estimated 10 to 18 million people, written in latin script.
</nextsent>
<nextsent>although some igbo texts use diacritics to mark tones, they are not part of the officialorthography and they are absent in most printed materials.
</nextsent>
<nextsent>other than grammar books, texts for igbo, even hardcopy, are extremely difficult to obtain.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J470">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>4.4 extrinsic evaluation: mt. while our post-processing methods reduce wer,our main interest is their impact on nlp applications.
</prevsent>
<prevsent>we have performed machine translation experiments to measure the effects of ocr errors and the post-processing approach on nlp application performance.for arabic, we trained statistical mt system using the first nine sections of the bible data.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
the language model is trained using the cmu-cambridge toolkit and the translation model using the giza++ toolkit (och and ney, 2000).<papid> P00-1056 </papid></citsent>
<aftsection>
<nextsent>we used the rewrite decoder (germann, 2003) <papid> N03-1010 </papid>for translation.</nextsent>
<nextsent>bleu scores for ocr, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J471">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we have performed machine translation experiments to measure the effects of ocr errors and the post-processing approach on nlp application performance.for arabic, we trained statistical mt system using the first nine sections of the bible data.
</prevsent>
<prevsent>the language model is trained using the cmu-cambridge toolkit and the translation model using the giza++ toolkit (och and ney, 2000).<papid> P00-1056 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1010 ">
we used the rewrite decoder (germann, 2003) <papid> N03-1010 </papid>for translation.</citsent>
<aftsection>
<nextsent>bleu scores for ocr, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively.
</nextsent>
<nextsent>this establishes that ocr errors degrade the performance of the mt system, and we are able to bring the performance much closer to the level of performance on clean text by using post-processing.
</nextsent>
<nextsent>clearly thebleu scores are quite low; we are planning to perform experiments on arabic using more advanced translation system, such as hiero (chiang, 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>872 mt system input text bleu score systran ocr 0.2000 systran corrected 0.2606 systran clean 0.3188 rewrite ocr 0.1792 rewrite corrected 0.2234 rewrite clean 0.2590 table 6: spanish-english translation resultsin order to test in scenario with better translation performance, we performed mt evaluations using spanish.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J472">
<title id=" H05-1109.xml">ocr postprocessing for low density languages </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>bleu scores for ocr, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively.
</prevsent>
<prevsent>this establishes that ocr errors degrade the performance of the mt system, and we are able to bring the performance much closer to the level of performance on clean text by using post-processing.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
clearly thebleu scores are quite low; we are planning to perform experiments on arabic using more advanced translation system, such as hiero (chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>872 mt system input text bleu score systran ocr 0.2000 systran corrected 0.2606 systran clean 0.3188 rewrite ocr 0.1792 rewrite corrected 0.2234 rewrite clean 0.2590 table 6: spanish-english translation resultsin order to test in scenario with better translation performance, we performed mt evaluations using spanish.
</nextsent>
<nextsent>we used commercial translation system, systran, in addition to statistical translation.
</nextsent>
<nextsent>more resources being available for this language, corrected text for spanish experiments was obtained using our original model that takes advantage of lexicon (2003).
</nextsent>
<nextsent>table 6 shows that scores are much higher compared to arabic, but the pattern of improvements using post-processing is the same.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J473">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with larger ele-mentary structures, generalizations are lost re-garding the internal structure of the elementary trees.
</prevsent>
<prevsent>since parse time depends on grammar size, this could have an adverse effect on parsing effi-ciency.
</prevsent>
</prevsection>
<citsent citstr=" P95-1011 ">
however, the problem of grammar size in tag has to some extent been addressed both with respect grammar encoding (evans et al , 1995; <papid> P95-1011 </papid>candito, 1996) and parsing (joshi and srinivas, 1994; <papid> C94-1024 </papid>evans and weir, 1998).<papid> P98-1061 </papid></citsent>
<aftsection>
<nextsent>on the other hand, if the edol hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature val-ues around during parsing will be less than in rule-based framework.
</nextsent>
<nextsent>if all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed ynamically during the parsing process by means of feature percola-tion.
</nextsent>
<nextsent>for example, there is no need to use slash feature to establish filler-gap dependencies over unbounded istances across the tree if the edol 217 proceedings of eacl  99 $ np\[?~ :,??\] (whom) snp vp figure 1: localizing filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure.
</nextsent>
<nextsent>this paper presents an investigation to the ex-tent to which the edol reduces the need for fea-ture passing in two existing wide-coverage ram- mars: the xtag grammar (xtag-group, 1995), and the lexsys grammar (carroll et al , 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J474">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with larger ele-mentary structures, generalizations are lost re-garding the internal structure of the elementary trees.
</prevsent>
<prevsent>since parse time depends on grammar size, this could have an adverse effect on parsing effi-ciency.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
however, the problem of grammar size in tag has to some extent been addressed both with respect grammar encoding (evans et al , 1995; <papid> P95-1011 </papid>candito, 1996) and parsing (joshi and srinivas, 1994; <papid> C94-1024 </papid>evans and weir, 1998).<papid> P98-1061 </papid></citsent>
<aftsection>
<nextsent>on the other hand, if the edol hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature val-ues around during parsing will be less than in rule-based framework.
</nextsent>
<nextsent>if all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed ynamically during the parsing process by means of feature percola-tion.
</nextsent>
<nextsent>for example, there is no need to use slash feature to establish filler-gap dependencies over unbounded istances across the tree if the edol 217 proceedings of eacl  99 $ np\[?~ :,??\] (whom) snp vp figure 1: localizing filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure.
</nextsent>
<nextsent>this paper presents an investigation to the ex-tent to which the edol reduces the need for fea-ture passing in two existing wide-coverage ram- mars: the xtag grammar (xtag-group, 1995), and the lexsys grammar (carroll et al , 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J475">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with larger ele-mentary structures, generalizations are lost re-garding the internal structure of the elementary trees.
</prevsent>
<prevsent>since parse time depends on grammar size, this could have an adverse effect on parsing effi-ciency.
</prevsent>
</prevsection>
<citsent citstr=" P98-1061 ">
however, the problem of grammar size in tag has to some extent been addressed both with respect grammar encoding (evans et al , 1995; <papid> P95-1011 </papid>candito, 1996) and parsing (joshi and srinivas, 1994; <papid> C94-1024 </papid>evans and weir, 1998).<papid> P98-1061 </papid></citsent>
<aftsection>
<nextsent>on the other hand, if the edol hypothesis holds for those dependencies that are being checked by the parser, then the burden of passing feature val-ues around during parsing will be less than in rule-based framework.
</nextsent>
<nextsent>if all dependencies that the parser is checking can be stated directly within the elementary structures of the grammar, they do not need to be computed ynamically during the parsing process by means of feature percola-tion.
</nextsent>
<nextsent>for example, there is no need to use slash feature to establish filler-gap dependencies over unbounded istances across the tree if the edol 217 proceedings of eacl  99 $ np\[?~ :,??\] (whom) snp vp figure 1: localizing filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure.
</nextsent>
<nextsent>this paper presents an investigation to the ex-tent to which the edol reduces the need for fea-ture passing in two existing wide-coverage ram- mars: the xtag grammar (xtag-group, 1995), and the lexsys grammar (carroll et al , 1998).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J476">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> parsing unification-based.  </section>
<citcontext>
<prevsection>
<prevsent>efficient graph unification must also ensure that it does not destructively modify the input struc-tures, since the same rule may be used several times within single derivation, and also the same constituent may be used within different partial analyses with features instantiated in dif-ferent ways.
</prevsent>
<prevsent>copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical.
</prevsent>
</prevsection>
<citsent citstr=" C86-1016 ">
therefore, various  quasi-destructive  algorithms (karttunen, 1986; <papid> C86-1016 </papid>kogure, 1990; <papid> C90-2039 </papid>tomabechi, 1991) <papid> P91-1041 </papid>and algorithms using  skeletal  dacs with updates (pereira, 1985; <papid> P85-1017 </papid>emele, 1991) <papid> P91-1042 </papid>have been proposed, which at-tempt to minimize copying.</citsent>
<aftsection>
<nextsent>but even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars us-ing large hpsg-style feature graphs pend around 85-90% of their time unifying and copying feature structures (tomabechi, 1991), <papid> P91-1041 </papid>and may allocate in the region of 1-2 mbytes memory while parsing sentences of only eight words or so (flickinger, p.c.).</nextsent>
<nextsent>although comfortably within main mem-ory capacities of current workstations, uch large amounts of short-term storage allocation overflow cpu caches, and storage management overheads become significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J477">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> parsing unification-based.  </section>
<citcontext>
<prevsection>
<prevsent>efficient graph unification must also ensure that it does not destructively modify the input struc-tures, since the same rule may be used several times within single derivation, and also the same constituent may be used within different partial analyses with features instantiated in dif-ferent ways.
</prevsent>
<prevsent>copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical.
</prevsent>
</prevsection>
<citsent citstr=" C90-2039 ">
therefore, various  quasi-destructive  algorithms (karttunen, 1986; <papid> C86-1016 </papid>kogure, 1990; <papid> C90-2039 </papid>tomabechi, 1991) <papid> P91-1041 </papid>and algorithms using  skeletal  dacs with updates (pereira, 1985; <papid> P85-1017 </papid>emele, 1991) <papid> P91-1042 </papid>have been proposed, which at-tempt to minimize copying.</citsent>
<aftsection>
<nextsent>but even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars us-ing large hpsg-style feature graphs pend around 85-90% of their time unifying and copying feature structures (tomabechi, 1991), <papid> P91-1041 </papid>and may allocate in the region of 1-2 mbytes memory while parsing sentences of only eight words or so (flickinger, p.c.).</nextsent>
<nextsent>although comfortably within main mem-ory capacities of current workstations, uch large amounts of short-term storage allocation overflow cpu caches, and storage management overheads become significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J478">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> parsing unification-based.  </section>
<citcontext>
<prevsection>
<prevsent>efficient graph unification must also ensure that it does not destructively modify the input struc-tures, since the same rule may be used several times within single derivation, and also the same constituent may be used within different partial analyses with features instantiated in dif-ferent ways.
</prevsent>
<prevsent>copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical.
</prevsent>
</prevsection>
<citsent citstr=" P91-1041 ">
therefore, various  quasi-destructive  algorithms (karttunen, 1986; <papid> C86-1016 </papid>kogure, 1990; <papid> C90-2039 </papid>tomabechi, 1991) <papid> P91-1041 </papid>and algorithms using  skeletal  dacs with updates (pereira, 1985; <papid> P85-1017 </papid>emele, 1991) <papid> P91-1042 </papid>have been proposed, which at-tempt to minimize copying.</citsent>
<aftsection>
<nextsent>but even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars us-ing large hpsg-style feature graphs pend around 85-90% of their time unifying and copying feature structures (tomabechi, 1991), <papid> P91-1041 </papid>and may allocate in the region of 1-2 mbytes memory while parsing sentences of only eight words or so (flickinger, p.c.).</nextsent>
<nextsent>although comfortably within main mem-ory capacities of current workstations, uch large amounts of short-term storage allocation overflow cpu caches, and storage management overheads become significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J479">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> parsing unification-based.  </section>
<citcontext>
<prevsection>
<prevsent>efficient graph unification must also ensure that it does not destructively modify the input struc-tures, since the same rule may be used several times within single derivation, and also the same constituent may be used within different partial analyses with features instantiated in dif-ferent ways.
</prevsent>
<prevsent>copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical.
</prevsent>
</prevsection>
<citsent citstr=" P85-1017 ">
therefore, various  quasi-destructive  algorithms (karttunen, 1986; <papid> C86-1016 </papid>kogure, 1990; <papid> C90-2039 </papid>tomabechi, 1991) <papid> P91-1041 </papid>and algorithms using  skeletal  dacs with updates (pereira, 1985; <papid> P85-1017 </papid>emele, 1991) <papid> P91-1042 </papid>have been proposed, which at-tempt to minimize copying.</citsent>
<aftsection>
<nextsent>but even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars us-ing large hpsg-style feature graphs pend around 85-90% of their time unifying and copying feature structures (tomabechi, 1991), <papid> P91-1041 </papid>and may allocate in the region of 1-2 mbytes memory while parsing sentences of only eight words or so (flickinger, p.c.).</nextsent>
<nextsent>although comfortably within main mem-ory capacities of current workstations, uch large amounts of short-term storage allocation overflow cpu caches, and storage management overheads become significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J480">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> parsing unification-based.  </section>
<citcontext>
<prevsection>
<prevsent>efficient graph unification must also ensure that it does not destructively modify the input struc-tures, since the same rule may be used several times within single derivation, and also the same constituent may be used within different partial analyses with features instantiated in dif-ferent ways.
</prevsent>
<prevsent>copying input feature structures in their entirety before each unification would solve this problem, but the space usage renders this approach impractical.
</prevsent>
</prevsection>
<citsent citstr=" P91-1042 ">
therefore, various  quasi-destructive  algorithms (karttunen, 1986; <papid> C86-1016 </papid>kogure, 1990; <papid> C90-2039 </papid>tomabechi, 1991) <papid> P91-1041 </papid>and algorithms using  skeletal  dacs with updates (pereira, 1985; <papid> P85-1017 </papid>emele, 1991) <papid> P91-1042 </papid>have been proposed, which at-tempt to minimize copying.</citsent>
<aftsection>
<nextsent>but even with good implementations of the best of these improved algorithms, parsers designed for wide-coverage unification-based phrase-structure grammars us-ing large hpsg-style feature graphs pend around 85-90% of their time unifying and copying feature structures (tomabechi, 1991), <papid> P91-1041 </papid>and may allocate in the region of 1-2 mbytes memory while parsing sentences of only eight words or so (flickinger, p.c.).</nextsent>
<nextsent>although comfortably within main mem-ory capacities of current workstations, uch large amounts of short-term storage allocation overflow cpu caches, and storage management overheads become significant.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J482">
<title id=" E99-1029.xml">parsing with an extended domain of locality </title>
<section> analysis of two wide-coverage.  </section>
<citcontext>
<prevsection>
<prevsent>this, however, does not violate the edol hypothesis ince this feature is not co referenced with any other feature in the tree.
</prevsent>
<prevsent>grammars as we have seen, the edol of ltags makes it pos-sible, at least in principle, to locally express de-pendencies which cannot be localized in cfg- based formalism.
</prevsent>
</prevsection>
<citsent citstr=" P95-1021 ">
in this section we consider two existing grammars: the xtag grammar, wide- coverage ltag, and the lexsys grammar, wide- coverage d-tree substitution grammar (rambow et al , 1995).<papid> P95-1021 </papid></citsent>
<aftsection>
<nextsent>for each grammar we investigate the extend to which they do not take full advantage of the edol and require percolation of features at parse time.
</nextsent>
<nextsent>there are number of instances in which depen-dencies are not localized in the xtag grammar, most of which involve auxiliary trees.
</nextsent>
<nextsent>there are 219 proceedings of eacl  99 three types of auxiliary trees: predicative, modi-fier and coordination auxiliary trees.
</nextsent>
<nextsent>in predica- tive auxiliary trees the anchor is also the head of the tree and becomes the head of the tree resulting from the adjunction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J483">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>goes beyond gb in that it allows the treat-ment of anaphora the text level of descrip-tion within the same grammar formalism as is used for sentence level anaphora, and, 4.
</prevsent>
<prevsent>goes beyond the anaphora-centered treatment of text structure characteristic of the drt ap-proach in that it already accounts for the reso-lution of text-level ellipsis (sometimes also re-ferred to as functional anaphora, cf.
</prevsent>
</prevsection>
<citsent citstr=" C92-1008 ">
hahn and strube (1995)) and the interpretation of text macro structures (a preliminary study is pre-sented in hahn (1992)).<papid> C92-1008 </papid></citsent>
<aftsection>
<nextsent>in this section, we present, quite informally, some constraints on intra-sentential naphora in terms of dependency grammar (dg).
</nextsent>
<nextsent>we will reconsider these constraints in section 3, where our grammar model is dealt with in more depth.
</nextsent>
<nextsent>we provide here definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases).
</nextsent>
<nextsent>these constraints cover approximately the same phenomena as the binding theory of gb (chomsky (1981); for computational treatment, cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J484">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> dg const ra in ts  on  anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>we provide here definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases).
</prevsent>
<prevsent>these constraints cover approximately the same phenomena as the binding theory of gb (chomsky (1981); for computational treatment, cf.
</prevsent>
</prevsection>
<citsent citstr=" C88-1026 ">
correa (1988)).<papid> C88-1026 </papid></citsent>
<aftsection>
<nextsent>dependency structures, by definition, refer to the sentence level of linguistic description only.
</nextsent>
<nextsent>the relation of dependency holds between lexi-cal head and one or several modifiers of that head, such that the occurrence of head allows for the occurrence of one or several modifiers (in some pre-specified linear ordering), but not vice versa.
</nextsent>
<nextsent>speaking in terms of dependency structure rep-resentations, the head always precedes and, thus, (transitively) governs its associated modifiers in the dependency tree.
</nextsent>
<nextsent>this basic notion of govern-ment must be further refined for the description of anaphoric relations in dependency trees (we do not claim universal status for the following con-straints, but restrict their validity to the descrip-tion of the german language): d-b ind lng: modifier is d-bound by some head h, if no node intervenes between and for which one of the following conditions holds: (i) node represents finite verb, or (ii) node represents noun with possessive modifier, i.e., possessive determiners, saxon geni- tive, genitival and prepositional attributes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J485">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> dg const ra in ts  on  anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, for pronominal anaphors the selected antecedent must be permitted in those conceptual roles con-necting the pronominal anaphors and its gram-matical head.
</prevsent>
<prevsent>the dg constraints for the use of reflex ives and intra-sentential anaphora cover approximately the same phenomena gb, but the structures used by dg analysis are less complex than those of gb and do not require the formal machinery of empty categories, binding chains and complex movements (cf.
</prevsent>
</prevsection>
<citsent citstr=" J90-4001 ">
lappin and mccord (1990, <papid> J90-4001 </papid>p.205) for similar argument).</citsent>
<aftsection>
<nextsent>hence, our proposal pro-vides more tractable basis for implementation.
</nextsent>
<nextsent>the parse talk model of dg (hahn et al, 1994) exploits inheritance as major abstraction mech-anism.
</nextsent>
<nextsent>the entire lexical system is organized as hierarchy of lexical classes (isac denoting the subclass relation among lexical classes), with con-crete lexical items forming the leave nodes of the corresponding lexicon grammar graph.
</nextsent>
<nextsent>valency constraints are attached to each lexical item, on which the local computation of concrete depen-dency relations between head and its associated modifier is based.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J486">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> major grammatical  predicates.  </section>
<citcontext>
<prevsection>
<prevsent>the parse talk specifi-cation language, in addition, incorporates topo- logical primitives for relations within dependency trees.
</prevsent>
<prevsent>the relations left and head denote  ~ oc-curs left of  and  is head of  , resp.
</prevsent>
</prevsection>
<citsent citstr=" P89-1032 ">
these primitive relations can be considered eclarative equivalents to the procedural specifications used in several tree-walking algorithms for anaphora resolution, e.g., by hobbs (1978) or ingria and stallard (1989).<papid> P89-1032 </papid></citsent>
<aftsection>
<nextsent>note that in the description be- low tel + and rel* denote the transitive and transi- tive/reflexive closure of relation rel, respectively.
</nextsent>
<nextsent>x d-binds : :~ (x head + y) -~q z: ((x head + z) (z head + y) (z isac* finite verb 3u: (z head a ((z spec a isac* detpossessive) (z saxgen a isac* noun) (z ppatt a isac* noun) (z genatt a isac* noun))))) box 1: d-binding the possible antecedents hat can be reached via anaphoric relations are described by the pred-icates ispotentialreflexi~eantecedentof (eft box 2) and ispotentialanaphoricantecedentof(cf.
</nextsent>
<nextsent>box 3).
</nextsent>
<nextsent>these incorporate the predicate d-binds (of.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J487">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> resolution of anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>pronanaphortest (pro, ante):?:~ ante isac* noun ((pro.features\selt~agr\gen) lj(ante.features\self~agr\gen) ? _l) ((pro.features \ elf\agr\num) u(ante.features\self~agr\nurn) # _l) ((pro.features\self~agr\pers) ll(ante.features\self~agr\pers) # .1_) ^ vx role r: (x head pro (x.concept, pro.concept) roles =~(x.concept, role, ante.concept) permit) box 4: pronanaphortest nomanaphortest (defnp, ante):~ ante isac* noun ((defnp.features \sel~agr\num) u(ante.features\sel~ag,\num) # _l) ^ ante.concept isaf* defnp.concept box 5: nomanaphortest
</prevsent>
<prevsent>the parsetaik environment builds on the actor computation model (agha and hewitt, 1987) as background for the procedural interpretation of lexicalized dependency specifications in terms of so-called word actors (of.
</prevsent>
</prevsection>
<citsent citstr=" C94-1080 ">
schacht et al 1994; <papid> C94-1080 </papid>hahn et al 1994).</citsent>
<aftsection>
<nextsent>word actors combine object- oriented features with con currency ielding strict lexical distribution and distributed computation in methodologically clean way.
</nextsent>
<nextsent>the model assumes word actors to communicate via asyn-chronous message passing.
</nextsent>
<nextsent>an actor can send mes-sages only to other actors it knows about, its so- called acquaintances.
</nextsent>
<nextsent>the arrival of message at an actor is called an event; it triggers the execu-tion of method that is composed of atomic ac-tions - among them the evaluation of grammatical predicates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J488">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> resolution of anaphora.  </section>
<citcontext>
<prevsection>
<prevsent>consequently, any of these sub protocols consti-tutes part of the grammar specification proper.
</prevsent>
<prevsent>we shall illustrate the linguistic aspects of word actor-based parsing by introducing the basic data structures for text-level anaphora acquain-tances of specific word actors, and then turn to the general message-passing protocol that accounts for intra- as well as inter-sentential naphora.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
our exposition builds on the well-known focus-ing mechanism (sidner, 1983; grosz and sidner, 1986).<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>accordingly, we distinguish each sentence unique focus, complementary list of alternate potential loci, and history list composed of dis-course elements not in the list of potential loci, but occurring in previous ente nces of the current discourse segment.
</nextsent>
<nextsent>these data structures are re-alized as acquaintances of sentence delimiters to restrict the search space beyond the sentence to the relevant word actors.
</nextsent>
<nextsent>the protocol evel of analysis encompasses the procedural interpretation of the declarative con-straints given in section 2.
</nextsent>
<nextsent>at that level, in the case of reflexive pronouns, the search for the an-tecedent is triggered by the occurrence of reflex-ive pronoun in the text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J490">
<title id=" E95-1033.xml">parse talk about sentence and text level anaphora </title>
<section> comparison to related work.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, they treat pronominal coreference and anaphora (i.e., reflex ives and reciprocals).
</prevsent>
<prevsent>our approach methodologically differs in three major aspects from that study: first, unlike the sg proposal, which is based on second-pass algorithm operat-ing on fully parsed clauses to determine anaphoric relationships, our proposal is basically an incre-mental single-pass parsing model.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
most impor- tan t, however, is that our model incorporates the text-level of anaphora resolution, ashortcoming of the original sg approach that has recently been removed (lappin and leass, 1994), <papid> J94-4002 </papid>but still is source of lots of problems.</citsent>
<aftsection>
<nextsent>third, unlike our ap-proach, even the current sg model for anaphora resolution does not incorporate conceptual knowl-edge and global discourse structures (for reasons discussed by lappin and laess).
</nextsent>
<nextsent>this decision might nevertheless cause trouble if more conceptu-ally rooted text cohesion and coherence structures have to be accounted for (e.g., textual ellipses).
</nextsent>
<nextsent>a particular problem we have not yet solved, the plausible ranking of single antecedents from candidate set, is dealt with indepth by lappin and laess (1994) and hajicova et al (1992).
</nextsent>
<nextsent>both define salience metrics capable of ordering alter-native antecedents according to structural crite-ria, several of which can directly be attributed to the topological structure and topic/comment an-notations of the underlying dependency trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J491">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments showed substantial improvement on the transliteration accuracy over state-of-the-art baseline system, significantly reducing the transliteration character error rate from 50.29% to 12.84%.
</prevsent>
<prevsent>named entity (ne) translation and transliteration are very important to many multilingual natural language processing tasks, such as machine translation, cross lingual information retrieval and question answering.
</prevsent>
</prevsection>
<citsent citstr=" W03-0317 ">
although some frequently occurring nes can be reliably translated using information from existing bilingual dictionaries and parallel or monolingual corpora (al-onaizan and knight, 2002; huang and vogel, 2002; lee and chang, 2003), <papid> W03-0317 </papid>less frequently occurring nes, especially new names, still relyon machine transliteration to generate their translations.</citsent>
<aftsection>
<nextsent>ne machine transliteration generates phoneti cally similar equivalent in the target language for source ne, and transliteration patterns highly depend on the names origin, e.g., the country or the language family this name is from.
</nextsent>
<nextsent>for example, when transliterating names 1 from chinese into english, as shown in the following example, the same chinese character ???
</nextsent>
<nextsent>is transliterated into different english letters according to the origin of each person.
</nextsent>
<nextsent>--- jin renqing (china) ???
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J492">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>--- kanemaru shin (japan) ??
</prevsent>
<prevsent>--- jose joaquin brunner (chile) several approaches have been proposed for name transliteration.
</prevsent>
</prevsection>
<citsent citstr=" P97-1017 ">
(knight and graehl, 1997) <papid> P97-1017 </papid>proposed generative transliteration model to trans literate foreign names in japanese back to english using finite state transducers.</citsent>
<aftsection>
<nextsent>(stalls and knight, 1998) expanded that model to arabic english transliteration.
</nextsent>
<nextsent>(meng et al 2001) developed an english-chinese ne transliteration technique using pronunciation lexicon and phonetic mapping rules.
</nextsent>
<nextsent>(virga and khudanpur, 2003) <papid> W03-1508 </papid>applied statistical machine translation models to translate?</nextsent>
<nextsent>english names into chinese characters for mandarin spoken document retrieval.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J493">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(stalls and knight, 1998) expanded that model to arabic english transliteration.
</prevsent>
<prevsent>(meng et al 2001) developed an english-chinese ne transliteration technique using pronunciation lexicon and phonetic mapping rules.
</prevsent>
</prevsection>
<citsent citstr=" W03-1508 ">
(virga and khudanpur, 2003) <papid> W03-1508 </papid>applied statistical machine translation models to translate?</citsent>
<aftsection>
<nextsent>english names into chinese characters for mandarin spoken document retrieval.
</nextsent>
<nextsent>all these approaches exploit general model for ne transliteration, where source names from different origins or language families are transliterated into the target language with the same rules or probability distributions, which fails to capture their different 1 assuming foreign names are already transliterated into chi-.
</nextsent>
<nextsent>nese.
</nextsent>
<nextsent>435transliteration patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J494">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>nese.
</prevsent>
<prevsent>435transliteration patterns.
</prevsent>
</prevsection>
<citsent citstr=" P04-1024 ">
alternatively, (qu and grefenstette, 2004) <papid> P04-1024 </papid>applied language identification of name origins to select language-specific tran slit era tions when back-transliterating japanese names from english to japanese.</citsent>
<aftsection>
<nextsent>however, they only classified names into three origins: chinese, japanese and english, and they used the unihan database to obtain the mapping between kenji characters and romanji representations.
</nextsent>
<nextsent>ideally, to explicitly model these transliteration differences we should construct transliteration model and language model for each origin.
</nextsent>
<nextsent>how ever, some origins lack enough name translation pairs for reliable model training.
</nextsent>
<nextsent>in this paper we propose cluster-specific ne transliteration framework.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J495">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> phrase-based name transliteration.  </section>
<citcontext>
<prevsection>
<prevsent>a naive choice of source transliteration unit is single character.
</prevsent>
<prevsent>however, single characters lack contextual information, and their combinations may generate too many unlikely candidates.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
motivated by the success of phrase-based machine translation approaches (wu 1997, <papid> J97-3002 </papid>och 1999, marcu and wong 2002 <papid> W02-1018 </papid>and vogel et. al., 2003), we select transliteration units which are long enough to capture contextual information while flexible enough to compose new names with other units.</citsent>
<aftsection>
<nextsent>we discover such source transliteration phrases based on character collocation likelihood ratio test (manning and schutze 1999).
</nextsent>
<nextsent>this test accepts or rejects null hypothesis that the occurrence of one character is independent of the other, , by calculating the likelihood ratio between the independent ( ) and dependent ( ) hypotheses: 1f 2f 0h 1h ),,(log),,(log ),,(log),,(log )( )( loglog 211221112 1122112 1 0 pcncclpccl pcncclpccl hl hl ????
</nextsent>
<nextsent>l is the likelihood of getting the observed character counts under each hypothesis.
</nextsent>
<nextsent>assuming the character occurrence frequency follows binomial distribution, knk xx n xnkl ?????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J496">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> phrase-based name transliteration.  </section>
<citcontext>
<prevsection>
<prevsent>a naive choice of source transliteration unit is single character.
</prevsent>
<prevsent>however, single characters lack contextual information, and their combinations may generate too many unlikely candidates.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
motivated by the success of phrase-based machine translation approaches (wu 1997, <papid> J97-3002 </papid>och 1999, marcu and wong 2002 <papid> W02-1018 </papid>and vogel et. al., 2003), we select transliteration units which are long enough to capture contextual information while flexible enough to compose new names with other units.</citsent>
<aftsection>
<nextsent>we discover such source transliteration phrases based on character collocation likelihood ratio test (manning and schutze 1999).
</nextsent>
<nextsent>this test accepts or rejects null hypothesis that the occurrence of one character is independent of the other, , by calculating the likelihood ratio between the independent ( ) and dependent ( ) hypotheses: 1f 2f 0h 1h ),,(log),,(log ),,(log),,(log )( )( loglog 211221112 1122112 1 0 pcncclpccl pcncclpccl hl hl ????
</nextsent>
<nextsent>l is the likelihood of getting the observed character counts under each hypothesis.
</nextsent>
<nextsent>assuming the character occurrence frequency follows binomial distribution, knk xx n xnkl ?????
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J497">
<title id=" H05-1055.xml">cluster specific named entity transliteration </title>
<section> phrase-based name transliteration.  </section>
<citcontext>
<prevsection>
<prevsent>into source transliteration phrase sequence based on maximum string matching using t; 2.
</prevsent>
<prevsent>convert chinese characters into their ro-.
</prevsent>
</prevsection>
<citsent citstr=" W03-1502 ">
man ization form, pinyin, then align the pinyin with english letters via phonetic string matching, as described in (huang et. al., 2003); <papid> W03-1502 </papid>3.</citsent>
<aftsection>
<nextsent>identify the initial phrase alignment path.
</nextsent>
<nextsent>based on the character alignment path; phrase alignment path, searching for the optimal alignment which minimizes the overall phrase alignment cost, defined as: ? ?
</nextsent>
<nextsent>= aa aia i efda ),(minarg* . here is the th source phrase in f, is its target candidate under alignment a. their alignment cost is defined as the linear interpolation of the phonetic transliteration cost log and semantic translation cost log : if iae trlp transp )|(log)1()|(log),( fepfepefd transtrl ??
</nextsent>
<nextsent>?+= , where is the trlp product of the letter transliteration probabilities over aligned pinyin-english letter pairs, transp is the phrase translation probability calculated from word translation probabilities, where word?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J500">
<title id=" E95-1043.xml">incorporating unconscious re analyses into an incremental monotonic parser </title>
<section> conscious and unconscious.  </section>
<citcontext>
<prevsection>
<prevsent>the research was supported by esrc grant r00429334338 (1) while john was eating the ice cream melted .
</prevsent>
<prevsent>(2) john knows the truth hurts . this binary distinction has often been used to mo-tivate two-level architecture in the human syn-tactic processing system, where what we will call the  core parser  performs tandard attachment, as well as being able to re analyse in the easy cases (such as on reaching hurts in (2)), but where the assistance of higher level resolver(to use abney terminology (1987), abney terminology (1989)), is required to solve the difficult cases, (such as on reaching melted in (1)).
</prevsent>
</prevsection>
<citsent citstr=" P83-1020 ">
this  core parser  has been the subject of number of computational implementations, in-cluding marcus deterministic parser (1980), de-scription theory (henceforth, d-theory) (marcus et al (1983)), <papid> P83-1020 </papid>and abney licensing based model (1987), licensing based model (1989).</citsent>
<aftsection>
<nextsent>it has also been the subject of number of psycho linguistic studies on more the-oretical level (pritchett (1992), gorrell (in press)).
</nextsent>
<nextsent>the implementation described in this paper is based on the most recent model, that of (gorrell (in press)).
</nextsent>
<nextsent>this model is interesting in that it does not allow the parser to employ delay tactics, such as using look ahead buffer (marcus (1980), marcus et al (1983)), <papid> P83-1020 </papid>or waiting for the head of phrase to appear in the input before construct-ing that phrase (abney (1987), abney (1989), pritehett (1992)).</nextsent>
<nextsent>instead, processing is guided by the prin-ciple of incremental licensing, which states that  the parser attempts incrementally to satisfy the principles of grammar .</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J504">
<title id=" E95-1043.xml">incorporating unconscious re analyses into an incremental monotonic parser </title>
<section> conscious and unconscious.  </section>
<citcontext>
<prevsection>
<prevsent>however, for the purposes of this implementation, we have taken the most constrained position.
</prevsent>
<prevsent>note that, since we do not deal with such 291 full attachment model has been argued for, es-pecially with regard to the processing of head- final languages, where evidence has been found of pre-head structuring (inoue &amp; fodor (1991), frazier (1987)).
</prevsent>
</prevsection>
<citsent citstr=" E95-1017 ">
such models have also been ex-plored computationally (milward (1995), <papid> E95-1017 </papid>crocker (1991)).</citsent>
<aftsection>
<nextsent>gorrell employs the d-theoretic device of building up set of dominance and precedence relations 2 between nodes, where the set is intended to be constrained by informational monotonicity, in that once asserted to the set, no relation may be deleted or overridden.
</nextsent>
<nextsent>gorrell restricts this con-straint to primary structural relations (i.e. domi-nance and precedence), while secondary relations (e.g. thematic and case dependencies) are not so constrained.
</nextsent>
<nextsent>recall (2), repeated below: (2) john knows the truth hurts.
</nextsent>
<nextsent>at the point where john knows the truth has been processed, complete clause will have been built: (3) is \[np1 john\] \[vp iv knows\] \[np2 the truth\]\] the description will include the information that the verb knows precedes ~ip2, and that the vp dora- . inates np2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J506">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that our alignment combination approach yields significant 20-34% relative error reduction over the best-known alignment combination technique on english spanish and english-chinese data.
</prevsent>
<prevsent>parallel texts are valuable resource in natural language processing and essential for projecting knowledge from one language onto another.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
word-level alignment is critical component of wide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when there is sufficient training data,the quality decreases as the size of training data decreases.
</nextsent>
<nextsent>even with large amounts of training data,statistical align ers have been shown to be susceptible to mis-aligning phrasal constructions (dorr et al,2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).this paper presents novel approach to alignment combination, neuralign, that treats each alignment system as black box and merges their outputs.
</nextsent>
<nextsent>we view word alignment as pattern classification problem and treat alignment combination as classifier ensemble (hansen and salamon, 1990; wolpert,1992).
</nextsent>
<nextsent>the ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce globally optimal solution (minsky, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J507">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that our alignment combination approach yields significant 20-34% relative error reduction over the best-known alignment combination technique on english spanish and english-chinese data.
</prevsent>
<prevsent>parallel texts are valuable resource in natural language processing and essential for projecting knowledge from one language onto another.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
word-level alignment is critical component of wide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when there is sufficient training data,the quality decreases as the size of training data decreases.
</nextsent>
<nextsent>even with large amounts of training data,statistical align ers have been shown to be susceptible to mis-aligning phrasal constructions (dorr et al,2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).this paper presents novel approach to alignment combination, neuralign, that treats each alignment system as black box and merges their outputs.
</nextsent>
<nextsent>we view word alignment as pattern classification problem and treat alignment combination as classifier ensemble (hansen and salamon, 1990; wolpert,1992).
</nextsent>
<nextsent>the ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce globally optimal solution (minsky, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J508">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show that our alignment combination approach yields significant 20-34% relative error reduction over the best-known alignment combination technique on english spanish and english-chinese data.
</prevsent>
<prevsent>parallel texts are valuable resource in natural language processing and essential for projecting knowledge from one language onto another.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
word-level alignment is critical component of wide range of nlp applications, such as construction of bilingual lexicons (melamed, 2000), <papid> J00-2004 </papid>word sense disambiguation (diab and resnik, 2002), <papid> P02-1033 </papid>projection of language resources (yarowsky et al, 2001), <papid> H01-1035 </papid>and statistical machine translation.</citsent>
<aftsection>
<nextsent>although word-level align ers tend to perform well when there is sufficient training data,the quality decreases as the size of training data decreases.
</nextsent>
<nextsent>even with large amounts of training data,statistical align ers have been shown to be susceptible to mis-aligning phrasal constructions (dorr et al,2002) due to many-to-many correspondences, morphological language distinctions, paraphrased and free translations, and high percentage of function words (about 50% of the tokens in most texts).this paper presents novel approach to alignment combination, neuralign, that treats each alignment system as black box and merges their outputs.
</nextsent>
<nextsent>we view word alignment as pattern classification problem and treat alignment combination as classifier ensemble (hansen and salamon, 1990; wolpert,1992).
</nextsent>
<nextsent>the ensemble-based approach was developed to select the best features of different learning algorithms, including those that may not produce globally optimal solution (minsky, 1991).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J509">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, liu (2005) usesa log-linear combination of linguistic features.
</prevsent>
<prevsent>additional linguistic knowledge can be in the form ofpart-of-speech tags.
</prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
(toutanova et al, 2002) <papid> W02-1012 </papid>or dependency relations (cherry and lin, 2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>other approaches to improving alignment have combined alignment models, e.g., using log-linear combination (och and ney, 2003) <papid> J03-1002 </papid>or mutually independent association clues (tiedemann, 2003).</nextsent>
<nextsent>a simpler approach was developed by ayan etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J510">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for example, liu (2005) usesa log-linear combination of linguistic features.
</prevsent>
<prevsent>additional linguistic knowledge can be in the form ofpart-of-speech tags.
</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
(toutanova et al, 2002) <papid> W02-1012 </papid>or dependency relations (cherry and lin, 2003).<papid> P03-1012 </papid></citsent>
<aftsection>
<nextsent>other approaches to improving alignment have combined alignment models, e.g., using log-linear combination (och and ney, 2003) <papid> J03-1002 </papid>or mutually independent association clues (tiedemann, 2003).</nextsent>
<nextsent>a simpler approach was developed by ayan etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J511">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>additional linguistic knowledge can be in the form ofpart-of-speech tags.
</prevsent>
<prevsent>(toutanova et al, 2002) <papid> W02-1012 </papid>or dependency relations (cherry and lin, 2003).<papid> P03-1012 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
other approaches to improving alignment have combined alignment models, e.g., using log-linear combination (och and ney, 2003) <papid> J03-1002 </papid>or mutually independent association clues (tiedemann, 2003).</citsent>
<aftsection>
<nextsent>a simpler approach was developed by ayan etal.
</nextsent>
<nextsent>(2004), where word alignment outputs are combined using linear combination of feature weights assigned to the individual aligners.
</nextsent>
<nextsent>our method is more general in that it uses neural network model that is capable of learning nonlinear functions.classifier ensembles are used in several nlp applications.
</nextsent>
<nextsent>some nlp applications for classifier ensembles are pos tagging (brill and wu, 1998; ab ney et al, 1999), <papid> W99-0606 </papid>pp attachment (abney et al, 1999), <papid> W99-0606 </papid>word sense disambiguation (florian and yarowsky, 2002), <papid> W02-1004 </papid>and parsing (henderson and brill, 2000).<papid> A00-2005 </papid>the work reported in this paper is the first application of classifier ensembles to the word-alignmentproblem.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J512">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004), where word alignment outputs are combined using linear combination of feature weights assigned to the individual aligners.
</prevsent>
<prevsent>our method is more general in that it uses neural network model that is capable of learning nonlinear functions.classifier ensembles are used in several nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" W99-0606 ">
some nlp applications for classifier ensembles are pos tagging (brill and wu, 1998; ab ney et al, 1999), <papid> W99-0606 </papid>pp attachment (abney et al, 1999), <papid> W99-0606 </papid>word sense disambiguation (florian and yarowsky, 2002), <papid> W02-1004 </papid>and parsing (henderson and brill, 2000).<papid> A00-2005 </papid>the work reported in this paper is the first application of classifier ensembles to the word-alignmentproblem.</citsent>
<aftsection>
<nextsent>we use different methodology to combine classifiers that is based on stacked generalization (wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers.
</nextsent>
<nextsent>a multi-layer perceptron (mlp) is feed-forwardneural network that consists of several units (neu rons) that are connected to each other by weighted links.
</nextsent>
<nextsent>as illustrated in figure 1, an mlp consists of one input layer, one or more hidden layers, and one output layer.
</nextsent>
<nextsent>the external input is presented tothe input layer, propagated forward through the hidden layers and creates the output vector in the output layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J514">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004), where word alignment outputs are combined using linear combination of feature weights assigned to the individual aligners.
</prevsent>
<prevsent>our method is more general in that it uses neural network model that is capable of learning nonlinear functions.classifier ensembles are used in several nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" W02-1004 ">
some nlp applications for classifier ensembles are pos tagging (brill and wu, 1998; ab ney et al, 1999), <papid> W99-0606 </papid>pp attachment (abney et al, 1999), <papid> W99-0606 </papid>word sense disambiguation (florian and yarowsky, 2002), <papid> W02-1004 </papid>and parsing (henderson and brill, 2000).<papid> A00-2005 </papid>the work reported in this paper is the first application of classifier ensembles to the word-alignmentproblem.</citsent>
<aftsection>
<nextsent>we use different methodology to combine classifiers that is based on stacked generalization (wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers.
</nextsent>
<nextsent>a multi-layer perceptron (mlp) is feed-forwardneural network that consists of several units (neu rons) that are connected to each other by weighted links.
</nextsent>
<nextsent>as illustrated in figure 1, an mlp consists of one input layer, one or more hidden layers, and one output layer.
</nextsent>
<nextsent>the external input is presented tothe input layer, propagated forward through the hidden layers and creates the output vector in the output layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J515">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2004), where word alignment outputs are combined using linear combination of feature weights assigned to the individual aligners.
</prevsent>
<prevsent>our method is more general in that it uses neural network model that is capable of learning nonlinear functions.classifier ensembles are used in several nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" A00-2005 ">
some nlp applications for classifier ensembles are pos tagging (brill and wu, 1998; ab ney et al, 1999), <papid> W99-0606 </papid>pp attachment (abney et al, 1999), <papid> W99-0606 </papid>word sense disambiguation (florian and yarowsky, 2002), <papid> W02-1004 </papid>and parsing (henderson and brill, 2000).<papid> A00-2005 </papid>the work reported in this paper is the first application of classifier ensembles to the word-alignmentproblem.</citsent>
<aftsection>
<nextsent>we use different methodology to combine classifiers that is based on stacked generalization (wolpert, 1992), i.e., learning an additional model on the outputs of individual classifiers.
</nextsent>
<nextsent>a multi-layer perceptron (mlp) is feed-forwardneural network that consists of several units (neu rons) that are connected to each other by weighted links.
</nextsent>
<nextsent>as illustrated in figure 1, an mlp consists of one input layer, one or more hidden layers, and one output layer.
</nextsent>
<nextsent>the external input is presented tothe input layer, propagated forward through the hidden layers and creates the output vector in the output layer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J516">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> neuralign approach.  </section>
<citcontext>
<prevsection>
<prevsent>while combining word alignments, we use two types of features to describe each instance (i, j): (1) linguistic features and (2) alignment features.
</prevsent>
<prevsent>linguistic features include pos tags of both words (ei and fj) and dependency relation for one of the words (ei).
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we generate pos tags using the mxpost tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>for english and chinese, and conn exor for spanish.</citsent>
<aftsection>
<nextsent>dependency relations are produced using version of the collins parser (collins, 1997) <papid> P97-1003 </papid>that has been adapted for building dependencies.alignment features consist of features that are extracted from the outputs of individual alignment sys tems.</nextsent>
<nextsent>for each alignmentak ? a, the following are some of the alignment features that can be used to describe an instance (i, j): 1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J517">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> neuralign approach.  </section>
<citcontext>
<prevsection>
<prevsent>linguistic features include pos tags of both words (ei and fj) and dependency relation for one of the words (ei).
</prevsent>
<prevsent>we generate pos tags using the mxpost tagger (ratnaparkhi, 1996) <papid> W96-0213 </papid>for english and chinese, and conn exor for spanish.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
dependency relations are produced using version of the collins parser (collins, 1997) <papid> P97-1003 </papid>that has been adapted for building dependencies.alignment features consist of features that are extracted from the outputs of individual alignment sys tems.</citsent>
<aftsection>
<nextsent>for each alignmentak ? a, the following are some of the alignment features that can be used to describe an instance (i, j): 1.
</nextsent>
<nextsent>whether (i, j) is an element of ak or not.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>translation probability p(fj |ei) computed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J518">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>a set of 491 english-chinese sentence pairs (nearly 13k words on each side) from 2002 nist mt evaluation test set.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
we computed precision, recall and error rate on the entire set of sentence pairs for each data set.5 to evaluate neuralign, we used giza++ in both directions (e-to-f and -to-e, where is either chinese (c) or spanish (s)) as input and refined alignment approach (och and ney, 2000) <papid> P00-1056 </papid>that usesa heuristic combination method called grow-diag final (koehn et al, 2003) <papid> N03-1017 </papid>for comparison.</citsent>
<aftsection>
<nextsent>(we henceforth refer to the refined-alignment approach as ra.?)
</nextsent>
<nextsent>for the english-spanish experiments, giza++ was trained on 48k sentence pairs from mixed corpus (un + bible + fbis), with nearly 1.2m of words on each side, using 10 iterations of model 1, 5 iterations of hmm, and 5 iterations of model 4.
</nextsent>
<nextsent>for the english-chinese experiments, we used 107ksentence pairs from fbis corpus (nearly 4.1m english and 3.3m chinese words) to train giza++, using 5 iterations of model 1, 5 iterations of hmm, 3 iterations of model 3, and 3 iterations of model 4.
</nextsent>
<nextsent>5.3 neural network settings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J519">
<title id=" H05-1009.xml">neuralign combining word alignments using neural networks </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>2.
</prevsent>
<prevsent>a set of 491 english-chinese sentence pairs (nearly 13k words on each side) from 2002 nist mt evaluation test set.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
we computed precision, recall and error rate on the entire set of sentence pairs for each data set.5 to evaluate neuralign, we used giza++ in both directions (e-to-f and -to-e, where is either chinese (c) or spanish (s)) as input and refined alignment approach (och and ney, 2000) <papid> P00-1056 </papid>that usesa heuristic combination method called grow-diag final (koehn et al, 2003) <papid> N03-1017 </papid>for comparison.</citsent>
<aftsection>
<nextsent>(we henceforth refer to the refined-alignment approach as ra.?)
</nextsent>
<nextsent>for the english-spanish experiments, giza++ was trained on 48k sentence pairs from mixed corpus (un + bible + fbis), with nearly 1.2m of words on each side, using 10 iterations of model 1, 5 iterations of hmm, and 5 iterations of model 4.
</nextsent>
<nextsent>for the english-chinese experiments, we used 107ksentence pairs from fbis corpus (nearly 4.1m english and 3.3m chinese words) to train giza++, using 5 iterations of model 1, 5 iterations of hmm, 3 iterations of model 3, and 3 iterations of model 4.
</nextsent>
<nextsent>5.3 neural network settings.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J520">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system builder must decide whether to adopt an existing technique or design new approach.
</prevsent>
<prevsent>a huge variety of techniques are described in the literature, many of them achieving high suc-cess rates on their own evaluation texts (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
hobbs 1986; strube 1998; <papid> P98-2204 </papid>mitkov 1998).<papid> P98-2143 </papid></citsent>
<aftsection>
<nextsent>each technique makes different assumptions about the data available to reference resolution, forex- ample, some assume perfect parses, others as-sume only pos-tagged input, some assume se-mantic information is available, etc. the chances are high that no published technique will ex-actly match the data available to particular sys-tem reference resolution component, so it may the authors thank james allen for help on this project, as well as the anonymous reviewers for helpful comments on the paper.
</nextsent>
<nextsent>this material is based on work supported by usaf/rome labs contract f30602-95-1-0025, onr grant n00014-95-1 - 1088, and columbia univ. grant opg: 1307.
</nextsent>
<nextsent>229 not be apparent which method will work best.
</nextsent>
<nextsent>choosing technique is especially problematic for designers of dialogue systems trying to pre-dict how anaphora resolution techniques devel-oped for written monologue will perform when adapted for spoken dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J521">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the system builder must decide whether to adopt an existing technique or design new approach.
</prevsent>
<prevsent>a huge variety of techniques are described in the literature, many of them achieving high suc-cess rates on their own evaluation texts (cf.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
hobbs 1986; strube 1998; <papid> P98-2204 </papid>mitkov 1998).<papid> P98-2143 </papid></citsent>
<aftsection>
<nextsent>each technique makes different assumptions about the data available to reference resolution, forex- ample, some assume perfect parses, others as-sume only pos-tagged input, some assume se-mantic information is available, etc. the chances are high that no published technique will ex-actly match the data available to particular sys-tem reference resolution component, so it may the authors thank james allen for help on this project, as well as the anonymous reviewers for helpful comments on the paper.
</nextsent>
<nextsent>this material is based on work supported by usaf/rome labs contract f30602-95-1-0025, onr grant n00014-95-1 - 1088, and columbia univ. grant opg: 1307.
</nextsent>
<nextsent>229 not be apparent which method will work best.
</nextsent>
<nextsent>choosing technique is especially problematic for designers of dialogue systems trying to pre-dict how anaphora resolution techniques devel-oped for written monologue will perform when adapted for spoken dialogue.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J522">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the supervisor layer controls which modules within layers 2 and 3 execute for particular ex-periment.
</prevsent>
<prevsent>we created two different supervisor see (byron and allen.
</prevsent>
</prevsection>
<citsent citstr=" P99-1079 ">
1999; tetreault, 1999) <papid> P99-1079 </papid>for results of pronoun resolution experiments run within the testbed.</citsent>
<aftsection>
<nextsent>proceedings of eacl  99 modules in the testbed.
</nextsent>
<nextsent>one of them simply reads configuration file with runtime flags hard-coded by the user.
</nextsent>
<nextsent>this allows the user to explicitly con-trol which parts of the system execute, and will be used when final reference resolution techniques is chosen for integration into the trips system parser (ferguson and allen, 1998).
</nextsent>
<nextsent>the second supervisor layer was coded as ge-netic algorithm (byron and allen, 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J523">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>there are number of design features that must be controlled in this layer, such as how the dis-course structure affects antecedent accessibility and which surface constituents rigger des.
</prevsent>
<prevsent>all these design decisions hould be implemented as independent modules so that they can be turned on or off for particular experiments.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
our experiments created translation modules for two evaluation corpora: written news sto-ries from the penn treebank corpus (marcus et al., 1993) <papid> J93-2004 </papid>and spoken task-oriented ialogues from the trains93 corpus (heeman and allen, 1995).</citsent>
<aftsection>
<nextsent>the input format and features added onto des from these two corpora are very different, but by encapsulating the translation layer, the same pronoun resolution code can be used for both domains.
</nextsent>
<nextsent>in both of our experiments only simple noun phrases in the surface form triggered des.
</nextsent>
<nextsent>treebank texts contain complete structural parsers, pos tags, and annotation of the antecedents of definite pronouns (added by ge et al 1998).<papid> W98-1119 </papid></nextsent>
<nextsent>because of the thorough syntac-tic information, des can be attributed with ex-plicit phrase structure information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J524">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>the input format and features added onto des from these two corpora are very different, but by encapsulating the translation layer, the same pronoun resolution code can be used for both domains.
</prevsent>
<prevsent>in both of our experiments only simple noun phrases in the surface form triggered des.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
treebank texts contain complete structural parsers, pos tags, and annotation of the antecedents of definite pronouns (added by ge et al 1998).<papid> W98-1119 </papid></citsent>
<aftsection>
<nextsent>because of the thorough syntac-tic information, des can be attributed with ex-plicit phrase structure information.
</nextsent>
<nextsent>this corpus contains unconstrained news stories, so semantic type information is not available.
</nextsent>
<nextsent>the treebank translator module adds the following features to each 1.
</nextsent>
<nextsent>de: whether its surface constituent is contained in reported speech; 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J525">
<title id=" E99-1031.xml">a flexible architecture for reference resolution </title>
<section> implementation.  </section>
<citcontext>
<prevsection>
<prevsent>modules within this layer can be coded to resolve variety of anaphoric phenomena in variety of ways.
</prevsent>
<prevsent>for example, particular experiment may be concerned only with resolving pronouns or it might also require determination of coreference between definite noun phrases.
</prevsent>
</prevsection>
<citsent citstr=" A88-1003 ">
this layer is rem-iniscent of the independent anaphora resolution modules in the lucy system (rich and luperfoy, 1988), <papid> A88-1003 </papid>except hat modules in that system were not designed to be easily turned on or off.</citsent>
<aftsection>
<nextsent>for our testbed, we implemented variety of pronoun resolution techniques.
</nextsent>
<nextsent>each technique 231 proceedings of eacl  99 pronoun resolution module baseline most-recent technique that chooses closest entity to the left of the pronoun choose most recent entity that matches sub-categorization restrictions on the verb strobe -list algorithm (strube, 1998) <papid> P98-2204 </papid>boost salience for the first entity in each sentence decrease salience for entities in prepositional phrases or relative clauses increase the salience for non-subject entities for demonstrative pronoun resolution (schiffman, 1985) decrease salience for indefinite ntities decrease salience for entities in reported speech increase the salience of entities in the subject of the previous sentence increase the salience of entities whose surface form is pronominal activated for treebank activated for trains93 x x x x x x x x table 1  pronoun resolution modules used in our experiments can run in isolation or with the addition of meta- modules that combine the output of multiple tech- niques.</nextsent>
<nextsent>we implemented meta-modules to in-terface to the genetic algorithm driver and to combine different salience factors into an over- all score (similar to (carbonell and brown, 1988; mitkov, 1998)).<papid> P98-2143 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J528">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the baseline model, which makes almost no use of morphology, achieves 81.2%/82.5%/77.0% in these same measures.we use the morphological model from the aforementioned experiments as base parser in second set of experiments.
</prevsent>
<prevsent>here we investigate the efficacy of reranking approach for parsing spanish by using 795arbitrary structural features.
</prevsent>
</prevsection>
<citsent citstr=" J05-1003 ">
previous work in statistical parsing (collins and koo, 2005) <papid> J05-1003 </papid>has shown that applying reranking techniques to the n-best output of base parser can improve parsing performance.</citsent>
<aftsection>
<nextsent>applying an exponentiated gradient reranking algorithm (bartlett et al , 2004) to the n-best output of our morphologically-informed spanish parsing model gives us similar improvements.
</nextsent>
<nextsent>using the reranking model combined with the morphological model raises performance to 85.1%/84.7%/80.2%f1 accuracy for labeled constituents, unlabeled dependencies, and labeled dependencies.
</nextsent>
<nextsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></nextsent>
<nextsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J530">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>applying an exponentiated gradient reranking algorithm (bartlett et al , 2004) to the n-best output of our morphologically-informed spanish parsing model gives us similar improvements.
</prevsent>
<prevsent>using the reranking model combined with the morphological model raises performance to 85.1%/84.7%/80.2%f1 accuracy for labeled constituents, unlabeled dependencies, and labeled dependencies.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></citsent>
<aftsection>
<nextsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.
</nextsent>
<nextsent>for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</nextsent>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J531">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></prevsent>
<prevsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</prevsent>
</prevsection>
<citsent citstr=" P03-1056 ">
for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</citsent>
<aftsection>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
<nextsent>the authors in (collins et al , 1999) <papid> P99-1065 </papid>describe an approach that gives 80% accuracy in recovering unlabeled dependencies in czech.1 the project that is arguably most akin to the work presented in this paper is that on spanish parsing(moreno et al , 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J532">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></prevsent>
<prevsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</prevsent>
</prevsection>
<citsent citstr=" C02-1126 ">
for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</citsent>
<aftsection>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
<nextsent>the authors in (collins et al , 1999) <papid> P99-1065 </papid>describe an approach that gives 80% accuracy in recovering unlabeled dependencies in czech.1 the project that is arguably most akin to the work presented in this paper is that on spanish parsing(moreno et al , 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J533">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></prevsent>
<prevsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</citsent>
<aftsection>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
<nextsent>the authors in (collins et al , 1999) <papid> P99-1065 </papid>describe an approach that gives 80% accuracy in recovering unlabeled dependencies in czech.1 the project that is arguably most akin to the work presented in this paper is that on spanish parsing(moreno et al , 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J534">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></prevsent>
<prevsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</prevsent>
</prevsection>
<citsent citstr=" P05-1038 ">
for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</citsent>
<aftsection>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
<nextsent>the authors in (collins et al , 1999) <papid> P99-1065 </papid>describe an approach that gives 80% accuracy in recovering unlabeled dependencies in czech.1 the project that is arguably most akin to the work presented in this paper is that on spanish parsing(moreno et al , 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J536">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the statistical parsing of english has surpassed 90%accuracy in the precision and recall of labeled constituents (e.g., (collins, 1999; charniak and johnson, 2005)).<papid> P05-1022 </papid></prevsent>
<prevsent>a recent proliferation of treebanks in various languages has fueled research in the parsing of other languages.</prevsent>
</prevsection>
<citsent citstr=" P03-1013 ">
for instance, work hasbeen done in chinese using the penn chinese tree bank (levy and manning, 2003; <papid> P03-1056 </papid>chiang and bikel,2002), <papid> C02-1126 </papid>in czech using the prague dependency tree bank (collins et al , 1999), <papid> P99-1065 </papid>in french using the french treebank (arun and keller, 2005), <papid> P05-1038 </papid>inger man using the negra treebank (dubey, 2005; dubey and keller, 2003), <papid> P03-1013 </papid>and in spanish using the uam spanish treebank (moreno et al , 2000).</citsent>
<aftsection>
<nextsent>the best reported f1 constituency scores from this work for each language are 79.9% (chinese (chiang and bikel, 2002)), <papid> C02-1126 </papid>81.0% (french (arun and keller, 2005), <papid> P05-1038 </papid>76.2% (german (dubey, 2005)), and 73.8% (spanish (moreno et al , 2000)).</nextsent>
<nextsent>the authors in (collins et al , 1999) <papid> P99-1065 </papid>describe an approach that gives 80% accuracy in recovering unlabeled dependencies in czech.1 the project that is arguably most akin to the work presented in this paper is that on spanish parsing(moreno et al , 2000).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J545">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>the trees contain information about both constituency structure and syntactic functions.
</prevsent>
<prevsent>4.1 preprocessing.
</prevsent>
</prevsection>
<citsent citstr=" J98-4004 ">
it is well-known that tree representation influences parsing performance (johnson, 1998).<papid> J98-4004 </papid></citsent>
<aftsection>
<nextsent>prior to training our models, we made some systematic modifications to the corpus trees in an effort to make it easier for model 1 to represent the linguistic phenomena present in the trees.
</nextsent>
<nextsent>for the convenience of the reader, table 2 gives key to the non-terminal labels in the 3lb treebank that are used in this section and the remainder of the paper.relative and subordinate clauses cases of relative and subordinate clauses appearing in the corpus trees have the basic structure of the example in figure 2a.
</nextsent>
<nextsent>figure 2b shows the modifications we im pose on such structures.
</nextsent>
<nextsent>the modified structure has the advantage that the sbar selects the cp node asits head, making the relative pronoun que the head word for the root of the subtree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J546">
<title id=" H05-1100.xml">morphology and reranking for the statistical parsing of spanish </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>row 3 gives the accuracy of the reranking approach, when applied to n-best output from the model in row 2.
</prevsent>
<prevsent>5.1 the effects of morphology.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
in our first experiments, we trained over 50 models, incorporating different morphological information into each in the way described in section 3.1.prior to running the parsers, we trained the pos tagger described in (collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>the output fromthe tagger was used to assign pos label for unknown words.
</nextsent>
<nextsent>we only attempted to parse sentences under 70 words in length.
</nextsent>
<nextsent>table 3 describes some of the models we tried during development and gives results for each.
</nextsent>
<nextsent>our baseline model, which we used to evaluate the effects of using morphology, was model 1 (collins, 1999) with simple pos tagset containing almost no morphological information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J551">
<title id=" H05-1004.xml">on coreference resolution performance metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>that is, there should be an intuitive sense of how good system is when metric suggests that certain percentage of coreference results are correct.
</prevsent>
<prevsent>for example, when metric reports
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
or above correct for system, we would expect that the vast majority of mentions are in right entities or coreference chains.a widely-used metric is the link-based f-measure (vi lain et al, 1995) <papid> M95-1005 </papid>adopted in the muc task.</citsent>
<aftsection>
<nextsent>it is computed by first counting the number of common links between the reference (or truth?)
</nextsent>
<nextsent>and the system output (or re sponse?); the link precision is the number of common links divided by the number of links in the system out put, and the link recall is the number of common links divided by the number of links in the reference.
</nextsent>
<nextsent>there are known problems associated with the link-based measure.
</nextsent>
<nextsent>first, it ignores single-mention entities since no link can be found in these entities; second, and more importantly, it fails to distinguish system outputs with different qualities: the link-based f-measure intrinsically favors systems producing fewer entities, and may result 25 in higher f-measures for worse systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J553">
<title id=" H05-1004.xml">on coreference resolution performance metrics </title>
<section> comparison with other metrics.  </section>
<citcontext>
<prevsection>
<prevsent>the second column contains the number of system-proposed entities.
</prevsent>
<prevsent>the column under muc-f is the muc f-measure while ? ceaf is the mention-based ceaf.
</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
the coreference system is similar to the one used in (luo et al, 2004).<papid> P04-1018 </papid></citsent>
<aftsection>
<nextsent>results in table 3 are produced by system trained on the muc6 training data and tested on the ? official muc6 test documents.
</nextsent>
<nextsent>the test set contains ?? reference entities.
</nextsent>
<nextsent>the coreference system uses penalty parameter to balance miss and false alarm errors: the smaller the parameter, the fewer entities will be generated.
</nextsent>
<nextsent>we vary the parameter from ?-2 ? to c  ,listed in the first column of table 3, and compare the system performance measured by the muc f-measure and the proposed mention-based ceaf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J554">
<title id=" H05-1073.xml">emotions from text machine learning for text based emotion prediction </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>again, while lexical items with clear emotional meaning,such as happy or sad, matter, emotion classification probably needs to consider additional inferencemechanisms.
</prevsent>
<prevsent>moreover, nave compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation.many nlp problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g.
</prevsent>
</prevsection>
<citsent citstr=" J04-3002 ">
(wiebe et al , 2004), <papid> J04-3002 </papid>measuring strength of subjective clauses (wilson, wiebe and hwa, 2004), determining word polarity (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>or texts?</citsent>
<aftsection>
<nextsent>attitudinal valence, e.g.
</nextsent>
<nextsent>(turney, 2002), (<papid> P02-1053 </papid>bai, padman and airoldi, 2004), (beineke, hastie and vaithyanathan, 2003), (mullen and collier, 2003), (pang and lee, 2003).</nextsent>
<nextsent>here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passage sin childrens stories, and eventually use this information for rendering expressive child-directed storytelling in text-to-speech application.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J555">
<title id=" H05-1073.xml">emotions from text machine learning for text based emotion prediction </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>again, while lexical items with clear emotional meaning,such as happy or sad, matter, emotion classification probably needs to consider additional inferencemechanisms.
</prevsent>
<prevsent>moreover, nave compositional approach to emotion recognition is risky due to simple linguistic facts, such as context-dependent semantics, domination of words with multiple meanings, and emotional negation.many nlp problems address attitudinal meaning distinctions in text, e.g. detecting subjective opinion documents or expressions, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
(wiebe et al , 2004), <papid> J04-3002 </papid>measuring strength of subjective clauses (wilson, wiebe and hwa, 2004), determining word polarity (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>or texts?</citsent>
<aftsection>
<nextsent>attitudinal valence, e.g.
</nextsent>
<nextsent>(turney, 2002), (<papid> P02-1053 </papid>bai, padman and airoldi, 2004), (beineke, hastie and vaithyanathan, 2003), (mullen and collier, 2003), (pang and lee, 2003).</nextsent>
<nextsent>here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passage sin childrens stories, and eventually use this information for rendering expressive child-directed storytelling in text-to-speech application.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J556">
<title id=" H05-1073.xml">emotions from text machine learning for text based emotion prediction </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>(wiebe et al , 2004), <papid> J04-3002 </papid>measuring strength of subjective clauses (wilson, wiebe and hwa, 2004), determining word polarity (hatzivassiloglou and mckeown, 1997) <papid> P97-1023 </papid>or texts?</prevsent>
<prevsent>attitudinal valence, e.g.</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
(turney, 2002), (<papid> P02-1053 </papid>bai, padman and airoldi, 2004), (beineke, hastie and vaithyanathan, 2003), (mullen and collier, 2003), (pang and lee, 2003).</citsent>
<aftsection>
<nextsent>here, it suffices to say that the targets, the domain, and the intended application differ; our goal is to classify emotional text passage sin childrens stories, and eventually use this information for rendering expressive child-directed storytelling in text-to-speech application.
</nextsent>
<nextsent>this can be useful, e.g. in therapeutic education of children with communication disorders (van santen et al , 2003).
</nextsent>
<nextsent>this part covers the experimental study with formal problem definition, computational implementation, data, features, and note on parameter tuning.
</nextsent>
<nextsent>4.1 machine learning model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J557">
<title id=" H05-1073.xml">emotions from text machine learning for text based emotion prediction </title>
<section> empirical study.  </section>
<citcontext>
<prevsection>
<prevsent>the mapping is based on = {f1, f2, .., fn}, where contains the features derived from the text.
</prevsent>
<prevsent>furthermore, if multiple emotion classes can characterize s, then given e?
</prevsent>
</prevsection>
<citsent citstr=" C02-1150 ">
e, the target of the mapping function becomes the ordered pair (s,e?).finally, as further discussed in section 6, the hierarchical case of label assignment requires sequen 580tial model that further defines levels of coarse versus fine-grained classifiers, as done by (li and roth, 2002) <papid> C02-1150 </papid>for the question classification problem.</citsent>
<aftsection>
<nextsent>4.2 implementation.
</nextsent>
<nextsent>whereas our goal is to predict finer emotional meaning distinctions according to emotional categories in speech; in this study, we focus on the basic task of recognizing emotional passages and on determining their valence (i.e. positive versus negative) because we currently do not have enough training data to explore finer-grained distinctions.
</nextsent>
<nextsent>the goal here is to get good understanding of the nature of the tep problem and explore features which may be useful.we explore two cases of flat classification, using variation of the winnow update rule implemented in the snow learning architecture (carl sonet al , 1999),1 which learns linear classifier in feature space, and has been successful in several nlp applications, e.g. semantic role labeling (koomen, punyakanok, roth and yih, 2005).
</nextsent>
<nextsent>in the first case, the set of emotion classes consists of emotional versus non-emotional or neutral, i.e. = {n,e}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J559">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also demonstrate how discourse chunking canbe successfully applied to sentence compression task.
</prevsent>
<prevsent>the computational treatment of discourse phenomena has recently attracted much attention, partly dueto their increasing importance for potential applications.
</prevsent>
</prevsection>
<citsent citstr=" J02-4002 ">
in summarisation, for example, the extraction of sentences to include in summary crucially depends on their rhetorical status (marcu, 2000; teufel and moens, 2002); <papid> J02-4002 </papid>one might want to extract contrastive or explanatory statements while omitting sentences that contain background information.in information extraction, discourse-level knowledge can be used to identify co-referring events(humphreys et al, 1997) <papid> W97-1311 </papid>and to determine their temporal order.</citsent>
<aftsection>
<nextsent>discourse processing could further enhance question answering systems by interpreting the users question either in isolation or in the context of preceding questions (chai and jing, 2004).
</nextsent>
<nextsent>discourse analysis is often viewed as parsing task.
</nextsent>
<nextsent>rhetorical structure theory (rst, mann and thomson, 1988), one of the most influential frameworks in discourse processing, represents texts bytrees whose leaves correspond to elementary discourse units (edus) and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., contrast, elaboration).
</nextsent>
<nextsent>discourse units are further characterised in terms of their text im portance: nuclei denote central segments, whereas satellites denote peripheral ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J560">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also demonstrate how discourse chunking canbe successfully applied to sentence compression task.
</prevsent>
<prevsent>the computational treatment of discourse phenomena has recently attracted much attention, partly dueto their increasing importance for potential applications.
</prevsent>
</prevsection>
<citsent citstr=" W97-1311 ">
in summarisation, for example, the extraction of sentences to include in summary crucially depends on their rhetorical status (marcu, 2000; teufel and moens, 2002); <papid> J02-4002 </papid>one might want to extract contrastive or explanatory statements while omitting sentences that contain background information.in information extraction, discourse-level knowledge can be used to identify co-referring events(humphreys et al, 1997) <papid> W97-1311 </papid>and to determine their temporal order.</citsent>
<aftsection>
<nextsent>discourse processing could further enhance question answering systems by interpreting the users question either in isolation or in the context of preceding questions (chai and jing, 2004).
</nextsent>
<nextsent>discourse analysis is often viewed as parsing task.
</nextsent>
<nextsent>rhetorical structure theory (rst, mann and thomson, 1988), one of the most influential frameworks in discourse processing, represents texts bytrees whose leaves correspond to elementary discourse units (edus) and whose nodes specify how these and larger units (e.g., multi-sentence segments) are linked to each other by rhetorical relations (e.g., contrast, elaboration).
</nextsent>
<nextsent>discourse units are further characterised in terms of their text im portance: nuclei denote central segments, whereas satellites denote peripheral ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J561">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>discourse units are further characterised in terms of their text im portance: nuclei denote central segments, whereas satellites denote peripheral ones.
</prevsent>
<prevsent>recent advances in discourse modelling have greatly benefited from the availability of resources annotated with discourse-level information such as the rst discourse treebank (rst-dt, carlson et al., 2002).
</prevsent>
</prevsection>
<citsent citstr=" N03-1030 ">
even though discourse parsing at the document-level still poses significant challenge todata-driven methods, sentence-level discourse models (e.g., soricut and marcu, 2003) <papid> N03-1030 </papid>trained on therst-dt have attained accuracies comparable to human performance.</citsent>
<aftsection>
<nextsent>the availability of discourse annotations is partly responsible for the success ofthese models.
</nextsent>
<nextsent>another important reason is the development of robust syntactic parsers (e.g., charniak, 2000) <papid> A00-2018 </papid>that can be used to provide critical structural and lexical information to the discourse parser.</nextsent>
<nextsent>unfortunately, discourse annotated corpora are largely absent for languages other than english.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J562">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even though discourse parsing at the document-level still poses significant challenge todata-driven methods, sentence-level discourse models (e.g., soricut and marcu, 2003) <papid> N03-1030 </papid>trained on therst-dt have attained accuracies comparable to human performance.</prevsent>
<prevsent>the availability of discourse annotations is partly responsible for the success ofthese models.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
another important reason is the development of robust syntactic parsers (e.g., charniak, 2000) <papid> A00-2018 </papid>that can be used to provide critical structural and lexical information to the discourse parser.</citsent>
<aftsection>
<nextsent>unfortunately, discourse annotated corpora are largely absent for languages other than english.
</nextsent>
<nextsent>furthermore, reliance on syntactic parsing renders discourse parsing practically impossible for languages for which state-of-the-art parsers are unavailable.
</nextsent>
<nextsent>in this paper we propose discourse chunking as an alternative to discourse parsing.
</nextsent>
<nextsent>analogous to sentence chunking, discourse chunking is an intermediate step towards full parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J566">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> discourse chunking.  </section>
<citcontext>
<prevsection>
<prevsent>we do not try to determine that the first two edus are merged at higher level and then function as the overall nucleus of the sentence.the discourse chunking task assumes non hierarchical representation.
</prevsent>
<prevsent>we converted eachsentence-level discourse tree into flat chunk representation by assigning each token (i.e., word or punctuation mark) tag encoding its nuclearity status at the edu level.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
we adopted the chunk representation proposed by ramshaw and marcus (1995) <papid> W95-0107 </papid>and used four different tags: b-nuc and b-sat for nucleus and satellite-initial tokens, and i-nuc andi-sat for non-initial tokens, i.e., tokens inside nucleus and satellite span.</citsent>
<aftsection>
<nextsent>as all tokens belong either 258to nucleus or satellite span, we do not need special tag (typically denoted by in syntactic chunk ing) to indicate elements outside chunk.
</nextsent>
<nextsent>the chunk representation for the sentence in figure 1 is thus: ?/b-nuc i/i-nuc am/i-nuc optimistic/i-nuc ?/i-nuc said/b-sat mr./i-sat smith/i-satas/b-sat the/i-sat market/i-sat plunged/i sat ./i-sat discourse and sentence structure do not always correspond, and for 5% of sentences in the rst-dt no discourse tree exists.
</nextsent>
<nextsent>we excluded these from our data.
</nextsent>
<nextsent>we also disregarded sentences without internal structure, i.e., those which consist of only one edu.the rst-dt is partitioned into training (342 arti cles) and test set (43 articles).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J567">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>we measured both un labelled and labelled agreement on the 52 doubly annotated rst-dt texts.
</prevsent>
<prevsent>the former measures whether humans agree in placing chunk boundaries, whereas the latter additionally measures 260 whether humans agree in assigning chunk labels.
</prevsent>
</prevsection>
<citsent citstr=" J02-1002 ">
to facilitate comparison with our models we report inter-annotator agreement in terms of accuracy andf-score.3 for the un labelled case we also report window difference (wdiff), commonly used evaluation measure for segmentation tasks (pevzner and hearst, 2002).<papid> J02-1002 </papid></citsent>
<aftsection>
<nextsent>it returns values between 0 (identicalsegmentations) and 1 (maximally different segmen tations) and differs from accuracy in that predicted boundaries which are only slightly off are penalised less than those which are completely wrong.human agreement is relatively high4 on both segmentation and span labelling (see table 1), which can be explained by the fact that (i) the rst-dtannotators were given very detailed and precise instructions and (ii) assigning boundaries and labels is an easier task than creating full-scale discourse trees.
</nextsent>
<nextsent>4.2 one-step chunking.
</nextsent>
<nextsent>for the one-step chunking method, our training set consists of approximately 130,000 instances (i.e., to kens).
</nextsent>
<nextsent>we set aside 10% as development set for optimising boostexters parameters (i.e., the number of training iterations and the maximal length of the n-grams considered for text-valued features).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J569">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>sentence compression can be likened to summarisation at the sentence level.
</prevsent>
<prevsent>the task has an immediate impact on several applications ranging from summarisation to audio scanning devices for the blind and caption generation (see knight and marcu,2002 and the references therein).
</prevsent>
</prevsection>
<citsent citstr=" N03-1026 ">
previous data driven approaches (knight and marcu, 2003; riezler et al, 2003) <papid> N03-1026 </papid>relied on parallel corpora to determine what is important in sentence.</citsent>
<aftsection>
<nextsent>the models learned correspondences between long sentences and their shorter counterparts, typically employing rich feature space induced from parse trees.
</nextsent>
<nextsent>the task is challenging since the compressed sentences should retain essential information and convey it grammatically.
</nextsent>
<nextsent>here, we propose complementary approach which utilises discourse chunking.
</nextsent>
<nextsent>a compressed sentence can be obtained from the output of thechunker simply by removing satellites.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J571">
<title id=" H05-1033.xml">discourse chunking and its application to sentence compression </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>spade administration officials traveling with president bush in costa rica interpreted mr. ortegas wavering as sign.
</prevsent>
<prevsent>human administration officials interpreted mr. ortegas wavering as sign that he is searching for ways to strengthen his hand prior to the elections.
</prevsent>
</prevsection>
<citsent citstr=" A00-2024 ">
table 3: example compress ions compression avglen rating baseline 9.70 1.93 bt-2-step 22.06 3.21 spade 19.09 3.10 humans 20.07 3.83 table 4: mean ratings for automatic compressionsnally, we added simple baseline compression algorithm proposed by jing and mckeown (2000)<papid> A00-2024 </papid>which removed all prepositional phrases, clauses, to infiniti ves, and gerunds.</citsent>
<aftsection>
<nextsent>both the baseline and spade operate on parse trees which were obtained from charniaks (2000) parser.
</nextsent>
<nextsent>our set of experimental materials contained 440 = 160 compressions.procedure and subjects we obtained compression ratings during an elicitation study completed by 45 unpaid volunteers, all native speaker of english.
</nextsent>
<nextsent>the study was conducted remotely overthe internet.
</nextsent>
<nextsent>participants first saw set of instructions that explained the task, and defined sentence compression using multiple examples.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J572">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>resolution of structural ambiguity problems such as noun compound bracketing, prepositional phrase(pp) attachment, and noun phrase coordination requires using information about lexical items and their cooccurrences.
</prevsent>
<prevsent>this in turn leads to the data sparseness problem, since algorithms that relyon making decisions based on individual lexical items must have statistics about every word that may be encountered.
</prevsent>
</prevsection>
<citsent citstr=" P01-1005 ">
past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived.more recently, banko and brill (2001) <papid> P01-1005 </papid>have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources.</citsent>
<aftsection>
<nextsent>they demonstrate the ideaon lexical disambiguation problem for which labeled examples are available for free?.
</nextsent>
<nextsent>the problem is to choose which of 2-3 commonly confused words (e.g., {principle, principal}) are appropriate forgiven context.
</nextsent>
<nextsent>the labeled data comes for free?
</nextsent>
<nextsent>by assuming that in most edited written text, the words are used correctly, so training can be done directly from the text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J578">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>banko and brill (2001) <papid> P01-1005 </papid>show that even using very simple algorithm, the results continue to improve log-linearly with more training data, even out to billion words.</prevsent>
<prevsent>a potential limitation of this approach is the question of how applicable it is for nlp problems more generally ? how can we treat large corpus as labeled collection for wide range of nlp tasks?</prevsent>
</prevsection>
<citsent citstr=" N04-1016 ">
in related strand of work, lapata and keller (2004) <papid> N04-1016 </papid>show that computing n-gram statistics over very large corpora yields results that are competitive with if not better than the best supervised and knowledge-based approaches on wide range of nlp tasks.</citsent>
<aftsection>
<nextsent>for example, they show that for the problem of noun compound bracketing, the performance of an n-gram based model computed using search engine statistics was not significantly different from the best supervised algorithm whose parameters were tuned and which used taxonomy.
</nextsent>
<nextsent>they find however that these approaches generally fail to outperform supervised state-of-the-art models that are trained on smaller corpora, and so conclude that web-based n-gram statistics should be the base line to beat.
</nextsent>
<nextsent>we feel the potential of these ideas is not yet fully realized.
</nextsent>
<nextsent>we are interested in finding ways to further exploit the availability of enormous web corpora as implicit training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J585">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is especially important for structural ambiguity problems in which the decisions must be made on the basis of the behavior 835 of individual lexical items.
</prevsent>
<prevsent>the trick is to figure out how to use information that is latent in the web as corpus, and web search engines as query interfaces to that corpus.in this paper we describe two techniques ? surface features and paraphrases ? that push the ideas of banko and brill (2001) <papid> P01-1005 </papid>and lapata and keller (2004) <papid> N04-1016 </papid>farther, enabling the use of statistics gathered from very large corpora in an unsupervised man ner.</prevsent>
</prevsection>
<citsent citstr=" W05-0603 ">
in recent work (nakov and hearst, 2005) <papid> W05-0603 </papid>we showed that variation of the techniques, when applied to the problem of noun compound bracketing, produces higher accuracy than lapata and keller(2004) <papid> N04-1016 </papid>and the best supervised results.</citsent>
<aftsection>
<nextsent>in this paper we adapt the techniques to the structural disambiguation problems of prepositional phrase attachment and noun compound coordination.
</nextsent>
<nextsent>a long-standing challenge for syntactic parsers is the attachment decision for prepositional phrases.
</nextsent>
<nextsent>ina configuration where verb takes noun complement that is followed by pp, the problem arises of whether the pp attaches to the noun or to the verb.
</nextsent>
<nextsent>consider the following contrastive pair of sentences: (1) peter spent millions of dollars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J587">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>most recent work can be divided into supervised and unsupervised approaches.
</prevsent>
<prevsent>supervised approaches tend to make use of semantic classes or thesauri in order to deal with data sparsenessproblems.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
brill and resnik (1994) <papid> C94-2195 </papid>used the supervised transformation-based learning method and lexical and conceptual classes derived from wordnet, achieving 82% precision on 500 randomly selected examples.</citsent>
<aftsection>
<nextsent>ratnaparkhi et al (1994) <papid> H94-1048 </papid>created benchmark dataset of 27,937 quadruples(v, n1, p, n2), extracted from the wall street jour nal.</nextsent>
<nextsent>they found the human performance on this task to be 88%1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J588">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>supervised approaches tend to make use of semantic classes or thesauri in order to deal with data sparsenessproblems.
</prevsent>
<prevsent>brill and resnik (1994) <papid> C94-2195 </papid>used the supervised transformation-based learning method and lexical and conceptual classes derived from wordnet, achieving 82% precision on 500 randomly selected examples.</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
ratnaparkhi et al (1994) <papid> H94-1048 </papid>created benchmark dataset of 27,937 quadruples(v, n1, p, n2), extracted from the wall street jour nal.</citsent>
<aftsection>
<nextsent>they found the human performance on this task to be 88%1.
</nextsent>
<nextsent>using this dataset, they trained maximum entropy model and binary hierarchy ofword classes derived by mutual information, achieving 81.6% precision.
</nextsent>
<nextsent>collins and brooks (1995) <papid> W95-0103 </papid>used supervised back-off model to achieve 84.5% precision on the ratnaparkhi test set.</nextsent>
<nextsent>stetina and makoto (1997) <papid> W97-0109 </papid>use supervised method with decision tree and wordnet classes to achieve 88.1% precision on the same test set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J589">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>they found the human performance on this task to be 88%1.
</prevsent>
<prevsent>using this dataset, they trained maximum entropy model and binary hierarchy ofword classes derived by mutual information, achieving 81.6% precision.
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
collins and brooks (1995) <papid> W95-0103 </papid>used supervised back-off model to achieve 84.5% precision on the ratnaparkhi test set.</citsent>
<aftsection>
<nextsent>stetina and makoto (1997) <papid> W97-0109 </papid>use supervised method with decision tree and wordnet classes to achieve 88.1% precision on the same test set.</nextsent>
<nextsent>toutanova et al (2004)use supervised method that makes use of morphological and syntactic analysis and wordnet synsets, yielding 87.5% accuracy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J590">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>using this dataset, they trained maximum entropy model and binary hierarchy ofword classes derived by mutual information, achieving 81.6% precision.
</prevsent>
<prevsent>collins and brooks (1995) <papid> W95-0103 </papid>used supervised back-off model to achieve 84.5% precision on the ratnaparkhi test set.</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
stetina and makoto (1997) <papid> W97-0109 </papid>use supervised method with decision tree and wordnet classes to achieve 88.1% precision on the same test set.</citsent>
<aftsection>
<nextsent>toutanova et al (2004)use supervised method that makes use of morphological and syntactic analysis and wordnet synsets, yielding 87.5% accuracy.
</nextsent>
<nextsent>in the unsupervised approaches, the attachment decision depends largely on co-occurrence statistics drawn from text collections.
</nextsent>
<nextsent>the pioneering work in this area was that of hindle and rooth (1993).<papid> J93-1005 </papid></nextsent>
<nextsent>using partially parsed corpus, they calculate and compare lexical associations over subsets of the tuple (v, n1, p), ignoring n2, and achieve 80% precision at 80% recall.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J591">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>toutanova et al (2004)use supervised method that makes use of morphological and syntactic analysis and wordnet synsets, yielding 87.5% accuracy.
</prevsent>
<prevsent>in the unsupervised approaches, the attachment decision depends largely on co-occurrence statistics drawn from text collections.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
the pioneering work in this area was that of hindle and rooth (1993).<papid> J93-1005 </papid></citsent>
<aftsection>
<nextsent>using partially parsed corpus, they calculate and compare lexical associations over subsets of the tuple (v, n1, p), ignoring n2, and achieve 80% precision at 80% recall.
</nextsent>
<nextsent>more recently, ratnaparkhi (1998) <papid> P98-2177 </papid>developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms.</nextsent>
<nextsent>an extraction heuristic is used to identify unambiguous attachment decisions,for example, the algorithm can assume noun attachment if there is no verb within words to the left of the preposition in given sentence, among other conditions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J592">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>the pioneering work in this area was that of hindle and rooth (1993).<papid> J93-1005 </papid></prevsent>
<prevsent>using partially parsed corpus, they calculate and compare lexical associations over subsets of the tuple (v, n1, p), ignoring n2, and achieve 80% precision at 80% recall.</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
more recently, ratnaparkhi (1998) <papid> P98-2177 </papid>developed an unsupervised method that collects statistics from text annotated with part-of-speech tags and morphological base forms.</citsent>
<aftsection>
<nextsent>an extraction heuristic is used to identify unambiguous attachment decisions,for example, the algorithm can assume noun attachment if there is no verb within words to the left of the preposition in given sentence, among other conditions.
</nextsent>
<nextsent>this extraction heuristic uncovered 910k unique tuples of the form (v, p, n2) and(n, p, n2), although the results are very noisy, suggesting the correct attachment only about 69% of thetime.
</nextsent>
<nextsent>the tuples are used as training data for classifiers, the best of which achieves 81.9% precision on the ratnaparkhi test set.
</nextsent>
<nextsent>pantel and lin (2000)<papid> P00-1014 </papid>describe an unsupervised method that uses collocation database, thesaurus, dependency parser, and large corpus (125m words), achieving 84.3%precision on the ratnaparkhi test set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J595">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> prepositional phrase attachment.  </section>
<citcontext>
<prevsection>
<prevsent>this extraction heuristic uncovered 910k unique tuples of the form (v, p, n2) and(n, p, n2), although the results are very noisy, suggesting the correct attachment only about 69% of thetime.
</prevsent>
<prevsent>the tuples are used as training data for classifiers, the best of which achieves 81.9% precision on the ratnaparkhi test set.
</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
pantel and lin (2000)<papid> P00-1014 </papid>describe an unsupervised method that uses collocation database, thesaurus, dependency parser, and large corpus (125m words), achieving 84.3%precision on the ratnaparkhi test set.</citsent>
<aftsection>
<nextsent>using sim 1when presented with whole sentence, average humans score 93%.
</nextsent>
<nextsent>836 ple combinations of web-based n-grams, lapata and keller (2005) achieve lower results, in the low 70s. using different collection consisting of german pp-attachment decisions, volk (2000) uses the web to obtain n-gram counts.
</nextsent>
<nextsent>he compared pr(p|n1) to pr(p|v), where pr(p|x) = #(x, p)/#(x).
</nextsent>
<nextsent>here can be n1 or v. the bigram frequencies #(x, p) were obtained using the alta vista near operator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J603">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> coordination.  </section>
<citcontext>
<prevsection>
<prevsent>63.65 30.54 n1 is pronoun 98.48 3.04 is to be 79.23 9.53 surface features (summed) 73.13 9.26 maj. vote, of ? noun 85.011.21 91.77 maj. vote, of ? noun, n/a ? verb 83.631.30 100.00 table 2: pp-attachment results, in percentages.
</prevsent>
<prevsent>ity] of life].
</prevsent>
</prevsection>
<citsent citstr=" P92-1003 ">
from semantic point of view, weneed to determine whether the or in chronic diseases or disabilities really means or or is used as an and (agarwal and boggess, 1992).<papid> P92-1003 </papid></citsent>
<aftsection>
<nextsent>finally, we needto choose between non-elided and an elided read ing: [[chronic diseases] or disabilities] vs. [chronic [diseases or disabilities]].
</nextsent>
<nextsent>below we focus on special case of the latterproblem: noun compound (nc) coordination.
</nextsent>
<nextsent>consider the nc car and truck production.
</nextsent>
<nextsent>its real meaning is car production and truck production.however, due to the principle of economy of expression, the first instance of production has been compressed out by means of ellipsis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J604">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> coordination.  </section>
<citcontext>
<prevsection>
<prevsent>syntactically, this can be expressed by the following brack etings: [[n1 n2] h] versus [n1 [n2 h]].
</prevsent>
<prevsent>(collins?
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
parser (collins, 1997) <papid> P97-1003 </papid>always predicts flat np for such configurations.)</citsent>
<aftsection>
<nextsent>in order to make the task more 4the configurations of the kind h1 h2 (e.g., company/n cars/h1 and/c trucks/h2) can be handled in similar way.
</nextsent>
<nextsent>839 realistic (from parsers perspective), we ignore the option of all-way coordination and try to predict the bracketing in penn treebank (marcus et al, 1994) for configurations of this kind.
</nextsent>
<nextsent>the penn treebank brackets ncs with ellipsis as, e.g., (np car/nn and/cc truck/nn production/nn).
</nextsent>
<nextsent>and without ellipsis as(np (np president/nn) and/cc (np chief/nn exec utive/nn))the nps with ellipsis are flat, while the others contain internal nps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J605">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> coordination.  </section>
<citcontext>
<prevsection>
<prevsent>the all-way coordinations can appear bracketed either way and make the task harder.
</prevsent>
<prevsent>3.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" J82-3004 ">
coordination ambiguity is under-explored, despite being one of the three major sources of structural ambiguity (together with prepositional phrase attachment and noun compound bracketing), and belonging to the class of ambiguities for which the number of analyses is the number of binary trees over the corresponding nodes (church and patil, 1982), <papid> J82-3004 </papid>and despite the fact that conjunctions are among the most frequent words.rus et al (2002) present deterministic rule based approach for bracketing in context of coordinated ncs of the kind n1 n2 h, as necessary step towards logical form derivation.</citsent>
<aftsection>
<nextsent>their algorithm uses pos tagging, syntactic parses, semantic senses of the nouns (manually annotated), lookups in semantic network (wordnet) and the type of the coordination conjunction to make 3-way clas sifi cation: ellipsis, no ellipsis and all-way coordination.
</nextsent>
<nextsent>using back-off sequence of 3 different heuristics, they achieve 83.52% precision (baseline 61.52%) ona set of 298 examples.
</nextsent>
<nextsent>when 3 additional context dependent heuristics and 224 additional examples with local contexts are added, the precision jumps to 87.42% (baseline 52.35%), with 71.05% recall.resnik (1999) disambiguates two kinds of pat terns: n1 and n2 n3 and n1 n2 and n3 n4 (e.g., [food/n1 [handling/n2 and/c storage/n3] procedures/n4]).
</nextsent>
<nextsent>while there are two options for the former (all-way coordinations are not allowed),there are 5 valid bracketings for the latter.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J606">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> coordination.  </section>
<citcontext>
<prevsection>
<prevsent>when 3 additional context dependent heuristics and 224 additional examples with local contexts are added, the precision jumps to 87.42% (baseline 52.35%), with 71.05% recall.resnik (1999) disambiguates two kinds of pat terns: n1 and n2 n3 and n1 n2 and n3 n4 (e.g., [food/n1 [handling/n2 and/c storage/n3] procedures/n4]).
</prevsent>
<prevsent>while there are two options for the former (all-way coordinations are not allowed),there are 5 valid bracketings for the latter.
</prevsent>
</prevsection>
<citsent citstr=" C92-1029 ">
following kurohashi and nagao (1992), <papid> C92-1029 </papid>resnik makes decisions based on similarity of form (i.e., numberagreement: p=53%, r=90.6%), similarity of meaning (p=66%, r=71.2%) and conceptual association example predicts p(%) r(%) (buy) and sell orders no ellipsis 33.33 1.40 buy (and sell orders) no ellipsis 70.00 4.67 buy: and sell orders no ellipsis 0.00 0.00 buy; and sell orders no ellipsis 66.67 2.80 buy.</citsent>
<aftsection>
<nextsent>and sell orders no ellipsis 68.57 8.18 buy[...]
</nextsent>
<nextsent>and sell orders no ellipsis 49.00 46.73 buy- and sell orders ellipsis 77.27 5.14 buy and sell / orders ellipsis 50.54 21.73 (buy and sell) orders ellipsis 92.31 3.04 buy and sell (orders) ellipsis 90.91 2.57 buy and sell, orders ellipsis 92.86 13.08 buy and sell: orders ellipsis 93.75 3.74 buy and sell; orders ellipsis 100.00 1.87 buy and sell.
</nextsent>
<nextsent>orders ellipsis 93.33 7.01 buy and sell[...]
</nextsent>
<nextsent>orders ellipsis 85.19 18.93 table 3: coordination surface features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J607">
<title id=" H05-1105.xml">using the web as an implicit training set application to structural ambiguity resolution </title>
<section> coordination.  </section>
<citcontext>
<prevsection>
<prevsent>for the 4-noun coordinations the precision is 81.6% (baseline 44.9%), 85.4% re call.chantree et al (2005) cover large set of ambiguities, not limited to nouns.
</prevsent>
<prevsent>they allow the head word to be noun, verb or an adjective, and the modifier to be an adjective, preposition, an adverb, etc. they extract distributional information from the british national corpus and distributional similarities between words, similarly to (resnik, 1999).
</prevsent>
</prevsection>
<citsent citstr=" P99-1081 ">
in two different experiments they achievep=88.2%, r=38.5% and p=80.8%, r=53.8% (base line p=75%).goldberg (1999) <papid> P99-1081 </papid>resolves the attachment of ambiguous coordinate phrases of the kind n1 n2 n3, e.g., box/n1 of/p chocolates/n2 and/c roses/n3.</citsent>
<aftsection>
<nextsent>using an adaptation of the algorithm proposed by ratnaparkhi (1998) <papid> P98-2177 </papid>for pp-attachment, she achieves p=72% (baseline p=64%), r=100.00%.agarwal and boggess (1992) <papid> P92-1003 </papid>focus on the identification of the conjuncts of coordinate conjunctions.using pos and case labels in deterministic algorithm, they achieve p=81.6%.</nextsent>
<nextsent>kurohashi and na gao (1992) <papid> C92-1029 </papid>work on the same problem for japanese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J613">
<title id=" H05-1123.xml">a generalized framework for revealing analogous themese across related topics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several previous computational models of analogy making (e.g. falkenhainer et al , 1989) suggested symbolic computational mechanisms for constructing detailed mappings that connect corresponding ingredients across analog ized systems.
</prevsent>
<prevsent>this work explores the identification of thematic correspondences in texts through an extension of the well known data clustering problem.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
previous works aimed at identifying ? through clusters of words ? concepts, sub-topics or themes that are prominent within corpus of texts (e.g., pereira et al ., 1993; <papid> P93-1024 </papid>li, 2002; lin and pantel, 2002).<papid> C02-1144 </papid></citsent>
<aftsection>
<nextsent>the current work deals with extending this line of research to identify corresponding themes across corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics.
</nextsent>
<nextsent>this research task has been defined quite recently (dagan et al , 2002), <papid> W02-2009 </papid>and has not been explored extensively yet.</nextsent>
<nextsent>one could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalen cies in news published in different countries, and so on.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J614">
<title id=" H05-1123.xml">a generalized framework for revealing analogous themese across related topics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>several previous computational models of analogy making (e.g. falkenhainer et al , 1989) suggested symbolic computational mechanisms for constructing detailed mappings that connect corresponding ingredients across analog ized systems.
</prevsent>
<prevsent>this work explores the identification of thematic correspondences in texts through an extension of the well known data clustering problem.
</prevsent>
</prevsection>
<citsent citstr=" C02-1144 ">
previous works aimed at identifying ? through clusters of words ? concepts, sub-topics or themes that are prominent within corpus of texts (e.g., pereira et al ., 1993; <papid> P93-1024 </papid>li, 2002; lin and pantel, 2002).<papid> C02-1144 </papid></citsent>
<aftsection>
<nextsent>the current work deals with extending this line of research to identify corresponding themes across corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics.
</nextsent>
<nextsent>this research task has been defined quite recently (dagan et al , 2002), <papid> W02-2009 </papid>and has not been explored extensively yet.</nextsent>
<nextsent>one could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalen cies in news published in different countries, and so on.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J615">
<title id=" H05-1123.xml">a generalized framework for revealing analogous themese across related topics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous works aimed at identifying ? through clusters of words ? concepts, sub-topics or themes that are prominent within corpus of texts (e.g., pereira et al ., 1993; <papid> P93-1024 </papid>li, 2002; lin and pantel, 2002).<papid> C02-1144 </papid></prevsent>
<prevsent>the current work deals with extending this line of research to identify corresponding themes across corpus pre-divided to several sub-corpora, which are focused on different, yet related, topics.</prevsent>
</prevsection>
<citsent citstr=" W02-2009 ">
this research task has been defined quite recently (dagan et al , 2002), <papid> W02-2009 </papid>and has not been explored extensively yet.</citsent>
<aftsection>
<nextsent>one could think, however, of many potential applications for drawing correspondences across textual resources: comparison of related firms or products, identifying equivalen cies in news published in different countries, and so on.
</nextsent>
<nextsent>the experimental part of our work deals with revealing correspondences between different religions: buddhism, christianity, hinduism, islam and judaism.
</nextsent>
<nextsent>given pre-partition of the corpus to sub-corpora, one for each religion, our method exposes common aspects for all religions, such as sacred writings, festivals and suffering.
</nextsent>
<nextsent>the mechanism we employ directs corresponding key terms in the different sub-corpora, such as names of festivals of different religions, to be included in the same cluster.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J622">
<title id=" H05-1110.xml">inducing a multilingual dictionary from a parallel multi text in related languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>875 the to model.
</prevsent>
<prevsent>there has been lot of work done on building dictionaries, by using variety of techniques.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
one good overview is melamed (2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>there is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, suchas mann and yarowsky (2001) <papid> N01-1020 </papid>or semantic comparison using resources such as wordnet (kondrak, 2001).<papid> N01-1014 </papid></nextsent>
<nextsent>such work, however, primarily focuses on finding cognates, whereas we are interested in translations of all words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J623">
<title id=" H05-1110.xml">inducing a multilingual dictionary from a parallel multi text in related languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been lot of work done on building dictionaries, by using variety of techniques.
</prevsent>
<prevsent>one good overview is melamed (2000).<papid> J00-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" N01-1020 ">
there is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, suchas mann and yarowsky (2001) <papid> N01-1020 </papid>or semantic comparison using resources such as wordnet (kondrak, 2001).<papid> N01-1014 </papid></citsent>
<aftsection>
<nextsent>such work, however, primarily focuses on finding cognates, whereas we are interested in translations of all words.
</nextsent>
<nextsent>moreover, while some techniques (e.g., mann and yarowsky (2001)) <papid> N01-1020 </papid>use multiple languages, the languages used have resources such as dictionaries between some language pairs.</nextsent>
<nextsent>we do not require any dictionaries for any language pair.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J624">
<title id=" H05-1110.xml">inducing a multilingual dictionary from a parallel multi text in related languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been lot of work done on building dictionaries, by using variety of techniques.
</prevsent>
<prevsent>one good overview is melamed (2000).<papid> J00-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" N01-1014 ">
there is work on lexicon induction using string distance or other phonetic/orthographic comparison techniques, suchas mann and yarowsky (2001) <papid> N01-1020 </papid>or semantic comparison using resources such as wordnet (kondrak, 2001).<papid> N01-1014 </papid></citsent>
<aftsection>
<nextsent>such work, however, primarily focuses on finding cognates, whereas we are interested in translations of all words.
</nextsent>
<nextsent>moreover, while some techniques (e.g., mann and yarowsky (2001)) <papid> N01-1020 </papid>use multiple languages, the languages used have resources such as dictionaries between some language pairs.</nextsent>
<nextsent>we do not require any dictionaries for any language pair.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J626">
<title id=" H05-1110.xml">inducing a multilingual dictionary from a parallel multi text in related languages </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>there is an active research area focusing on multi-source translation(e.g., och and ney (2001)).
</prevsent>
<prevsent>our setting is there verse: we do not use multiple dictionaries in order to translate, but translate (in very crude way) in order to build multiple dictionaries.many machine translation techniques require dictionary building as step of the process, and therefore have also attacked this problem.
</prevsent>
</prevsection>
<citsent citstr=" W01-0504 ">
they use variety of approaches (a good overview is koehn and knight (2001)), <papid> W01-0504 </papid>many of which require advanced tools for both languages which we are not able to use.</citsent>
<aftsection>
<nextsent>they also use bilingual (and to some extent monolingual) corpora, which we do have available.
</nextsent>
<nextsent>they do not, however, focus on related languages, and tend to ignore lexical similarity 4, nor are they able to work on more than pair of languages at time.
</nextsent>
<nextsent>it is also worth noting that there has been somemt work on related languages which explores language similarity in an opposite way: by using dictionaries and tools for both languages, and assuming that near word-for-word approach is reasonable (hajic et al , 2000).
</nextsent>
<nextsent>4much of recent mt research focuses on pairs of languages which are not related, such as english-chinese, english-arabic, etc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J627">
<title id=" H05-1110.xml">inducing a multilingual dictionary from a parallel multi text in related languages </title>
<section> building pairwise models.  </section>
<citcontext>
<prevsection>
<prevsent>let la and lb be the portions of the parallel text in languages and respectively, and la = (xi)i=1...n and lb = (yi)i=1...m. we can define (lb|la) as max pab max paligns n?
</prevsent>
<prevsent>i=1 m?
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
j=1 pab (yj |xi)paligns (xi|j) the giza software does the maximization by building variety of models, mostly described by brown et al  (1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>giza can be tuned in various ways, most importantly by choosing which models to run and for how many iterations.
</nextsent>
<nextsent>we treat these parameters as free, to be set al ng with the rest at later stage.
</nextsent>
<nextsent>as side effect of gizas optimization, we obtain the pab(y|x) that maximizes the above expression.
</nextsent>
<nextsent>it is quite reasonable to believe that model of this sort is also good model for our purposes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J628">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, we obtain the best published results on semantic function labels.
</prevsent>
<prevsent>this suggests that current statistical parsing methods are sufficiently general to produce accurate shallow semantic annotation.
</prevsent>
</prevsection>
<citsent citstr=" A00-1010 ">
with recent advances in speech recognition, parsing, and information extraction, some domain-specific interactive systems are now of practical use for tasks such as question-answering, flight booking, or restaurant reservation (stallard, 2000).<papid> A00-1010 </papid></citsent>
<aftsection>
<nextsent>one of the challenges ahead lies in moving from hand-craftedprograms of limited scope to robust systems independent of given domain.
</nextsent>
<nextsent>while this ambitious goal will remain in the future for some time to come,recent efforts to develop language processing systems producing richer semantic outputs will likely be the cornerstone of many successful developments in natural language understanding.in this paper, we present parser that outputs labels indicating the syntactic or semantic function of constituent in the tree, such as np-sbj or pp-tmp shown in boldface in the tree in figure 1.
</nextsent>
<nextsent>these labels indicate that the np is the subject of the sentence and that the pp conveys temporal information.
</nextsent>
<nextsent>(labels in parentheses will be explained later in thepaper.)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J629">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(labels in parentheses will be explained later in thepaper.)
</prevsent>
<prevsent>output annotated with such informative labels underlies all domain-independent question an    hh hh np-sbj  ppp the authority vp           @ @ @ pp pp pp pp vbd dropped pp-tmp  in(tmp) at np nn midnight np-tmp nnp(tmp) tuesday pp-dir  hh to(dir) to np qp  ppp $ 2.8 trillion figure 1: sample syntactic structure with function labels.
</prevsent>
</prevsection>
<citsent citstr=" C04-1188 ">
swering (jijkoun et al, 2004) <papid> C04-1188 </papid>or shallow semantic interpretation systems (collins and miller, 1998; <papid> W98-1105 </papid>ge and mooney, 2005).<papid> W05-0602 </papid></citsent>
<aftsection>
<nextsent>we test the hypothesis that acurrent statistical parser can output such richer information without any degradation of the parsers accuracy on the original parsing task.
</nextsent>
<nextsent>briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</nextsent>
<nextsent>we achieve state-of-the-art results both for parsing and function labelling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J630">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(labels in parentheses will be explained later in thepaper.)
</prevsent>
<prevsent>output annotated with such informative labels underlies all domain-independent question an    hh hh np-sbj  ppp the authority vp           @ @ @ pp pp pp pp vbd dropped pp-tmp  in(tmp) at np nn midnight np-tmp nnp(tmp) tuesday pp-dir  hh to(dir) to np qp  ppp $ 2.8 trillion figure 1: sample syntactic structure with function labels.
</prevsent>
</prevsection>
<citsent citstr=" W98-1105 ">
swering (jijkoun et al, 2004) <papid> C04-1188 </papid>or shallow semantic interpretation systems (collins and miller, 1998; <papid> W98-1105 </papid>ge and mooney, 2005).<papid> W05-0602 </papid></citsent>
<aftsection>
<nextsent>we test the hypothesis that acurrent statistical parser can output such richer information without any degradation of the parsers accuracy on the original parsing task.
</nextsent>
<nextsent>briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</nextsent>
<nextsent>we achieve state-of-the-art results both for parsing and function labelling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J631">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(labels in parentheses will be explained later in thepaper.)
</prevsent>
<prevsent>output annotated with such informative labels underlies all domain-independent question an    hh hh np-sbj  ppp the authority vp           @ @ @ pp pp pp pp vbd dropped pp-tmp  in(tmp) at np nn midnight np-tmp nnp(tmp) tuesday pp-dir  hh to(dir) to np qp  ppp $ 2.8 trillion figure 1: sample syntactic structure with function labels.
</prevsent>
</prevsection>
<citsent citstr=" W05-0602 ">
swering (jijkoun et al, 2004) <papid> C04-1188 </papid>or shallow semantic interpretation systems (collins and miller, 1998; <papid> W98-1105 </papid>ge and mooney, 2005).<papid> W05-0602 </papid></citsent>
<aftsection>
<nextsent>we test the hypothesis that acurrent statistical parser can output such richer information without any degradation of the parsers accuracy on the original parsing task.
</nextsent>
<nextsent>briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</nextsent>
<nextsent>we achieve state-of-the-art results both for parsing and function labelling.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J632">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>swering (jijkoun et al, 2004) <papid> C04-1188 </papid>or shallow semantic interpretation systems (collins and miller, 1998; <papid> W98-1105 </papid>ge and mooney, 2005).<papid> W05-0602 </papid></prevsent>
<prevsent>we test the hypothesis that acurrent statistical parser can output such richer information without any degradation of the parsers accuracy on the original parsing task.</prevsent>
</prevsection>
<citsent citstr=" N03-1014 ">
briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</citsent>
<aftsection>
<nextsent>we achieve state-of-the-art results both for parsing and function labelling.
</nextsent>
<nextsent>statistical parsers trained on the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>produce trees annotated with bare phrase structure labels (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></nextsent>
<nextsent>the trees of the penn treebank, however, are also decorated with function labels.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J635">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</prevsent>
<prevsent>we achieve state-of-the-art results both for parsing and function labelling.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
statistical parsers trained on the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>produce trees annotated with bare phrase structure labels (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>the trees of the penn treebank, however, are also decorated with function labels.
</nextsent>
<nextsent>figure 1 shows the simplified tree representation with function labels for sample sentence fromthe penn treebank corpus (section 00) the governments borrowing authority dropped at midnight tuesday to 2.8 trillion from 2.87 trillion.
</nextsent>
<nextsent>table 1 illustrates the complete list of function labels in thepenn treebank.
</nextsent>
<nextsent>unlike phrase structure labels, func 620 syntactic labels semantic labels dtv dative adv adverbial lgs logical subject bnf benefactive prd predicate dir direction put compl of put ext extent sbj surface subject loc locative voc vocative mnr manner miscellaneous labels nom nominal clf it-cleft prp purpose or reason hln headline tmp temporal ttl title topic labels clr closely related tpc topical ized table 1: complete set of function labels in the penn treebank.tion labels are context-dependent and encode shallow level of phrasal and lexical semantics, as observed first in (blaheta and charniak, 2000).<papid> A00-2031 </papid>1 to large extent, they overlap with semantic role labels as defined in propbank (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J636">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>briefly, our method consists in augmenting state-of-the-art statistical parser (henderson, 2003), <papid> N03-1014 </papid>whose architecture and properties make it particularly adaptive to new tasks.</prevsent>
<prevsent>we achieve state-of-the-art results both for parsing and function labelling.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
statistical parsers trained on the penn treebank (ptb) (marcus et al, 1993) <papid> J93-2004 </papid>produce trees annotated with bare phrase structure labels (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>the trees of the penn treebank, however, are also decorated with function labels.
</nextsent>
<nextsent>figure 1 shows the simplified tree representation with function labels for sample sentence fromthe penn treebank corpus (section 00) the governments borrowing authority dropped at midnight tuesday to 2.8 trillion from 2.87 trillion.
</nextsent>
<nextsent>table 1 illustrates the complete list of function labels in thepenn treebank.
</nextsent>
<nextsent>unlike phrase structure labels, func 620 syntactic labels semantic labels dtv dative adv adverbial lgs logical subject bnf benefactive prd predicate dir direction put compl of put ext extent sbj surface subject loc locative voc vocative mnr manner miscellaneous labels nom nominal clf it-cleft prp purpose or reason hln headline tmp temporal ttl title topic labels clr closely related tpc topical ized table 1: complete set of function labels in the penn treebank.tion labels are context-dependent and encode shallow level of phrasal and lexical semantics, as observed first in (blaheta and charniak, 2000).<papid> A00-2031 </papid>1 to large extent, they overlap with semantic role labels as defined in propbank (palmer et al, 2005).<papid> J05-1004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J639">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows the simplified tree representation with function labels for sample sentence fromthe penn treebank corpus (section 00) the governments borrowing authority dropped at midnight tuesday to 2.8 trillion from 2.87 trillion.
</prevsent>
<prevsent>table 1 illustrates the complete list of function labels in thepenn treebank.
</prevsent>
</prevsection>
<citsent citstr=" A00-2031 ">
unlike phrase structure labels, func 620 syntactic labels semantic labels dtv dative adv adverbial lgs logical subject bnf benefactive prd predicate dir direction put compl of put ext extent sbj surface subject loc locative voc vocative mnr manner miscellaneous labels nom nominal clf it-cleft prp purpose or reason hln headline tmp temporal ttl title topic labels clr closely related tpc topical ized table 1: complete set of function labels in the penn treebank.tion labels are context-dependent and encode shallow level of phrasal and lexical semantics, as observed first in (blaheta and charniak, 2000).<papid> A00-2031 </papid>1 to large extent, they overlap with semantic role labels as defined in propbank (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>current statistical parsers do not use this richer information because performance of the parser usually decreases considerably, since more complex task is being solved.
</nextsent>
<nextsent>(klein and manning, 2003)<papid> P03-1054 </papid>for instance report reduction in parsing accuracy of an un lexicalised pcfg from 77.8% to 72.9% if using function labels in training.</nextsent>
<nextsent>(blaheta, 2004)also reports decrease in performance when attempting to integrate his function labelling system with full parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J643">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>figure 1 shows the simplified tree representation with function labels for sample sentence fromthe penn treebank corpus (section 00) the governments borrowing authority dropped at midnight tuesday to 2.8 trillion from 2.87 trillion.
</prevsent>
<prevsent>table 1 illustrates the complete list of function labels in thepenn treebank.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
unlike phrase structure labels, func 620 syntactic labels semantic labels dtv dative adv adverbial lgs logical subject bnf benefactive prd predicate dir direction put compl of put ext extent sbj surface subject loc locative voc vocative mnr manner miscellaneous labels nom nominal clf it-cleft prp purpose or reason hln headline tmp temporal ttl title topic labels clr closely related tpc topical ized table 1: complete set of function labels in the penn treebank.tion labels are context-dependent and encode shallow level of phrasal and lexical semantics, as observed first in (blaheta and charniak, 2000).<papid> A00-2031 </papid>1 to large extent, they overlap with semantic role labels as defined in propbank (palmer et al, 2005).<papid> J05-1004 </papid></citsent>
<aftsection>
<nextsent>current statistical parsers do not use this richer information because performance of the parser usually decreases considerably, since more complex task is being solved.
</nextsent>
<nextsent>(klein and manning, 2003)<papid> P03-1054 </papid>for instance report reduction in parsing accuracy of an un lexicalised pcfg from 77.8% to 72.9% if using function labels in training.</nextsent>
<nextsent>(blaheta, 2004)also reports decrease in performance when attempting to integrate his function labelling system with full parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J644">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>unlike phrase structure labels, func 620 syntactic labels semantic labels dtv dative adv adverbial lgs logical subject bnf benefactive prd predicate dir direction put compl of put ext extent sbj surface subject loc locative voc vocative mnr manner miscellaneous labels nom nominal clf it-cleft prp purpose or reason hln headline tmp temporal ttl title topic labels clr closely related tpc topical ized table 1: complete set of function labels in the penn treebank.tion labels are context-dependent and encode shallow level of phrasal and lexical semantics, as observed first in (blaheta and charniak, 2000).<papid> A00-2031 </papid>1 to large extent, they overlap with semantic role labels as defined in propbank (palmer et al, 2005).<papid> J05-1004 </papid></prevsent>
<prevsent>current statistical parsers do not use this richer information because performance of the parser usually decreases considerably, since more complex task is being solved.</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
(klein and manning, 2003)<papid> P03-1054 </papid>for instance report reduction in parsing accuracy of an un lexicalised pcfg from 77.8% to 72.9% if using function labels in training.</citsent>
<aftsection>
<nextsent>(blaheta, 2004)also reports decrease in performance when attempting to integrate his function labelling system with full parser.
</nextsent>
<nextsent>conversely, researchers interested in producing richer semantic outputs have concentrated on two-stage systems, where the semantic labelling task is performed on the output of parser, in pipeline architecture divided in several stages (gildea and jurafsky, 2002).<papid> J02-3001 </papid></nextsent>
<nextsent>see also the common task of (conll, 2004 2005; senseval, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J647">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(klein and manning, 2003)<papid> P03-1054 </papid>for instance report reduction in parsing accuracy of an un lexicalised pcfg from 77.8% to 72.9% if using function labels in training.</prevsent>
<prevsent>(blaheta, 2004)also reports decrease in performance when attempting to integrate his function labelling system with full parser.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
conversely, researchers interested in producing richer semantic outputs have concentrated on two-stage systems, where the semantic labelling task is performed on the output of parser, in pipeline architecture divided in several stages (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>see also the common task of (conll, 2004 2005; senseval, 2004).
</nextsent>
<nextsent>our approach maintains state-of-the-art results in parsing, while also reaching state-of-the-art resultsin function labelling, by suitably extending simple synch rony network (ssn) parser (henderson, 2003) <papid> N03-1014 </papid>into single integrated system.</nextsent>
<nextsent>this is an interesting result, as task combining function labelling and parsing is more complex than simple parsing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J696">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this difference in input size doesnot give rise to an appreciable difference in performance.
</prevsent>
<prevsent>on the contrary, we observe that introducing 122 new words and 83 new tags improves resultsconsiderably.
</prevsent>
</prevsection>
<citsent citstr=" C00-2137 ">
this leads us to conclude that the performance of the augmented model is not simply due to larger vocabulary.we think that our tag-word pairs are effective because they are selected by linguistically meaning7significance was measured with the randomized significance test described in (yeh, 2000).<papid> C00-2137 </papid></citsent>
<aftsection>
<nextsent>syntactic labels semantic labels r f p validation set base 95.3 93.9 96.7 73.1 70.2 76.3 aug 95.7 95.0 96.5 80.1 77.0 83.5 test set aug 96.4 95.3 97.4 86.3 82.4 90.5 bc00 95.7 95.8 95.5 79.0 77.6 80.4 b04 ft 95.9 95.3 96.4 83.4 80.3 86.7 b04 kp 98.7 98.4 99.0 78.0 73.2 83.5 table 4: percentage f-measure (f), recall (r), and precision (p) function labelling, separated for syntactic and semantic labels, for our models and blaheta and charniaks (bc00) and blah etas models(b04 ft, b04 kp).
</nextsent>
<nextsent>the feature trees (ft) and kernel perceptrons (kp) are optimised separately for the two different sets of labels.
</nextsent>
<nextsent>ful criterion and are more informative exemplars for the parser.
</nextsent>
<nextsent>instead, simply decreasing the frequency cut-off adds mostly types of words for which the parser already possesses enough evidence (in general, nouns).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J700">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we will therefore discuss separately those pieces of work that have made limited use of function labels for parsing (klein and manning,2003), <papid> P03-1054 </papid>and those that have concentrated on recovering function labels as separate task (blaheta and charniak, 2000; <papid> A00-2031 </papid>blaheta, 2004).</prevsent>
<prevsent>we cannot discus shere the large recent literature on semantic role labelling for reasons of space, apart from work that 8see also (musillo and merlo, 2005) for more detail and comparisons on the labelling task.</prevsent>
</prevsection>
<citsent citstr=" P04-1040 ">
also recovers function labels (jijkoun and de rijke,2004) <papid> P04-1040 </papid>and work that trains parser on propbank labels as the first stage of semantic role labelling pipeline (yi and palmer, 2005).<papid> W05-0639 </papid></citsent>
<aftsection>
<nextsent>(klein and manning, 2003)<papid> P03-1054 </papid> and, to much more limited extent, (collins, 1999) are the only researchers we are aware of who used function labels for parsing.</nextsent>
<nextsent>in both cases, the aim was actually to improve parser performance, consequently only few carefully chosen labels were used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J701">
<title id=" H05-1078.xml">accurate function parsing </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we will therefore discuss separately those pieces of work that have made limited use of function labels for parsing (klein and manning,2003), <papid> P03-1054 </papid>and those that have concentrated on recovering function labels as separate task (blaheta and charniak, 2000; <papid> A00-2031 </papid>blaheta, 2004).</prevsent>
<prevsent>we cannot discus shere the large recent literature on semantic role labelling for reasons of space, apart from work that 8see also (musillo and merlo, 2005) for more detail and comparisons on the labelling task.</prevsent>
</prevsection>
<citsent citstr=" W05-0639 ">
also recovers function labels (jijkoun and de rijke,2004) <papid> P04-1040 </papid>and work that trains parser on propbank labels as the first stage of semantic role labelling pipeline (yi and palmer, 2005).<papid> W05-0639 </papid></citsent>
<aftsection>
<nextsent>(klein and manning, 2003)<papid> P03-1054 </papid> and, to much more limited extent, (collins, 1999) are the only researchers we are aware of who used function labels for parsing.</nextsent>
<nextsent>in both cases, the aim was actually to improve parser performance, consequently only few carefully chosen labels were used.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J713">
<title id=" H05-1056.xml">extracting personal names from email applying named entity recognition to informal text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we present two methods for improving performance of person name recognizers for email: email specific structural features and recall enhancing method which exploits name repetition across multiple documents.
</prevsent>
<prevsent>named entity recognition (ner), the identification of entity names in free text, is well-studied problem.
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
in most previous work, ner has been applied to news articles (e.g., (bikel et al, 1999; mccallum and li, 2003)), <papid> W03-0430 </papid>scientific articles (e.g., (craven and kumlien, 1999; bunescu and mooney, 2004)), or web pages (e.g., (freitag, 1998)).</citsent>
<aftsection>
<nextsent>these genres of text share two important properties: documents are written for fairly broad audience, and writers take care in preparing documents.
</nextsent>
<nextsent>important genres thatdo not share these properties include instant messaging logs, news group postings and email messages.
</nextsent>
<nextsent>we refer to these genres as informal?
</nextsent>
<nextsent>text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J714">
<title id=" H05-1056.xml">extracting personal names from email applying named entity recognition to informal text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>techniques for exploiting name repetition within documents have been recently applied to newswire text 1two of these are publicly available.
</prevsent>
<prevsent>the others can not be distributed due to privacy considerations.
</prevsent>
</prevsection>
<citsent citstr=" M98-1007 ">
443 (e.g., (humphreys et al, 1998)), <papid> M98-1007 </papid>scientific abstracts(e.g., (bunescu and mooney, 2004)) and seminar announcements (sutton and mccallum, 2004); how ever, these techniques relyon either np analysis or capitalization information to pre-identify candidatecoreferent name mentions, features which are notre liable in email.</citsent>
<aftsection>
<nextsent>furthermore, we argue that name repetition in email should be inferred by examining multiple documents in corpus, which is not common practice.
</nextsent>
<nextsent>we therefore present an alternative efficient scheme for increasing recall in email, using the whole corpus.
</nextsent>
<nextsent>this technique is shown to always improve recall substantially, and to almost always improve f1 performance.
</nextsent>
<nextsent>two email corpora used in our experiments were extracted from the cspace email corpus (kraut et al., 2004), which contains email messages collected from management course conducted at carnegie mellon university in 1997.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J715">
<title id=" H05-1056.xml">extracting personal names from email applying named entity recognition to informal text </title>
<section> existing ner methods.  </section>
<citcontext>
<prevsection>
<prevsent>given that in email corpora capitalization patterns are not followed to large extent, there is no adequate heuristic that would link candidate entities prior to extraction.
</prevsent>
<prevsent>further, it is not clear if collective classification approach can scale to modeling multiple-document repetition.we suggest an alternative approach of recall enhancing name matching, which is appropriate for email.
</prevsent>
</prevsection>
<citsent citstr=" A00-1040 ">
our approach has points of similarity to the methods described by stevenson and gaizauskas(2000), <papid> A00-1040 </papid>who suggest matching text against name dictionaries, filtering out names that are also common words or appear as non-names in high proportion in the training data.</citsent>
<aftsection>
<nextsent>the approach described here is more systematic and general.
</nextsent>
<nextsent>in nutshell, we suggest applying the noisy dictionary of predicted names over the test corpus, and use the approximate (predicted) name to non-name proportions over thetest set itself to filter out ambiguous names.
</nextsent>
<nextsent>therefore, our approach does not require large amount of annotated training data.
</nextsent>
<nextsent>it also does not require word distribution to be similar between train and test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J716">
<title id=" E91-1052.xml">an extension of earleys algorithm for sattributed grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this case, we study the conditions on the underlying base grammar under which the extended algorithm may be guaranteed to terminate.
</prevsent>
<prevsent>finite partitioning of attribute domains is proposed to guarantee the termination of the algorithm, without he need for any restrictions on the context-free base.
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
earley (1970) algorithm is general algorithm for context-free languages, widely used in natural language processing (king, 1983; shieber, 1985) <papid> P85-1018 </papid>and syntactic pattern recognition (fu, 1982), where the full generative power of context-free grammar is required.</citsent>
<aftsection>
<nextsent>the original algorithm and its common implementations, however, assume the atomic symbols of context-free grammars, thus limiting its applicability to systems with attributed symbols, or attribute grammars (knuth, 1968).
</nextsent>
<nextsent>attribute grammar is an elegant formalization of the augmented context-free grammars characteristic ofmost current nlp systems.
</nextsent>
<nextsent>it is more general than members of the family of unification-based grammar formalisms (kay, 1985; shieber, 1986), mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of well-understood declarative formalism.
</nextsent>
<nextsent>attribute grammar has been used in the past by the author to define computational models of chomsky government-binding theory, from which practical parsing programs were developed (correa, 1987<papid> P87-1007 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J717">
<title id=" E91-1052.xml">an extension of earleys algorithm for sattributed grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>attribute grammar is an elegant formalization of the augmented context-free grammars characteristic ofmost current nlp systems.
</prevsent>
<prevsent>it is more general than members of the family of unification-based grammar formalisms (kay, 1985; shieber, 1986), mainly in that it allows and encourages the use of simpler attribution functions than unification for the definition of attribute values, and hence can lead to computationally efficient grammatical definitions, while maintaining the advantages of well-understood declarative formalism.
</prevsent>
</prevsection>
<citsent citstr=" P87-1007 ">
attribute grammar has been used in the past by the author to define computational models of chomsky government-binding theory, from which practical parsing programs were developed (correa, 1987<papid> P87-1007 </papid>a).</citsent>
<aftsection>
<nextsent>many systems based on earley algorithm have clear division between the phases of syntactic and semantic analysis.
</nextsent>
<nextsent>yellin (1988), for instance, uses syntactic analysis phase in which earley algorithm builds factored parse tree (fpt) for the given input, which is then followed by up to two phases of semantic analysis, during which the fpt is attributed and evaluated.
</nextsent>
<nextsent>watt (1980) and jones and madsen (1980) propose close interaction between syntactic and semantic analysis in the form of  attribute-directed  parsing.
</nextsent>
<nextsent>however, their particular ealization of the technique is severely restricted for nlp applications, since it uses deterministic one-path (lr) algorithm, applicable only to semantically unambiguous grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J718">
<title id=" E91-1052.xml">an extension of earleys algorithm for sattributed grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>watt (1980) and jones and madsen (1980) propose close interaction between syntactic and semantic analysis in the form of  attribute-directed  parsing.
</prevsent>
<prevsent>however, their particular ealization of the technique is severely restricted for nlp applications, since it uses deterministic one-path (lr) algorithm, applicable only to semantically unambiguous grammars.
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
pereira and warren (1983) <papid> P83-1021 </papid>and shieber (1985) <papid> P85-1018 </papid>present v6rsions of earley algorithm for unification grammars, in which unification is the sole operation responsible for attribute valuation.</citsent>
<aftsection>
<nextsent>however, given the high computational cost of unification, important differences between attribute and unification grammars in their respective attribution domains and functions (correa, forthcoming), and the more general nature of attribute grammars in this regard, it is of interest to investigate the extension of earley algorithm directly to the main subclasses of attribute grammar.
</nextsent>
<nextsent>the paper is organized as follows: section 2 presents pieliminary elements, including definition of attribute grammar and earley algorithm.
</nextsent>
<nextsent>section 3 presents the extension of the algorithm for s-attributed grammars.
</nextsent>
<nextsent>in section 4, we consider the conditions on the underlying rammar under which the extended algorithm may be guaranteed to terminate for each input.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J723">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even with only 100 labeled training examples and simple features which incorporate counts from large unlabeled corpus, we achieve aer performance close to ibm model 4, in much less time.
</prevsent>
<prevsent>including model 4 predictions as features, we achieve relative aer reduction of 22% in over intersected model 4 alignments.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
the standard approach to word alignment from sentence-aligned bitexts has been to construct models which generate sentences of one language from the other, then fitting those generative models with em (brown et al, 1990; <papid> J90-2002 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>this approach has two primary advantages and two primary drawbacks.
</nextsent>
<nextsent>in its favor, generative models of alignment are well suited for use in noisy-channel translation system.
</nextsent>
<nextsent>in addition, they can be trained in an unsupervised fashion, though in practice they do require labeled validation alignments for tuning model hyper-parameters, such as null counts or smoothing amounts, which are crucial to producing alignments of good quality.
</nextsent>
<nextsent>a primary drawback of the generative approach to alignment is that, as in all generative models, explicitly incorporating arbitrary features of the in put is difficult.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J724">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>even with only 100 labeled training examples and simple features which incorporate counts from large unlabeled corpus, we achieve aer performance close to ibm model 4, in much less time.
</prevsent>
<prevsent>including model 4 predictions as features, we achieve relative aer reduction of 22% in over intersected model 4 alignments.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the standard approach to word alignment from sentence-aligned bitexts has been to construct models which generate sentences of one language from the other, then fitting those generative models with em (brown et al, 1990; <papid> J90-2002 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>this approach has two primary advantages and two primary drawbacks.
</nextsent>
<nextsent>in its favor, generative models of alignment are well suited for use in noisy-channel translation system.
</nextsent>
<nextsent>in addition, they can be trained in an unsupervised fashion, though in practice they do require labeled validation alignments for tuning model hyper-parameters, such as null counts or smoothing amounts, which are crucial to producing alignments of good quality.
</nextsent>
<nextsent>a primary drawback of the generative approach to alignment is that, as in all generative models, explicitly incorporating arbitrary features of the in put is difficult.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J727">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment is cast as maximum weighted matching problem (cormen et al, 1990) in which each pair of words (e , k ) in sentence pair (e, f) is associated with score jk (e, f) reflecting the desirability of the alignment of that pair.
</prevsent>
<prevsent>the alignment 73 for the sentence pair is then the highest scoring matching under some constraints, for example the requirement that matchings be one-to-one.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
this view of alignment as graph matching isnot, in itself, new: melamed (2000) <papid> J00-2004 </papid>uses competitive linking to greedily construct matchingswhere the pair score is measure of word to-word association, and matusov et al (2004) <papid> C04-1032 </papid>find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</citsent>
<aftsection>
<nextsent>tiedemann (2003) proposes incorporating variety of word association clues?
</nextsent>
<nextsent>into greedy linking algorithm.what we contribute here is principled approach for tractable and efficient learning of the alignment score jk(e, f) as function of arbitrary features of that token pair.
</nextsent>
<nextsent>this contribution opens up the possibility of doing the kind of feature engineering for alignment that has been so successful for other nlp tasks.
</nextsent>
<nextsent>wefirst present the algorithm for large margin estimation of the scoring function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J728">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>word alignment is cast as maximum weighted matching problem (cormen et al, 1990) in which each pair of words (e , k ) in sentence pair (e, f) is associated with score jk (e, f) reflecting the desirability of the alignment of that pair.
</prevsent>
<prevsent>the alignment 73 for the sentence pair is then the highest scoring matching under some constraints, for example the requirement that matchings be one-to-one.
</prevsent>
</prevsection>
<citsent citstr=" C04-1032 ">
this view of alignment as graph matching isnot, in itself, new: melamed (2000) <papid> J00-2004 </papid>uses competitive linking to greedily construct matchingswhere the pair score is measure of word to-word association, and matusov et al (2004) <papid> C04-1032 </papid>find exact maximum matchings where the pair scores come from the alignment posteriors of generative models.</citsent>
<aftsection>
<nextsent>tiedemann (2003) proposes incorporating variety of word association clues?
</nextsent>
<nextsent>into greedy linking algorithm.what we contribute here is principled approach for tractable and efficient learning of the alignment score jk(e, f) as function of arbitrary features of that token pair.
</nextsent>
<nextsent>this contribution opens up the possibility of doing the kind of feature engineering for alignment that has been so successful for other nlp tasks.
</nextsent>
<nextsent>wefirst present the algorithm for large margin estimation of the scoring function.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J729">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>s jk = wf(x jk) for some user provided feature mapping and abbreviate wf(x,y) = ? jk jk wf(x jk).
</prevsent>
<prevsent>we can include in the feature vector the identity of the two words, their relative positions in their respective sentences, their part-of-speech tags, their string similarity (for detecting cognates), and so on.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
at this point, one can imagine estimating alinear matching model in multiple ways, including using conditional likelihood estimation, an averaged perceptron update (see which matchings are proposed and adjust the weights according to the difference between the guessed and target structures (collins, 2002)), <papid> W02-1001 </papid>or inlarge-margin fashion.</citsent>
<aftsection>
<nextsent>conditional likelihood estimation using log-linear model (y | x) = 1 w (x) exp{wf(x,y)} requires summing over all matchings to compute the normalization zw(x), which is #p-complete (valiant, 1979).
</nextsent>
<nextsent>in our experiments, we therefore investigated the averaged perceptron in addition to the large-margin method outlined below.
</nextsent>
<nextsent>2.1 large-margin estimation.
</nextsent>
<nextsent>we follow the large-margin formulation of taskar et al (2005a).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J730">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>now the minimum cost flow from the source to the sink computes projection of z?
</prevsent>
<prevsent>i onto i we use standard, publicly-available code for solving this problem (guerriero and tseng, 2002).
</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
we applied this matching algorithm to word level alignment using the english-french hansa rds data from the 2003 naacl shared task (mihalcea and pedersen, 2003).<papid> W03-0301 </papid></citsent>
<aftsection>
<nextsent>this corpus consists of 1.1m automatically aligned sentences, and comes with validation set of 39 sentence pairs and test set of 447 sentences.
</nextsent>
<nextsent>the validation and test sentences have been hand-aligned (see och and ney (2003)) <papid> J03-1002 </papid>and are marked with both sure and possible alignments.</nextsent>
<nextsent>using these alignments, alignment error rate (aer) is calculated as: aer(a,s, ) = 1 ? |a ? s| + |a ? | |a| + |s| here, is set of proposed index pairs, is the sure gold pairs, and is the possible goldpairs.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J737">
<title id=" H05-1010.xml">a discriminative matching approach to word alignment </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>a clearer example of these features making difference is shown in figure 2, where both the exact-match and character overlap fea 3the learned response was in fact close to gaussian, but harsher near zero displacement.
</prevsent>
<prevsent>78 tures are used.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
one source of constraint which our model still does not explicitly capture is the first-order dependency between alignment positions, as in thehmm model (vogel et al, 1996) <papid> C96-2141 </papid>and ibm models 4+.</citsent>
<aftsection>
<nextsent>the the-le error in figure 1(c) is symptomatic of this lack.
</nextsent>
<nextsent>in particular, it is slightly better pair according to the dice value than the correct the-les.
</nextsent>
<nextsent>however, the latter alignment has the advantage that major-grands follows it.
</nextsent>
<nextsent>to use this information source, we included feature which gives the dice value of the words following the pair.4 we also added word frequency feature whose value is the absolute difference in log rank of the words, discouraging very common words from translating to very rare ones.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J740">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also show that for an all words wsd task this automatic method is best focussed on words that are salient to the domain, and on words with different acquired predominant sense in that domain compared to that acquired from balanced corpus.
</prevsent>
<prevsent>from analysis of manually sense tagged corpora, kilgarriff (2004) has demonstrated that distributions of the senses of words are often highly skewed.
</prevsent>
</prevsection>
<citsent citstr=" H93-1061 ">
most researchers working on word sense disambiguation(wsd) use manually sense tagged data such as semcor (miller et al , 1993) <papid> H93-1061 </papid>to train statistical classifiers, but also use the information in semcor on the overall sense distribution for each word as back off model.</citsent>
<aftsection>
<nextsent>in wsd, the heuristic of just choosing themost frequent sense of word is very powerful, especially for words with highly skewed sense distributions (yarowsky and florian, 2002).
</nextsent>
<nextsent>indeed, only 5 out of the 26 systems in the recent senseval-3 english all words task (snyder and palmer, 2004)<papid> W04-0811 </papid>outperformed the heuristic of choosing the most frequent sense as derived from semcor (which would give 61.5% precision and recall1).</nextsent>
<nextsent>furthermore, systems that did outperform the first sense heuristic did so only by small margin (the top score being 65% precision and recall).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J741">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most researchers working on word sense disambiguation(wsd) use manually sense tagged data such as semcor (miller et al , 1993) <papid> H93-1061 </papid>to train statistical classifiers, but also use the information in semcor on the overall sense distribution for each word as back off model.</prevsent>
<prevsent>in wsd, the heuristic of just choosing themost frequent sense of word is very powerful, especially for words with highly skewed sense distributions (yarowsky and florian, 2002).</prevsent>
</prevsection>
<citsent citstr=" W04-0811 ">
indeed, only 5 out of the 26 systems in the recent senseval-3 english all words task (snyder and palmer, 2004)<papid> W04-0811 </papid>outperformed the heuristic of choosing the most frequent sense as derived from semcor (which would give 61.5% precision and recall1).</citsent>
<aftsection>
<nextsent>furthermore, systems that did outperform the first sense heuristic did so only by small margin (the top score being 65% precision and recall).
</nextsent>
<nextsent>over decade ago, gale et al  (1992) <papid> H92-1045 </papid>observed the tendency for one sense of word to prevail in given discourse.</nextsent>
<nextsent>to take advantage of this, method for automatically determining the one sense?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J742">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>indeed, only 5 out of the 26 systems in the recent senseval-3 english all words task (snyder and palmer, 2004)<papid> W04-0811 </papid>outperformed the heuristic of choosing the most frequent sense as derived from semcor (which would give 61.5% precision and recall1).</prevsent>
<prevsent>furthermore, systems that did outperform the first sense heuristic did so only by small margin (the top score being 65% precision and recall).</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
over decade ago, gale et al  (1992) <papid> H92-1045 </papid>observed the tendency for one sense of word to prevail in given discourse.</citsent>
<aftsection>
<nextsent>to take advantage of this, method for automatically determining the one sense?
</nextsent>
<nextsent>given discourse or document is required.
</nextsent>
<nextsent>magnini et al (2002) have shown that information about the do main of document is very useful for wsd.
</nextsent>
<nextsent>this is because many concepts are specific to particular domains, and for many words their most likely meaning in context is strongly correlated to the domain of the document they appear in.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J743">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are no extant domain-specificsense tagged corpora to derive such sense distribution information from.
</prevsent>
<prevsent>producing them would be extremely costly, since substantial corpus would have to be annotated by hand for every domain of interest.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
in response to this problem, mccarthy et al  (2004) <papid> P04-1036 </papid>proposed method for automatically inducing the1this figure is the mean of two different estimates (sny der and palmer, 2004), <papid> W04-0811 </papid>the difference being due to multiword handling.</citsent>
<aftsection>
<nextsent>419 predominant sense of word from raw text.
</nextsent>
<nextsent>they carried out limited test of their method on text in two domains using subject field codes (magnini andcavaglia`, 2000) to assess whether the acquired predominant sense information was broadly consistent with the domain of the text it was acquired from.but they did not evaluate their method on hand tagged domain-specific corpora since there was no such data publicly available.
</nextsent>
<nextsent>in this paper, we evaluate the method on domain specific text by creating sense-annotated gold standard2 for sample of words.
</nextsent>
<nextsent>we used lexical sample because the cost of hand tagging several corpora for an all-words task would be prohibitive.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J748">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> finding predominant senses.  </section>
<citcontext>
<prevsection>
<prevsent>the method uses thesaurus obtained from the text by parsing, extracting grammatical relations and then listing each word (  ) with its top  nearest neighbours, where  is constant.
</prevsent>
<prevsent>like mccarthy 2this resource will be made publicly available for research purposes in the near future.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
et al  (2004) we use   and obtain our thesaurus using the distributional similarity metric described by lin (1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>we use wordnet (wn) as our sense inventory.
</nextsent>
<nextsent>the senses of word  are each assigned ranking score which sums over the distributional similarity scores of the neighbours and weights each neighbours score by wn similarity score (pat wardhan and pedersen, 2003) between the sense of  and the sense of the neighbour that maximises the wn similarity score.
</nextsent>
<nextsent>this weight is normalised by the sum of such wn similarity scores between all senses of  and and the senses of the neighbour that maximises this score.
</nextsent>
<nextsent>we use the wn similarity jcnscore (jiang and conrath, 1997) since this gave reasonable results for mccarthy et al  and it is efficient at run time given pre compilation of frequency information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J749">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> creating the three gold standards.  </section>
<citcontext>
<prevsection>
<prevsent>the complete set of data comprises 33225 tagging acts.
</prevsent>
<prevsent>the inter-annotator agreement on the complete set of data was 65%6.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
for the bnc data it was 60%, for the sports data 65% and for the finance data 69%.this is lower than reported for other sets of annotated data (for example it was 75% for the nouns in the senseval-2 english all-words task), but quite close to the reported 62.8% agreement between the first two taggings for single noun tagging for the senseval-3 english lexical sample task (mihalceaet al , 2004).<papid> W04-0807 </papid></citsent>
<aftsection>
<nextsent>the fairest comparison is probably between the latter and the inter-annotator agreement for the bnc data.
</nextsent>
<nextsent>reasons why our agreement is relatively low include the fact that almost all of the sentences are annotated by three people, and also the high degree of polysemy of this set of words.
</nextsent>
<nextsent>problematic cases the unlisted category was used as miscellaneous category.
</nextsent>
<nextsent>in some cases sense was truly missing from the inventory (e.g. the word tie?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J751">
<title id=" H05-1053.xml">domain specific sense distributions and predominant sense acquisition </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>above.
</prevsent>
<prevsent>one could look at differences in the ranking over all words, using mea training testing finance sports finance 35.5 sports - 40.9 semcor 14.2 (15.3) 10.0 table 4: wsd accuracy for words with different first sense to the bnc.
</prevsent>
</prevsection>
<citsent citstr=" P99-1004 ">
sure such as pairwise agreement of rankings or ranking correlation coefficient, such as spearmans.one could also use the rankings to estimate probability distributions and compare the distributions with measures such as alpha-skew divergence (lee, 1999).<papid> P99-1004 </papid></citsent>
<aftsection>
<nextsent>a simple definition would be where the rankings assign different predominant senses to word.
</nextsent>
<nextsent>taking this simple definition of deviation, we demonstrate how this might be done for our corpora.
</nextsent>
<nextsent>we compared the automatic rankings from the bnc with those from each domain specific corpus (sports and finance) for all polysemous nouns in semcor.
</nextsent>
<nextsent>although the majority are assigned thesame first sense in the bnc as in the domain specific corpora, significant proportion (31% sports and 34% finance) are not.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J752">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting system identifies opinion sources with 79.3% precision and 59.5% recall using head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
</prevsent>
<prevsent>in recent years, there has been great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text.
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
much of this research explores sentiment classification, text categorization task in which the goal is to classifya document as having positive or negative polarity (e.g., das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al  (2003), pang and lee(2004)).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g., dave et al  (2003), pang and lee (2004), <papid> P04-1035 </papid>wilson et al .</nextsent>
<nextsent>(2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebeand riloff (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J753">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting system identifies opinion sources with 79.3% precision and 59.5% recall using head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
</prevsent>
<prevsent>in recent years, there has been great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
much of this research explores sentiment classification, text categorization task in which the goal is to classifya document as having positive or negative polarity (e.g., das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al  (2003), pang and lee(2004)).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g., dave et al  (2003), pang and lee (2004), <papid> P04-1035 </papid>wilson et al .</nextsent>
<nextsent>(2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebeand riloff (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J754">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the resulting system identifies opinion sources with 79.3% precision and 59.5% recall using head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
</prevsent>
<prevsent>in recent years, there has been great deal of interest in methods for automatically identifying opinions, emotions, and sentiments in text.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
much of this research explores sentiment classification, text categorization task in which the goal is to classifya document as having positive or negative polarity (e.g., das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al  (2003), pang and lee(2004)).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g., dave et al  (2003), pang and lee (2004), <papid> P04-1035 </papid>wilson et al .</nextsent>
<nextsent>(2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebeand riloff (2005)).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J756">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>much of this research explores sentiment classification, text categorization task in which the goal is to classifya document as having positive or negative polarity (e.g., das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al  (2003), pang and lee(2004)).<papid> P04-1035 </papid></prevsent>
<prevsent>other research efforts analyze opinion expressions at the sentence level or below to recognize opinions, their polarity, and their strength (e.g., dave et al  (2003), pang and lee (2004), <papid> P04-1035 </papid>wilson et al .</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
(2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebeand riloff (2005)).</citsent>
<aftsection>
<nextsent>many applications could benefit from these opinion analyzers, including product reputation tracking (e.g., morinaga et al  (2002), yi et al  (2003)), opinion-oriented summarization (e.g., cardie et al  (2004)), and question answering (e.g., bethard et al  (2004), yu and hatzivassiloglou (2003)).<papid> W03-1017 </papid></nextsent>
<nextsent>we focus here on another aspect of opinion analysis: automatically identifying the sources of the opinions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J758">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 the goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text.
</prevsent>
<prevsent>to illustrate the nature of this problem, consider the examples below: s1: taiwan-born voters favoring independence...
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
1in related work, we investigate methods to identify the opinion expressions (e.g., riloff and wiebe (2003), <papid> W03-1014 </papid>wiebe and riloff (2005), wilson et al  (2005)) <papid> H05-2018 </papid>and the nesting structure of sources (e.g., breck and cardie (2004)).<papid> C04-1018 </papid></citsent>
<aftsection>
<nextsent>the target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus.
</nextsent>
<nextsent>355 s2: according to the report, the human rights record in china is horrendous.
</nextsent>
<nextsent>s3: international officers believe that the eu will prevail.
</nextsent>
<nextsent>s4: international officers said us officials want the eu to prevail.in s1, the phrase taiwan-born voters?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J759">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 the goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text.
</prevsent>
<prevsent>to illustrate the nature of this problem, consider the examples below: s1: taiwan-born voters favoring independence...
</prevsent>
</prevsection>
<citsent citstr=" H05-2018 ">
1in related work, we investigate methods to identify the opinion expressions (e.g., riloff and wiebe (2003), <papid> W03-1014 </papid>wiebe and riloff (2005), wilson et al  (2005)) <papid> H05-2018 </papid>and the nesting structure of sources (e.g., breck and cardie (2004)).<papid> C04-1018 </papid></citsent>
<aftsection>
<nextsent>the target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus.
</nextsent>
<nextsent>355 s2: according to the report, the human rights record in china is horrendous.
</nextsent>
<nextsent>s3: international officers believe that the eu will prevail.
</nextsent>
<nextsent>s4: international officers said us officials want the eu to prevail.in s1, the phrase taiwan-born voters?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J760">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and opinion-oriented summarization systems, both of which need to distinguish the opinions of one source from those of another.1 the goal of our research is to identify direct and indirect sources of opinions, emotions, sentiments, and other private states that are expressed in text.
</prevsent>
<prevsent>to illustrate the nature of this problem, consider the examples below: s1: taiwan-born voters favoring independence...
</prevsent>
</prevsection>
<citsent citstr=" C04-1018 ">
1in related work, we investigate methods to identify the opinion expressions (e.g., riloff and wiebe (2003), <papid> W03-1014 </papid>wiebe and riloff (2005), wilson et al  (2005)) <papid> H05-2018 </papid>and the nesting structure of sources (e.g., breck and cardie (2004)).<papid> C04-1018 </papid></citsent>
<aftsection>
<nextsent>the target of each opinion, i.e., what the opinion is directed towards, is currently being annotated manually for our corpus.
</nextsent>
<nextsent>355 s2: according to the report, the human rights record in china is horrendous.
</nextsent>
<nextsent>s3: international officers believe that the eu will prevail.
</nextsent>
<nextsent>s4: international officers said us officials want the eu to prevail.in s1, the phrase taiwan-born voters?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J761">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> the big picture.  </section>
<citcontext>
<prevsection>
<prevsent>the resulting system identifies opinion sources with 79.3% precision and 59.5% recall using head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.
</prevsent>
<prevsent>the goal of information extraction (ie) systems is to extract information about events, including the participants of the events.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
this task goes beyond named entity recognition (e.g., bikel et al  (1997))<papid> A97-1029 </papid>because it requires the recognition of role relationships.</citsent>
<aftsection>
<nextsent>for example, an ie system that extracts information about corporate acquisitions must distinguish between the company that is doing the acquiring and the company that is being acquired.
</nextsent>
<nextsent>similarly, an ie system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim.we hypothesized that ie techniques would be well suited for source identification because an opinion statement can be viewed as kind of speech event with the source as the agent.
</nextsent>
<nextsent>we investigate two very different learning-basedmethods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning.
</nextsent>
<nextsent>in particular, we consider conditional random fields (lafferty et al , 2001) and variation of auto slog (riloff, 1996a).crfs have been used successfully for named entity recognition (e.g., mccallum and li (2003),<papid> W03-0430 </papid>sarawagi and cohen (2004)), and auto slog has performed well on information extraction tasks in several domains (riloff, 1996a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J762">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> the big picture.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, an ie system that extracts information about terrorism must distinguish between the person who is the perpetrator and the person who is the victim.we hypothesized that ie techniques would be well suited for source identification because an opinion statement can be viewed as kind of speech event with the source as the agent.
</prevsent>
<prevsent>we investigate two very different learning-basedmethods from information extraction for the problem of opinion source identification: graphical models and extraction pattern learning.
</prevsent>
</prevsection>
<citsent citstr=" W03-0430 ">
in particular, we consider conditional random fields (lafferty et al , 2001) and variation of auto slog (riloff, 1996a).crfs have been used successfully for named entity recognition (e.g., mccallum and li (2003),<papid> W03-0430 </papid>sarawagi and cohen (2004)), and auto slog has performed well on information extraction tasks in several domains (riloff, 1996a).</citsent>
<aftsection>
<nextsent>while crfs treat source identification as sequence tagging task, au toslog views the problem as pattern-matching task, acquiring symbolic patterns that relyon both the syntax and lexical semantics of sentence.
</nextsent>
<nextsent>we hypothesized that combination of the two techniques would perform better than either one alone.section 3 describes the crf approach to identifying opinion sources and the features that the systemuses.
</nextsent>
<nextsent>section 4 then presents new variation of au toslog, autoslog-se, which generates ie patterns to extract sources.
</nextsent>
<nextsent>section 5 describes the hybrid sys tem: we encode the ie patterns as additional features in the crf model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J763">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> semantic tagging via conditional.  </section>
<citcontext>
<prevsection>
<prevsent>because of the third condition, however, the task requires the recognition of opinion expressions and more sophisticated encoding of sentence structure to capture relationships between source phrases and opinion ex pressions.with these properties in mind, we define the following features for each token/word xi in an input sentence.
</prevsent>
<prevsent>for pedagogical reasons, we will describe some of the features as being multi-valued or categorical features.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
in practice, however, all features are binarized for the crf model.capitalization features we use two boolean features to represent the capitalization of word: all-capital, initial-capital.part-of-speech features based on the lexical categories produced by gate (cunningham et al , 2002), <papid> P02-1022 </papid>each token xi is classified into one of set of coarse part-of-speech tags: noun, verb, adverb, wh-word, determiner, punctuation, etc. we do the same for neighboring words in [2,+2] window in order to assist noun phrase segmentation.opinion lexicon features for each token xi, we include binary feature that indicates whether or not the word is in our opinion lexicon ? set of words that indicate the presence of an opinion.</citsent>
<aftsection>
<nextsent>we do the same for neighboring words in [1,+1] window.additionally, we include for xi feature that indicates the opinion subclass associated with xi, if available from the lexicon.
</nextsent>
<nextsent>(e.g., bless?
</nextsent>
<nextsent>is classified as moderately subjective?
</nextsent>
<nextsent>according to the lexicon, while accuse?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J764">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> semantic tagging via conditional.  </section>
<citcontext>
<prevsection>
<prevsent>dependency tree features for each token xi, we create features based on the parse tree produced by the collins (1999) dependency parser.
</prevsent>
<prevsent>the purpose of the features is to (1) encode structural information, and (2) indicate whether xi is involved in any grammatical relations with an opinion word.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
two pre-processing steps are required before features can be constructed: 3some words are drawn from levin (1993); others are from framenet lemmas (baker et al  1998) <papid> P98-1013 </papid>associated with communication verbs.</citsent>
<aftsection>
<nextsent>357 1.
</nextsent>
<nextsent>syntactic chunking.
</nextsent>
<nextsent>we traverse the depen-.
</nextsent>
<nextsent>dency tree using breadth-first search to identify and group syntactically related nodes, producing flatter, more concise tree.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J767">
<title id=" H05-1045.xml">identifying sources of opinions with conditional random fields and extraction patterns </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our work aims to find sources for all opinions, emotions, and sentiments, including those that are not related to verb at all.
</prevsent>
<prevsent>furthermore, berthardet al task definition only requires the identification of direct sources, while our task requires the identification of both direct and indirect sources.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
bethard et al  evaluate their system on manually annotated framenet (baker et al , 1998) <papid> P98-1013 </papid>and propbank (palmer et al , 2005) <papid> J05-1004 </papid>sentences and achieve 48% recall with 57% precision.</citsent>
<aftsection>
<nextsent>our ie pattern learner can be viewed as cross between auto slog (riloff, 1996a) and auto slog ts (riloff, 1996b).
</nextsent>
<nextsent>auto slog is supervised learner that requires annotated training data but does not compute statistics.
</nextsent>
<nextsent>autoslog-ts is weakly supervised learner that does not require annotated databut generates coarse statistics that measure each patterns correlation with relevant and irrelevant documents.
</nextsent>
<nextsent>consequently, the patterns learned by bothautoslog and autoslog-ts need to be manually reviewed by person to achieve good accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J768">
<title id=" E93-1025.xml">a discourse copying algorithm for ellipsis and anaphora resolution </title>
<section> past approaches.  </section>
<citcontext>
<prevsection>
<prevsent>several discourse copying phenomena the event and np level are shown to be accounted for by our algorithm.
</prevsent>
<prevsent>we conclude in section 6.
</prevsent>
</prevsection>
<citsent citstr=" C92-1048 ">
much attention has been paid to the ellipsis prob-lem in linguistics (e.g., \[dahl, 1972; dahl, 1974; fiengo and may, 1990; gawron and peters, 1990; ha lk, 1987; hellan, 1988; klein, 1987; lappin, 1992; sag, 1976; williams, 1977\]), as well as in natural language processing (e.g., \[dalrymple al., 1991; hardt, 1992; <papid> C92-1048 </papid>lappin and mccord, 1990; <papid> J90-4001 </papid>priist et al., 1991\]).</citsent>
<aftsection>
<nextsent>we begin by briefly pointing out several problems with some of these approaches.
</nextsent>
<nextsent>.syntactic accounts of ellipsis (e.g., \[fiengo and may, 1990; ha /k, 1987; hellan, 1988; lappin, 1992; 203 lappin and mccord, 1990\]) <papid> J90-4001 </papid>posit the copying of syn-tactic structure from the source clause representation to the target clause representation.</nextsent>
<nextsent>2 such accounts fail to explain certain empirical facts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J769">
<title id=" E93-1025.xml">a discourse copying algorithm for ellipsis and anaphora resolution </title>
<section> past approaches.  </section>
<citcontext>
<prevsection>
<prevsent>several discourse copying phenomena the event and np level are shown to be accounted for by our algorithm.
</prevsent>
<prevsent>we conclude in section 6.
</prevsent>
</prevsection>
<citsent citstr=" J90-4001 ">
much attention has been paid to the ellipsis prob-lem in linguistics (e.g., \[dahl, 1972; dahl, 1974; fiengo and may, 1990; gawron and peters, 1990; ha lk, 1987; hellan, 1988; klein, 1987; lappin, 1992; sag, 1976; williams, 1977\]), as well as in natural language processing (e.g., \[dalrymple al., 1991; hardt, 1992; <papid> C92-1048 </papid>lappin and mccord, 1990; <papid> J90-4001 </papid>priist et al., 1991\]).</citsent>
<aftsection>
<nextsent>we begin by briefly pointing out several problems with some of these approaches.
</nextsent>
<nextsent>.syntactic accounts of ellipsis (e.g., \[fiengo and may, 1990; ha /k, 1987; hellan, 1988; lappin, 1992; 203 lappin and mccord, 1990\]) <papid> J90-4001 </papid>posit the copying of syn-tactic structure from the source clause representation to the target clause representation.</nextsent>
<nextsent>2 such accounts fail to explain certain empirical facts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J771">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>these models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data.
</prevsent>
<prevsent>bilingual word alignment is the first step of most current approaches to statistical machine translation.although the best performing systems are phrase based?
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
(e.g, och and ney, 2004), <papid> J04-4002 </papid>possible phrase translations are normally first extracted from word aligned bilingual text segments.</citsent>
<aftsection>
<nextsent>the standard approach to word alignment makes use of various combinations of five generative models developed at ibm by brown et al (1993), <papid> J93-2003 </papid>sometimes augmented by an hmm-based model or och and neys model 6?</nextsent>
<nextsent>(och and ney, 2003).<papid> J03-1002 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J772">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>bilingual word alignment is the first step of most current approaches to statistical machine translation.although the best performing systems are phrase based?
</prevsent>
<prevsent>(e.g, och and ney, 2004), <papid> J04-4002 </papid>possible phrase translations are normally first extracted from word aligned bilingual text segments.</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the standard approach to word alignment makes use of various combinations of five generative models developed at ibm by brown et al (1993), <papid> J93-2003 </papid>sometimes augmented by an hmm-based model or och and neys model 6?</citsent>
<aftsection>
<nextsent>(och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>the best combinations of these models can produce high accuracy alignments,at least when trained on large corpus of fairly direct translations in related languages.these standard models are less than ideal, how ever, in number of ways, two of which we address in this paper.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J773">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>(e.g, och and ney, 2004), <papid> J04-4002 </papid>possible phrase translations are normally first extracted from word aligned bilingual text segments.</prevsent>
<prevsent>the standard approach to word alignment makes use of various combinations of five generative models developed at ibm by brown et al (1993), <papid> J93-2003 </papid>sometimes augmented by an hmm-based model or och and neys model 6?</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
(och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the best combinations of these models can produce high accuracy alignments,at least when trained on large corpus of fairly direct translations in related languages.these standard models are less than ideal, how ever, in number of ways, two of which we address in this paper.
</nextsent>
<nextsent>first, although the standard models can theoretically be trained without supervision, in practice various parameters are introduced that should be optimized using annotated data.
</nextsent>
<nextsent>for, example,och and ney (2003)<papid> J03-1002 </papid>suggest supervised optimization of number of parameters, including the prob ablity of jumping to the empty word in the hmm model, as well as smoothing parameters for the distortion probabilities and fertility probabilities of themore complex models.</nextsent>
<nextsent>since the values of these parameters affect the values of the translation, alignment, and fertility probabilities trained by em, there is no effective way to optimize them other than torun the training procedure with particular combination of values and evaluate the accuracy of the resulting alignments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J779">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>such that a?
</prevsent>
<prevsent>= argmaxa ? i=1 ifi(a, e, f) where the fi are features and the are weights.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we optimize the model weights using modified version of averaged perceptron learning as described by collins (2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>this is fast to train, because selecting the feature weights is the last step in building the model and the online?
</nextsent>
<nextsent>nature of perceptron learning allows the parameter optimization to converge quickly.
</nextsent>
<nextsent>furthermore, no generative story has to be invented to explain how the features generate the data, so new features can be easily added without having to change the overall structure of the model.in theory, disadvantage of discrimintative approach compared to generative approach is that it requires annotated data for training.
</nextsent>
<nextsent>in practice, however, effective discriminative models for word alignment require only few parameters, which can be optimized on set of annotated sentence pairs comparable in size to what is needed to tune the free parameters used in the generative approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J781">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> discriminative alignment models.  </section>
<citcontext>
<prevsection>
<prevsent>2.1 the log-likelihood-based model.
</prevsent>
<prevsent>in our first model, we use log-likelihood-ratio (llr) statistic as our measure of word association.
</prevsent>
</prevsection>
<citsent citstr=" J00-2004 ">
we chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., melamed, 2000).<papid> J00-2004 </papid></citsent>
<aftsection>
<nextsent>we compute llr scores using the following formula presented by moore (2004): <papid> W04-3243 </papid>llr(f, e) = ? f??{f,f} ? e??{e,e} c(f?, e?)</nextsent>
<nextsent>log p(f?|e?)p(f?) in this formula and mean that the words whose degree of association is being measured occur in the respective target and source sentences of an aligned sentence pair, and mean that the corresponding words do not occur in the respective sentences, f?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J782">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> discriminative alignment models.  </section>
<citcontext>
<prevsection>
<prevsent>in our first model, we use log-likelihood-ratio (llr) statistic as our measure of word association.
</prevsent>
<prevsent>we chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons (e.g., melamed, 2000).<papid> J00-2004 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3243 ">
we compute llr scores using the following formula presented by moore (2004): <papid> W04-3243 </papid>llr(f, e) = ? f??{f,f} ? e??{e,e} c(f?, e?)</citsent>
<aftsection>
<nextsent>log p(f?|e?)p(f?) in this formula and mean that the words whose degree of association is being measured occur in the respective target and source sentences of an aligned sentence pair, and mean that the corresponding words do not occur in the respective sentences, f?
</nextsent>
<nextsent>and e?
</nextsent>
<nextsent>are variables ranging over these values,and c(f?, e?)
</nextsent>
<nextsent>is the observed joint count for the values of f?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J784">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> discriminative alignment models.  </section>
<citcontext>
<prevsection>
<prevsent>however, when we make this estimate in the clp-based model, we do not count link between not and ne...pas if the same instance of not, ne, or pas is linked to any other words.the clp-based model incorporates the same ad dtional features as the llr-based model, except that it omits the one-to-many feature, since we assume that the one-to-one vs. one-to-many trade-off is already modeled in the conditional link probabilities for particular one-to-one and one-to-many clusters.we have developed two versions of the clp based model, using two different estimates for the conditional link probabilities.
</prevsent>
<prevsent>one estimate of the conditional link probabilities comes from the llrbased model described above, optimized on an annotated development set.
</prevsent>
</prevsection>
<citsent citstr=" W05-0801 ">
the other estimate comes from heuristic alignment model that we previously developed (moore, 2005).<papid> W05-0801 </papid>2 space does not permit full description of this heuristic model here, butin brief, it utilizes series of greedy searches inspired by mela meds competitive linking algorithm (2000), in which constraints limiting alignments tobeing one-to-one and monotonic are applied at different thresholds of the llr score, with final cut off of the llr score below which no alignments are made.</citsent>
<aftsection>
<nextsent>while the discriminative models presented above are very simple to describe, finding the optimal alignment according to these models is non-trivial.
</nextsent>
<nextsent>adding link for new pair of words can affect the nonmonotonicity scores, the one-to-many score, and the un linked word score differently, depending on 2the conditional link probabilities used in the current work are those used in method 4 of the earlier work.
</nextsent>
<nextsent>full details are provided in the reference.what other links are present in the alignment.
</nextsent>
<nextsent>nevertheless, we have found beam-search procedure that seems highly effective in finding good alignments when used with these models.for each sentence pair, we create list of association types and their corresponding scores, consisting of the associations for which we have determined ascore and for which the words involved in the association type occur in the sentence pair.3 we sort the resulting list of association types from best to worst according to their scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J787">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> data and methodology for evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>we experimented with both the downhill simplex method (press et al, 2002, section 10.4) and powells method (press et al, 2002, section 10.5), but we obtained slightly better results with more heuristic method designed to look past minor local minima.
</prevsent>
<prevsent>we found that using this approach on top of perceptron learning led to slightly lower error rates on the development set with the clp-based model, but not with the llr-base model, so we used it only with the former in our final evaluations.
</prevsent>
</prevsection>
<citsent citstr=" W03-0301 ">
we evaluated our models using data from the bilingual word alignment workshop held at hlt-naacl 2003 (mihalcea and pedersen, 2003).<papid> W03-0301 </papid></citsent>
<aftsection>
<nextsent>we useda subset of the canadian hansa rds bilingual corpus supplied for the workshop, comprising 500,000english-french sentences pairs, including 447 manually word-aligned sentence pairs designated as test data.
</nextsent>
<nextsent>the test data annotates particular pairs ofwords either as sure?
</nextsent>
<nextsent>or possible?
</nextsent>
<nextsent>links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J797">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>when the first version of this paper was submitted for review, we could honestly state, we are not aware of any previous work on discriminative word alignment models.?
</prevsent>
<prevsent>callison-burch et al (2004) had investigated the use of small amounts of annotated data to help train the ibm and hmm models, but the models were still generative and were trained using maximum-likelihood methods.recently, however, three efforts nearly simultaneous with ours have made use of discriminative methods to train alignment models.
</prevsent>
</prevsection>
<citsent citstr=" W05-0814 ">
fraser and marcu(2005) <papid> W05-0814 </papid>modify model 4 to be log-linear combination of 11 sub models (5 based on standard model 4 parameters, and 6 based on additional features) and discriminatively optimize the sub model weights on each iteration of viterbi approximation to em.</citsent>
<aftsection>
<nextsent>liu et al (2005) <papid> P05-1057 </papid>also develop log-linear model,based on ibm model 3.</nextsent>
<nextsent>they train model 3 using giza++, and then use the model 3 score of apos sible alignment as feature value in discriminatively trained log-linear model, along with fea 87 tures incorporating part-of-speech information, and whether the aligned words are given as translations in bilingual dictionary.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J798">
<title id=" H05-1011.xml">a discriminative framework for bilingual word alignment </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>callison-burch et al (2004) had investigated the use of small amounts of annotated data to help train the ibm and hmm models, but the models were still generative and were trained using maximum-likelihood methods.recently, however, three efforts nearly simultaneous with ours have made use of discriminative methods to train alignment models.
</prevsent>
<prevsent>fraser and marcu(2005) <papid> W05-0814 </papid>modify model 4 to be log-linear combination of 11 sub models (5 based on standard model 4 parameters, and 6 based on additional features) and discriminatively optimize the sub model weights on each iteration of viterbi approximation to em.</prevsent>
</prevsection>
<citsent citstr=" P05-1057 ">
liu et al (2005) <papid> P05-1057 </papid>also develop log-linear model,based on ibm model 3.</citsent>
<aftsection>
<nextsent>they train model 3 using giza++, and then use the model 3 score of apos sible alignment as feature value in discriminatively trained log-linear model, along with fea 87 tures incorporating part-of-speech information, and whether the aligned words are given as translations in bilingual dictionary.
</nextsent>
<nextsent>the log-linear model is trained by standard maximum-entropy methods.klein and taskar (2005), in tutorial on maximum margin methods for natural-language processing, described weighted linear model incorporating association, position, and orthography features,with its parameters trained by structured-supportvector-machine method.
</nextsent>
<nextsent>this model is in some respects very similar to our llr-based model, using dice coefficient association scores where we use llr scores, and absolute position differences where we use nonmonotonicity measures.
</nextsent>
<nextsent>the results of our work and other recent efforts on discriminatively trained alignment models show that results comparable to or better than those obtained with the ibm models are possible within aframework that makes it easy to add arbitrary additional features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J799">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the difficulty lies in the fact that there are often many ambiguous ways to refer to the same entity.
</prevsent>
<prevsent>for example, consider the two sentences below: bill clintonnamper1 gave speech today tothe senatenamorg2 . the presidentnomper1 outlinedhisproper1 plan for budget reform to themproorg2 .there are five entity mentions in these two sentences, each of which is underlined (the corresponding mention type and entity type appear as super scripts and sub scripts, respectively, with coreference chains marked in the subscripts), but only two enti ties:  bill clinton, the president, his  and  the senate, them  . the mention detection task is to identify the entity mentions and their types, without regard for the underlying entity sets, while coreference resolution groups given mentions into sets.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
current state-of-the-art solutions to this problem split it into two parts: mention detection and coreference (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>florian et al, 2004).<papid> N04-1001 </papid></citsent>
<aftsection>
<nextsent>first, model is run that attempts to identify each mention in text and assign it type (person, organization, etc.).
</nextsent>
<nextsent>then, one holds these mentions fixed and attempts to identify which ones refer to the same entity.
</nextsent>
<nextsent>this is typically accomplished through some form of clustering, with clustering weights often tuned through some local learning procedure.
</nextsent>
<nextsent>this pipe lining scheme has the significant drawback that the mention detection module cannot take advantage of information from the coreference module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J800">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the difficulty lies in the fact that there are often many ambiguous ways to refer to the same entity.
</prevsent>
<prevsent>for example, consider the two sentences below: bill clintonnamper1 gave speech today tothe senatenamorg2 . the presidentnomper1 outlinedhisproper1 plan for budget reform to themproorg2 .there are five entity mentions in these two sentences, each of which is underlined (the corresponding mention type and entity type appear as super scripts and sub scripts, respectively, with coreference chains marked in the subscripts), but only two enti ties:  bill clinton, the president, his  and  the senate, them  . the mention detection task is to identify the entity mentions and their types, without regard for the underlying entity sets, while coreference resolution groups given mentions into sets.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
current state-of-the-art solutions to this problem split it into two parts: mention detection and coreference (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>florian et al, 2004).<papid> N04-1001 </papid></citsent>
<aftsection>
<nextsent>first, model is run that attempts to identify each mention in text and assign it type (person, organization, etc.).
</nextsent>
<nextsent>then, one holds these mentions fixed and attempts to identify which ones refer to the same entity.
</nextsent>
<nextsent>this is typically accomplished through some form of clustering, with clustering weights often tuned through some local learning procedure.
</nextsent>
<nextsent>this pipe lining scheme has the significant drawback that the mention detection module cannot take advantage of information from the coreference module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J801">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the difficulty lies in the fact that there are often many ambiguous ways to refer to the same entity.
</prevsent>
<prevsent>for example, consider the two sentences below: bill clintonnamper1 gave speech today tothe senatenamorg2 . the presidentnomper1 outlinedhisproper1 plan for budget reform to themproorg2 .there are five entity mentions in these two sentences, each of which is underlined (the corresponding mention type and entity type appear as super scripts and sub scripts, respectively, with coreference chains marked in the subscripts), but only two enti ties:  bill clinton, the president, his  and  the senate, them  . the mention detection task is to identify the entity mentions and their types, without regard for the underlying entity sets, while coreference resolution groups given mentions into sets.
</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
current state-of-the-art solutions to this problem split it into two parts: mention detection and coreference (soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>florian et al, 2004).<papid> N04-1001 </papid></citsent>
<aftsection>
<nextsent>first, model is run that attempts to identify each mention in text and assign it type (person, organization, etc.).
</nextsent>
<nextsent>then, one holds these mentions fixed and attempts to identify which ones refer to the same entity.
</nextsent>
<nextsent>this is typically accomplished through some form of clustering, with clustering weights often tuned through some local learning procedure.
</nextsent>
<nextsent>this pipe lining scheme has the significant drawback that the mention detection module cannot take advantage of information from the coreference module.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J802">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> feature functions.  </section>
<citcontext>
<prevsection>
<prevsent>based on the hypothesis that many name to nominal coreference chains are best understood in terms of background knowledge (for instance, that george w. bush?
</prevsent>
<prevsent>is the president?), we have attempted to take advantage of recent techniques from large scale data mining to extract lists of such pairs.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
in particular, we use the name/instance lists described by (fleischman et al., 2003) <papid> P03-1001 </papid>and available on fleisch mans web page to generate features between names and nominals (this list contains nou pairs mined from pi` gbs of news data).</citsent>
<aftsection>
<nextsent>since this dataset tends to focus mostly on person instances from news, we have additionally used similar data mined from piq1r gb web corpus, for which more general isa?
</nextsent>
<nextsent>relations were mined (ravichandran et al, 2005).<papid> P05-1077 </papid></nextsent>
<nextsent>class-based features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J803">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> feature functions.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, we use the name/instance lists described by (fleischman et al., 2003) <papid> P03-1001 </papid>and available on fleisch mans web page to generate features between names and nominals (this list contains nou pairs mined from pi` gbs of news data).</prevsent>
<prevsent>since this dataset tends to focus mostly on person instances from news, we have additionally used similar data mined from piq1r gb web corpus, for which more general isa?</prevsent>
</prevsection>
<citsent citstr=" P05-1077 ">
relations were mined (ravichandran et al, 2005).<papid> P05-1077 </papid></citsent>
<aftsection>
<nextsent>class-based features.
</nextsent>
<nextsent>the class-based features we employ are designed to get around the sparsity of data problem while simultaneously providing new information about word usage.
</nextsent>
<nextsent>the first class-based feature we use is based on word classes derived from the web corpus mentioned earlier and computed as described by (ravichandran et al, 2005).<papid> P05-1077 </papid></nextsent>
<nextsent>the second attempts to instill knowledge of collocations inthe data; we use the technique described by (dun ning, 1993) <papid> J93-1003 </papid>to compute multi-word expressions and then mark words that are commonly used as such with feature that expresses this fact.list-based features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J807">
<title id=" H05-1013.xml">a largescale exploration of effective global features for a joint entity detection and tracking model </title>
<section> feature functions.  </section>
<citcontext>
<prevsection>
<prevsent>the class-based features we employ are designed to get around the sparsity of data problem while simultaneously providing new information about word usage.
</prevsent>
<prevsent>the first class-based feature we use is based on word classes derived from the web corpus mentioned earlier and computed as described by (ravichandran et al, 2005).<papid> P05-1077 </papid></prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
the second attempts to instill knowledge of collocations inthe data; we use the technique described by (dun ning, 1993) <papid> J93-1003 </papid>to compute multi-word expressions and then mark words that are commonly used as such with feature that expresses this fact.list-based features.</citsent>
<aftsection>
<nextsent>we have gathered collection of about 40 lists of commonplaces, organization, names, etc. these include the standard lists of names gathered from census data and baby name books, as well as standard gazetteer information listing countries, cities, islands, ports, provinces and states.
</nextsent>
<nextsent>we supplement these standard lists with lists of airport locations (gathered from the faa) and company names (mined from the nasdaq and nyse web pages).
</nextsent>
<nextsent>we additionally include lists of semantically plural but syntactically singular words(e.g., group?)
</nextsent>
<nextsent>which were mined from large corpus by looking for patterns such as (members of the . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J808">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> modeling framework.  </section>
<citcontext>
<prevsection>
<prevsent>in such model, each word is independent of its neighbors (because the dictionary ignores context).
</prevsent>
<prevsent>estimation.
</prevsent>
</prevsection>
<citsent citstr=" J95-3004 ">
a unigram channel model defines2probabilistic modeling of what we call the morphological channel was first carried out by levinger et al  (1995), <papid> J95-3004 </papid>who used unlabeled data to estimate p(~y | x) for hebrew, with the support defined by dictionary.</citsent>
<aftsection>
<nextsent>pc(x | y) def = |x|?
</nextsent>
<nextsent>i=1 p(xi | yi) (2) the simplest estimate of this model is to make p(?, ?) uniform over (x, ~y) such that ~y ? d(x).
</nextsent>
<nextsent>doing so and marginalizing to get p(x | ~y) makes the channel model encode categorial information only, leaving all learning to the source model.3 another way to estimate this model is, of course, from data.
</nextsent>
<nextsent>this is troublesome, becausemodulooptionalityx is expected to be known given ~y, resulting in huge model with mostly 1-valued probabilities.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J809">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> modeling framework.  </section>
<citcontext>
<prevsection>
<prevsent>3) is distinct from the source-channel structure of the model (eq.
</prevsent>
<prevsent>1).
</prevsent>
</prevsection>
<citsent citstr=" W04-3230 ">
the lattice-conditional estimation approach was first used by kudo et al  (2004) <papid> W04-3230 </papid>for japanese segmentation and hierarchical pos-tagging and by smith and smith (2004) <papid> W04-3207 </papid>for korean morphological disambiguation.</citsent>
<aftsection>
<nextsent>the resulting model is an instance of conditional random field (crf; lafferty et al , 2001).
</nextsent>
<nextsent>when training crf for pos tagging, iob chunking (sha and pereira, 2003), <papid> N03-1028 </papid>or word segmentation (peng et al , 2004), <papid> C04-1081 </papid>one typically structures the conditional probabilities (in the objective function) using domain knowledge: in pos tagging, the set of allowed tags for word is used; in iob chunking, the bigram i?</nextsent>
<nextsent>is disallowed; and in segmentation, lexicon is used to enumerate the possible word boundaries.44this refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J810">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> modeling framework.  </section>
<citcontext>
<prevsection>
<prevsent>3) is distinct from the source-channel structure of the model (eq.
</prevsent>
<prevsent>1).
</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
the lattice-conditional estimation approach was first used by kudo et al  (2004) <papid> W04-3230 </papid>for japanese segmentation and hierarchical pos-tagging and by smith and smith (2004) <papid> W04-3207 </papid>for korean morphological disambiguation.</citsent>
<aftsection>
<nextsent>the resulting model is an instance of conditional random field (crf; lafferty et al , 2001).
</nextsent>
<nextsent>when training crf for pos tagging, iob chunking (sha and pereira, 2003), <papid> N03-1028 </papid>or word segmentation (peng et al , 2004), <papid> C04-1081 </papid>one typically structures the conditional probabilities (in the objective function) using domain knowledge: in pos tagging, the set of allowed tags for word is used; in iob chunking, the bigram i?</nextsent>
<nextsent>is disallowed; and in segmentation, lexicon is used to enumerate the possible word boundaries.44this refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J812">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> modeling framework.  </section>
<citcontext>
<prevsection>
<prevsent>the lattice-conditional estimation approach was first used by kudo et al  (2004) <papid> W04-3230 </papid>for japanese segmentation and hierarchical pos-tagging and by smith and smith (2004) <papid> W04-3207 </papid>for korean morphological disambiguation.</prevsent>
<prevsent>the resulting model is an instance of conditional random field (crf; lafferty et al , 2001).</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
when training crf for pos tagging, iob chunking (sha and pereira, 2003), <papid> N03-1028 </papid>or word segmentation (peng et al , 2004), <papid> C04-1081 </papid>one typically structures the conditional probabilities (in the objective function) using domain knowledge: in pos tagging, the set of allowed tags for word is used; in iob chunking, the bigram i?</citsent>
<aftsection>
<nextsent>is disallowed; and in segmentation, lexicon is used to enumerate the possible word boundaries.44this refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation.
</nextsent>
<nextsent>mle would make the sum in the denominator of eq.
</nextsent>
<nextsent>4 y+, which for log-linear models is often intractable to compute (and for sequence models may not converge).
</nextsent>
<nextsent>conditional estimation limits the sum to the subset of y+ that is consistent with x, andour variant further stipulates consistency with the dictionary entries for x. our approach is the same, with two modifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J813">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> modeling framework.  </section>
<citcontext>
<prevsection>
<prevsent>the lattice-conditional estimation approach was first used by kudo et al  (2004) <papid> W04-3230 </papid>for japanese segmentation and hierarchical pos-tagging and by smith and smith (2004) <papid> W04-3207 </papid>for korean morphological disambiguation.</prevsent>
<prevsent>the resulting model is an instance of conditional random field (crf; lafferty et al , 2001).</prevsent>
</prevsection>
<citsent citstr=" C04-1081 ">
when training crf for pos tagging, iob chunking (sha and pereira, 2003), <papid> N03-1028 </papid>or word segmentation (peng et al , 2004), <papid> C04-1081 </papid>one typically structures the conditional probabilities (in the objective function) using domain knowledge: in pos tagging, the set of allowed tags for word is used; in iob chunking, the bigram i?</citsent>
<aftsection>
<nextsent>is disallowed; and in segmentation, lexicon is used to enumerate the possible word boundaries.44this refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation.
</nextsent>
<nextsent>mle would make the sum in the denominator of eq.
</nextsent>
<nextsent>4 y+, which for log-linear models is often intractable to compute (and for sequence models may not converge).
</nextsent>
<nextsent>conditional estimation limits the sum to the subset of y+ that is consistent with x, andour variant further stipulates consistency with the dictionary entries for x. our approach is the same, with two modifications.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J815">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>similar results were presented by smith and smith (2004), <papid> W04-3207 </papid>using similar estimation strategy with model that included far more feature tem plates.</prevsent>
<prevsent>tm3 has about third as many parameters andtm3h about half; performance is roughly the same (num bers omitted for space).</prevsent>
</prevsection>
<citsent citstr=" W98-1110 ">
korean disambiguation results were also reported by cha et al  (1998), <papid> W98-1110 </papid>who applied deterministic morpheme pattern dictionary to segment words, then used bigram hmm tagger.</citsent>
<aftsection>
<nextsent>they also applied transformation-based learning to fix commoner rors.
</nextsent>
<nextsent>due to differences in tag set and data, we cannot compare to that model; bigram baseline is included.
</nextsent>
<nextsent>3.3 arabic experiments.
</nextsent>
<nextsent>we applied tm3 and tm3h to arabic.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J816">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>we applied tm3 and tm3h to arabic.
</prevsent>
<prevsent>the dataset is the arabic treebank (maamouri et al , 2003), with up to 90%used for training and 10% (13k words) for test.
</prevsent>
</prevsection>
<citsent citstr=" J00-1006 ">
the morphological dictionary is buck walters analyzer (version 2), made available by the ldc (buckwalter, 2004).7 this analyzer has total coverage of the corpus; there are no 7arabic morphological processing was also addressed by kiraz (2000), <papid> J00-1006 </papid>who gives detailed review of symbolic work in that area, and by darwish (2002).<papid> W02-0506 </papid></citsent>
<aftsection>
<nextsent>478 korean arabic pos tagging morpheme lemma pos tagging morpheme lemma accuracy accuracy accuracy accuracy accuracy accuracy 2 32k 49k 32k 49k 32k 49k 38k 76k 114k 38k 76k 114k 38k 76k 114k most likely ~y 86.0 86.9 87.5 88.8 95.3 95.7 84.5 87.0 88.3 83.2 86.2 87.0 37.9 39.8 40.9 channel only 62.6 62.6 70.3 70.8 86.4 86.4 43.7 43.7 43.7 41.2 41.2 41.2 27.2 27.2 27.2 bigram hmm 90.7 91.2 83.2 86.1 96.9 97.2 90.3 92.0 92.8 89.2 91.4 91.6 85.7?
</nextsent>
<nextsent>87.8?
</nextsent>
<nextsent>87.9?
</nextsent>
<nextsent>trigram hmm 91.5 91.8 83.3 86.0 97.0 97.2 89.8 92.0 93.0 88.5 91.3 91.3 85.2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J817">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>we applied tm3 and tm3h to arabic.
</prevsent>
<prevsent>the dataset is the arabic treebank (maamouri et al , 2003), with up to 90%used for training and 10% (13k words) for test.
</prevsent>
</prevsection>
<citsent citstr=" W02-0506 ">
the morphological dictionary is buck walters analyzer (version 2), made available by the ldc (buckwalter, 2004).7 this analyzer has total coverage of the corpus; there are no 7arabic morphological processing was also addressed by kiraz (2000), <papid> J00-1006 </papid>who gives detailed review of symbolic work in that area, and by darwish (2002).<papid> W02-0506 </papid></citsent>
<aftsection>
<nextsent>478 korean arabic pos tagging morpheme lemma pos tagging morpheme lemma accuracy accuracy accuracy accuracy accuracy accuracy 2 32k 49k 32k 49k 32k 49k 38k 76k 114k 38k 76k 114k 38k 76k 114k most likely ~y 86.0 86.9 87.5 88.8 95.3 95.7 84.5 87.0 88.3 83.2 86.2 87.0 37.9 39.8 40.9 channel only 62.6 62.6 70.3 70.8 86.4 86.4 43.7 43.7 43.7 41.2 41.2 41.2 27.2 27.2 27.2 bigram hmm 90.7 91.2 83.2 86.1 96.9 97.2 90.3 92.0 92.8 89.2 91.4 91.6 85.7?
</nextsent>
<nextsent>87.8?
</nextsent>
<nextsent>87.9?
</nextsent>
<nextsent>trigram hmm 91.5 91.8 83.3 86.0 97.0 97.2 89.8 92.0 93.0 88.5 91.3 91.3 85.2?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J818">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>there are 139 distinct pos tags; these contain some inflectional information which we treat atom ically.
</prevsent>
<prevsent>for speed, tm3h was trained in two separate pieces: tm3 and the lemma features added by tm3h.arabic has templatic morphology in which conso nantal roots are transformed into surface words by the insertion of vowels and ancillary consonants.
</prevsent>
</prevsection>
<citsent citstr=" W04-3246 ">
our system does not model this process except through the use of buck walters dictionary to define the set of analyses for each word (cf., daya et al , 2004, <papid> W04-3246 </papid>who modeled interdigitation in hebrew).</citsent>
<aftsection>
<nextsent>we treat the analysis of an arabic word as sequence ~y of pairs of morphemes andpos tags, plus lemma.
</nextsent>
<nextsent>the lemma, given in the dictionary, provides further disambiguation beyond the headmorpheme.
</nextsent>
<nextsent>the lemma is standalone dictionary head word and not merely the consonantal root, as in some other work.
</nextsent>
<nextsent>the heads?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J819">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>the tm3h model and the tag channel offer slight gains over the base tm3 model (especially on lemmatization), though the tag channel offers no help in pos tagging.
</prevsent>
<prevsent>prior work (arabic).
</prevsent>
</prevsection>
<citsent citstr=" N04-4038 ">
both diab et al  (2004) <papid> N04-4038 </papid>and habash and rambow (2005) <papid> P05-1071 </papid>use support-vector machines with local features; the former for tokenization, pos tagging, and base phrase chunking; the latter for full morphological disambiguation.</citsent>
<aftsection>
<nextsent>diab et al  report results for coarsened 24-tag set, while we use the full 139 tags from the arabic treebank, so the systems are not directly comparable.
</nextsent>
<nextsent>habash and rambow present even better results on the same pos tag set.
</nextsent>
<nextsent>our full disambiguation results appear to be competitive with theirs.
</nextsent>
<nextsent>khoja (2001) and freeman (2001) describe arabic pos taggers and many of the issues involved in developing them, but because tagged corpora did not yet exist, there are no comparable quantitative results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J820">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> concatenative models.  </section>
<citcontext>
<prevsection>
<prevsent>the tm3h model and the tag channel offer slight gains over the base tm3 model (especially on lemmatization), though the tag channel offers no help in pos tagging.
</prevsent>
<prevsent>prior work (arabic).
</prevsent>
</prevsection>
<citsent citstr=" P05-1071 ">
both diab et al  (2004) <papid> N04-4038 </papid>and habash and rambow (2005) <papid> P05-1071 </papid>use support-vector machines with local features; the former for tokenization, pos tagging, and base phrase chunking; the latter for full morphological disambiguation.</citsent>
<aftsection>
<nextsent>diab et al  report results for coarsened 24-tag set, while we use the full 139 tags from the arabic treebank, so the systems are not directly comparable.
</nextsent>
<nextsent>habash and rambow present even better results on the same pos tag set.
</nextsent>
<nextsent>our full disambiguation results appear to be competitive with theirs.
</nextsent>
<nextsent>khoja (2001) and freeman (2001) describe arabic pos taggers and many of the issues involved in developing them, but because tagged corpora did not yet exist, there are no comparable quantitative results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J821">
<title id=" H05-1060.xml">context based morphological disambiguation with random fields </title>
<section> czech: model and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>models has turned to cooperative techniques (hin ton, 1999).
</prevsent>
<prevsent>decoding that finds (given x) to maximize some weighted average of log-probabilities is known as logarithmic opinion pool (lop).
</prevsent>
</prevsection>
<citsent citstr=" P05-1003 ">
lops were applied to crfs (for named entity recognition and tagging) by smith et al  (2005), <papid> P05-1003 </papid>with an eye toward regularization.</citsent>
<aftsection>
<nextsent>their experts (each crf) contained overlapping feature sets, and the combined model achieved much the same effect as training single model with smoothing.
</nextsent>
<nextsent>note that our models, unlike theirs, partition the feature space; there is only one crf, but some parameters are ignored when estimating other parameters.
</nextsent>
<nextsent>we have not estimated log-domain mixing coefficient swe weight all models?
</nextsent>
<nextsent>contributions equally.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J824">
<title id=" H05-1008.xml">redundancy based correction of automatically extracted facts </title>
<section> name recognition and classification.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline system is obtained by supervised learning from few hundred manually annotated examples.
</prevsent>
<prevsent>then the ie system is applied to successively larger sets of unlabeled examples, and association rules are mined from the extracted facts.
</prevsent>
</prevsection>
<citsent citstr=" W04-0705 ">
the resulting combined system (trained model plus association rules) showed an improvement in performance on test set, which correlated with the size of the unlabeled corpus.in work on improving (chinese) named entity tagging, (ji and grishman, 2004; <papid> W04-0705 </papid>ji and grishman,2005), <papid> P05-1051 </papid>show benefits to this component from integrating decisions made in later stages, viz.</citsent>
<aftsection>
<nextsent>coreference, and relation extraction.1 tighter coupling and integration between ie and kdd components for mutual benefit is advocated by(mccallum and jensen, 2003), which present models based on crfs and supervised training.this work is related in spirit to the work presented in this paper, in its focus on leveraging cross document information that information though it is inherently noisyto improve local decisions.
</nextsent>
<nextsent>we expect that the approach could be quite powerful when these ideas are used in combination, and our experiments seem to confirm this expectation.
</nextsent>
<nextsent>in this section we describe the text corpus, theun derlying ie process, the form of the extracted facts, and the specific problem under studyi.e., which aspects of these facts we first try to improve.
</nextsent>
<nextsent>1performance on english named entity tasks reaches mid to high 90s in many domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J825">
<title id=" H05-1008.xml">redundancy based correction of automatically extracted facts </title>
<section> name recognition and classification.  </section>
<citcontext>
<prevsection>
<prevsent>the baseline system is obtained by supervised learning from few hundred manually annotated examples.
</prevsent>
<prevsent>then the ie system is applied to successively larger sets of unlabeled examples, and association rules are mined from the extracted facts.
</prevsent>
</prevsection>
<citsent citstr=" P05-1051 ">
the resulting combined system (trained model plus association rules) showed an improvement in performance on test set, which correlated with the size of the unlabeled corpus.in work on improving (chinese) named entity tagging, (ji and grishman, 2004; <papid> W04-0705 </papid>ji and grishman,2005), <papid> P05-1051 </papid>show benefits to this component from integrating decisions made in later stages, viz.</citsent>
<aftsection>
<nextsent>coreference, and relation extraction.1 tighter coupling and integration between ie and kdd components for mutual benefit is advocated by(mccallum and jensen, 2003), which present models based on crfs and supervised training.this work is related in spirit to the work presented in this paper, in its focus on leveraging cross document information that information though it is inherently noisyto improve local decisions.
</nextsent>
<nextsent>we expect that the approach could be quite powerful when these ideas are used in combination, and our experiments seem to confirm this expectation.
</nextsent>
<nextsent>in this section we describe the text corpus, theun derlying ie process, the form of the extracted facts, and the specific problem under studyi.e., which aspects of these facts we first try to improve.
</nextsent>
<nextsent>1performance on english named entity tasks reaches mid to high 90s in many domains.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J826">
<title id=" H05-1008.xml">redundancy based correction of automatically extracted facts </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>58 3.1 corpus.
</prevsent>
<prevsent>we conducted experiments with redundancy-basedauto-correction over large database of facts extracted from the texts in promed-mail, mailing list which carries reports about outbreaks of infectious epidemics around the world and the efforts to contain them.
</prevsent>
</prevsection>
<citsent citstr=" H05-2012 ">
this domain has been explored earlier; see, e.g., (grishman et al, 2003) for an overview.our underlying ie system is described in (yan garber et al, 2005).<papid> H05-2012 </papid></citsent>
<aftsection>
<nextsent>the system is hybrid automatically- and manually-built pattern base for finding facts, an hmm-based name tagger, automatically compiled and manually verified domain specific ontology, based in part on mesh, (mes, 2004), and rule-based co-reference module, that uses the ontology.
</nextsent>
<nextsent>the database is live on-line, and is continuously updated with new incoming reports; it can be accessed at doremi.cs.helsinki.fi/plus/.text reports have been collected by promed mail for over 10 years.
</nextsent>
<nextsent>the quality of reporting (and editing) has been rising over time, which is easy to observe in the text data.
</nextsent>
<nextsent>the distribution of the data, aggregated by month is shown in figure 1, where one can see steady increase in volume over time.2 3.2 extracted facts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J827">
<title id=" H05-1008.xml">redundancy based correction of automatically extracted facts </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>the key fields of the incident table are:   disease name   location   date (start and end)where possible, the system also extracts information about the victims affected in the incident their count, severity (affected or dead), and descriptor (people, animals, etc.).
</prevsent>
<prevsent>the system also extracts bookkeeping information about each incident: locations of mentions of the key fields in the text, etc.the systems performance is currently at 71.16 measure: 67% recall, 74% precision.
</prevsent>
</prevsection>
<citsent citstr=" M98-1030 ">
this score is obtained by muc scorer (douthat, 1998) <papid> M98-1030 </papid>on 50 document test corpus, which was manually tagged with correct incidents with these slots.</citsent>
<aftsection>
<nextsent>we have 4in this paper, we use the terms fact, incident, and event interchangeably.
</nextsent>
<nextsent>5this problem is addressed in, e.g., (huttunen et al, 2002).<papid> C02-1165 </papid></nextsent>
<nextsent>59 no blind-test corpus at present, but prior experience suggests that we ought to expect about 10% reduction in f-measure on unseen data; this is approximately borne out by our informal evaluations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J828">
<title id=" H05-1008.xml">redundancy based correction of automatically extracted facts </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>this score is obtained by muc scorer (douthat, 1998) <papid> M98-1030 </papid>on 50 document test corpus, which was manually tagged with correct incidents with these slots.</prevsent>
<prevsent>we have 4in this paper, we use the terms fact, incident, and event interchangeably.</prevsent>
</prevsection>
<citsent citstr=" C02-1165 ">
5this problem is addressed in, e.g., (huttunen et al, 2002).<papid> C02-1165 </papid></citsent>
<aftsection>
<nextsent>59 no blind-test corpus at present, but prior experience suggests that we ought to expect about 10% reduction in f-measure on unseen data; this is approximately borne out by our informal evaluations.
</nextsent>
<nextsent>further, the system attempts to normalize?
</nextsent>
<nextsent>the key fields.
</nextsent>
<nextsent>an alias for disease name (e.g., birdflu?)
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J829">
<title id=" H05-2017.xml">opine extracting product features and opinions from reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the previous review-mining systems most relevant to our work are (hu and liu, 2004) and (kobayashi etal., 2004).
</prevsent>
<prevsent>the formers precision on the explicit feature extraction task is 22% lower than opines while the latter employs an iterative semi-automatic approach which requires significant human input; neither handles implicit features.
</prevsent>
</prevsection>
<citsent citstr=" C00-1044 ">
unlike previous research on identifying the subjective character and the polarity of phrases and sentences ((hatzivassiloglou and wiebe, 2000; <papid> C00-1044 </papid>turney,2003) and many others), opine identifies the context sensitive polarity of opinion phrases.</citsent>
<aftsection>
<nextsent>in contrast to supervised methods which distinguish among strength levels for sentences or clauses ((wilson et al, 2004) and others), opine uses an unsupervised constraint-based opinion ranking approach.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J830">
<title id=" E99-1045.xml">encoding a parallel corpus for automatic terminology extraction </title>
<section> d iscuss ion   </section>
<citcontext>
<prevsection>
<prevsent>the multext tagger mr- tag will be used for the disambiguation of pos tags.
</prevsent>
<prevsent>word alignment still requires the study of various approaches, e.g.
</prevsent>
</prevsection>
<citsent citstr=" W93-0301 ">
(dagan et al, 1993; <papid> W93-0301 </papid>melamed, 1997).<papid> P97-1039 </papid></citsent>
<aftsection>
<nextsent>finally, we are working on so-phisticated interface to navigate through parallel documents to disseminate the text corpus before terminology extraction has been completed.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J831">
<title id=" E99-1045.xml">encoding a parallel corpus for automatic terminology extraction </title>
<section> d iscuss ion   </section>
<citcontext>
<prevsection>
<prevsent>the multext tagger mr- tag will be used for the disambiguation of pos tags.
</prevsent>
<prevsent>word alignment still requires the study of various approaches, e.g.
</prevsent>
</prevsection>
<citsent citstr=" P97-1039 ">
(dagan et al, 1993; <papid> W93-0301 </papid>melamed, 1997).<papid> P97-1039 </papid></citsent>
<aftsection>
<nextsent>finally, we are working on so-phisticated interface to navigate through parallel documents to disseminate the text corpus before terminology extraction has been completed.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J832">
<title id=" E95-1008.xml">collocation map for overcoming data sparseness </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>smoothing methods (church and gale 1991) readjust he dis-tribution of frequencies of word occurrences ob-tained from sample texts, and verify the distri-bution through held-out exts.
</prevsent>
<prevsent>as dagan (1992) pointed out, however, the values from the smooth-ing methods closely agree with the probability of bigram consisting of two independent words.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
class based methods (pereira et al 1993) <papid> P93-1024 </papid>approximate the likelihood of unobserved words based on similar words.</citsent>
<aftsection>
<nextsent>dagan and et al (1992) proposed non-hierarchical class based method.
</nextsent>
<nextsent>the two approaches report limited successes of purely experimental nature.
</nextsent>
<nextsent>this is so because they are based on strong assumptions.
</nextsent>
<nextsent>in the case of smoothing methods, frequency readjustment is somewhat arbitrary and will not be good for heav-ily dependent bigrams.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J833">
<title id=" E95-1008.xml">collocation map for overcoming data sparseness </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>from figure 3 that shows the oc-currence distribution of 378,888 unique bigrams, about 70% of them occur only one time.
</prevsent>
<prevsent>one in-teresting and important observation is that those of 1 to 3 frequency range that take about 90% of the population have very high mi values.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
this re-sults also agree with dunning argument about overestimation on the infrequent occurrences in which many infrequent pairs tend to get higher estimation (dunning 1993).<papid> J93-1003 </papid></citsent>
<aftsection>
<nextsent>the problem is due to the assumption of normality in naive frequency based statistics according to dunning (1993).<papid> J93-1003 </papid></nextsent>
<nextsent>ap-proximated values, thus, do not indicate the level of data quality.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J835">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>moreover, although the need to assemble and extend technical and scientific terminologies is currently most pressing in the biomedical domain, virtually any (sub-)field of human research/expertise in which we deal with terminologically structured knowledge calls for high-performance terminology identification and extraction methods.
</prevsent>
<prevsent>we want to target exactly this challenge.
</prevsent>
</prevsection>
<citsent citstr=" W02-0308 ">
the automatic extraction of complex multi-word terms from domain-specific corpora is already an active field of research (cf., e.g., for the biomedical domain rindflesch et al (1999), collier et al (2002), bodenreider et al (2002), <papid> W02-0308 </papid>or nenadic?</citsent>
<aftsection>
<nextsent>et al.
</nextsent>
<nextsent>(2003)).
</nextsent>
<nextsent>typically, in all of these approaches term candidates are collected from texts by various forms of linguistic filtering (part-of-speech tagging, phrase chunking, etc.), through which candidates obeying various linguistic patterns are identified (e.g., noun-noun, adjective-noun-noun com binations).
</nextsent>
<nextsent>these candidates are then submitted to frequency- or statistically-based evidence measures 1nakagawa and mori (2002) <papid> W02-1407 </papid>claim that more than 85% of domain-specific terms are multi-word units.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J836">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2003)).
</prevsent>
<prevsent>typically, in all of these approaches term candidates are collected from texts by various forms of linguistic filtering (part-of-speech tagging, phrase chunking, etc.), through which candidates obeying various linguistic patterns are identified (e.g., noun-noun, adjective-noun-noun com binations).
</prevsent>
</prevsection>
<citsent citstr=" W02-1407 ">
these candidates are then submitted to frequency- or statistically-based evidence measures 1nakagawa and mori (2002) <papid> W02-1407 </papid>claim that more than 85% of domain-specific terms are multi-word units.</citsent>
<aftsection>
<nextsent>843 (such as the c-value (frantzi et al, 2000)), which compute scores indicating to what degree candidate qualifies as term.
</nextsent>
<nextsent>term mining, as whole,is complex process involving several other components (orthographic and morphological normalization, acronym detection, conflation of term variants, term context, term clustering; cf.
</nextsent>
<nextsent>nenadic?
</nextsent>
<nextsent>et al (2003)).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J837">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>for multi-word automatic term recognition (atr), the c-value approach (frantzi et al, 2000; nenadic?
</prevsent>
<prevsent>et al, 2004), which aims at improving the extraction of nested terms, has been one of the most widely used techniques in recent years.
</prevsent>
</prevsection>
<citsent citstr=" P01-1025 ">
other potential association measures are mutual information(damerau, 1993) and the whole battery of statistical and information-theoretic measures (t-test, log likelihood, entropy) which are typically employed for the extraction of general-language collocations (manning and schutze, 1999; evert and krenn, 2001).<papid> P01-1025 </papid></citsent>
<aftsection>
<nextsent>while these measures have their statistical merits in terminology identification, it is interesting to note that they only make little use of linguistic properties inherent to complex terms.2 more linguistically oriented work on atr by daille (1996) or on term variation by jacquemin (1999) <papid> P99-1044 </papid>builds on the deep syntactic analysis of termcandidates.</nextsent>
<nextsent>this includes morphological and head modifier dependency analysis and thus presupposes accurate, high-quality parsing which, for sublanguages at least, can only be achieved by highlydomain-dependent type of grammar.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J838">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>et al, 2004), which aims at improving the extraction of nested terms, has been one of the most widely used techniques in recent years.
</prevsent>
<prevsent>other potential association measures are mutual information(damerau, 1993) and the whole battery of statistical and information-theoretic measures (t-test, log likelihood, entropy) which are typically employed for the extraction of general-language collocations (manning and schutze, 1999; evert and krenn, 2001).<papid> P01-1025 </papid></prevsent>
</prevsection>
<citsent citstr=" P99-1044 ">
while these measures have their statistical merits in terminology identification, it is interesting to note that they only make little use of linguistic properties inherent to complex terms.2 more linguistically oriented work on atr by daille (1996) or on term variation by jacquemin (1999) <papid> P99-1044 </papid>builds on the deep syntactic analysis of termcandidates.</citsent>
<aftsection>
<nextsent>this includes morphological and head modifier dependency analysis and thus presupposes accurate, high-quality parsing which, for sublanguages at least, can only be achieved by highlydomain-dependent type of grammar.
</nextsent>
<nextsent>as sublanguages from different domains usually reveal high degree of syntactic variability among each other(e.g., in terms of pos distribution, syntactic patterns), this property makes it difficult to port grammatical specifications to different domains.
</nextsent>
<nextsent>therefore, one may wonder whether there are cross-domain linguistic properties which might be beneficial to atr and still could be accounted for by only shallow syntactic analysis.
</nextsent>
<nextsent>in this paper, we propose the limited paradigmatic modiability of terms as criterion which meets these requirements and will elaborate on it in detail in subsection 3.3.2a notable exception is the c-value method which incorporates terms likelihood of being nested in other multi-word units.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J839">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> methods and experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper, we propose the limited paradigmatic modiability of terms as criterion which meets these requirements and will elaborate on it in detail in subsection 3.3.2a notable exception is the c-value method which incorporates terms likelihood of being nested in other multi-word units.
</prevsent>
<prevsent>3.1 text corpus.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
we collected biomedical training corpus of approximately 513,000 medline abstracts using the following query composed of mesh terms from the biomedical domain: transcription factors, blood cells and human.3 we then annotated the resulting 104-million-word corpus with the genia part of-speech tagger4 and identified noun phrases (nps) with the yamcha chunker (kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>we restrict our study to np recognition (i.e., determining the extension of noun phrase but refraining from assigning any internal constituent structure to that phrase), because the vast majority of technical or scientific terms surface as noun phrases(justeson and katz, 1995).
</nextsent>
<nextsent>we filtered out number of stop words (determiners, pronouns, measure symbols, etc.) and also ignored noun phrases with coordination markers (and?, or?, etc.).5 n-gram cut-off np term candidates length tokens types no cut-off 5,920,018 1,055,820bigrams ? 10 4,185,427 67,308 no cut-off 3,110,786 1,655,440trigrams ? 8 1,053,651 31,017 no cut-off 1,686,745 1,356,547quadgrams ? 6 222,255 10,838table 1: frequency distribution for noun phrase term candidate tokens and types for the medline text corp usin order to obtain the term candidate sets (see table 1), we counted the frequency of occurrence of noun phrases in our training corpus and categorized them according to their length.
</nextsent>
<nextsent>for this study, we restricted ourselves to noun phrases of length 2 (word bigrams), length 3 (word trigrams) and length 4 (word quadgrams).
</nextsent>
<nextsent>morphological normalization of term candidates has shown to be beneficial for atr(nenadic?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J843">
<title id=" H05-1106.xml">paradigmatic modifiability statistics for the extraction of complex multiword terms </title>
<section> conclusions.  </section>
<citcontext>
<prevsection>
<prevsent>we here proposed new terminology extraction method and showed that it significantly outperforms 849 two of the standard approaches in distinguishing terms from non-terms in the biomedical literature.while mining scientific literature for new terminological units and assembling those in controlled vocabularies is task involving several components,one essential building block is to measure the degree of termhood of candidate.
</prevsent>
<prevsent>in this respect, our study has shown that criterion which incorporatesa vital linguistic property of terms, viz.
</prevsent>
</prevsection>
<citsent citstr=" C04-1141 ">
their limited paradigmatic modiability, is much more powerful than linguistically more uninformed measures.this is in line with our previous work on general language collocation extraction (wermter and hahn,2004), <papid> C04-1141 </papid>in which we showed that linguistically motivated criterion based on the limited syntagmatic modifiability of collocations outperforms alternative standard association measures as well.</citsent>
<aftsection>
<nextsent>we also collected evidence that the superiority of the -mod method relative to other term extraction approaches holds independent of the underlying corpus size (given reasonable offset).
</nextsent>
<nextsent>this is crucial finding because other domains might lack large volumes of free-text material but still provide sufficient corpus sizes for valid term extraction.
</nextsent>
<nextsent>finally, since we only require shallow syntactic analysis (in terms of np chunking), our approach might be well suited to be easily portable to other domains.
</nextsent>
<nextsent>hence, we may conclude that, although our methodology has been tested on the biomedical domain only, there are essentially no inherent domain-specific restrictions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J844">
<title id=" H05-1120.xml">learning a spelling error model from search query logs </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>since large proportion of queries contain more than one word, wordn-gram statistics can be used to provide context sensitive spelling correction.
</prevsent>
<prevsent>finally, large proportion of typos involve letter trans positions, and other operations that can not be captured by single-letter substitution model.
</prevsent>
</prevsection>
<citsent citstr=" P00-1037 ">
in (brill and moore, 2000), <papid> P00-1037 </papid>more general model allowing generic string to stringed its is used, allowing many-to-one and one-to-many character substitution edits.</citsent>
<aftsection>
<nextsent>pronunciation modeling in (toutanova and moore, 2002) <papid> P02-1019 </papid>further improves spelling correction performance.</nextsent>
<nextsent>acknowledgments support for this work was provided by the natural sciences and engineering research council of canada.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J845">
<title id=" H05-1120.xml">learning a spelling error model from search query logs </title>
<section> conclusions and future work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, large proportion of typos involve letter trans positions, and other operations that can not be captured by single-letter substitution model.
</prevsent>
<prevsent>in (brill and moore, 2000), <papid> P00-1037 </papid>more general model allowing generic string to stringed its is used, allowing many-to-one and one-to-many character substitution edits.</prevsent>
</prevsection>
<citsent citstr=" P02-1019 ">
pronunciation modeling in (toutanova and moore, 2002) <papid> P02-1019 </papid>further improves spelling correction performance.</citsent>
<aftsection>
<nextsent>acknowledgments support for this work was provided by the natural sciences and engineering research council of canada.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J846">
<title id=" E91-1048.xml">lexical transfer based on bilingual signs towards interaction during transfer </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the bil-ingual signs not only relate the local linguistic structures of two languages but also play central role in connecting the linguistic processes of transla-tion with knowledge based inferences.
</prevsent>
<prevsent>we also show that they can be effectively used to formulate appropriate questions for disambiguating  transfer ambiguities , which is crucial in interactive mt sys-tems.
</prevsent>
</prevsection>
<citsent citstr=" C86-1022 ">
lexical transfer  has always been one of the main sources of problems in machine translation (mt)\[melby, 1986<papid> C86-1022 </papid>1\[nirenburg, 1988\].</citsent>
<aftsection>
<nextsent>research in transfer-based mt systems has focussed on discovering an appropriate vel of linguistic description for translation, at which we can specify  translation relations  (or transfer ules) in simple manner.
</nextsent>
<nextsent>however, lexical differences between languages have caused problems in this attempt.
</nextsent>
<nextsent>besides structural changes caused by lexi-cal irans fer, selecting appropriate translations of source lexical items has been one of the hardest problems in mt. because languages have their own ways of reflecting the structure of the world in their lexi-cons, and the process of lexicalization is more or less arbitrary, bilingual knowledge about lexical correspondences is highly dependent on language pairs and individual words.
</nextsent>
<nextsent>we have to .prepare framework in which such idiosyncratic bilingual knowledge about lexical items can be systemati-cally accumulated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J847">
<title id=" E95-1014.xml">corpus based method for automatic identification of support verbs for nominalizations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>in order to extract corpus evidence related to these phenomena, we proceed as follows: 1.
</prevsent>
<prevsent>we generate all the morphologically.
</prevsent>
</prevsection>
<citsent citstr=" C92-1025 ">
related forms of the word pair using lexical transducer for english (karttunen et al, 1992).<papid> C92-1025 </papid></citsent>
<aftsection>
<nextsent>this list of words will be used as corpus filter.
</nextsent>
<nextsent>2.
</nextsent>
<nextsent>the lines of the corpus are tokenized.
</nextsent>
<nextsent>(grefenstette and tapanainen, 1994), and only sentences containing one of the word forms in the filter are retained.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J848">
<title id=" E95-1014.xml">corpus based method for automatic identification of support verbs for nominalizations </title>
<section> method.  </section>
<citcontext>
<prevsection>
<prevsent>3.
</prevsent>
<prevsent>the corpus lines retained are.
</prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
part-of-speech tagged (cutting et al, 1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>this allows us to divide the corpus evidence into verb evidence and noun evidence.
</nextsent>
<nextsent>(grefenstette, 1994), we derive the local syntactic patterns involving the verbal form and the nominal ized form.
</nextsent>
<nextsent>retain some of the verbal characteristics of the underlying predicate, we want to extract the most common argument/adjunct structures found around verbal uses of the predicate.
</nextsent>
<nextsent>as an approximation, we extract here all the prepositional phrases found after the verb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J850">
<title id=" E99-1033.xml">investigating nlg architectures taking style into consideration </title>
<section> relating style to architecture.  </section>
<citcontext>
<prevsection>
<prevsent>13 ? at the global level: assuming that tasks are normally encapsulated inside modules, what characteristics of texts force the in-teraction between modules to be more in- tense?
</prevsent>
<prevsent>~2the statistical method by which arrow in figure 2 is derived gives measure of how important the linguistic feature isfor certain stylistic parameter.
</prevsent>
</prevsection>
<citsent citstr=" P84-1107 ">
~ssee danlos (1984) <papid> P84-1107 </papid>for examples of how the order of execution of tasks can favour certain textual result over another.</citsent>
<aftsection>
<nextsent>proceedings ofeacl  99 faced with this classification we will propose solutions that can be used in the specification of an architecture that supports the generation of texts in different styles.
</nextsent>
<nextsent>we expect hese solu-tions to lead to useful guidelines for helping de-signers of nlg systems to choose the appropriate architecture for the type of text they want their system to generate.
</nextsent>
<nextsent>one may question why we are repeating biber experiment, when he has already obtained set of stylistic parameters and set of text types.
</nextsent>
<nextsent>it is possible that other results emerge from apply-ing his methodology to our corpus, and the only way to know this will be by re-doing the analy-sis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J851">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>hierarchical organization is well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recentcommunity-wide evaluations.
</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
in this paper, we discuss new hierarchical phrase-based statistical machine translation system (chiang, 2005), <papid> P05-1033 </papid>presenting recent extensions to the original proposal, new evaluation results in community-wide evaluation, and novel technique for fine-grained comparative analysis of mt systems.</citsent>
<aftsection>
<nextsent>hierarchical organization is well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations.
</nextsent>
<nextsent>statistical phrase-based models (e.g.
</nextsent>
<nextsent>(och and ney, 2004;<papid> J04-4002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>marcu andwong, 2002)) <papid> W02-1018 </papid>characterize source sentence as flat partition of nonoverlapping sub sequences, or phrases?, f1 ? ?</nextsent>
<nextsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J852">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hierarchical organization is well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations.
</prevsent>
<prevsent>statistical phrase-based models (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
(och and ney, 2004;<papid> J04-4002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>marcu andwong, 2002)) <papid> W02-1018 </papid>characterize source sentence as flat partition of nonoverlapping sub sequences, or phrases?, f1 ? ?</citsent>
<aftsection>
<nextsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</nextsent>
<nextsent>j and modifying their sequential order.
</nextsent>
<nextsent>the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</nextsent>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J853">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hierarchical organization is well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations.
</prevsent>
<prevsent>statistical phrase-based models (e.g.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
(och and ney, 2004;<papid> J04-4002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>marcu andwong, 2002)) <papid> W02-1018 </papid>characterize source sentence as flat partition of nonoverlapping sub sequences, or phrases?, f1 ? ?</citsent>
<aftsection>
<nextsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</nextsent>
<nextsent>j and modifying their sequential order.
</nextsent>
<nextsent>the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</nextsent>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J854">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hierarchical organization is well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations.
</prevsent>
<prevsent>statistical phrase-based models (e.g.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
(och and ney, 2004;<papid> J04-4002 </papid>koehn et al, 2003; <papid> N03-1017 </papid>marcu andwong, 2002)) <papid> W02-1018 </papid>characterize source sentence as flat partition of nonoverlapping sub sequences, or phrases?, f1 ? ?</citsent>
<aftsection>
<nextsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</nextsent>
<nextsent>j and modifying their sequential order.
</nextsent>
<nextsent>the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</nextsent>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J855">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</prevsent>
<prevsent>j and modifying their sequential order.
</prevsent>
</prevsection>
<citsent citstr=" W02-1039 ">
the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</citsent>
<aftsection>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
<nextsent>chiang (2005) <papid> P05-1033 </papid>introduces hiero, hierarchicalphrase-based model for statistical machine transla tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J857">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</prevsent>
<prevsent>j and modifying their sequential order.
</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</citsent>
<aftsection>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
<nextsent>chiang (2005) <papid> P05-1033 </papid>introduces hiero, hierarchicalphrase-based model for statistical machine transla tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J858">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>fj ,and the process of translation involves selecting target phrases ei corresponding to the f?
</prevsent>
<prevsent>j and modifying their sequential order.
</prevsent>
</prevsection>
<citsent citstr=" P03-1040 ">
the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</citsent>
<aftsection>
<nextsent>nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</nextsent>
<nextsent>chiang (2005) <papid> P05-1033 </papid>introduces hiero, hierarchicalphrase-based model for statistical machine transla tion.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J859">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>j and modifying their sequential order.
</prevsent>
<prevsent>the need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as unit, is widely recognized the role of syntactic units iswell attested in recent systematic studies of translation (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002; <papid> P02-1050 </papid>koehn and knight, 2003), <papid> P03-1040 </papid>and their absence in phrase-based models is quite evident when looking at mt system output.</prevsent>
</prevsection>
<citsent citstr=" N04-1021 ">
nonetheless, attempts to incorporate richer linguistic features have generally met with little success (och et al, 2004<papid> N04-1021 </papid>a).</citsent>
<aftsection>
<nextsent>chiang (2005) <papid> P05-1033 </papid>introduces hiero, hierarchicalphrase-based model for statistical machine transla tion.</nextsent>
<nextsent>hiero extends the standard, non-hierarchicalnotion of phrases?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J864">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>section 4 presents new evaluation results for chinese-english as well as arabic-english translation, obtained in the context of the 2005 nistmt eval exercise.
</prevsent>
<prevsent>in section 5, we introduce novel technique for fine-grained comparative analysis of mt systems, which we employ in analyzing differences between hieros and pharaohs translations.
</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
hiero is stochastic synchronous cfg, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form log-linear model learned by minimum-errorrate training; together with modified cky beam search decoder (similar to that of wu (1996)).<papid> P96-1021 </papid></citsent>
<aftsection>
<nextsent>we describe these components in brief below.
</nextsent>
<nextsent>779 ? 1 2 ,s 1 2 ? ? 1 ,x 1 ? ? yu 1 you 2 , have 2 with 1 ? ? 1 de 2 , the 2 that 1 ? ? 1 zhiyi, one of 1 ? ? aozhou,australia?
</nextsent>
<nextsent>x ? shi, is? ? shaoshu guojia, few countries?
</nextsent>
<nextsent>x ? bang jiao, diplomatic relations?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J865">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>we denote this alignment by co indexation with boxed numbers (figure 1).
</prevsent>
<prevsent>a derivation starts with pairof aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (figure 2).
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
training begins with phrase pairs, obtained as by och, koehn, and others: giza++ (och and ney,2000) <papid> P00-1056 </papid>is used to obtain one-to-many word alignments in both directions, which are combined into single set of refined alignments using the final-and?</citsent>
<aftsection>
<nextsent>method of koehn et al (2003); <papid> N03-1017 </papid>then those pairs of sub strings that are exclusively aligned to each other are extracted as phrase pairs.</nextsent>
<nextsent>then, synchronous cfg rules are constructed out of the initial phrase pairs by subtraction: every phrase pair ? f? , e??</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J867">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>to form new rule ? 1x 2, 1x 2?, where is an index not already used.
</prevsent>
<prevsent>various filters are also applied to reduce the number of extracted rules.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (wu, 1997).<papid> J97-3002 </papid></citsent>
<aftsection>
<nextsent>2.2 model.
</nextsent>
<nextsent>the model is log-linear model (och and ney, 2002) <papid> P02-1038 </papid>over synchronous cfg derivations.</nextsent>
<nextsent>the weight of derivation is plm(e)lm , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J868">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (wu, 1997).<papid> J97-3002 </papid></prevsent>
<prevsent>2.2 model.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
the model is log-linear model (och and ney, 2002) <papid> P02-1038 </papid>over synchronous cfg derivations.</citsent>
<aftsection>
<nextsent>the weight of derivation is plm(e)lm , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation.
</nextsent>
<nextsent>the weight of each rule is, in turn: (1) w(x ? ??, ??)
</nextsent>
<nextsent>= ? i(x ? ??, ??)iwhere the are features defined on rules.
</nextsent>
<nextsent>the basic model uses the following features, analogous to pharaohs default feature set: ? p(?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J871">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>aozhou shi 8 de 9 zhiyi,australia is one of the 9 that 8 ? ?
</prevsent>
<prevsent>aozhou shi yu 1 you 2 de 9 zhiyi,australia is one of the 9 that have 2 with 1 ? figure 2: example partial derivation of synchronous cfg.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the feature weights are learned by maximizing the bleu score (papineni et al, 2002) <papid> P02-1040 </papid>on held-out data, using minimum-error-rate training (och, 2003) <papid> P03-1021 </papid>as implemented by koehn.</citsent>
<aftsection>
<nextsent>the implementation was slightly modified to ensure that the bleu scoring matches nists definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector.
</nextsent>
<nextsent>in this section we describe our extensions to the basehiero system that improve its performance significantly.
</nextsent>
<nextsent>first, we describe the addition of two new features to the chinese model, in manner similar to that of och et al (2004<papid> N04-1021 </papid>b); then we describe how we scaled the system up to much larger training set.</nextsent>
<nextsent>3.1 new features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J872">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> hiero.  </section>
<citcontext>
<prevsection>
<prevsent>aozhou shi 8 de 9 zhiyi,australia is one of the 9 that 8 ? ?
</prevsent>
<prevsent>aozhou shi yu 1 you 2 de 9 zhiyi,australia is one of the 9 that have 2 with 1 ? figure 2: example partial derivation of synchronous cfg.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
the feature weights are learned by maximizing the bleu score (papineni et al, 2002) <papid> P02-1040 </papid>on held-out data, using minimum-error-rate training (och, 2003) <papid> P03-1021 </papid>as implemented by koehn.</citsent>
<aftsection>
<nextsent>the implementation was slightly modified to ensure that the bleu scoring matches nists definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector.
</nextsent>
<nextsent>in this section we describe our extensions to the basehiero system that improve its performance significantly.
</nextsent>
<nextsent>first, we describe the addition of two new features to the chinese model, in manner similar to that of och et al (2004<papid> N04-1021 </papid>b); then we describe how we scaled the system up to much larger training set.</nextsent>
<nextsent>3.1 new features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J876">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>since this amounted to only about 1.5m+1.5m words, we used higher initial phrase limit of 15 during both training and decoding.
</prevsent>
<prevsent>figure 1 shows the performance of several systems on nist mt eval 2003 chinese test data: pharaoh(2004 version), trained only on the fbis data; hi ero, with various combinations of the new features and the larger training data.4 this table also showshieros performance on the nist 2005 mt evaluation task.5 the metric here is case-sensitive bleu.6 figure 2 shows the performance of two systems on arabic in the nist 2005 mt evaluation task: dc, phrase-based decoder for model trained by pharaoh, and hiero.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (lin and och, 2004; <papid> P04-1077 </papid>melamed et al, 2003; <papid> N03-2021 </papid>papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>all are predicated on the concept4the third line, corresponding to the model without new features trained on the larger data, may be slightly depressed be cause the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model.
</nextsent>
<nextsent>5full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm.
</nextsent>
<nextsent>for this test, phrase length limit of 15 was used during decoding.
</nextsent>
<nextsent>6for this task, the translation output was upper cased using the sri-lm toolkit: essentially, it was decoded again using an hmm whose states and transitions are trigram language model of cased english, and whose emission probabilities are reversed, i.e., probability of cased word given lower cased word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J877">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>since this amounted to only about 1.5m+1.5m words, we used higher initial phrase limit of 15 during both training and decoding.
</prevsent>
<prevsent>figure 1 shows the performance of several systems on nist mt eval 2003 chinese test data: pharaoh(2004 version), trained only on the fbis data; hi ero, with various combinations of the new features and the larger training data.4 this table also showshieros performance on the nist 2005 mt evaluation task.5 the metric here is case-sensitive bleu.6 figure 2 shows the performance of two systems on arabic in the nist 2005 mt evaluation task: dc, phrase-based decoder for model trained by pharaoh, and hiero.
</prevsent>
</prevsection>
<citsent citstr=" N03-2021 ">
over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (lin and och, 2004; <papid> P04-1077 </papid>melamed et al, 2003; <papid> N03-2021 </papid>papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>all are predicated on the concept4the third line, corresponding to the model without new features trained on the larger data, may be slightly depressed be cause the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model.
</nextsent>
<nextsent>5full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm.
</nextsent>
<nextsent>for this test, phrase length limit of 15 was used during decoding.
</nextsent>
<nextsent>6for this task, the translation output was upper cased using the sri-lm toolkit: essentially, it was decoded again using an hmm whose states and transitions are trigram language model of cased english, and whose emission probabilities are reversed, i.e., probability of cased word given lower cased word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J882">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>782 5.1 new analysis meth odin developing new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains high degree of flexibility (fox, 2002; <papid> W02-1039 </papid>hwa et al, 2002).<papid> P02-1050 </papid></prevsent>
<prevsent>this suggests that in comparative analysis of two mt systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using syntax based metric.we propose to summarize reordering patterns using part-of-speech sequences.</prevsent>
</prevsection>
<citsent citstr=" P02-1039 ">
unfortunately, recent work has shown that applying statistical parsers to ungrammatical mt output is unreliable at best, withthe parser often assigning unreasonable probabilities and in congruent structure (yamada and knight, 2002; <papid> P02-1039 </papid>och et al, 2004<papid> N04-1021 </papid>a).</citsent>
<aftsection>
<nextsent>anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus.
</nextsent>
<nextsent>word gram correspondences with reference translation are used to infer the part-of-speech tags for words in the system output.
</nextsent>
<nextsent>first, we tagged the reference corpus with parts of speech.
</nextsent>
<nextsent>we used mxpost (ratnaparkhi, 1996), <papid> W96-0213 </papid>and in order to discover more general patterns, we map the tag set down after tagging, e.g. nn, nnp,nnps and nns all map to nn.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J885">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>word gram correspondences with reference translation are used to infer the part-of-speech tags for words in the system output.
</prevsent>
<prevsent>first, we tagged the reference corpus with parts of speech.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
we used mxpost (ratnaparkhi, 1996), <papid> W96-0213 </papid>and in order to discover more general patterns, we map the tag set down after tagging, e.g. nn, nnp,nnps and nns all map to nn.</citsent>
<aftsection>
<nextsent>second, we computed the frequency freq(ti . . .
</nextsent>
<nextsent>t j) of every possible tag sequence ti . . .
</nextsent>
<nextsent>t in the reference corpus.
</nextsent>
<nextsent>third,we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (melamed et al, 2003).<papid> N03-2021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J887">
<title id=" H05-1098.xml">the hiero machine translation system extensions evaluation and analysis </title>
<section> analysis.  </section>
<citcontext>
<prevsection>
<prevsent>by examining examples of these tag sequences inthe reference corpus and their hypothesized translations, we expect to gain some insight into the comparative strengths and weaknesses of the mtsystems?
</prevsent>
<prevsent>reordering models.
</prevsent>
</prevsection>
<citsent citstr=" H05-2007 ">
(an interactive platform for this analysis is demonstrated by lopez and resnik (2005).)<papid> H05-2007 </papid></citsent>
<aftsection>
<nextsent>5.2 chinese.
</nextsent>
<nextsent>we performed tag sequence analysis on the hiero and pharaoh systems trained on the fbis data only.
</nextsent>
<nextsent>table 3 shows those n-grams for which hiero and pharaohs recall differed significantly (p   0.01).
</nextsent>
<nextsent>the numbers shown are the ratio of hieros recall to pharaohs. note that the n-grams on which hi ero had better recall are dominated by fragments of prepositional phrases (in the penn treebank tagset, prepositions are tagged in or to).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J889">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one success in factoid question answering is pattern based systems, either manually constructed (soubbotin and soubbotin, 2002) or machine learned (cui et al, 2004).
</prevsent>
<prevsent>however, it is unknown whether such pure pattern based systems work well on definitional questions where answers are more diverse.
</prevsent>
</prevsection>
<citsent citstr=" P02-1005 ">
deep linguistic analysis has been found useful in factoid question answering (moldovan et al, 2002) <papid> P02-1005 </papid>and has been used for definitional questions (xu etal., 2004; harabagiu et al, 2003).</citsent>
<aftsection>
<nextsent>linguistic analy 307sis is useful because full parsing captures long distance dependencies between the answers and the query terms, and provides more information for inference.
</nextsent>
<nextsent>however, merely linguistic analysis maynot be enough.
</nextsent>
<nextsent>first, current state of the art linguistic analysis such as parsing, co-reference, and relation extraction is still far below human performance.
</nextsent>
<nextsent>errors made in this stage will propagate and lower system accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J890">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>thus we believe that combining linguistic analysis and pattern learning would be complementary and be beneficial to the whole system.work in combining linguistic analysis with patterns include weischedel et al (2004) and jijkoun et al.
</prevsent>
<prevsent>(2004) where manually constructed patterns areused to augment linguistic features.
</prevsent>
</prevsection>
<citsent citstr=" C04-1188 ">
however, manual pattern construction critically depends on the do main knowledge of the pattern designer and often has low coverage (jijkoun et al, 2004).<papid> C04-1188 </papid></citsent>
<aftsection>
<nextsent>automatic pattern derivation is more appealing (ravichandran and hovy, 2002).<papid> P02-1006 </papid></nextsent>
<nextsent>in this work, we explore hybrid approach to combining deep linguistic analysis with automatic pattern learning.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J891">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(2004) where manually constructed patterns areused to augment linguistic features.
</prevsent>
<prevsent>however, manual pattern construction critically depends on the do main knowledge of the pattern designer and often has low coverage (jijkoun et al, 2004).<papid> C04-1188 </papid></prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
automatic pattern derivation is more appealing (ravichandran and hovy, 2002).<papid> P02-1006 </papid></citsent>
<aftsection>
<nextsent>in this work, we explore hybrid approach to combining deep linguistic analysis with automatic pattern learning.
</nextsent>
<nextsent>we are interested in answering the following four questions for chinese definitional question answering:   how helpful are linguistic analysis and pattern learning in definitional question answering?  if pattern learning is useful, what kind of question can pattern matching answer?
</nextsent>
<nextsent>  how much human annotation is required for apattern based system to achieve reasonable per formance?
</nextsent>
<nextsent>  if linguistic analysis is helpful, what linguistic features are most useful?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J892">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> a hybrid approach to definitional ques- </section>
<citcontext>
<prevsection>
<prevsent>we then select relevant sentences among the returned relevant documents.
</prevsent>
<prevsent>a sentence is considered relevant if it contains the query key word or contains word that is co-referent to the query term.
</prevsent>
</prevsection>
<citsent citstr=" H01-1024 ">
coreference is determined using an information extraction engine, serif (ramshaw etal., 2001).<papid> H01-1024 </papid></citsent>
<aftsection>
<nextsent>we then conduct deep linguistic analysis and pattern matching to extract candidate answers.
</nextsent>
<nextsent>we rank all candidate answers by predetermined feature ordering.
</nextsent>
<nextsent>at the same time, we perform redundancy detection based on
</nextsent>
<nextsent>-gram overlap.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J895">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> relations: the serif linguistic analysis en-.  </section>
<citcontext>
<prevsection>
<prevsent>the trec question answering evaluation is based on human judgments (voorhees, 2004).
</prevsent>
<prevsent>however,such manual procedure is costly and time consuming.
</prevsent>
</prevsection>
<citsent citstr=" P04-1078 ">
recently, researchers have started automatic question answering evaluation (xu et al, 2004; lin and demner-fushman, 2005; soricut and brill,2004).<papid> P04-1078 </papid></citsent>
<aftsection>
<nextsent>we use rouge, an automatic evaluation metric that was originally used for summarization evaluation (lin and hovy, 2003) <papid> N03-1020 </papid>and was recently found useful for evaluating definitional question answering (xu et al, 2004).</nextsent>
<nextsent>rouge is based on</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J896">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> relations: the serif linguistic analysis en-.  </section>
<citcontext>
<prevsection>
<prevsent>however,such manual procedure is costly and time consuming.
</prevsent>
<prevsent>recently, researchers have started automatic question answering evaluation (xu et al, 2004; lin and demner-fushman, 2005; soricut and brill,2004).<papid> P04-1078 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
we use rouge, an automatic evaluation metric that was originally used for summarization evaluation (lin and hovy, 2003) <papid> N03-1020 </papid>and was recently found useful for evaluating definitional question answering (xu et al, 2004).</citsent>
<aftsection>
<nextsent>rouge is based on
</nextsent>
<nextsent>-gramco-occurrence.
</nextsent>
<nextsent>an
</nextsent>
<nextsent>-gram is sequence of
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J898">
<title id=" H05-1039.xml">combining deep linguistics analysis and surface pattern learning a hybrid approach to chinese definitional question answering </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>jikoun et al (2004) shows that information extraction can help improve the recall of pattern based system.
</prevsent>
<prevsent>xu et al (2004) also shows that manually constructed patterns are very important in answering english definitional questions.
</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
hildebrand tet al (2004) <papid> N04-1007 </papid>uses manual surface patterns for target extraction to augment database and dictionarylookup.</citsent>
<aftsection>
<nextsent>blair-goldensohn et al (2004) apply supervised learning for definitional predicates and then apply summarization methods for question answering.
</nextsent>
<nextsent>we have explored hybrid approach for definitional question answering by combining deep linguistic analysis and surface pattern learning.
</nextsent>
<nextsent>for the first time, we have answered four questions regarding chinese definitional qa: deep linguistic analysis and automatic pattern learning are complementary and may be combined; patterns are powerful in answering biographical questions; only small amount of annotation (2 days) is required to obtain good performance in biographical qa system; copulas and appositions are the most useful linguistic features; relation extraction also helps.answering what-is? questions is more challenging than answering who-is? questions.
</nextsent>
<nextsent>to improve the performance on what-is? questions, we could divide what-is? questions into finer classes suchas organization, location, disease, and general substance, and process them specifically.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J899">
<title id=" E93-1024.xml">restriction and correspondence based translation </title>
<section> abstract </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" E89-1037 ">
kaplan et al  (1989) <papid> E89-1037 </papid>present framework for translation based on the description and correspondence oncepts of lexical- functional grammar (kaplan and bresnan, 1982).</citsent>
<aftsection>
<nextsent>certain phenomena, in particular the head-switching of adverbs and verbs, seem to be problematic for that approach.
</nextsent>
<nextsent>in this paper we suggest hat these difficulties are more properly considered as the result of defective monolingual nalyses.
</nextsent>
<nextsent>we propose new description-language op rator, restriction, to permit succinct formal encoding of the informal intuition that semantic units sometimes correspond to subsets of functional information.
</nextsent>
<nextsent>this operator, in conjunction with an additional recur sion provided by description-by-analysis rule, is the basis of more adequate account of head-switching that preserves the advantages of correspondence-based translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J929">
<title id=" E93-1024.xml">restriction and correspondence based translation </title>
<section> difficulties with the.  </section>
<citcontext>
<prevsection>
<prevsent>the co description device crucially involves both composition of correspondences and single recursive analysis of common ancestor structures.
</prevsent>
<prevsent>this contrasts with description- by-analysis, another technique mentioned by kaplan and halvorsen (1988) and kaplan et al  (1989) <papid> E89-1037 </papid>for deriving descriptions of abstract structures.</prevsent>
</prevsection>
<citsent citstr=" E91-1051 ">
correspondence approach this proposal for correspondence-based translation has been scrutinized by number of researchers (e.g. sadler et al , 1989, sadler et al , 1990, sadler and thompson, 1991), <papid> E91-1051 </papid>and several difficulties have been pointed out.</citsent>
<aftsection>
<nextsent>these difficulties arise particularly in cases where the independently motivated source and target structures are not very closely aligned, where single units in source structure map to multiple units in the target (so-called splitting) or where hierarchical relationships are interchanged in mapping from source to target (switching).
</nextsent>
<nextsent>if such discrepancies are both locally bounded and predictable, then they can in principle be handled by means of co description statements involving the ordinary monolingual description-language constructs of function-application d equality.
</nextsent>
<nextsent>but even if possible, such conservative treatments may not permit obvious generalizations about he translation relation to be naturally expressed.
</nextsent>
<nextsent>sadler et al  (1990) demonstrate this point by examples in which the translation of lexical head differs according to its dependents in the source sentence (english commit suicide translates to french (se) sui cider whereas commit crime translates to commettre une crime).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J946">
<title id=" H05-1051.xml">differentiating homonymy and polysemy in information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>several studies (schtze, 1998; gaustad, 2001) suggest that this failure to model polysemy has significant impact.
</prevsent>
<prevsent>disambiguation algorithms evaluated using pseudo words show much better performance than when subsequently applied to real words.
</prevsent>
</prevsection>
<citsent citstr=" W98-0705 ">
gonzalo et al (1998) <papid> W98-0705 </papid>cite this failure to model related senses in order to explain why their study into the effects of ambiguity showed radically different results to sanderson (1994).</citsent>
<aftsection>
<nextsent>they performed known item retrieval on 256 manually disambigu ated documents and showed increased retrieval effectiveness where disambiguation was over 60% accurate.
</nextsent>
<nextsent>whilst sandersons results no longer fit the empirical data, his pseudo word methodology does allow us to explore the effects of ambiguity without the overhead of manual disambiguation.
</nextsent>
<nextsent>gaustad (2001) highlighted that the challenge lies in adapting pseudo words to account for polysemy.
</nextsent>
<nextsent>404 krovetz (1997) performed the only study to date which has explicitly attempted to differentiate between homonymy and polysemy in ir.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J947">
<title id=" H05-1051.xml">differentiating homonymy and polysemy in information retrieval </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the final step was to measure the effects of attempting to resolve the additional ambiguity which had been added to the collection.
</prevsent>
<prevsent>in order to do this, the author simulated disambiguation to varying degrees of accuracy and measured the impact that this had on retrieval effectiveness.
</prevsent>
</prevsection>
<citsent citstr=" N03-2023 ">
to date only nakov and hearst (2003) <papid> N03-2023 </papid>have looked into creating more plausible pseudowords.</citsent>
<aftsection>
<nextsent>working with medical abstracts (medline) and the controlled vocabulary contained in the mesh hierarchy they created pseudo sense pairings that are related.
</nextsent>
<nextsent>by identifying pairs of mesh subject categories which frequently co-occurred and selecting constituents for their pseudo words from these pairings they produced disambiguation test collection.
</nextsent>
<nextsent>their results showed that category based pseudo words provided more realistic test dataset for disambiguation, in that evaluation using them more closely resembled real words.
</nextsent>
<nextsent>the challenge in this study lay in adapting these ideas for open domain text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J948">
<title id=" H05-1051.xml">differentiating homonymy and polysemy in information retrieval </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>whilst this is undoubtedly true these results suggest that homonymy has to be resolved to much higher level of accuracy than polysemy in order to be of benefit in ir.
</prevsent>
<prevsent>it would seem prudent to consider the results of this study in relation to the state-of-the-art in disambiguation.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
at senseval-3 (mihalcea et al, 2004) <papid> W04-0807 </papid>the top systems were considered to have reached ceiling, in terms of performance, at 72% for fine grained disambiguation and 80% for coarse grained.</citsent>
<aftsection>
<nextsent>when producing the english language test collections the rate of agreement between humans performing manual disambiguation was approximately 74%.
</nextsent>
<nextsent>this suggests that machine disambiguation has reached levels comparable to the performance of humans.
</nextsent>
<nextsent>in parallel with this their community has begun to report increased retrieval effectiveness through explicitly performing disambiguation to varying levels of granularity.
</nextsent>
<nextsent>a final point of discussion is the way in which we simulate disambiguation both in this study and those previously (sanderson, 1994; gonzalo et al, 1998).<papid> W98-0705 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J950">
<title id=" H05-1051.xml">differentiating homonymy and polysemy in information retrieval </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in parallel with this their community has begun to report increased retrieval effectiveness through explicitly performing disambiguation to varying levels of granularity.
</prevsent>
<prevsent>a final point of discussion is the way in which we simulate disambiguation both in this study and those previously (sanderson, 1994; gonzalo et al, 1998).<papid> W98-0705 </papid></prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
there is growing evidence (leacock et al, 1998; <papid> J98-1006 </papid>agirre and martinez, 2004) <papid> W04-3204 </papid>to suggest that simulating uniform rates of accuracy and error across both words and senses may not reflect the performance of modern disambiguation systems.</citsent>
<aftsection>
<nextsent>supervised approaches are known to exhibit inherent bias that exists in their training data.
</nextsent>
<nextsent>examples include zipfs law (zipf, 1949) which denotes that small number of words make up large percentage of word use and krovetz and crofts (1992) observation that one sense of word accounts for the majority of all use.
</nextsent>
<nextsent>it would seem logical to presume that supervised systems show their best performance over the most frequent senses of the most frequent words in their training data.
</nextsent>
<nextsent>not enough is known about the potential impact of these biases to allow for them to be incorporated into this simulation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J951">
<title id=" H05-1051.xml">differentiating homonymy and polysemy in information retrieval </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in parallel with this their community has begun to report increased retrieval effectiveness through explicitly performing disambiguation to varying levels of granularity.
</prevsent>
<prevsent>a final point of discussion is the way in which we simulate disambiguation both in this study and those previously (sanderson, 1994; gonzalo et al, 1998).<papid> W98-0705 </papid></prevsent>
</prevsection>
<citsent citstr=" W04-3204 ">
there is growing evidence (leacock et al, 1998; <papid> J98-1006 </papid>agirre and martinez, 2004) <papid> W04-3204 </papid>to suggest that simulating uniform rates of accuracy and error across both words and senses may not reflect the performance of modern disambiguation systems.</citsent>
<aftsection>
<nextsent>supervised approaches are known to exhibit inherent bias that exists in their training data.
</nextsent>
<nextsent>examples include zipfs law (zipf, 1949) which denotes that small number of words make up large percentage of word use and krovetz and crofts (1992) observation that one sense of word accounts for the majority of all use.
</nextsent>
<nextsent>it would seem logical to presume that supervised systems show their best performance over the most frequent senses of the most frequent words in their training data.
</nextsent>
<nextsent>not enough is known about the potential impact of these biases to allow for them to be incorporated into this simulation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J952">
<title id=" H05-1027.xml">minimum sample risk methods for language modeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while most existing discriminative training methods use loss function that can be optimized easily but approaches only approximately to the objective of minimum error rate, msr minimizes the training error directly using heuristic training procedure.
</prevsent>
<prevsent>evaluations on the task of japanese text input show that msr can handle large number of features and training samples; it significantly outperforms regular trigram model trained using maximum likelihood estimation, and it also outperforms the two widely applied discriminative methods, the boosting and the perceptron algorithms, by small but statistically significant margin.
</prevsent>
</prevsection>
<citsent citstr=" W02-1032 ">
language modeling (lm) is fundamental to wide range of applications, such as speech recognition and asian language text input (jelinek 1997; gao et al. 2002).<papid> W02-1032 </papid></citsent>
<aftsection>
<nextsent>the traditional approach uses parametric model with maximum likelihood estimation (mle), usually with smoothing methods to deal with data sparseness problems.
</nextsent>
<nextsent>this approach is optimal under the assumption that the true distribution of data on which the parametric model is based is known.
</nextsent>
<nextsent>unfortunately, such an assumption rarely holds in realistic applications.
</nextsent>
<nextsent>an alternative approach to lm is based on the framework of discriminative training, which uses much weaker assumption that training and test data are generated from the same distribution but the form of the distribution is unknown.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J954">
<title id=" H05-1027.xml">minimum sample risk methods for language modeling </title>
<section> minimum sample risk.  </section>
<citcontext>
<prevsection>
<prevsent>3.1 problem definition.
</prevsent>
<prevsent>we follow the general framework of linear dis criminant models described in (duda et al 2001).
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
in the rest of the paper we use the following notation, adapted from collins (2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>training data is set of example input/output pairs.
</nextsent>
<nextsent>in lm for ime, training samples are represented as {ai, wir}, for = 1m, where each ai is an input phonetic string and wir is the reference transcript of ai.
</nextsent>
<nextsent>we assume some way of generating set of candidate word strings given a, denoted by gen(a).
</nextsent>
<nextsent>in our experiments, gen(a) consists of top word strings converted from using base line ime system that uses only word trigram model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J955">
<title id=" H05-1027.xml">minimum sample risk methods for language modeling </title>
<section> minimum sample risk.  </section>
<citcontext>
<prevsection>
<prevsent>we address these issues respectively in the next two subsections.
</prevsent>
<prevsent>3.3 grid line search.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
our implementation of grid search is modified version of that proposed in (och 2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>the modifications are made to deal with the efficiency issue due to the fact that there is very large number of features and training samples in our task, compared to only 8 features used in (och 2003).<papid> P03-1021 </papid></nextsent>
<nextsent>unlike simple grid search where the intervals between any two adjacent grids are equal and fixed, we determine for each feature sequence of grids with differently sized intervals, each corresponding to different value of sample risk.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J964">
<title id=" H05-1027.xml">minimum sample risk methods for language modeling </title>
<section> for each = 1k.  </section>
<citcontext>
<prevsection>
<prevsent>though msr achieves impressive performance in cer reduction over the comparison methods, as described in section 4.2, the experiments are all performed using newspaper text for both training and testing, which is not realistic scenario if we are to deploy the model in an application.
</prevsent>
<prevsent>this section reports the results of additional experiments in which we adapt model trained on one domain to different domain, i.e., in so-called cross-domain lm adaptation paradigm.
</prevsent>
</prevsection>
<citsent citstr=" H05-1034 ">
see (su zuki and gao 2005) <papid> H05-1034 </papid>for detailed report.</citsent>
<aftsection>
<nextsent>the datasets we used stem from five distinct sources of text.
</nextsent>
<nextsent>the nikkei newspaper corpus described in section 4.1 was used as the background domain, on which the word trigram model was trained.
</nextsent>
<nextsent>we used four adaptation domains: yomiuri (newspaper corpus), tuneup (balanced corpus containing newspapers and other sources of text), encarta (encyclopedia) and shincho (collection of novels).
</nextsent>
<nextsent>for each of the four domains, we used an 72,000-sentence subset as adaptation training data, 5,000-sentence subset as held-out data and an other 5,000-sentence subset as test data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J967">
<title id=" H05-1027.xml">minimum sample risk methods for language modeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>msr shares many similarities with previous methods.
</prevsent>
<prevsent>the basic training algorithm described in section 3.2 follows the general framework of multi-dimensional optimization (e.g., press et al 1992).
</prevsent>
</prevsection>
<citsent citstr=" P05-1034 ">
the line search is an extension of that described in (och 2003; <papid> P03-1021 </papid>quirk et al 2005.<papid> P05-1034 </papid></citsent>
<aftsection>
<nextsent>the extension lies in the way of handling large number of features and training samples.
</nextsent>
<nextsent>previous algorithms were used to optimize linear models with less than 10 features.
</nextsent>
<nextsent>the feature selection method described in section 3.4 is particular implementation of the feature selection methods described in (e.g., theodoridis and koutroumbas 2003).
</nextsent>
<nextsent>the major difference between the msr and other methods is that it estimates the effectiveness of each feature in terms of its expected training error reduction while previous methods used metrics that are loosely coupled with reducing training errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J968">
<title id=" H05-1085.xml">improving statistical mt through morphological analysis </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>until recently, most machine translation projects involved translating between languages with relatively little morphological structure.
</prevsent>
<prevsent>nevertheless, few research projects have investigated the use of morphology to improve translation quality.
</prevsent>
</prevsection>
<citsent citstr=" C00-2162 ">
niessen and ney (2000), <papid> C00-2162 </papid>niessen and ney (2004) <papid> J04-2003 </papid>report work on german-english translation, where they investigate various types of morphosyntactic restructuring, including merging german verbs with their detached prefixes, annotating handful of frequent ambiguous german words with pos tags, combining idiomatic multi-word expressions into single words, and undoing question in 676version and do-insertion in both german and english.</citsent>
<aftsection>
<nextsent>in addition, niessen and ney (2004) <papid> J04-2003 </papid>decompose german words into hierarchical representation using lemmas and morphological tags, and use maxent model to combine the different levels of representation in the translation model.</nextsent>
<nextsent>the results from these papers indicate that on corpus sizes upto 60,000 parallel sentences, the restructuring operations yielded large improvement in translation quality, but the morphological decomposition provided only slight additional benefit.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J969">
<title id=" H05-1085.xml">improving statistical mt through morphological analysis </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>until recently, most machine translation projects involved translating between languages with relatively little morphological structure.
</prevsent>
<prevsent>nevertheless, few research projects have investigated the use of morphology to improve translation quality.
</prevsent>
</prevsection>
<citsent citstr=" J04-2003 ">
niessen and ney (2000), <papid> C00-2162 </papid>niessen and ney (2004) <papid> J04-2003 </papid>report work on german-english translation, where they investigate various types of morphosyntactic restructuring, including merging german verbs with their detached prefixes, annotating handful of frequent ambiguous german words with pos tags, combining idiomatic multi-word expressions into single words, and undoing question in 676version and do-insertion in both german and english.</citsent>
<aftsection>
<nextsent>in addition, niessen and ney (2004) <papid> J04-2003 </papid>decompose german words into hierarchical representation using lemmas and morphological tags, and use maxent model to combine the different levels of representation in the translation model.</nextsent>
<nextsent>the results from these papers indicate that on corpus sizes upto 60,000 parallel sentences, the restructuring operations yielded large improvement in translation quality, but the morphological decomposition provided only slight additional benefit.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J973">
<title id=" H05-1085.xml">improving statistical mt through morphological analysis </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the results from these papers indicate that on corpus sizes upto 60,000 parallel sentences, the restructuring operations yielded large improvement in translation quality, but the morphological decomposition provided only slight additional benefit.
</prevsent>
<prevsent>however, since german is not as morphologically complex as czech, we might expect larger benefit from morphological analysis in czech.
</prevsent>
</prevsection>
<citsent citstr=" N04-4015 ">
another project utilizing morphological analysis for statistical machine translation is described by lee (2004).<papid> N04-4015 </papid></citsent>
<aftsection>
<nextsent>lees system for arabic-english translation takes as input pos-tagged english and arabic text, where the arabic words have been pre-segmentedinto stems and affixes.
</nextsent>
<nextsent>the system performs an initial alignment of the arabic morphemes to the english words.
</nextsent>
<nextsent>based on the consistency of the english pos tag that each arabic morpheme aligns to, the system determines whether to keep that morpheme as separate item, merge it back onto the stem,or delete it altogether.
</nextsent>
<nextsent>in addition, multiple occurrences of the determiner al within single arabic noun phrase are deleted (i.e. only one occurrence is allowed).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J974">
<title id=" H05-1085.xml">improving statistical mt through morphological analysis </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>this result is consistent with anecdotal evidence suggesting that morphological analysis becomes less helpful as corpus sizes increase.
</prevsent>
<prevsent>how ever, since parallel corpora of hundreds of thousands of sentences or more are often difficult to obtain, it would still be worthwhile to develop method for improving systems trained on smaller corpora.previous results on czech-english machine translation suggest that morphological analysis may be quite productive for this highly inflected language where there is only small amount of closely translated material.
</prevsent>
</prevsection>
<citsent citstr=" E03-1004 ">
cmejrek et al (2003), <papid> E03-1004 </papid>while not focusing on the use of morphology, give results indicating that lemmatization of the czech input improves bleu score relative to baseline.</citsent>
<aftsection>
<nextsent>these results support the earlier findings of al-onaizan et al (1999), who used subjective scoring measures.
</nextsent>
<nextsent>al-onaizan et al measured translation accuracy not only forlemmatized input, but for an input form theyre fer to as czech?.
</nextsent>
<nextsent>czech?
</nextsent>
<nextsent>is intended to capture many of the morphological distinctions of english, while discarding those distinctions that are czech-specific.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J976">
<title id=" H05-1085.xml">improving statistical mt through morphological analysis </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>this graph illustrates the sparse data problem in czech that our morphological analysis is intended to address.
</prevsent>
<prevsent>although the number of infrequently occurring lemmas is about the same in both english and czech, the number of infrequently occurring inflected word forms is approximately twice as high in czech.1for all of our experiments, we used the same language model, trained with the cmu statistical language modelling toolkit (clarkson and rosenfeld, 1997).
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
our translation models were trained usinggiza++ (och and ney, 2003), <papid> J03-1002 </papid>which we modi 1although we did not use it for the experiments inthis paper, the pcedt corpus does contain lemma information for the english data.</citsent>
<aftsection>
<nextsent>there is slight discrepancy between the english and czech data in the lemma information for pronouns, in that english pronouns (including accusitive, possessive, and other forms) are assigned themselves as lemmas, whereas czech pronouns are reduced to uninflected forms.
</nextsent>
<nextsent>given that pronouns generally have many tokens, this discrepancy should not affect the data in figure 3.
</nextsent>
<nextsent>679 1 2 3 4 5 6 7 8 9 10 0 0.5 1 1.5 2 2.5 3 3.5 104 token count ite c ou nt english word forms czech word forms english lemmas czech lemmas figure 3: the number of items (full word forms or lemmas) appearing in the parallel corpus with token count of x.fied as necessary for the morpheme-based experiments.
</nextsent>
<nextsent>we used the isi rewrite decoder (marcuand germann, 2005) for producing translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J977">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the accuracy level of current syntactic and semantic parsers on natural language text from different domains limit the extent to which syntactic and semantic information can be used in real ie systems.
</prevsent>
<prevsent>nevertheless, various linesof work on relation extraction have shown experimentally that the use of automatically derived syntactic information can lead to significant improvements in extraction accuracy.
</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
the amount of syntactic knowledge used in ie systems varies from part of-speech only (ray and craven, 2001) to chunking(ray and craven, 2001) to shallow parse trees (ze lenko et al, 2003) to dependency trees derived from full parse trees (culotta and sorensen, 2004).<papid> P04-1054 </papid></citsent>
<aftsection>
<nextsent>even though exhaustive experiments comparing the performance of relation extraction system based on these four levels of syntactic information are yet tobe conducted, reasonable assumption is that the extraction accuracy increases with the amount of syntactic information used.
</nextsent>
<nextsent>the performance however depends not only on the amount of syntactic information, but also on the details of the exact models using this information.
</nextsent>
<nextsent>training machine learning system in setting where the information usedfor representing the examples is only partially relevant to the actual task often leads to overfitting.
</nextsent>
<nextsent>it is therefore important to design the ie system so that the input data is stripped of unnecessary features as much as possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J980">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>syntactic categories are combined using small set of typed combinatory rules such as functional application, composition and type raising.
</prevsent>
<prevsent>in table 3we show sample derivation based on three functional applications.
</prevsent>
</prevsection>
<citsent citstr=" P02-1043 ">
protesters seized several stations np (s\np )/np np/np np np (s\np )/np np np s\np table 3: sample derivation.in order to obtain ccg derivations for all sentences in the ace corpus, we used the ccg parser introduced in (hockenmaier and steedman,2002)<papid> P02-1043 </papid>3.</citsent>
<aftsection>
<nextsent>this parser also outputs list of dependencies, with each dependency represented as 4-tuple f, a, wf , wa?, where is the syntactic category of the functor, is the argument number, wf is the head word of the functor, and wa is the head word of theargument.
</nextsent>
<nextsent>for example, the three functional applications from table 3 result in the functor-argument dependencies enumerated below in table 4.
</nextsent>
<nextsent>3url:http://www.ircs.upenn.edu/juliahr/parser/ a wf wa np/np 1 several?
</nextsent>
<nextsent>stations?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J981">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>alternatively, head-modifier dependencies can be directly output by parser whose model is based on lexical dependencies.
</prevsent>
<prevsent>in our experiments, we used the full parse output from collins?
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
parser (collins, 1997), <papid> P97-1003 </papid>in which every non-terminal node is already annotated with head information.</citsent>
<aftsection>
<nextsent>because local dependencies assemble into tree for each sentence, there is onlyone (shortest) path between any two entities in dependency tree.
</nextsent>
<nextsent>5.3 experimental results.
</nextsent>
<nextsent>a recent approach to extracting relations is described in (culotta and sorensen, 2004).<papid> P04-1054 </papid></nextsent>
<nextsent>the authors use generalized version of the tree kernel from (zelenko et al, 2003) to compute kernel over 729 relation examples, where relation example consists of the smallest dependency tree containing the two entities of the relation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J990">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>defining positive instance are the 5 types of entities.
</prevsent>
<prevsent>local and non-local (deep) dependencies are equally important for finding relations.
</prevsent>
</prevsection>
<citsent citstr=" P04-1082 ">
in this paper we tried extracting both types of dependencies using ccg parser, however another approach is to recover deep dependencies from syntactic parses, as in (camp bell, 2004; <papid> P04-1082 </papid>levy and manning, 2004).<papid> P04-1042 </papid></citsent>
<aftsection>
<nextsent>this mayhave the advantage of preserving the quality of local dependencies while completing the representation with non-local dependencies.currently, the method assumes that the named entities are known.
</nextsent>
<nextsent>a natural extension is to automatically extract both the entities and their relationships.
</nextsent>
<nextsent>recent research (roth and yih, 2004) <papid> W04-2401 </papid>indicates that integrating entity recognition with relation extraction in global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy.</nextsent>
<nextsent>we have presented new kernel for relation extraction based on the shortest-path between the two relation entities in the dependency graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J991">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>defining positive instance are the 5 types of entities.
</prevsent>
<prevsent>local and non-local (deep) dependencies are equally important for finding relations.
</prevsent>
</prevsection>
<citsent citstr=" P04-1042 ">
in this paper we tried extracting both types of dependencies using ccg parser, however another approach is to recover deep dependencies from syntactic parses, as in (camp bell, 2004; <papid> P04-1082 </papid>levy and manning, 2004).<papid> P04-1042 </papid></citsent>
<aftsection>
<nextsent>this mayhave the advantage of preserving the quality of local dependencies while completing the representation with non-local dependencies.currently, the method assumes that the named entities are known.
</nextsent>
<nextsent>a natural extension is to automatically extract both the entities and their relationships.
</nextsent>
<nextsent>recent research (roth and yih, 2004) <papid> W04-2401 </papid>indicates that integrating entity recognition with relation extraction in global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy.</nextsent>
<nextsent>we have presented new kernel for relation extraction based on the shortest-path between the two relation entities in the dependency graph.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J992">
<title id=" H05-1091.xml">a shortest path dependency kernel for relation extraction </title>
<section> future work.  </section>
<citcontext>
<prevsection>
<prevsent>this mayhave the advantage of preserving the quality of local dependencies while completing the representation with non-local dependencies.currently, the method assumes that the named entities are known.
</prevsent>
<prevsent>a natural extension is to automatically extract both the entities and their relationships.
</prevsent>
</prevsection>
<citsent citstr=" W04-2401 ">
recent research (roth and yih, 2004) <papid> W04-2401 </papid>indicates that integrating entity recognition with relation extraction in global model that captures the mutual influences between the two tasks can lead to significant improvements in accuracy.</citsent>
<aftsection>
<nextsent>we have presented new kernel for relation extraction based on the shortest-path between the two relation entities in the dependency graph.
</nextsent>
<nextsent>comparative experiments on extracting top-level relations from the ace corpus show significant improvements over recent dependency tree kernel.
</nextsent>
<nextsent>this work was supported by grants iis-0117308 and iis-0325116 from the nsf.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J993">
<title id=" E99-1038.xml">focusing on focus a formalization </title>
<section> defining focus: eognito-pragmatie.  </section>
<citcontext>
<prevsection>
<prevsent>the fda to be proposed is largely based on human-human dialogue (though space consideration precludes the full presentation of data), but is believed to be applicable to human-computer interaction as well.
</prevsent>
<prevsent>the study is characterized by its interdisciplinary approach, combining insights and inputs from linguistics, neuroscience and social psychology.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
category the term focus has been used in various senses, at least six of which can be identified, i.e., phonological (pierrehumbert, 1980; ladd, 1996), semantic (jackendoff, 1972; prince, 1985), syntactic (rochemont, 1986), cognitive (sanford &amp; garrod, 1981; musseler et al, 1995), pragmatic (halliday, 1967), and ai-focus (grosz &amp; sidner, 1986) ~.<papid> J86-3001 </papid></citsent>
<aftsection>
<nextsent>we argue that, first, these multiple uses of focus, though resulting in conceptual confusion, hint at the central status of the notion in coreas well as peripheral linguistics.
</nextsent>
<nextsent>second, focus as occurs in discourse is best captured by referring to both the interlocutors  cognitive computation and constant interaction, in accordance with the dual (i.e., cognitive and pragmatic) nature of discourse perse (nuyts, 1992).
</nextsent>
<nextsent>of the six above-mentioned senses, the cognitive and pragmatic ones serve as the basis for the present definition, although the caveat is immediately made that the two aspects are to be fully integrated rather than merely added together.
</nextsent>
<nextsent>moreover, neither is to be adopted blindly given certain shortcomings of previous accounts of each, such as general vagueness militating against their effective application in speech technology.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J994">
<title id=" E99-1038.xml">focusing on focus a formalization </title>
<section> evaluator.  </section>
<citcontext>
<prevsection>
<prevsent>259 proceedings of eacl  99 this representation dovetails nicely with our present account of focus and fda.
</prevsent>
<prevsent>specifically, rough parallel may be drawn between, first, wm and az, second, early episodic memory and s~ &amp; iaz, third, long-term episodic memory &amp; semantic memory and iaz &amp; ks, and fourth, the dynamic working of knowledge processing and that of fda, in particular the card manager which takes charge of the make-up of dm by constantly monitoring the timing and subsequently shuffling and reshuffling cards.
</prevsent>
</prevsection>
<citsent citstr=" C92-3128 ">
synthesis ystems fda, presented here on the basis of an operable definition of focus, enables the integration of focus module into speech synthesis ystem; specifically, the output of fda, i.e., the focus pattern of the message conveyed by the utterance, may be fed into subsequent accent assignment module, one in the spirit of the focus-accent theory of dirksen (1992) <papid> C92-3128 </papid>and dirksen &amp; quene (1993).</citsent>
<aftsection>
<nextsent>in this way, fda entertains great potential for the integration of discourse-level information into prosody generation system, and thereby the production of more discourse-felicitous prosody.
</nextsent>
<nextsent>moreover, given that fda starts with conceptual planning of message, its integration is particularly suitable for concept-to-speech systems.
</nextsent>
<nextsent>as final note, we suggest hat its fundamental rationale is arguably also highly pertinent to text-to-speech systems, which, however, cannot be elaborated here.
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J995">
<title id=" E99-1011.xml">the tipster summac text summarization evaluation </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>methods for evaluating text summarization can be broadly classified into two categories.
</prevsent>
<prevsent>the first, an intrinsic (or normative) evalua-tion, judges the quality of the summary directly based on analysis in terms of some set of norms.
</prevsent>
</prevsection>
<citsent citstr=" W97-0705 ">
this can involve user judgments of fluency of the summary (minel et al 1997), (<papid> W97-0705 </papid>brandow et al 1994), coverage of stipulated  key/essential ideas  in the source (paice 1990), (brandow et al 1994), or similarity to an  ideal  summary, e.g., (ed- mundson 1969), (kupiec et al 1995).</citsent>
<aftsection>
<nextsent>the problem with matching system summary against an ideal summary is that the ideal sum-mary is hard to establish.
</nextsent>
<nextsent>there can be large number of generic and topic-related abstracts that could summarize given document.
</nextsent>
<nextsent>also, there have been several reports of low inter-annotator agreement on sentence xtracts, e.g., (rath et al 1961), (salton et al 1997), although judges may agree more on the most important sentences to include (jing et al 1998).
</nextsent>
<nextsent>the second category, an extrinsic evaluation, judges the quality of the summarization based on how it affects the completion of some other task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J996">
<title id=" E99-1011.xml">the tipster summac text summarization evaluation </title>
<section> results: adhoc and.  </section>
<citcontext>
<prevsection>
<prevsent>we may surmise that in this task, where performance on full-text was hard to begin with, the systems were al~l find-ing the categorization task equally hard, with no particular technique for producing generic sum-maries standing out.
</prevsent>
<prevsent>5.3 agreement between subjects.
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
as indicated in table 9, the unanimous agreement of just 16.6% and 19.5% in the adhoc and cat-egorization tasks respectively is low: the agree-ment data has kappa (carletta et al 1997) <papid> J97-1002 </papid>of .38 for adhoc and .29 for categorization 4.</citsent>
<aftsection>
<nextsent>the ad- hoc pairwise and 3-way agreement (i.e., agreement between groups of 3 subjects) is consistent with 3-subject  dry-run  adhoc consistency task car-ried out earlier.
</nextsent>
<nextsent>however, it is much lower than reported in 3-subject adhoc experiments in trec (harman and voorhees 1996).
</nextsent>
<nextsent>one possible xpla- nation is that in contrast our subjects, trec subjects had years of experience in this task.
</nextsent>
<nextsent>it is also possible that our mix of documents had fewer obviously relevant or obviously irrelevant docu-ments than trec.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J997">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> prior-polarity subjectivity lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>subjectivity clues are words and phrases that may be used to express private states, i.e., they have subjective usages (though they may have objective usages as well).
</prevsent>
<prevsent>for this work, only single-word clues are used.
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
to compile the lexicon, we began with list of subjectivity clues from (riloff and wiebe, 2003).<papid> W03-1014 </papid></citsent>
<aftsection>
<nextsent>the words in this list were grouped in previous work according to their reliability as subjectivity clues.
</nextsent>
<nextsent>words that are subjective in most contexts were marked strongly subjective (strongsubj), and those that may only have certain subjective usages were marked weakly subjective (weaksubj).
</nextsent>
<nextsent>we expanded the list using dictionary and thesaurus, and also added words from the general inquirer positive and negative word lists (general inquirer, 2000) which we judged to be potentially subjective.
</nextsent>
<nextsent>we also gave the new words reliability tags, either strongsubj or weaksubj.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J998">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> prior-polarity subjectivity lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>we also gave the new words reliability tags, either strongsubj or weaksubj.
</prevsent>
<prevsent>the next step was to tag the clues in the lexicon with their prior polarity.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
for words that came from positive and negative word lists (general-inquirer, 2000; hatzivassiloglou and mckeown, 1997), <papid> P97-1023 </papid>we largely retained their original polarity, either positive or negative.</citsent>
<aftsection>
<nextsent>we assigned the remaining words one of the tags positive, negative, both or neutral.
</nextsent>
<nextsent>by far, the majority of clues, 92.8%, are marked as having either positive (33.1%) or negative (59.7%) prior polarity.
</nextsent>
<nextsent>only small number of clues (0.3%) are marked as having both positive and negative polarity.
</nextsent>
<nextsent>6.9% of the clues in the lexicon are marked as neutral.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J999">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the first four involve relationships with the word immediately before or after: if theword is noun preceded by an adjective, if the preceding word is an adverb other than not, if the preceding word is an intensifier, and if the word itself is an intensifier.
</prevsent>
<prevsent>a word is considered an intensifier if it appears in list of intensifiers and if it precedesa word of the appropriate part-of-speech (e.g., an in tensifier adjective must come before noun).
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (collins, 1997) <papid> P97-1003 </papid>and then converting the tree into its dependency representation (xia and palmer, 2001).<papid> H01-1014 </papid></citsent>
<aftsection>
<nextsent>in dependency representation, every node in the tree structure is surface word (i.e., there areno abstract nodes such as np or vp).
</nextsent>
<nextsent>the edge between parent and child specifies the grammatical relationship between the two words.
</nextsent>
<nextsent>figure 1 shows 350 word features sentence features structure features word token strongsubj clues in current sentence: count in subject: binary word part-of-speech strongsubj clues in previous sentence: count in copular: binary word context strongsubj clues in next sentence: count in passive: binary prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count modification features weaksubj clues in next sentence: count document feature preceeded by adjective: binary adjectives in sentence: count document topic preceeded by adverb (other than not): binary adverbs in sentence (other than not): count preceeded by intensifier: binary cardinal number in sentence: binary is intensifier: binary pronoun in sentence: binary modifies strongsubj: binary modal in sentence (other than will): binary modifies weaksubj: binary modified by strongsubj: binary modified by weaksubj: binary table 3: features for neutral-polar classification the human rights report poses substantial challenge to usthe interpretation of good and evil det det det adj adj objsubj mod mod conj conjpobj pobj p (pos) (neg) (pos) (neg) (pos) figure 1: the dependency tree for the sentence the human rights report poses substantial challenge to the us interpretation of good and evil.
</nextsent>
<nextsent>prior polarity is marked in parentheses for words that match clues from the lexicon.an example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1000">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the first four involve relationships with the word immediately before or after: if theword is noun preceded by an adjective, if the preceding word is an adverb other than not, if the preceding word is an intensifier, and if the word itself is an intensifier.
</prevsent>
<prevsent>a word is considered an intensifier if it appears in list of intensifiers and if it precedesa word of the appropriate part-of-speech (e.g., an in tensifier adjective must come before noun).
</prevsent>
</prevsection>
<citsent citstr=" H01-1014 ">
the modify features involve the dependency parse tree for the sentence, obtained by first parsing the sentence (collins, 1997) <papid> P97-1003 </papid>and then converting the tree into its dependency representation (xia and palmer, 2001).<papid> H01-1014 </papid></citsent>
<aftsection>
<nextsent>in dependency representation, every node in the tree structure is surface word (i.e., there areno abstract nodes such as np or vp).
</nextsent>
<nextsent>the edge between parent and child specifies the grammatical relationship between the two words.
</nextsent>
<nextsent>figure 1 shows 350 word features sentence features structure features word token strongsubj clues in current sentence: count in subject: binary word part-of-speech strongsubj clues in previous sentence: count in copular: binary word context strongsubj clues in next sentence: count in passive: binary prior polarity: positive, negative, both, neutral weaksubj clues in current sentence: count reliability class: strongsubj or weaksubj weaksubj clues in previous sentence: count modification features weaksubj clues in next sentence: count document feature preceeded by adjective: binary adjectives in sentence: count document topic preceeded by adverb (other than not): binary adverbs in sentence (other than not): count preceeded by intensifier: binary cardinal number in sentence: binary is intensifier: binary pronoun in sentence: binary modifies strongsubj: binary modal in sentence (other than will): binary modifies weaksubj: binary modified by strongsubj: binary modified by weaksubj: binary table 3: features for neutral-polar classification the human rights report poses substantial challenge to usthe interpretation of good and evil det det det adj adj objsubj mod mod conj conjpobj pobj p (pos) (neg) (pos) (neg) (pos) figure 1: the dependency tree for the sentence the human rights report poses substantial challenge to the us interpretation of good and evil.
</nextsent>
<nextsent>prior polarity is marked in parentheses for words that match clues from the lexicon.an example.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1001">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only significant difference in performance in these experiments is neutral f-measure when the modification features (ab2) are removed.
</prevsent>
<prevsent>these ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether review is positive or negative (e.g., (turney, 2002; <papid> P02-1053 </papid>dave et al, 2003; pang and lee,2004; <papid> P04-1035 </papid>beineke et al, 2004)).<papid> P04-1034 </papid></citsent>
<aftsection>
<nextsent>in contrast, our experiments classify individual words and phrases.
</nextsent>
<nextsent>a number of researchers have explored learning words and phrases with prior positive or negative polarity(another term is semantic orientation) (e.g., (hatzi vassiloglou and mckeown, 1997; <papid> P97-1023 </papid>kamps and marx, 2002; turney, 2002)).<papid> P02-1053 </papid></nextsent>
<nextsent>in contrast, we begin with lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1002">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only significant difference in performance in these experiments is neutral f-measure when the modification features (ab2) are removed.
</prevsent>
<prevsent>these ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether review is positive or negative (e.g., (turney, 2002; <papid> P02-1053 </papid>dave et al, 2003; pang and lee,2004; <papid> P04-1035 </papid>beineke et al, 2004)).<papid> P04-1034 </papid></citsent>
<aftsection>
<nextsent>in contrast, our experiments classify individual words and phrases.
</nextsent>
<nextsent>a number of researchers have explored learning words and phrases with prior positive or negative polarity(another term is semantic orientation) (e.g., (hatzi vassiloglou and mckeown, 1997; <papid> P97-1023 </papid>kamps and marx, 2002; turney, 2002)).<papid> P02-1053 </papid></nextsent>
<nextsent>in contrast, we begin with lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1003">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the only significant difference in performance in these experiments is neutral f-measure when the modification features (ab2) are removed.
</prevsent>
<prevsent>these ablation experiments show that the combination of features is needed to achieve significant results over baseline for polarity classification.
</prevsent>
</prevsection>
<citsent citstr=" P04-1034 ">
much work on sentiment analysis classifies documents by their overall sentiment, for example determining whether review is positive or negative (e.g., (turney, 2002; <papid> P02-1053 </papid>dave et al, 2003; pang and lee,2004; <papid> P04-1035 </papid>beineke et al, 2004)).<papid> P04-1034 </papid></citsent>
<aftsection>
<nextsent>in contrast, our experiments classify individual words and phrases.
</nextsent>
<nextsent>a number of researchers have explored learning words and phrases with prior positive or negative polarity(another term is semantic orientation) (e.g., (hatzi vassiloglou and mckeown, 1997; <papid> P97-1023 </papid>kamps and marx, 2002; turney, 2002)).<papid> P02-1053 </papid></nextsent>
<nextsent>in contrast, we begin with lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the cor pus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1006">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we begin with lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the corpus.
</prevsent>
<prevsent>to make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity are included in our prior-polarity lexicon (general inquirer lists (general-inquirer, 2000) used for evaluation by turney, and lists of manually identified positive and negative adjectives, used for evaluation by hatzivassiloglou and mckeown).some research classifies the sentiments of sentences.
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>kim and hovy (2004), <papid> C04-1200 </papid>hu and liu (2004), and grefenstette et al.</citsent>
<aftsection>
<nextsent>(2001)4 all begin by first creating prior-polaritylexicons.
</nextsent>
<nextsent>yu and hatzivassiloglou then assign sentiment to sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence.
</nextsent>
<nextsent>thus, they do not identify the contextual polarity of individual phrases containing clues, as we 4in (grefenstette et al, 2001), the units that are classified are fixed windows around named entities rather than sentences.
</nextsent>
<nextsent>353 do in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1007">
<title id=" H05-1044.xml">recognizing contextual polarity in phrase level sentiment analysis </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, we begin with lexicon of words with established prior polarities, and identify the contextual polarity of phrases inwhich instances of those words appear in the corpus.
</prevsent>
<prevsent>to make the relationship between that task and ours clearer, note that some word lists used to evaluate methods for recognizing prior polarity are included in our prior-polarity lexicon (general inquirer lists (general-inquirer, 2000) used for evaluation by turney, and lists of manually identified positive and negative adjectives, used for evaluation by hatzivassiloglou and mckeown).some research classifies the sentiments of sentences.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>kim and hovy (2004), <papid> C04-1200 </papid>hu and liu (2004), and grefenstette et al.</citsent>
<aftsection>
<nextsent>(2001)4 all begin by first creating prior-polaritylexicons.
</nextsent>
<nextsent>yu and hatzivassiloglou then assign sentiment to sentence by averaging the prior semantic orientations of instances of lexicon words in the sentence.
</nextsent>
<nextsent>thus, they do not identify the contextual polarity of individual phrases containing clues, as we 4in (grefenstette et al, 2001), the units that are classified are fixed windows around named entities rather than sentences.
</nextsent>
<nextsent>353 do in this paper.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1008">
<title id=" H05-2012.xml">extracting information about outbreaks of infectious epidemics </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>it is one of the most comprehensive sources of reports about the spread of infectious epidemics around the world, collected for over 10 years.
</prevsent>
<prevsent>5confidence for individual fields of extracted facts, and for entire facts, is based on document-local and global information.
</prevsent>
</prevsection>
<citsent citstr=" C02-1154 ">
22 ie engine customization environment lexicon ontology patterns inference rules unsupervised learning extracted facts candidate knowledge db server user query response publisher user data collection web server customizer noise reduction/ data correction/ cross-validation other corpora text documents knowledge bases: figure 1: system architecture of promed-plus acquisition, (yangarber et al, 2002; <papid> C02-1154 </papid>yangarber, 2003) <papid> P03-1044 </papid>requires large corpus of domain-specific and general-topic texts.</citsent>
<aftsection>
<nextsent>on the other hand, automatic error reduction requires critical mass of extractedfacts.
</nextsent>
<nextsent>tighter integration between ie and kdd components, for mutual benefit, is advocated in recent related research, e.g., (nahm and mooney, 2000; mccallum and jensen, 2003).
</nextsent>
<nextsent>in this system we have demonstrated that redundancy in the extracted data (despite the noise) can be leveraged to improve quality, by analyzing global trends and correcting erroneous fills which are due to local mis-analysis,(yangarber and jokipii, 2005).<papid> H05-1008 </papid></nextsent>
<nextsent>for this kind of approach to work, it is necessary to aggregate over large body of extracted records.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1009">
<title id=" H05-2012.xml">extracting information about outbreaks of infectious epidemics </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>it is one of the most comprehensive sources of reports about the spread of infectious epidemics around the world, collected for over 10 years.
</prevsent>
<prevsent>5confidence for individual fields of extracted facts, and for entire facts, is based on document-local and global information.
</prevsent>
</prevsection>
<citsent citstr=" P03-1044 ">
22 ie engine customization environment lexicon ontology patterns inference rules unsupervised learning extracted facts candidate knowledge db server user query response publisher user data collection web server customizer noise reduction/ data correction/ cross-validation other corpora text documents knowledge bases: figure 1: system architecture of promed-plus acquisition, (yangarber et al, 2002; <papid> C02-1154 </papid>yangarber, 2003) <papid> P03-1044 </papid>requires large corpus of domain-specific and general-topic texts.</citsent>
<aftsection>
<nextsent>on the other hand, automatic error reduction requires critical mass of extractedfacts.
</nextsent>
<nextsent>tighter integration between ie and kdd components, for mutual benefit, is advocated in recent related research, e.g., (nahm and mooney, 2000; mccallum and jensen, 2003).
</nextsent>
<nextsent>in this system we have demonstrated that redundancy in the extracted data (despite the noise) can be leveraged to improve quality, by analyzing global trends and correcting erroneous fills which are due to local mis-analysis,(yangarber and jokipii, 2005).<papid> H05-1008 </papid></nextsent>
<nextsent>for this kind of approach to work, it is necessary to aggregate over large body of extracted records.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1010">
<title id=" H05-2012.xml">extracting information about outbreaks of infectious epidemics </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, automatic error reduction requires critical mass of extractedfacts.
</prevsent>
<prevsent>tighter integration between ie and kdd components, for mutual benefit, is advocated in recent related research, e.g., (nahm and mooney, 2000; mccallum and jensen, 2003).
</prevsent>
</prevsection>
<citsent citstr=" H05-1008 ">
in this system we have demonstrated that redundancy in the extracted data (despite the noise) can be leveraged to improve quality, by analyzing global trends and correcting erroneous fills which are due to local mis-analysis,(yangarber and jokipii, 2005).<papid> H05-1008 </papid></citsent>
<aftsection>
<nextsent>for this kind of approach to work, it is necessary to aggregate over large body of extracted records.
</nextsent>
<nextsent>the interface to the db is accessible on-line at doremi.cs.helsinki.fi/plus/ (lower-right of fig.
</nextsent>
<nextsent>1).
</nextsent>
<nextsent>it allows the user to view, select and sortthe extracted outbreaks, as well as the individual incidents that make up the aggregated outbreaks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1011">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word.
</prevsent>
<prevsent>given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve number of purposes for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering (pereira et al, 1993; <papid> P93-1024 </papid>merlo and stevenson, 2001; <papid> J01-3003 </papid>lin, 1998; <papid> P98-2127 </papid>schulte im walde, 2003), thesaurus extraction (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003),<papid> W03-1810 </papid>word sense discrimination (schutze, 1998), text indexing (deerwester et al, 1990), and summarisation(barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</nextsent>
<nextsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</nextsent>
<nextsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</nextsent>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1012">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word.
</prevsent>
<prevsent>given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve number of purposes for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" J01-3003 ">
first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering (pereira et al, 1993; <papid> P93-1024 </papid>merlo and stevenson, 2001; <papid> J01-3003 </papid>lin, 1998; <papid> P98-2127 </papid>schulte im walde, 2003), thesaurus extraction (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003),<papid> W03-1810 </papid>word sense discrimination (schutze, 1998), text indexing (deerwester et al, 1990), and summarisation(barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</nextsent>
<nextsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</nextsent>
<nextsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</nextsent>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1013">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word.
</prevsent>
<prevsent>given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve number of purposes for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering (pereira et al, 1993; <papid> P93-1024 </papid>merlo and stevenson, 2001; <papid> J01-3003 </papid>lin, 1998; <papid> P98-2127 </papid>schulte im walde, 2003), thesaurus extraction (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003),<papid> W03-1810 </papid>word sense discrimination (schutze, 1998), text indexing (deerwester et al, 1990), and summarisation(barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</nextsent>
<nextsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</nextsent>
<nextsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</nextsent>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1014">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word.
</prevsent>
<prevsent>given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve number of purposes for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" P99-1041 ">
first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering (pereira et al, 1993; <papid> P93-1024 </papid>merlo and stevenson, 2001; <papid> J01-3003 </papid>lin, 1998; <papid> P98-2127 </papid>schulte im walde, 2003), thesaurus extraction (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003),<papid> W03-1810 </papid>word sense discrimination (schutze, 1998), text indexing (deerwester et al, 1990), and summarisation(barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</nextsent>
<nextsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</nextsent>
<nextsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</nextsent>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1015">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in the current investigation, we assume that these evoked concepts reflect highly salient linguistic and conceptual features of the stimulus word.
</prevsent>
<prevsent>given this assumption, identifying the types of information provided by speakers and distinguishing and quantifying the relationships between stimulus and response can serve number of purposes for nlp applications.
</prevsent>
</prevsection>
<citsent citstr=" W03-1810 ">
first, the notion of semantic verb relations is crucial for many nlp tasks and applications such as verb clustering (pereira et al, 1993; <papid> P93-1024 </papid>merlo and stevenson, 2001; <papid> J01-3003 </papid>lin, 1998; <papid> P98-2127 </papid>schulte im walde, 2003), thesaurus extraction (lin, 1999; <papid> P99-1041 </papid>mccarthy et al, 2003),<papid> W03-1810 </papid>word sense discrimination (schutze, 1998), text indexing (deerwester et al, 1990), and summarisation(barzilay et al, 2002).</citsent>
<aftsection>
<nextsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</nextsent>
<nextsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</nextsent>
<nextsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</nextsent>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1016">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>different applications incorporate different semantic verb relations, varying with respect to their demands.
</prevsent>
<prevsent>to date, limited effort has been spent on specifying the range of verb-verb relations.
</prevsent>
</prevsection>
<citsent citstr=" W04-2607 ">
morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</citsent>
<aftsection>
<nextsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).
</nextsent>
<nextsent>chklovski and pantel (2004) <papid> W04-3205 </papid>address the automatic acquisition of verb-verb pairs and their relations from the web.</nextsent>
<nextsent>they define syntagmatic patterns to cover strength, enable ment and temporal relations in addition to synonymy and antonymy, but they do not perform an exhaustive study.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1018">
<title id=" H05-1077.xml">identifying semantic relations and functional properties of human verb associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>morris and hirst (2004) <papid> W04-2607 </papid>perform studyon lexical semantic relations which ensure text co hesion.</prevsent>
<prevsent>their relations are not specific to verb-verb pairs, but include e.g. descriptive noun-adjective pairs (such as professors/brilliant), or stereotypical relations (such as homeless/drunk).</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
chklovski and pantel (2004) <papid> W04-3205 </papid>address the automatic acquisition of verb-verb pairs and their relations from the web.</citsent>
<aftsection>
<nextsent>they define syntagmatic patterns to cover strength, enable ment and temporal relations in addition to synonymy and antonymy, but they do not perform an exhaustive study.
</nextsent>
<nextsent>we suggest that an analysis of human verb-verb associations may identify the rangeof semantic relations which are crucial in nlp applications.
</nextsent>
<nextsent>we present preparatory study where the lexical semantic taxonymy germanet (kunze, 2000;kunze, 2004) is checked on the types of classical semantic verb relations1 in our data; verb-verb pairs not covered by germanet can help to detect missing links in the taxonomy, and provide an empirical basis for defining non-classical relations.1we follow morris and hirst (2004) <papid> W04-2607 </papid>and refer to the paradig matic wordnet relations as the classical?</nextsent>
<nextsent>relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1031">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>specialized algorithms are provided for the automatic estimation of these parameters from corpora of translation pairs.
</prevsent>
<prevsent>besides the task of natural language translation, statistical translation models are also exploited in other applications, such as word alignment, multilingual document retrieval and automatic dictionary construction.
</prevsent>
</prevsection>
<citsent citstr=" C88-1016 ">
the most successful translation models that are found in the literature exploit finite-state machinery.the approach started with the so-called ibm models (brown et al , 1988), <papid> C88-1016 </papid>implementing set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence.</citsent>
<aftsection>
<nextsent>these word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (och et al , 1999; <papid> W99-0604 </papid>och and ney, 2002).<papid> P02-1038 </papid></nextsent>
<nextsent>still, the generative capacity of these models lies within the realm of finite-state machinery (kumar and byrne, 2003), <papid> N03-1019 </papid>so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1032">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides the task of natural language translation, statistical translation models are also exploited in other applications, such as word alignment, multilingual document retrieval and automatic dictionary construction.
</prevsent>
<prevsent>the most successful translation models that are found in the literature exploit finite-state machinery.the approach started with the so-called ibm models (brown et al , 1988), <papid> C88-1016 </papid>implementing set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence.</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
these word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (och et al , 1999; <papid> W99-0604 </papid>och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>still, the generative capacity of these models lies within the realm of finite-state machinery (kumar and byrne, 2003), <papid> N03-1019 </papid>so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings.</nextsent>
<nextsent>recently, more sophisticated translation models have been proposed, borrowing from the theory of compilers and making use of synchronous rewriting.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1033">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>besides the task of natural language translation, statistical translation models are also exploited in other applications, such as word alignment, multilingual document retrieval and automatic dictionary construction.
</prevsent>
<prevsent>the most successful translation models that are found in the literature exploit finite-state machinery.the approach started with the so-called ibm models (brown et al , 1988), <papid> C88-1016 </papid>implementing set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
these word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (och et al , 1999; <papid> W99-0604 </papid>och and ney, 2002).<papid> P02-1038 </papid></citsent>
<aftsection>
<nextsent>still, the generative capacity of these models lies within the realm of finite-state machinery (kumar and byrne, 2003), <papid> N03-1019 </papid>so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings.</nextsent>
<nextsent>recently, more sophisticated translation models have been proposed, borrowing from the theory of compilers and making use of synchronous rewriting.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1034">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the most successful translation models that are found in the literature exploit finite-state machinery.the approach started with the so-called ibm models (brown et al , 1988), <papid> C88-1016 </papid>implementing set of elementary operations, such as movement, duplication and translation, that independently act on individual words in the source sentence.</prevsent>
<prevsent>these word-toword models have been later enriched with the introduction of larger units such as phrases; see for instance (och et al , 1999; <papid> W99-0604 </papid>och and ney, 2002).<papid> P02-1038 </papid></prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
still, the generative capacity of these models lies within the realm of finite-state machinery (kumar and byrne, 2003), <papid> N03-1019 </papid>so they are unable to handle nested structures and do not provide the expressivity required to process language pairs with very different word orderings.</citsent>
<aftsection>
<nextsent>recently, more sophisticated translation models have been proposed, borrowing from the theory of compilers and making use of synchronous rewriting.
</nextsent>
<nextsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</nextsent>
<nextsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</nextsent>
<nextsent>among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1035">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1036">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" P98-2230 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1037">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" J00-1004 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1038">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1041">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" P03-1011 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1042">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in synchronous rewriting, two formal grammars are exploited, one describing the source language and the other describing the target language.furthermore, the productions of the two grammars are paired and, in the rewriting process, such pairs are always applied synchronously.
</prevsent>
<prevsent>formalisms based on synchronous rewriting have been empowered with the use of statistical parameters, and specialized estimation and translation (decoding) algorithms were newly developed.
</prevsent>
</prevsection>
<citsent citstr=" N03-1021 ">
among the several proposals, we mention here the models presented in (wu, 1997; <papid> J97-3002 </papid>wu and wong, 1998), (<papid> P98-2230 </papid>alshawi et al , 2000), (<papid> J00-1004 </papid>yamada and knight, 2001), <papid> P01-1067 </papid>gildea, 2003) <papid> P03-1011 </papid>and (melamed, 2003).<papid> N03-1021 </papid></citsent>
<aftsection>
<nextsent>in this paper we consider synchronous models based on context-free grammars and probabilistic extensions thereof.
</nextsent>
<nextsent>this is the most common choice 803in statistical translation models that exceed the generative power of finite-state machinery.
</nextsent>
<nextsent>we focus on two associated computational problems that have been defined in the literature.
</nextsent>
<nextsent>one is the membership problem, which involves testing whether an in put string pair can be generated by the model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1043">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we show here that sometimes this is not the case.the contribution of this paper can be stated as fol lows:?
</prevsent>
<prevsent>we show that the membership problem is np hard, unless constant bound is imposed on the length of the productions (section 3); ? we show an exponential time lower bound for the membership problem, in case chart parsing is adopted (section 3); ? we show that translating an input string intothe best parse tree in the target language is np hard, even in case productions are bounded in length (section 4).
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
investigation of the computational complexity of translation models has started in (knight, 1999) <papid> J99-4005 </papid>for word-to-word models.</citsent>
<aftsection>
<nextsent>this paper can be seen as the continuation of that line of research.
</nextsent>
<nextsent>several definitions for synchronous context-free grammars have been proposed in the literature; see for instance (chiang, 2004; chiang, 2005).<papid> P05-1033 </papid></nextsent>
<nextsent>our definition is based on syntax-directed translation schemata (sdts; aho and ullman, 1972), with the difference that we do not impose the restriction that two paired context-free productions have the same left-hand side.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1044">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> synchronous context-free grammars.  </section>
<citcontext>
<prevsection>
<prevsent>investigation of the computational complexity of translation models has started in (knight, 1999) <papid> J99-4005 </papid>for word-to-word models.</prevsent>
<prevsent>this paper can be seen as the continuation of that line of research.</prevsent>
</prevsection>
<citsent citstr=" P05-1033 ">
several definitions for synchronous context-free grammars have been proposed in the literature; see for instance (chiang, 2004; chiang, 2005).<papid> P05-1033 </papid></citsent>
<aftsection>
<nextsent>our definition is based on syntax-directed translation schemata (sdts; aho and ullman, 1972), with the difference that we do not impose the restriction that two paired context-free productions have the same left-hand side.
</nextsent>
<nextsent>as it will be discussed in section 4, this results in an enriched generative capacity when probabilistic extensions are considered.
</nextsent>
<nextsent>we assume the reader is familiar with the definition of context free grammar (cfg) and with the associated notion of derivation.let vn and vt besets of nonterminal and terminal symbols, respectively.
</nextsent>
<nextsent>in what follows we need to represent bijections between all the occurrences of nonterminals in two strings over vn ? vt . this can be done by annotating nonterminals with indices from an infinite set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1048">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> the membership problem.  </section>
<citcontext>
<prevsection>
<prevsent>this problem has been considered for instance in (wu, 1997) <papid> J97-3002 </papid>for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora, as for instance segmentation, bracketing, phrasal and word alignment.</prevsent>
<prevsent>we show that the membership problem for scfgs is np hard.</prevsent>
</prevsection>
<citsent citstr=" P04-1084 ">
the result could be derived from the findings in (melamed et al , 2004) <papid> P04-1084 </papid>that synchronous rewriting systems as scfgs are related to the class of so called linear context-free rewriting systems (lcfrss) and from the result that the membership problem for 805 lcfrss is np-hard (satta, 1992; <papid> P92-1012 </papid>kaji and others, 1994).</citsent>
<aftsection>
<nextsent>however, we provide here direct proof, to simplify the presentation.
</nextsent>
<nextsent>theorem 1 the membership problem for scfgs is np-hard.
</nextsent>
<nextsent>proof.
</nextsent>
<nextsent>we reduce from the three-satisfiability problem (3sat, garey and johnson, 1979).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1049">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> the membership problem.  </section>
<citcontext>
<prevsection>
<prevsent>this problem has been considered for instance in (wu, 1997) <papid> J97-3002 </papid>for his inversion transduction grammars and has applications in the support of several tasks of automatic annotation of parallel corpora, as for instance segmentation, bracketing, phrasal and word alignment.</prevsent>
<prevsent>we show that the membership problem for scfgs is np hard.</prevsent>
</prevsection>
<citsent citstr=" P92-1012 ">
the result could be derived from the findings in (melamed et al , 2004) <papid> P04-1084 </papid>that synchronous rewriting systems as scfgs are related to the class of so called linear context-free rewriting systems (lcfrss) and from the result that the membership problem for 805 lcfrss is np-hard (satta, 1992; <papid> P92-1012 </papid>kaji and others, 1994).</citsent>
<aftsection>
<nextsent>however, we provide here direct proof, to simplify the presentation.
</nextsent>
<nextsent>theorem 1 the membership problem for scfgs is np-hard.
</nextsent>
<nextsent>proof.
</nextsent>
<nextsent>we reduce from the three-satisfiability problem (3sat, garey and johnson, 1979).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1055">
<title id=" H05-1101.xml">some computational complexity results for synchronous context free grammars </title>
<section> the translation problem.  </section>
<citcontext>
<prevsection>
<prevsent>given as input pscfg gp = (g, pg) and two strings w1, w2 ? t , output the pair of parse trees argmax y(t1) = w1, y(t2) = w2 pg([t1, t2]).
</prevsent>
<prevsent>(3) if the synchronous productions in the underlying scfg have length bounded by some constant, then the above problem can be solved in polynomial time using extensions of the viterbi search strategy to parse forests.
</prevsent>
</prevsection>
<citsent citstr=" P04-1083 ">
this has been shown for instance in (wu and wong, 1998; <papid> P98-2230 </papid>yamada and knight, 2001; <papid> P01-1067 </papid>melamed, 2004).<papid> P04-1083 </papid>a second interesting problem is defined as fol lows.</citsent>
<aftsection>
<nextsent>given as input pscfg gp = (g, pg) and string ? t , output the parse tree argmax pg([w, t]).
</nextsent>
<nextsent>(4) even in case we impose some constant bound on the length of the synchronous productions in g, the above problem is np-hard, as we show in what fol lows.we assume the reader is familiar with the definition of probabilistic context-free grammar (pcfg)and with the associated notion of derivation probability (wetherell, 1980).
</nextsent>
<nextsent>we denote pcfg asa pair (g, pg), with = (vn , vt , p, s) the underlying context-free grammar and pg the associated function providing the probability distributions for the productions in , conditioned on their left hand side.
</nextsent>
<nextsent>a probabilistic regular grammar (prg) is pcfg with underlying productions of the form ? ab or ? ?, with a,b nonterminal symbols and a terminal symbol.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1056">
<title id=" E95-1025.xml">profit prolog with features inheritance and templates </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>4.
</prevsent>
<prevsent>specification of the subsort relationship is. more convenient han constructing prolog terms which mirror these subsumption rela-tionships.
</prevsent>
</prevsection>
<citsent citstr=" C90-3052 ">
implementations of sorted feature formalisms such as tdl (krieger and sch~ifer, 1994), ale (carpenter, 1993), cuf (dsrre and dorna, 1993), tfs (emele and zajac, 1990) <papid> C90-3052 </papid>and others have been used successfully for the development and testing of large grammars and lexicons, but they may be too slow for actual use in applications 180 because they are generally built on top of pro- log or lisp, and can therefore not be as efficient as the built-in unification of prolog.</citsent>
<aftsection>
<nextsent>there are few logic programming lan gauges, uch as life (ait-kaci and lincoln, 1989) or oz (smolka et al, 1995), that provide sorted feature terms, but no commercial implementations of these languages with efficient compilers are yet available.
</nextsent>
<nextsent>1.2 efficient processing based on logic.
</nextsent>
<nextsent>grammars much work on efficient processing algorithms has been done in the logic grammar framework.
</nextsent>
<nextsent>this includes work on ? compiling grammars into efficient parsers and generators: compilation of dcgs into (top-down) prolog programs, left-corner parsers (bup), lr parsers, head-corner parsers, and semantic-head driven genera-tors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1057">
<title id=" E95-1025.xml">profit prolog with features inheritance and templates </title>
<section> the  profit  language.  </section>
<citcontext>
<prevsection>
<prevsent>sem_p ( ( head_adj    cont!x ~    adj dtr!   cont!x ) or ( (  head_comp or  head_marker or  head_fi ler ) ~    coat !y ~    head_dtr !   coat !y ) ).
</prevsent>
<prevsent>for dis junctions of atoms, there exists prolog term representation, which is described below.
</prevsent>
</prevsection>
<citsent citstr=" J91-4002 ">
4see the complexity analysis by brew (brew, 1991).<papid> J91-4002 </papid></citsent>
<aftsection>
<nextsent>183 2.7 in te domains for domains involving only finite set of atoms as possible values, it is possible to provide prolog term representation (due to colmerauer, and de-scribed by mellish (mellish, 1988)) <papid> J88-1004 </papid>to encode any subset of the possible values in one term.</nextsent>
<nextsent>consider the agreement features person (with values 1, 2 and 3) and number (with values sg and pl) . for the two features together there are six possible combinations of values (l&amp;;sg, 2&sg;, 3&sg;, l&pl;, 2&pl;, 3&pl;).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1058">
<title id=" E95-1025.xml">profit prolog with features inheritance and templates </title>
<section> the  profit  language.  </section>
<citcontext>
<prevsection>
<prevsent>for dis junctions of atoms, there exists prolog term representation, which is described below.
</prevsent>
<prevsent>4see the complexity analysis by brew (brew, 1991).<papid> J91-4002 </papid></prevsent>
</prevsection>
<citsent citstr=" J88-1004 ">
183 2.7 in te domains for domains involving only finite set of atoms as possible values, it is possible to provide prolog term representation (due to colmerauer, and de-scribed by mellish (mellish, 1988)) <papid> J88-1004 </papid>to encode any subset of the possible values in one term.</citsent>
<aftsection>
<nextsent>consider the agreement features person (with values 1, 2 and 3) and number (with values sg and pl) . for the two features together there are six possible combinations of values (l&amp;;sg, 2&sg;, 3&sg;, l&pl;, 2&pl;, 3&pl;).
</nextsent>
<nextsent>any subset of this set of possible values can be encoded as one prolog term.
</nextsent>
<nextsent>the following example shows the declaration needed for this finite domain, and some clauses that refer to subsets of the possi-ble agreement values by making use of the logi-cal connectives   (negation), &amp; (conjunction), or (disjunction).5 agr fin_dom \ [1,2,3\] * \[sg,pl\].
</nextsent>
<nextsent>verb(sleeps,3&sg;).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1061">
<title id=" E95-1025.xml">profit prolog with features inheritance and templates </title>
<section> acknowledgements.  </section>
<citcontext>
<prevsection>
<prevsent>profit is not grammar formalism, but rather aims to extend current and future for-malisms and processing models in the logic gram-mar tradition with the expressive power of sorted feature terms.
</prevsent>
<prevsent>since the output of profit com-pilation are prolog programs, all the techniques developed for the optimisation of logic programs (partial evaluation, tabulation, indexing, program transformation techniques etc.) can be applied straightforwardly to improve the performance of sorted feature grammars.
</prevsent>
</prevsection>
<citsent citstr=" E95-1023 ">
this work was supported by ? the commission of the european com-munities through the project lre-61-061  reusable gratnmatical resources , where it has been (ab-)used in creative ways to proto-type extensions for the alep formalism such as set descriptions, linear precedence con-straints and guarded constraints (manand- hat, 1994; manandhar, 1995).<papid> E95-1023 </papid></citsent>
<aftsection>
<nextsent>* deutsche forschungsgemeinschaft, special research division 314  artificial intelli-gence - knowledge-based systems  through project n3  bidirectional linguistic deduc-tion  (bild), where it is used to compile typed feature grammars into logic grammars, for which bidirectional nlp algorithms are developed, and ? cray systems (formerly pe-luxembourg), with whom we had fruitful interaction con-cerning the future development of the alep system-.
</nextsent>
<nextsent>some code for handling of finite domains was adapted from program by gertjan van noord.
</nextsent>
<nextsent>wojciech skut and christian braun were great help in testing and improving the system.
</nextsent>
<nextsent>thanks to all the early users and ~-testers for discover-ing bugs and inconsistencies, and for providing feedback and encouragement.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1062">
<title id=" E93-1021.xml">towards a proper treatment of coercion phenomena </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for kinds, etc.) classify the domain of entities (cf.
</prevsent>
<prevsent>\[bach, 1986; carlson, 1977; chierchia, 1984\]).
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
pustejovsky pro-poses in particular that the mismatch is solved by the operation of  type coercion  (cf.\[pustejovsky, 1991; <papid> J91-4003 </papid>pustejovsky and anick, 1988; <papid> C88-2110 </papid>boguraev and puste-jovsky, 1991\]).<papid> J91-4003 </papid></citsent>
<aftsection>
<nextsent>in essence, it confers to the predicate the ability to change the argument ype.
</nextsent>
<nextsent>forex- ample, the sequence in (1) is accounted for in the following way: (1) john began the book.
</nextsent>
<nextsent>the predicate associated with begin requires that the argument corresponding to the complement be an event (type e).
</nextsent>
<nextsent>since the type associated with book is different (we will suppose it is  material object , p) it is coerced to e. accordingly, (1) is given an event reading, which, in this case, is associated with two possible interpretations:  john began to read the book , and  john began to write the book .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1063">
<title id=" E93-1021.xml">towards a proper treatment of coercion phenomena </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for kinds, etc.) classify the domain of entities (cf.
</prevsent>
<prevsent>\[bach, 1986; carlson, 1977; chierchia, 1984\]).
</prevsent>
</prevsection>
<citsent citstr=" C88-2110 ">
pustejovsky pro-poses in particular that the mismatch is solved by the operation of  type coercion  (cf.\[pustejovsky, 1991; <papid> J91-4003 </papid>pustejovsky and anick, 1988; <papid> C88-2110 </papid>boguraev and puste-jovsky, 1991\]).<papid> J91-4003 </papid></citsent>
<aftsection>
<nextsent>in essence, it confers to the predicate the ability to change the argument ype.
</nextsent>
<nextsent>forex- ample, the sequence in (1) is accounted for in the following way: (1) john began the book.
</nextsent>
<nextsent>the predicate associated with begin requires that the argument corresponding to the complement be an event (type e).
</nextsent>
<nextsent>since the type associated with book is different (we will suppose it is  material object , p) it is coerced to e. accordingly, (1) is given an event reading, which, in this case, is associated with two possible interpretations:  john began to read the book , and  john began to write the book .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1066">
<title id=" H89-1036.xml">lexicalized tags parsing and lexicons </title>
<section> lexicalized tree adjoin ing grammar.  </section>
<citcontext>
<prevsection>

<prevsent>most current linguistic theories give lexical accounts of several phenomena that used to be considered purely syntactic.
</prevsent>
</prevsection>
<citsent citstr=" P84-1058 ">
the information put in the lexicon is thereby increased both in amount and complexity: for example, lexical rules in lfg (kaplan and bresnan, 1983), gpsg (gazdar, klein, pullum and sag, 1985), hpsg (pollard and sag, 1987), comhinatory categoriai grammars (steedman 1988), karttunen version of categorial grammar (karttunen 1986, 1988), some versions of gb theory (chomsky 1981), and lexicon- grammars (gross 1984).<papid> P84-1058 </papid></citsent>
<aftsection>
<nextsent>we say that grammar is  lexicalized  if it consists of: 1 ? finite set of structures associated with each lexical item, which is intended to be the head of these structures; ? an operation or operations for composing the structures.
</nextsent>
<nextsent>the finite set of structures define the domain of locality over which constraints are specified, and these are local with respect their lexical heads.
</nextsent>
<nextsent>context free grammars cannot be in general be lexicalized.
</nextsent>
<nextsent>however tags are  naturally  lexicalized because they use an extended domain of locality (schabes, abeilld and joshi, 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1067">
<title id=" H89-1036.xml">lexicalized tags parsing and lexicons </title>
<section> lexicalized tree adjoin ing grammar.  </section>
<citcontext>
<prevsection>
<prevsent>trees 6-7 are examples of s-type initial trees: they are usually considered as projections of verb and usually take nominal complements.
</prevsent>
<prevsent>the np-type tree  mary  (tree 5), and the np-type tree   john  (similar to tree 5), for example, will be inserted by substitution in the tree 6 corresponding to  np0 saw npi   to produce  john saw mary .
</prevsent>
</prevsection>
<citsent citstr=" C69-4701 ">
examples of auxiliary trees (they are predicates taking sentential complements, 8-10, or modifiers, 11-12): s npo$ vp npo$ vp npo$ vp n a (9) ~ (10) (11) v sl (8) sl npi$ s2 adv a i i i thinks saw promise maybe beautiful (12) 2in some earlier work of joshi (1969), <papid> C69-4701 </papid>work of joshi (1973), the use of the two operations  adjoining  and   replacement  (a restricted case of subst tut ion) was investigated both mathemat ica ly and linguistically.</citsent>
<aftsection>
<nextsent>however, these investigations dealt with string rewriting systems and not tree rewriting systems.
</nextsent>
<nextsent>3we put indices on some non-terminals to express syntactic roles (0 for subject, 1 for first object, etc.).
</nextsent>
<nextsent>the index shown on the empty string (e) and the corresponding filler in the same tree is for the purpose of indicating the filler-gap dependency.
</nextsent>
<nextsent>we use the convention of marking subst tut ion odes by down arrow (.~).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1068">
<title id=" H89-1036.xml">lexicalized tags parsing and lexicons </title>
<section> parsing lexicalized tags.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, we can use the top-down parsing strategy without encountering the usual problems due to recursion.
</prevsent>
<prevsent>by assuming that the number of structures associated with lexical item is finite, since each structure has lexical item attached to it, we implicitly make the assumption that an input string of finite length cannot be syntactically infinitely ambiguous.
</prevsent>
</prevsection>
<citsent citstr=" P88-1032 ">
an earley-type parser for tags has been investigated by schabes and joshi (1988).<papid> P88-1032 </papid></citsent>
<aftsection>
<nextsent>the algorithm has linear best time behavior and an o(n 9) worst time behavior.
</nextsent>
<nextsent>this is the first practical parser for tags because as is well known for cfgs, the average behavior of earley-type parsers is superior to its worst time behavior.
</nextsent>
<nextsent>we extended it to deal with substitution and feature structures for tags.
</nextsent>
<nextsent>by doing this, we have built system that parses unification formalisms that have cfg skeleton and also those that have tag skeleton.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1069">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our experiments show that the reordering model yields substantial improvements in translation performance on arabic-to-english and chinese-to-english mt tasks.
</prevsent>
<prevsent>we also show that the procedure scales as the bitext size is increased.
</prevsent>
</prevsection>
<citsent citstr=" J99-4005 ">
word and phrase reordering is crucial component of statistical machine translation (smt) systems.however allowing reordering in translation is computationally expensive and in some cases even prov ably np-complete (knight, 1999).<papid> J99-4005 </papid></citsent>
<aftsection>
<nextsent>therefore any translation scheme that incorporates reordering must necessarily balance model complexity against the ability to realize the model without approximation.in this paper our goal is to formulate models of local phrase reordering in such way that they can be embedded inside generative phrase-based model ? this work was supported by an onr muri grant n00014-01-1-0685.
</nextsent>
<nextsent>of translation (kumar et al, 2005).
</nextsent>
<nextsent>although this model of reordering is somewhat limited and can not capture all possible phrase movement, it forms proper parameterized probability distribution over reorderings of phrase sequences.
</nextsent>
<nextsent>we show that with this model it is possible to perform maximum aposteriori (map) decoding (with pruning) and expectation maximization (em) style re-estimation of model parameters over large bitext collections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1070">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we now discuss prior work on word and phrase reordering in translation.
</prevsent>
<prevsent>we focus on smt systems that do not require phrases to form syntactic constituents.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the ibm translation models (brown et al, 1993)<papid> J93-2003 </papid>describe word reordering via distortion model defined over word positions within sentence pairs.</citsent>
<aftsection>
<nextsent>the alignment template model (och et al, 1999) <papid> W99-0604 </papid>uses phrases rather than words as the basis for translation, and defines movement at the level of phrases.</nextsent>
<nextsent>phrase reordering is modeled as first order markov process with single parameter that controls the degree of movement.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1071">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we focus on smt systems that do not require phrases to form syntactic constituents.
</prevsent>
<prevsent>the ibm translation models (brown et al, 1993)<papid> J93-2003 </papid>describe word reordering via distortion model defined over word positions within sentence pairs.</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
the alignment template model (och et al, 1999) <papid> W99-0604 </papid>uses phrases rather than words as the basis for translation, and defines movement at the level of phrases.</citsent>
<aftsection>
<nextsent>phrase reordering is modeled as first order markov process with single parameter that controls the degree of movement.
</nextsent>
<nextsent>our current work is inspired by the block(phrase-pair) orientation model introduced by tillmann (2004) in which reordering allows neighboring blocks to swap.
</nextsent>
<nextsent>this is described as sequence of orientations (left, right, neutral) relative to the monotone block order.
</nextsent>
<nextsent>model parameters are block specific and estimated over word aligned trained bi text using simple heuristics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1072">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is described as sequence of orientations (left, right, neutral) relative to the monotone block order.
</prevsent>
<prevsent>model parameters are block specific and estimated over word aligned trained bi text using simple heuristics.
</prevsent>
</prevsection>
<citsent citstr=" P03-1019 ">
other researchers (vogel, 2003; zens and ney, 2003; <papid> P03-1019 </papid>zens et al, 2004) <papid> C04-1030 </papid>have reported performance gains in translation by allowing deviations from monotone word and phrase order.</citsent>
<aftsection>
<nextsent>in these cases, 161 0c 4c 5c 0d 1d 1v 2v 3v 4v 5v 6v 7v 1f 2f 3f 4f 5f 6f 7f 8f 9f 2d 3d 4d 5d 2c 3c1c 1 2 3 4 5 1e 5e 7e2e 3e 4e 6e 9e8e 1 2 3 4 5 1 5y 4y 3y 2 doivent de_25_%exportationsgrains flchir exportations grains de_25_%doivent flchir 1 les exportations de les exportations de grains doivent flchir de 25 % grains doivent flchir de_25_% 1.exportations doiventgrains flchir de_25_% grain exports are_projected_to by_25_% grain exports are projected to fall by 25 %sentence fall source language target language sentence figure 1: ttm generative translation process; here, = 9,k = 5, = 7, = 9.reordering is not governed by an explicit probabilistic model over reordered phrases; language model is employed to select the translation hypothesis.
</nextsent>
<nextsent>wealso note the prior work of wu (1996), <papid> P96-1021 </papid>closely related to till manns model.</nextsent>
<nextsent>the translation template model (ttm) is generative model of phrase-based translation (brown et al, 1993)<papid> J93-2003 </papid>.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1073">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is described as sequence of orientations (left, right, neutral) relative to the monotone block order.
</prevsent>
<prevsent>model parameters are block specific and estimated over word aligned trained bi text using simple heuristics.
</prevsent>
</prevsection>
<citsent citstr=" C04-1030 ">
other researchers (vogel, 2003; zens and ney, 2003; <papid> P03-1019 </papid>zens et al, 2004) <papid> C04-1030 </papid>have reported performance gains in translation by allowing deviations from monotone word and phrase order.</citsent>
<aftsection>
<nextsent>in these cases, 161 0c 4c 5c 0d 1d 1v 2v 3v 4v 5v 6v 7v 1f 2f 3f 4f 5f 6f 7f 8f 9f 2d 3d 4d 5d 2c 3c1c 1 2 3 4 5 1e 5e 7e2e 3e 4e 6e 9e8e 1 2 3 4 5 1 5y 4y 3y 2 doivent de_25_%exportationsgrains flchir exportations grains de_25_%doivent flchir 1 les exportations de les exportations de grains doivent flchir de 25 % grains doivent flchir de_25_% 1.exportations doiventgrains flchir de_25_% grain exports are_projected_to by_25_% grain exports are projected to fall by 25 %sentence fall source language target language sentence figure 1: ttm generative translation process; here, = 9,k = 5, = 7, = 9.reordering is not governed by an explicit probabilistic model over reordered phrases; language model is employed to select the translation hypothesis.
</nextsent>
<nextsent>wealso note the prior work of wu (1996), <papid> P96-1021 </papid>closely related to till manns model.</nextsent>
<nextsent>the translation template model (ttm) is generative model of phrase-based translation (brown et al, 1993)<papid> J93-2003 </papid>.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1074">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>other researchers (vogel, 2003; zens and ney, 2003; <papid> P03-1019 </papid>zens et al, 2004) <papid> C04-1030 </papid>have reported performance gains in translation by allowing deviations from monotone word and phrase order.</prevsent>
<prevsent>in these cases, 161 0c 4c 5c 0d 1d 1v 2v 3v 4v 5v 6v 7v 1f 2f 3f 4f 5f 6f 7f 8f 9f 2d 3d 4d 5d 2c 3c1c 1 2 3 4 5 1e 5e 7e2e 3e 4e 6e 9e8e 1 2 3 4 5 1 5y 4y 3y 2 doivent de_25_%exportationsgrains flchir exportations grains de_25_%doivent flchir 1 les exportations de les exportations de grains doivent flchir de 25 % grains doivent flchir de_25_% 1.exportations doiventgrains flchir de_25_% grain exports are_projected_to by_25_% grain exports are projected to fall by 25 %sentence fall source language target language sentence figure 1: ttm generative translation process; here, = 9,k = 5, = 7, = 9.reordering is not governed by an explicit probabilistic model over reordered phrases; language model is employed to select the translation hypothesis.</prevsent>
</prevsection>
<citsent citstr=" P96-1021 ">
wealso note the prior work of wu (1996), <papid> P96-1021 </papid>closely related to till manns model.</citsent>
<aftsection>
<nextsent>the translation template model (ttm) is generative model of phrase-based translation (brown et al, 1993)<papid> J93-2003 </papid>.</nextsent>
<nextsent>bitext is described via stochastic process that generates source (english) sentences and transforms them into target (french) sentences (fig 1 and eqn 1).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1076">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>reordering bleu (%) arabic-english chinese-english 02 03 04 all 02 03 04 all none 37.5 40.3 36.8 37.8 ? 0.6 24.2 23.7 26.0 25.0 ? 0.5 mj-1 flat 40.4 43.9 39.4 40.7 ? 0.6 25.7 24.5 27.4 26.2 ? 0.5 mj-1 vt 41.3 44.8 40.3 41.6 ? 0.6 25.8 24.5 27.8 26.5 ? 0.5 mj-2 flat 41.0 44.4 39.7 41.1 ? 0.6 26.4 24.9 27.7 26.7 ? 0.5 mj-2 vt 41.7 45.3 40.6 42.0 ? 0.6 26.5 24.9 27.9 26.8 ? 0.5 table 5: performance of mj-1 and mj-2 reordering models with 4-gram lm.
</prevsent>
<prevsent>(all=02+03+04).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
for the combined set (all), wealso show the 95% bleu confidence interval computed using bootstrap re sampling (och, 2003).<papid> P03-1021 </papid>row 1 gives the performance when no reordering model is used.</citsent>
<aftsection>
<nextsent>the next two rows show the influence of the mj-1 reordering model; in row 2, flat probability of 1(x, u) = 0.05 is used for all phrase-pairs; in row 3, reordering probability is estimated for each phrase-pair using viterbi training (eqn 13).
</nextsent>
<nextsent>the last two rows show the effect ofthe mj-2 reordering model; row 4 uses flat probabilities (1(x, u) = 0.05, 2(x, u) = 0.01) for all phrase-pairs; row 5 applies reordering probabilities estimating with viterbi training for each phrase-pair (table 2).on both language-pairs, we observe that reordering yields significant improvements.
</nextsent>
<nextsent>the gains from phrase reordering are much higher on a-e relative to c-e; this could be related to the fact that the word order differences between english and arabic are much higher than the differences between english and chinese.
</nextsent>
<nextsent>mj-1 vt outperforms flat mj-1 showing that there is value in estimating the reordering parameters from bitext.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1077">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the text processing and language models have been described in ? 4.1.
</prevsent>
<prevsent>alignment models are trained on all available bitext (7.6m chunk pairs/207.4m english words/175.7m chinese words on c-e and 5.1m chunk pairs/132.6m english words/123.0m arabic words on a-e), and word alignments are obtained over the bitext.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
phrase-pairs are then extracted from the word alignments (koehn et al, 2003).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>mj-1 model parameters are estimated over all bitext on a-e and over the non-un bitext on c-e.
</nextsent>
<nextsent>finally we use minimum error training(met) (och, 2003) <papid> P03-1021 </papid>to train log-linear scaling factors that are applied to the wfsts in equation 1.</nextsent>
<nextsent>04news (04n) is used as the met training set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1080">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we have described local phrase reordering models developed for use in statistical machine translation.
</prevsent>
<prevsent>the models are carefully formulated so that they can be implemented as wfsts, and we show how the models can be incorporated into the translation template model to perform phrase alignment and translation using standard wfst operations.
</prevsent>
</prevsection>
<citsent citstr=" N03-1019 ">
previous approaches to wfst-based reordering (knight and al-onaizan, 1998; kumar and byrne, 2003; <papid> N03-1019 </papid>tsukada and nagata, 2004) <papid> W04-3255 </papid>constructed permutation accept ors whose state spaces grow exponentially with the length of the sentence to be translated.</citsent>
<aftsection>
<nextsent>as result, these accept ors have to be pruned heavily for use in translation.
</nextsent>
<nextsent>in contrast, our models of local phrase movement do not grow explosively and do not require any pruning or approximation in their construction.
</nextsent>
<nextsent>in other related work,bangalore and ricardi (2001) have trained wf sts for modeling reordering within translation; their wfst parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.
</nextsent>
<nextsent>unlike this approach, our model formulation does not use tree representation and also ensures that the output sequences are valid permutations of input phrase sequences; we emphasize again that the probability distribution induced over reordered phrase sequences is not degenerate.our reordering models do resemble those of (till mann, 2004; tillmann and zhang, 2005) <papid> P05-1069 </papid>in that we 167 treat the reordering as sequence of jumps relative to the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1081">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we have described local phrase reordering models developed for use in statistical machine translation.
</prevsent>
<prevsent>the models are carefully formulated so that they can be implemented as wfsts, and we show how the models can be incorporated into the translation template model to perform phrase alignment and translation using standard wfst operations.
</prevsent>
</prevsection>
<citsent citstr=" W04-3255 ">
previous approaches to wfst-based reordering (knight and al-onaizan, 1998; kumar and byrne, 2003; <papid> N03-1019 </papid>tsukada and nagata, 2004) <papid> W04-3255 </papid>constructed permutation accept ors whose state spaces grow exponentially with the length of the sentence to be translated.</citsent>
<aftsection>
<nextsent>as result, these accept ors have to be pruned heavily for use in translation.
</nextsent>
<nextsent>in contrast, our models of local phrase movement do not grow explosively and do not require any pruning or approximation in their construction.
</nextsent>
<nextsent>in other related work,bangalore and ricardi (2001) have trained wf sts for modeling reordering within translation; their wfst parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.
</nextsent>
<nextsent>unlike this approach, our model formulation does not use tree representation and also ensures that the output sequences are valid permutations of input phrase sequences; we emphasize again that the probability distribution induced over reordered phrase sequences is not degenerate.our reordering models do resemble those of (till mann, 2004; tillmann and zhang, 2005) <papid> P05-1069 </papid>in that we 167 treat the reordering as sequence of jumps relative to the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1082">
<title id=" H05-1021.xml">local phrase reordering models for statistical machine translation </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, our models of local phrase movement do not grow explosively and do not require any pruning or approximation in their construction.
</prevsent>
<prevsent>in other related work,bangalore and ricardi (2001) have trained wf sts for modeling reordering within translation; their wfst parses word sequences into trees containing reordering information, which are then checked for well-formed brackets.
</prevsent>
</prevsection>
<citsent citstr=" P05-1069 ">
unlike this approach, our model formulation does not use tree representation and also ensures that the output sequences are valid permutations of input phrase sequences; we emphasize again that the probability distribution induced over reordered phrase sequences is not degenerate.our reordering models do resemble those of (till mann, 2004; tillmann and zhang, 2005) <papid> P05-1069 </papid>in that we 167 treat the reordering as sequence of jumps relative to the original phrase sequence, and that the likelihood of the reordering is assigned through phrase pair specific parameterized models.</citsent>
<aftsection>
<nextsent>we note thatour implementation allows phrase reordering beyond simply 1-phrase window, as was done by tillmann.
</nextsent>
<nextsent>more importantly, our model implements generative model of phrase reordering which can be incorporated directly into generative model of the overall translation process.
</nextsent>
<nextsent>this allows us to perform embedded?
</nextsent>
<nextsent>em-style parameter estimation, in which the parameters of the phrase reordering model are estimated using statistics gathered under the complete model that will actually be used in translation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1083">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>we compare the performances of decision tree, nave bayes, svm (support vector machine), and me (maxi mum entropy) classification methods.
</prevsent>
<prevsent>the integration of the cascading guidance strategy can help extract answers for questions with implicatures and produce satisfactory results in our experiments.
</prevsent>
</prevsection>
<citsent citstr=" W04-3215 ">
question answering has emerged as key area in natural language processing (nlp) to apply question parsing, information extraction, summarization, and language generation techniques (clark et al., 2004; <papid> W04-3215 </papid>fleischman et al, 2003; <papid> P03-1001 </papid>echihabi et al, 2003; yang et al, 2003; hermjakob et al, 2002; dumais et al, 2002).</citsent>
<aftsection>
<nextsent>traditional question answering systems adopt the framework of parsing questions, searching for relevant documents, and then pinpointing and generating answers.
</nextsent>
<nextsent>however, this framework includes potential dangers.
</nextsent>
<nextsent>for example, to answer the question when did beethoven get married??, typical qa system would identify the question target to be date?
</nextsent>
<nextsent>and would apply techniques to identify the date beethoven got married.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1084">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>we compare the performances of decision tree, nave bayes, svm (support vector machine), and me (maxi mum entropy) classification methods.
</prevsent>
<prevsent>the integration of the cascading guidance strategy can help extract answers for questions with implicatures and produce satisfactory results in our experiments.
</prevsent>
</prevsection>
<citsent citstr=" P03-1001 ">
question answering has emerged as key area in natural language processing (nlp) to apply question parsing, information extraction, summarization, and language generation techniques (clark et al., 2004; <papid> W04-3215 </papid>fleischman et al, 2003; <papid> P03-1001 </papid>echihabi et al, 2003; yang et al, 2003; hermjakob et al, 2002; dumais et al, 2002).</citsent>
<aftsection>
<nextsent>traditional question answering systems adopt the framework of parsing questions, searching for relevant documents, and then pinpointing and generating answers.
</nextsent>
<nextsent>however, this framework includes potential dangers.
</nextsent>
<nextsent>for example, to answer the question when did beethoven get married??, typical qa system would identify the question target to be date?
</nextsent>
<nextsent>and would apply techniques to identify the date beethoven got married.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1086">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>question answering has attracted much attention from the areas of natural language processing, information retrieval and data mining (fleisch manet al, 2003; <papid> P03-1001 </papid>echihabi et al, 2003; yang et al, 2003; hermjakob et al, 2002; dumais et al, 2002; hermjakob et al, 2000).</prevsent>
<prevsent>it is tested in several venues, including the trec and clef question answering tracks (voorhees, 2003; magnini et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" P04-1073 ">
most research efforts in the question answering community have focused on factoid questions and successful question answering systems tend to have similar underlying pipelines structures (prager et al, 2004; <papid> P04-1073 </papid>xu et al, 2003; hovy et al, 2000; moldovan et al, 2000).<papid> P00-1071 </papid></citsent>
<aftsection>
<nextsent>recently more techniques for answer extraction, answer selection, and answer validation have been proposed (lita et al, 2004; soricut and brill, 2004; <papid> N04-1008 </papid>clark et al, 2004).<papid> W04-3215 </papid></nextsent>
<nextsent>prager et al (2004) <papid> P04-1073 </papid>proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1087">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>question answering has attracted much attention from the areas of natural language processing, information retrieval and data mining (fleisch manet al, 2003; <papid> P03-1001 </papid>echihabi et al, 2003; yang et al, 2003; hermjakob et al, 2002; dumais et al, 2002; hermjakob et al, 2000).</prevsent>
<prevsent>it is tested in several venues, including the trec and clef question answering tracks (voorhees, 2003; magnini et al, 2003).</prevsent>
</prevsection>
<citsent citstr=" P00-1071 ">
most research efforts in the question answering community have focused on factoid questions and successful question answering systems tend to have similar underlying pipelines structures (prager et al, 2004; <papid> P04-1073 </papid>xu et al, 2003; hovy et al, 2000; moldovan et al, 2000).<papid> P00-1071 </papid></citsent>
<aftsection>
<nextsent>recently more techniques for answer extraction, answer selection, and answer validation have been proposed (lita et al, 2004; soricut and brill, 2004; <papid> N04-1008 </papid>clark et al, 2004).<papid> W04-3215 </papid></nextsent>
<nextsent>prager et al (2004) <papid> P04-1073 </papid>proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1088">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>it is tested in several venues, including the trec and clef question answering tracks (voorhees, 2003; magnini et al, 2003).
</prevsent>
<prevsent>most research efforts in the question answering community have focused on factoid questions and successful question answering systems tend to have similar underlying pipelines structures (prager et al, 2004; <papid> P04-1073 </papid>xu et al, 2003; hovy et al, 2000; moldovan et al, 2000).<papid> P00-1071 </papid></prevsent>
</prevsection>
<citsent citstr=" N04-1008 ">
recently more techniques for answer extraction, answer selection, and answer validation have been proposed (lita et al, 2004; soricut and brill, 2004; <papid> N04-1008 </papid>clark et al, 2004).<papid> W04-3215 </papid></citsent>
<aftsection>
<nextsent>prager et al (2004) <papid> P04-1073 </papid>proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance.</nextsent>
<nextsent>this approach requires the creation of auxiliary questions, which may be complex to automate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1091">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>prager et al (2004) <papid> P04-1073 </papid>proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance.</prevsent>
<prevsent>this approach requires the creation of auxiliary questions, which may be complex to automate.</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
ravichandran and hovy (2002) <papid> P02-1006 </papid>proposed automatically learning surface text patterns for answer extraction.</citsent>
<aftsection>
<nextsent>however, this approach will not work if no explicit answers exist in the source.
</nextsent>
<nextsent>the first reason is that in that situation the anchors to learn the patterns cannot be determined.
</nextsent>
<nextsent>secondly, most of the facts without explicit values are not expressed with long patterns including anchors.
</nextsent>
<nextsent>for example, the phrase the childless marriage?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1093">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>who is  person child?
</prevsent>
<prevsent>597 (pasca and harabagiu, 2001; hermjakob et al, 2000).
</prevsent>
</prevsection>
<citsent citstr=" P04-1072 ">
saquete et al (2004) <papid> P04-1072 </papid>decompose complex temporal questions into simpler ones based on the temporal relationships in the question.</citsent>
<aftsection>
<nextsent>to date, there has been little published work on handling implicatures in questions.
</nextsent>
<nextsent>just-in-time information seeking agents (jitisa) was proposed by harabagiu (2001) to process questions in dialogue and implicatures.
</nextsent>
<nextsent>the agents are created based on pragmatic knowledge.
</nextsent>
<nextsent>traditional answer extraction and answer fusion approaches assume the question is always correct and explicit answers do exist in the corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1094">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>reported work attempts to rank the candidate answer list to boost the correct one into top position.
</prevsent>
<prevsent>this is not enough when there may not be an answer for the question posed.
</prevsent>
</prevsection>
<citsent citstr=" W04-3256 ">
for biographical fact extraction and generation, zhou et al (2004) <papid> W04-3256 </papid>and schiffman et al (2001) <papid> P01-1059 </papid>use summarization techniques to generate human biographies.</citsent>
<aftsection>
<nextsent>mann and yarowsky (2005) <papid> P05-1060 </papid>propose fusing the extracted information across documents to return consensus answer.</nextsent>
<nextsent>in their approach, they did not consider multiple values or no values for biography facts, although multiple facts are common for some biography attributes, such as multiple occupations, children, books, places of residence, etc. in these cases consensus answer is not adequate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1095">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>reported work attempts to rank the candidate answer list to boost the correct one into top position.
</prevsent>
<prevsent>this is not enough when there may not be an answer for the question posed.
</prevsent>
</prevsection>
<citsent citstr=" P01-1059 ">
for biographical fact extraction and generation, zhou et al (2004) <papid> W04-3256 </papid>and schiffman et al (2001) <papid> P01-1059 </papid>use summarization techniques to generate human biographies.</citsent>
<aftsection>
<nextsent>mann and yarowsky (2005) <papid> P05-1060 </papid>propose fusing the extracted information across documents to return consensus answer.</nextsent>
<nextsent>in their approach, they did not consider multiple values or no values for biography facts, although multiple facts are common for some biography attributes, such as multiple occupations, children, books, places of residence, etc. in these cases consensus answer is not adequate.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1096">
<title id=" H05-1075.xml">handling biographical questions with implicature </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>this is not enough when there may not be an answer for the question posed.
</prevsent>
<prevsent>for biographical fact extraction and generation, zhou et al (2004) <papid> W04-3256 </papid>and schiffman et al (2001) <papid> P01-1059 </papid>use summarization techniques to generate human biographies.</prevsent>
</prevsection>
<citsent citstr=" P05-1060 ">
mann and yarowsky (2005) <papid> P05-1060 </papid>propose fusing the extracted information across documents to return consensus answer.</citsent>
<aftsection>
<nextsent>in their approach, they did not consider multiple values or no values for biography facts, although multiple facts are common for some biography attributes, such as multiple occupations, children, books, places of residence, etc. in these cases consensus answer is not adequate.
</nextsent>
<nextsent>our work differs from theirs because we are not only working on information/answer extraction; the focus in this paper is the guidance for answer extraction of questions (or ie task for values) with implicatures.
</nextsent>
<nextsent>this work can be of great help for immediate biographical information extraction.
</nextsent>
<nextsent>we describe details of the cascading guidance technique and investigate how it will help for question answering in section 3.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1097">
<title id=" H05-1025.xml">predicting sentences using ngram language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using an instance-based method as baseline, we empirically study the predictability of call-center emails, personal emails, weather reports, and cooking recipes.
</prevsent>
<prevsent>prediction of user behavior is basis for the construction of assistance systems; it has therefore been investigated in diverse application areas.
</prevsent>
</prevsection>
<citsent citstr=" W04-3225 ">
previous studies have shed light on the predictability of the next unix command that user will enter (motoda and yoshida, 1997; davison and hirsch, 1998), the next keystrokes on small input device such as apda (darragh and witten, 1992), and of the translation that human translator will choose forgiven foreign sentence (nepveu et al, 2004).<papid> W04-3225 </papid>we address the problem of predicting the subsequent words, given an initial fragment of text.</citsent>
<aftsection>
<nextsent>this problem is motivated by the perspective of assistance systems for repetitive tasks such as answering emails in call centers or letters in an administrative environment.
</nextsent>
<nextsent>both instance-based learning and -gram models can conjecture completions of sentences.
</nextsent>
<nextsent>the use of -gram models requires the application of the viterbi principle to this particular decoding problem.
</nextsent>
<nextsent>quantifying the benefit of editing assistance to user is challenging because it depends not only on an observed distribution over documents, but alsoon the reading and writing speed, personal preference, and training status of the user.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1101">
<title id=" H05-1025.xml">predicting sentences using ngram language models </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>assistance tools have furthermore been developed for translators.
</prevsent>
<prevsent>computer aided translation systems combine translation and language model in orderto provide (human) translator with list of suggestions (langlais et al, 2000; langlais et al, 2004; nepveu et al, 2004).<papid> W04-3225 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1020 ">
foster et al (2002) <papid> W02-1020 </papid>introducea model that adapts to users typing speed in order to achieve better trade-off between distractions and keystroke savings.</citsent>
<aftsection>
<nextsent>grabski and scheffer (2004) have previously developed an indexing method that efficiently retrieves the sentence from collection that is most similar to given initial fragment.
</nextsent>
<nextsent>given an initial text fragment, predictor that solves the sentence completion problem has to conjecture as much of the sentence that the user currently intends to write, as is possible with high confidence?
</nextsent>
<nextsent>preferably, but not necessarily, the entire remainder.
</nextsent>
<nextsent>the perceived benefit of an assistance system is highly subjective, because it depends on the expenditure of time for scanning and deciding on suggestions, and on the time saved due to helpful assistance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1102">
<title id=" H05-1025.xml">predicting sentences using ngram language models </title>
<section> algorithms for sentence completion.  </section>
<citcontext>
<prevsection>
<prevsent>, wt+j1) (5) the individual factors of equation 5 are provided bythe model.
</prevsent>
<prevsent>the markov order has to balance sufficient context information and sparsity of the training data.
</prevsent>
</prevsection>
<citsent citstr=" J92-1002 ">
a standard solution is to use weighted linear mixture of -gram models, 1 ? ? , (brown et al., 1992).<papid> J92-1002 </papid></citsent>
<aftsection>
<nextsent>we use an em algorithm to select mixing weights that maximize the generation probability of tuning set of sentences that have not been used for training.
</nextsent>
<nextsent>we are left with the following questions: (a)how can we decode the most likely completion effi ciently; and (b) how many words should we predict?
</nextsent>
<nextsent>4.1 efficient prediction.
</nextsent>
<nextsent>we have to address the problem of finding the most likely completion, argmaxwt+1,...,wt+t (wt+1, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1103">
<title id=" H89-2012.xml">parsing word associations and typical predicate argument relations </title>
<section> mutual information.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P89-1010 ">
church and hanks (1989) <papid> P89-1010 </papid>discussed the use of the mutual information statistic in order to identify variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence onstraints between verbs and prepositions (content word/function word).</citsent>
<aftsection>
<nextsent>mutual information, l(x;y), compares the probability of observing word and word together (the joint probability) with the probabilities of observing and independently (chance).
</nextsent>
<nextsent>l(x;y) -= log 2 p(x,y) e(x) e(y) if there is genuine association between and y, then the joint probability p(x,y) will he much larger than chance p(x) p(y), and consequently l(x;y)    0, as illustrated in the table below.
</nextsent>
<nextsent>if there is no interesting relationship between and y, then p(x,y) = p(x) p(y), and thus, i(x;y) = 0.
</nextsent>
<nextsent>if and are in complementary distribution, then p(x,y) will be much less than p(x) p(y), forcing l(x;y)    0.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1107">
<title id=" H89-2012.xml">parsing word associations and typical predicate argument relations </title>
<section> phrasal verbs.  </section>
<citcontext>
<prevsection>
<prevsent>this association is relatively strong; the other particles that sinclair mentions have scores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 ap corpus of 15 million words.
</prevsent>
<prevsent>phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
we have found that if we first tag every word in the corpus with part of speech using method such as church (1988) <papid> A88-1019 </papid>or derose (1988), <papid> J88-1003 </papid>and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with following preposition to~in and verbs associated with following infinitive marker to~to.</citsent>
<aftsection>
<nextsent>(part of speech notation is borrowed from francis and kucera (1982); = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)
</nextsent>
<nextsent>the score identifies quite number of verbs associated in an interesting way with to; restricting our attention to pairs with score of 3.0 or more, there are 768 verbs associated with the preposition to~in and 551 verbs with the infinitive marker to~to.
</nextsent>
<nextsent>the ten verbs found to be most associated before to~in are: 76 ? to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/vbg, amounting/vbg, revert/vb, re- verted/vbn, resorting/vbg, relegated/vbn ? to/to: obligated/vbn, trying/vbg, compelled/vbn, enables/vbz, supposed/vbn, intends/vbz, vow- ing/vbg, tried\]vbd, enabling/vbg, tends/vbz, tend\]vb, intend\]vb, tries/vbz thus, we see there is considerable verage to be gained by preprocessing the corpus and manipulating the inventory of tokens.
</nextsent>
<nextsent>4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1108">
<title id=" H89-2012.xml">parsing word associations and typical predicate argument relations </title>
<section> phrasal verbs.  </section>
<citcontext>
<prevsection>
<prevsent>this association is relatively strong; the other particles that sinclair mentions have scores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 ap corpus of 15 million words.
</prevsent>
<prevsent>phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
we have found that if we first tag every word in the corpus with part of speech using method such as church (1988) <papid> A88-1019 </papid>or derose (1988), <papid> J88-1003 </papid>and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with following preposition to~in and verbs associated with following infinitive marker to~to.</citsent>
<aftsection>
<nextsent>(part of speech notation is borrowed from francis and kucera (1982); = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.)
</nextsent>
<nextsent>the score identifies quite number of verbs associated in an interesting way with to; restricting our attention to pairs with score of 3.0 or more, there are 768 verbs associated with the preposition to~in and 551 verbs with the infinitive marker to~to.
</nextsent>
<nextsent>the ten verbs found to be most associated before to~in are: 76 ? to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/vbg, amounting/vbg, revert/vb, re- verted/vbn, resorting/vbg, relegated/vbn ? to/to: obligated/vbn, trying/vbg, compelled/vbn, enables/vbz, supposed/vbn, intends/vbz, vow- ing/vbg, tried\]vbd, enabling/vbg, tends/vbz, tend\]vb, intend\]vb, tries/vbz thus, we see there is considerable verage to be gained by preprocessing the corpus and manipulating the inventory of tokens.
</nextsent>
<nextsent>4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1109">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for which reference is to be established.
</prevsent>
<prevsent>these attributes may involve string matching, agreement, syntactic distance, and positional information, and they tend to rely primarily on the immediate context of the noun phrases (with the possible exception of sen tence-spanning distance measures such as hobbs distance).
</prevsent>
</prevsection>
<citsent citstr=" J01-4003 ">
though gains have been made with such methods (tetreault 2001; <papid> J01-4003 </papid>mitkov 2000; soon et al  2001; <papid> J01-4004 </papid>ng and cardie 2002), <papid> P02-1014 </papid>there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly.</citsent>
<aftsection>
<nextsent>coreference is by definition semantic relationship: two noun phrases corefer if they both refer to the same real-world entity.
</nextsent>
<nextsent>we should therefore expect successful coreference system to exploit world knowledge, inference, and other forms of semantic information in order to resolve hard cases.
</nextsent>
<nextsent>if, for example, two nouns refer to people who work for two different organizations, we want our system to infer that these noun phrases cannot corefer.
</nextsent>
<nextsent>further progress will likely be aided by flexible frameworks for representing and using the information provided by this kind of semantic relation between noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1110">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for which reference is to be established.
</prevsent>
<prevsent>these attributes may involve string matching, agreement, syntactic distance, and positional information, and they tend to rely primarily on the immediate context of the noun phrases (with the possible exception of sen tence-spanning distance measures such as hobbs distance).
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
though gains have been made with such methods (tetreault 2001; <papid> J01-4003 </papid>mitkov 2000; soon et al  2001; <papid> J01-4004 </papid>ng and cardie 2002), <papid> P02-1014 </papid>there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly.</citsent>
<aftsection>
<nextsent>coreference is by definition semantic relationship: two noun phrases corefer if they both refer to the same real-world entity.
</nextsent>
<nextsent>we should therefore expect successful coreference system to exploit world knowledge, inference, and other forms of semantic information in order to resolve hard cases.
</nextsent>
<nextsent>if, for example, two nouns refer to people who work for two different organizations, we want our system to infer that these noun phrases cannot corefer.
</nextsent>
<nextsent>further progress will likely be aided by flexible frameworks for representing and using the information provided by this kind of semantic relation between noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1111">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for which reference is to be established.
</prevsent>
<prevsent>these attributes may involve string matching, agreement, syntactic distance, and positional information, and they tend to rely primarily on the immediate context of the noun phrases (with the possible exception of sen tence-spanning distance measures such as hobbs distance).
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
though gains have been made with such methods (tetreault 2001; <papid> J01-4003 </papid>mitkov 2000; soon et al  2001; <papid> J01-4004 </papid>ng and cardie 2002), <papid> P02-1014 </papid>there are clearly cases where this sort of local information will not be sufficient to resolve coreference correctly.</citsent>
<aftsection>
<nextsent>coreference is by definition semantic relationship: two noun phrases corefer if they both refer to the same real-world entity.
</nextsent>
<nextsent>we should therefore expect successful coreference system to exploit world knowledge, inference, and other forms of semantic information in order to resolve hard cases.
</nextsent>
<nextsent>if, for example, two nouns refer to people who work for two different organizations, we want our system to infer that these noun phrases cannot corefer.
</nextsent>
<nextsent>further progress will likely be aided by flexible frameworks for representing and using the information provided by this kind of semantic relation between noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1112">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>the focus for the last decade has been primarily on broad coverage systems using relatively shallow knowledge, and in particular on corpus-trained statistical models.
</prevsent>
<prevsent>some of these systems attempt to apply shallow semantic information.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
(ge et al  1998) <papid> W98-1119 </papid>incorporate gender, number, and animaticity information into statistical model for anaphora resolution by gathering coreference statistics on particular nominal-pronoun pairs.</citsent>
<aftsection>
<nextsent>(tetreault and allen 2004) use semantic parser to add semantic constraints to the syntactic and agreement constraints in their left-right centering algorithm.
</nextsent>
<nextsent>(soon et al  2001) <papid> J01-4004 </papid>use wordnet to test the semantic compatibility of individual noun phrase pairs.</nextsent>
<nextsent>in general these approaches do not explore the possibility of exploiting the global semantic context provided by the document as whole.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1114">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> prior work.  </section>
<citcontext>
<prevsection>
<prevsent>(soon et al  2001) <papid> J01-4004 </papid>use wordnet to test the semantic compatibility of individual noun phrase pairs.</prevsent>
<prevsent>in general these approaches do not explore the possibility of exploiting the global semantic context provided by the document as whole.</prevsent>
</prevsection>
<citsent citstr=" N04-1038 ">
recently bean and riloff (2004) <papid> N04-1038 </papid>have sought to acquire automatically some semantic patterns that can be used as contextual information to improve reference resolution, using techniques adapted from information extraction.</citsent>
<aftsection>
<nextsent>their experiments were conducted on collections of texts in two topic areas (terrorism and natural disasters).
</nextsent>
<nextsent>our central goal is to model semantic and coreference structures in such way that we can take advantage of semantic context larger than the individual noun phrase when making coreference decisions.
</nextsent>
<nextsent>ideally, this model should make it possible to pick out important features in the context and to distinguish useful signals from background noise.
</nextsent>
<nextsent>it should, for example, be able to represent such basic relational facts as whether the (possibly identical) people referenced by two noun phrases work in the same organization, whether they own the same car, etc. and it should be able to use this information to resolve references even when surface features such as lexical or grammatical attributes are imperfect or fail altogether.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1115">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> algorithms.  </section>
<citcontext>
<prevsection>
<prevsent>whether the intervening words match to tag test example, we find the nearest training examples, use the distance to weight each neighbor, and then select the most heavily weighted class in the weighted neighbor set.
</prevsent>
<prevsent>name tagger and noun phrase chunker our baseline name tagger consists of hmm tagger augmented with set of post-processing rules.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
the hmm tagger generally follows the nymble model (bikel et al  1997), <papid> A97-1029 </papid>but with larger number of states (12 for chinese, 30 for english) to handle name prefixes and suffixes, and, for chinese, transliterated foreign names separately.</citsent>
<aftsection>
<nextsent>for chinese it operates on the output of word segmenter from tsing hua university.
</nextsent>
<nextsent>our nominal mention tagger (noun phrase chunker) is maximum entropy tagger trained on treebanks from the university of pennsylvania.
</nextsent>
<nextsent>5.2 rescoring stage.
</nextsent>
<nextsent>to incorporate information from the relation tagger into the final coreference decision, we split the maxent classification into two stages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1116">
<title id=" H05-1003.xml">using semantic relations to refine coreference decisions </title>
<section> evaluation results.  </section>
<citcontext>
<prevsection>
<prevsent>we used 100 ace texts for final blind test.
</prevsent>
<prevsent>6.2 experiments.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
we used the muc coreference scoring metric (vilain et al 1995) <papid> M95-1005 </papid>to evaluate3 our systems.</citsent>
<aftsection>
<nextsent>to establish an upper limit for the possible improvement offered by our models, we first did experiments using perfect (hand-tagged) mentions and perfect relations as inputs.
</nextsent>
<nextsent>the algorithms for 3 in our scoring, we use the ace keys and only score mentions which appear in both the key and system response.
</nextsent>
<nextsent>this therefore includes only mentions identified as being in the ace semantic categories by both the key and the system response.
</nextsent>
<nextsent>thus these scores cannot be directly compared against coreference scores involving all noun phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1121">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar.
</prevsent>
<prevsent>in the data-driven approach, frequency-based information is automatically derived from corpora.
</prevsent>
</prevsection>
<citsent citstr=" J94-2001 ">
the learning corpus can consist of plain text, but the best results seem achiev-able with annotated corpora (merialdo 1994; <papid> J94-2001 </papid>elworthy 1994).<papid> A94-1009 </papid></citsent>
<aftsection>
<nextsent>this corpus-based informa-tion typically concerns equences of 1-3 tags or words (with some well-known exceptions, e.g. cutting et al 1992).<papid> A92-1018 </papid></nextsent>
<nextsent>corpus-based infor-mation can be represented e.g. as neural net-works (eineborg and gamb/~c 1994; schmid 1994), <papid> C94-1027 </papid>local rules (brill 1992), <papid> A92-1021 </papid>or collocational matrices (garside 1987).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1123">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the linguistic approach is labour-intensive: skill and effort is needed for writing an exhaustive grammar.
</prevsent>
<prevsent>in the data-driven approach, frequency-based information is automatically derived from corpora.
</prevsent>
</prevsection>
<citsent citstr=" A94-1009 ">
the learning corpus can consist of plain text, but the best results seem achiev-able with annotated corpora (merialdo 1994; <papid> J94-2001 </papid>elworthy 1994).<papid> A94-1009 </papid></citsent>
<aftsection>
<nextsent>this corpus-based informa-tion typically concerns equences of 1-3 tags or words (with some well-known exceptions, e.g. cutting et al 1992).<papid> A92-1018 </papid></nextsent>
<nextsent>corpus-based infor-mation can be represented e.g. as neural net-works (eineborg and gamb/~c 1994; schmid 1994), <papid> C94-1027 </papid>local rules (brill 1992), <papid> A92-1021 </papid>or collocational matrices (garside 1987).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1124">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>in the data-driven approach, frequency-based information is automatically derived from corpora.
</prevsent>
<prevsent>the learning corpus can consist of plain text, but the best results seem achiev-able with annotated corpora (merialdo 1994; <papid> J94-2001 </papid>elworthy 1994).<papid> A94-1009 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
this corpus-based informa-tion typically concerns equences of 1-3 tags or words (with some well-known exceptions, e.g. cutting et al 1992).<papid> A92-1018 </papid></citsent>
<aftsection>
<nextsent>corpus-based infor-mation can be represented e.g. as neural net-works (eineborg and gamb/~c 1994; schmid 1994), <papid> C94-1027 </papid>local rules (brill 1992), <papid> A92-1021 </papid>or collocational matrices (garside 1987).</nextsent>
<nextsent>in the data-driven approach, no human effort is needed for rule- writing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1125">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the learning corpus can consist of plain text, but the best results seem achiev-able with annotated corpora (merialdo 1994; <papid> J94-2001 </papid>elworthy 1994).<papid> A94-1009 </papid></prevsent>
<prevsent>this corpus-based informa-tion typically concerns equences of 1-3 tags or words (with some well-known exceptions, e.g. cutting et al 1992).<papid> A92-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
corpus-based infor-mation can be represented e.g. as neural net-works (eineborg and gamb/~c 1994; schmid 1994), <papid> C94-1027 </papid>local rules (brill 1992), <papid> A92-1021 </papid>or collocational matrices (garside 1987).</citsent>
<aftsection>
<nextsent>in the data-driven approach, no human effort is needed for rule- writing.
</nextsent>
<nextsent>however, considerable effort may be needed for determining workable tag set (cf.
</nextsent>
<nextsent>cutting 1994) and annotating the training corpus.
</nextsent>
<nextsent>at the first flush, the linguistic approach may seem an obvious choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1126">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>the learning corpus can consist of plain text, but the best results seem achiev-able with annotated corpora (merialdo 1994; <papid> J94-2001 </papid>elworthy 1994).<papid> A94-1009 </papid></prevsent>
<prevsent>this corpus-based informa-tion typically concerns equences of 1-3 tags or words (with some well-known exceptions, e.g. cutting et al 1992).<papid> A92-1018 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1021 ">
corpus-based infor-mation can be represented e.g. as neural net-works (eineborg and gamb/~c 1994; schmid 1994), <papid> C94-1027 </papid>local rules (brill 1992), <papid> A92-1021 </papid>or collocational matrices (garside 1987).</citsent>
<aftsection>
<nextsent>in the data-driven approach, no human effort is needed for rule- writing.
</nextsent>
<nextsent>however, considerable effort may be needed for determining workable tag set (cf.
</nextsent>
<nextsent>cutting 1994) and annotating the training corpus.
</nextsent>
<nextsent>at the first flush, the linguistic approach may seem an obvious choice.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1127">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>and anttila 1992).
</prevsent>
<prevsent>its recall is very high (99.7% of all words receive the correct morphologi-cal analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.
</prevsent>
</prevsection>
<citsent citstr=" H92-1023 ">
157 ena or the linguist abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach 95-97% accuracy in the anal-ysis of several anguages, in particular english (marshall 1983; black et al 1992; <papid> H92-1023 </papid>church 1988; <papid> A88-1019 </papid>cutting et al 1992; <papid> A92-1018 </papid>de marcken 1990; derose 1988; <papid> J88-1003 </papid>hindle 1989; <papid> P89-1015 </papid>merialdo 1994; <papid> J94-2001 </papid>weischedel et al. 1993; <papid> J93-2006 </papid>brill 1992; <papid> A92-1021 </papid>samuelsson 1994; eineborg and gamb~ick 1994, etc.).</citsent>
<aftsection>
<nextsent>interestingly, no sig-nificant improvement beyond the 97%  barrier  by means of purely data-driven systems has been reported so far.
</nextsent>
<nextsent>in terms of the accuracy of known systems, the data-driven approach seems then to pro-vide the best model of part-of-speech distribu-tion.
</nextsent>
<nextsent>this should appear little curious because very competitive results have been achieved us-ing the linguistic approach at related levels of de-scription.
</nextsent>
<nextsent>with respect computational mor-phology, witness for instance the success of the two-level paradigm introduced by koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typo logically dif-ferent languages (kimmo koskenniemi, personal communication).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1128">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>and anttila 1992).
</prevsent>
<prevsent>its recall is very high (99.7% of all words receive the correct morphologi-cal analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.
</prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
157 ena or the linguist abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach 95-97% accuracy in the anal-ysis of several anguages, in particular english (marshall 1983; black et al 1992; <papid> H92-1023 </papid>church 1988; <papid> A88-1019 </papid>cutting et al 1992; <papid> A92-1018 </papid>de marcken 1990; derose 1988; <papid> J88-1003 </papid>hindle 1989; <papid> P89-1015 </papid>merialdo 1994; <papid> J94-2001 </papid>weischedel et al. 1993; <papid> J93-2006 </papid>brill 1992; <papid> A92-1021 </papid>samuelsson 1994; eineborg and gamb~ick 1994, etc.).</citsent>
<aftsection>
<nextsent>interestingly, no sig-nificant improvement beyond the 97%  barrier  by means of purely data-driven systems has been reported so far.
</nextsent>
<nextsent>in terms of the accuracy of known systems, the data-driven approach seems then to pro-vide the best model of part-of-speech distribu-tion.
</nextsent>
<nextsent>this should appear little curious because very competitive results have been achieved us-ing the linguistic approach at related levels of de-scription.
</nextsent>
<nextsent>with respect computational mor-phology, witness for instance the success of the two-level paradigm introduced by koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typo logically dif-ferent languages (kimmo koskenniemi, personal communication).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1130">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>and anttila 1992).
</prevsent>
<prevsent>its recall is very high (99.7% of all words receive the correct morphologi-cal analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
157 ena or the linguist abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach 95-97% accuracy in the anal-ysis of several anguages, in particular english (marshall 1983; black et al 1992; <papid> H92-1023 </papid>church 1988; <papid> A88-1019 </papid>cutting et al 1992; <papid> A92-1018 </papid>de marcken 1990; derose 1988; <papid> J88-1003 </papid>hindle 1989; <papid> P89-1015 </papid>merialdo 1994; <papid> J94-2001 </papid>weischedel et al. 1993; <papid> J93-2006 </papid>brill 1992; <papid> A92-1021 </papid>samuelsson 1994; eineborg and gamb~ick 1994, etc.).</citsent>
<aftsection>
<nextsent>interestingly, no sig-nificant improvement beyond the 97%  barrier  by means of purely data-driven systems has been reported so far.
</nextsent>
<nextsent>in terms of the accuracy of known systems, the data-driven approach seems then to pro-vide the best model of part-of-speech distribu-tion.
</nextsent>
<nextsent>this should appear little curious because very competitive results have been achieved us-ing the linguistic approach at related levels of de-scription.
</nextsent>
<nextsent>with respect computational mor-phology, witness for instance the success of the two-level paradigm introduced by koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typo logically dif-ferent languages (kimmo koskenniemi, personal communication).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1131">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>and anttila 1992).
</prevsent>
<prevsent>its recall is very high (99.7% of all words receive the correct morphologi-cal analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.
</prevsent>
</prevsection>
<citsent citstr=" P89-1015 ">
157 ena or the linguist abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach 95-97% accuracy in the anal-ysis of several anguages, in particular english (marshall 1983; black et al 1992; <papid> H92-1023 </papid>church 1988; <papid> A88-1019 </papid>cutting et al 1992; <papid> A92-1018 </papid>de marcken 1990; derose 1988; <papid> J88-1003 </papid>hindle 1989; <papid> P89-1015 </papid>merialdo 1994; <papid> J94-2001 </papid>weischedel et al. 1993; <papid> J93-2006 </papid>brill 1992; <papid> A92-1021 </papid>samuelsson 1994; eineborg and gamb~ick 1994, etc.).</citsent>
<aftsection>
<nextsent>interestingly, no sig-nificant improvement beyond the 97%  barrier  by means of purely data-driven systems has been reported so far.
</nextsent>
<nextsent>in terms of the accuracy of known systems, the data-driven approach seems then to pro-vide the best model of part-of-speech distribu-tion.
</nextsent>
<nextsent>this should appear little curious because very competitive results have been achieved us-ing the linguistic approach at related levels of de-scription.
</nextsent>
<nextsent>with respect computational mor-phology, witness for instance the success of the two-level paradigm introduced by koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typo logically dif-ferent languages (kimmo koskenniemi, personal communication).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1134">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>and anttila 1992).
</prevsent>
<prevsent>its recall is very high (99.7% of all words receive the correct morphologi-cal analysis), but this system leaves 3-7% of all words ambiguous, trading precision for recall.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
157 ena or the linguist abstraction capabilities (e.g. knowledge about what is relevant in the context), they tend to reach 95-97% accuracy in the anal-ysis of several anguages, in particular english (marshall 1983; black et al 1992; <papid> H92-1023 </papid>church 1988; <papid> A88-1019 </papid>cutting et al 1992; <papid> A92-1018 </papid>de marcken 1990; derose 1988; <papid> J88-1003 </papid>hindle 1989; <papid> P89-1015 </papid>merialdo 1994; <papid> J94-2001 </papid>weischedel et al. 1993; <papid> J93-2006 </papid>brill 1992; <papid> A92-1021 </papid>samuelsson 1994; eineborg and gamb~ick 1994, etc.).</citsent>
<aftsection>
<nextsent>interestingly, no sig-nificant improvement beyond the 97%  barrier  by means of purely data-driven systems has been reported so far.
</nextsent>
<nextsent>in terms of the accuracy of known systems, the data-driven approach seems then to pro-vide the best model of part-of-speech distribu-tion.
</nextsent>
<nextsent>this should appear little curious because very competitive results have been achieved us-ing the linguistic approach at related levels of de-scription.
</nextsent>
<nextsent>with respect computational mor-phology, witness for instance the success of the two-level paradigm introduced by koskenniemi (1983): extensive morphological descriptions have been made of more than 15 typo logically dif-ferent languages (kimmo koskenniemi, personal communication).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1136">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>with regard t.o computational syntax, see for instance (giingsrdii and oflazer 1994; hindle 1983; jensen, heidorn and richard-son (eds.)
</prevsent>
<prevsent>1993; mccord 1990; sleator and tem-perley 1991; alshawi (ed.)
</prevsent>
</prevsection>
<citsent citstr=" C92-1033 ">
1992; strzalkowski 1992).<papid> C92-1033 </papid></citsent>
<aftsection>
<nextsent>the present success of the statistical ap-proach in part-of-speech analysis eems then to form an exception to the general feasibility of the rule-based linguistic approach.
</nextsent>
<nextsent>is the level of parts of speech somehow different, perhaps less rule- governed, than related levels?
</nextsent>
<nextsent>2 we do not need to assume this idiosyncratic sta-tus entirely.
</nextsent>
<nextsent>the rest of this paper argues that also parts of speech can be viewed as rule-governed phenomenon, possible to model using the linguis-tic approach.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1138">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>engcg leaves them pending mainly because it is prohibitively diffi-cult to express certain kinds of structural gener-alisation using the available rule formalism and grammatical representation.
</prevsent>
<prevsent>2.3 syntactic analysis.
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
2.3.1 in te -s ta te intersect ion grammar syntactic analysis is carried out in another e- ductionistic parsing framework nown as finite- state intersection grammar (koskenniemi 1990; <papid> C90-2040 </papid>koskenniemi, tapanainen and voutilainen 1992; tapanainen 1992; voutilainen and tapanainen 1993; <papid> E93-1046 </papid>voutilainen 1994).</citsent>
<aftsection>
<nextsent>a short introduction: ? also here syntactic analysis means resolu-tion of structural ambiguities.
</nextsent>
<nextsent>morphologi-cal, syntactic and clause boundary descrip-tors are introduced as ambiguities with sim-ple mappings; these ambiguities are then re-solved in parallel.
</nextsent>
<nextsent>the formalism does not distinguish between various types of ambiguity; nor are ambiguity class specific rule sets needed.
</nextsent>
<nextsent>a single rule often resolves all types of ambiguity, though superficially it may look e.g. like rule about syntactic functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1139">
<title id=" E95-1022.xml">a syntax based partofspeech analyser </title>
<section> system description.  </section>
<citcontext>
<prevsection>
<prevsent>engcg leaves them pending mainly because it is prohibitively diffi-cult to express certain kinds of structural gener-alisation using the available rule formalism and grammatical representation.
</prevsent>
<prevsent>2.3 syntactic analysis.
</prevsent>
</prevsection>
<citsent citstr=" E93-1046 ">
2.3.1 in te -s ta te intersect ion grammar syntactic analysis is carried out in another e- ductionistic parsing framework nown as finite- state intersection grammar (koskenniemi 1990; <papid> C90-2040 </papid>koskenniemi, tapanainen and voutilainen 1992; tapanainen 1992; voutilainen and tapanainen 1993; <papid> E93-1046 </papid>voutilainen 1994).</citsent>
<aftsection>
<nextsent>a short introduction: ? also here syntactic analysis means resolu-tion of structural ambiguities.
</nextsent>
<nextsent>morphologi-cal, syntactic and clause boundary descrip-tors are introduced as ambiguities with sim-ple mappings; these ambiguities are then re-solved in parallel.
</nextsent>
<nextsent>the formalism does not distinguish between various types of ambiguity; nor are ambiguity class specific rule sets needed.
</nextsent>
<nextsent>a single rule often resolves all types of ambiguity, though superficially it may look e.g. like rule about syntactic functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1143">
<title id=" H89-2014.xml">augmenting a hidden markov model for phrase dependent word tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>application areas include speech recognition/synthesis and information retrieval.
</prevsent>
<prevsent>several workers have addressed the problem of tagging text.
</prevsent>
</prevsection>
<citsent citstr=" J88-1003 ">
methods have ranged from locally-operating rules (greene and rubin, 1971), to statistical methods (church, 1989; derose, 1988; <papid> J88-1003 </papid>garside, leech and sampson, 1987; jelinek, 1985) and back-propagation (benello, mackie and anderson, 1989; nakamura nd shikano, 1989).</citsent>
<aftsection>
<nextsent>the statistical methods can be described in terms of markov models.
</nextsent>
<nextsent>states in model represent categories {cl...c=} (n is the number of different categories used).
</nextsent>
<nextsent>in first order model, ci and ci_l are random variables denoting the categories of the words at position and (i - 1) in text.
</nextsent>
<nextsent>the transition probability p(ci = cz \] ci_~ = %) linking two states cz and cy, represents he probability of category cx following category %.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1144">
<title id=" H89-2014.xml">augmenting a hidden markov model for phrase dependent word tagging </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the work described here also makes use of hidden markov model.
</prevsent>
<prevsent>one aim of the work is to investigate he quality and performance of models with minimal parameter descriptions.
</prevsent>
</prevsection>
<citsent citstr=" H89-1054 ">
in this regard, word equivalence 92 classes were used (kupiec, 1989).<papid> H89-1054 </papid></citsent>
<aftsection>
<nextsent>there it is assumed that the distribution of the use of word depends on the set of categories it can assume, and words are partitioned accordingly.
</nextsent>
<nextsent>thus the words  play  and  touch  are considered to behave identically, as members of the class noun-or-verb, and  clay  and  zinc are members of the class noun.
</nextsent>
<nextsent>this partitioning drastically reduces the number of parameters equired in the model, and aids reliable stimation using moderate amounts of training data.
</nextsent>
<nextsent>equivalence classes {eqvl ...eqvm} replace the words {wl...wv} (m    v) and p(eqvi ci) replace the parameters p(wi ci).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1146">
<title id=" H05-1076.xml">the use of meta data web derived answer patterns and passage context to improve reading comprehension performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, performance analysis based on remedia shows that relative performances of 20.7% is due to meta data matching and further 10.9% is due to the application of web-derived answer patterns.
</prevsent>
<prevsent>a reading comprehension (rc) system attempts to understand document and returns an answer sentence when posed with question.
</prevsent>
</prevsection>
<citsent citstr=" P99-1042 ">
the rc task was first proposed by the mitre corporation which developed the deep read reading comprehension system (hirschman et al, 1999).<papid> P99-1042 </papid></citsent>
<aftsection>
<nextsent>deep read was evaluated on the remedia corpus that contains set of stories, each with an average of 20 sentences and five questions (of types who, where, when, what and why).
</nextsent>
<nextsent>the mitre group also defined the humsent scoring metric, i.e. the percentage of test questions for which the system has chosen correct sentence as the answer.
</nextsent>
<nextsent>humsent answers were compiled by human annotator, who examined the stories and chose the sentence(s) that best answered the questions.
</nextsent>
<nextsent>it was judged that for 11% of the remedia test questions, there is no single sentence in the story that is judged to be an appropriate answer sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1148">
<title id=" H05-1076.xml">the use of meta data web derived answer patterns and passage context to improve reading comprehension performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>hence the upper bound for rc on remedia should by 89% humsent accuracy.
</prevsent>
<prevsent>(hirschman et al 1999) <papid> P99-1042 </papid>reported humsent accuracy of 36.6% on the remedia test set.</prevsent>
</prevsection>
<citsent citstr=" W00-1316 ">
subsequently, (ng et al, 2000) <papid> W00-1316 </papid>used machine learning approach of decision tree and achieved the accuracy of 39.3%.</citsent>
<aftsection>
<nextsent>then (riloff and thelen, 2000) <papid> W00-0603 </papid>and (charniak et al, 2000) <papid> W00-0601 </papid>reported improvements to 39.7% and 41%, respectively.</nextsent>
<nextsent>they made use of handcrafted heuristics such as the when rule: if contain(s, time), then score(s)+=4 i.e. when questions reward candidate answer sentences with four extra points if they contain name entity time.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1149">
<title id=" H05-1076.xml">the use of meta data web derived answer patterns and passage context to improve reading comprehension performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(hirschman et al 1999) <papid> P99-1042 </papid>reported humsent accuracy of 36.6% on the remedia test set.</prevsent>
<prevsent>subsequently, (ng et al, 2000) <papid> W00-1316 </papid>used machine learning approach of decision tree and achieved the accuracy of 39.3%.</prevsent>
</prevsection>
<citsent citstr=" W00-0603 ">
then (riloff and thelen, 2000) <papid> W00-0603 </papid>and (charniak et al, 2000) <papid> W00-0601 </papid>reported improvements to 39.7% and 41%, respectively.</citsent>
<aftsection>
<nextsent>they made use of handcrafted heuristics such as the when rule: if contain(s, time), then score(s)+=4 i.e. when questions reward candidate answer sentences with four extra points if they contain name entity time.
</nextsent>
<nextsent>rc resembles the ad hoc question answering (qa) task in trec.1 the qa task finds answers to set of questions from collection of documents, while rc focuses on single 1 http://www.nist.gov. 604 document.
</nextsent>
<nextsent>(light et al 1998) conducted detailed compared between the two tasks.
</nextsent>
<nextsent>they found that the answers of most questions in the trec qa task appear more than once within the document collection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1151">
<title id=" H05-1076.xml">the use of meta data web derived answer patterns and passage context to improve reading comprehension performance </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>(hirschman et al 1999) <papid> P99-1042 </papid>reported humsent accuracy of 36.6% on the remedia test set.</prevsent>
<prevsent>subsequently, (ng et al, 2000) <papid> W00-1316 </papid>used machine learning approach of decision tree and achieved the accuracy of 39.3%.</prevsent>
</prevsection>
<citsent citstr=" W00-0601 ">
then (riloff and thelen, 2000) <papid> W00-0603 </papid>and (charniak et al, 2000) <papid> W00-0601 </papid>reported improvements to 39.7% and 41%, respectively.</citsent>
<aftsection>
<nextsent>they made use of handcrafted heuristics such as the when rule: if contain(s, time), then score(s)+=4 i.e. when questions reward candidate answer sentences with four extra points if they contain name entity time.
</nextsent>
<nextsent>rc resembles the ad hoc question answering (qa) task in trec.1 the qa task finds answers to set of questions from collection of documents, while rc focuses on single 1 http://www.nist.gov. 604 document.
</nextsent>
<nextsent>(light et al 1998) conducted detailed compared between the two tasks.
</nextsent>
<nextsent>they found that the answers of most questions in the trec qa task appear more than once within the document collection.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1158">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal.
</prevsent>
<prevsent>however, statistical advances have made it possible to address core pieces of the problem.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
recent years have seen wealth of research on one important component of semantic interpretation automatic role labelling (e.g.,gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>hacioglu et al, 2004, <papid> W04-2416 </papid>and additional papers from carreras and marquez, 2004).</citsent>
<aftsection>
<nextsent>such work aims to annotate each constituent in clause with semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [yuka]agent [whispered]pred to [dar]recipient semantic role labelling systems address crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up.most work thus far on domain-general role labelling depends on supervised learning over statistical features extracted from hand-labelled corpus.
</nextsent>
<nextsent>the reliance on such resource one in which the arguments of each predicate are manually identified and assigned semantic role limits the portability of such methods to other languages or even to other genres of corpora.
</nextsent>
<nextsent>in this study, we explore the possibility of using verb lexicon, rather than hand-labelled corpus, as the primary resource in the semantic role labelling task.
</nextsent>
<nextsent>perhaps because of the focus on what can be gleaned from labelled data, existing supervised approaches have made little use of the additional knowledge available in the predicate lexicon associated with the labelled corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1159">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal.
</prevsent>
<prevsent>however, statistical advances have made it possible to address core pieces of the problem.
</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
recent years have seen wealth of research on one important component of semantic interpretation automatic role labelling (e.g.,gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>hacioglu et al, 2004, <papid> W04-2416 </papid>and additional papers from carreras and marquez, 2004).</citsent>
<aftsection>
<nextsent>such work aims to annotate each constituent in clause with semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [yuka]agent [whispered]pred to [dar]recipient semantic role labelling systems address crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up.most work thus far on domain-general role labelling depends on supervised learning over statistical features extracted from hand-labelled corpus.
</nextsent>
<nextsent>the reliance on such resource one in which the arguments of each predicate are manually identified and assigned semantic role limits the portability of such methods to other languages or even to other genres of corpora.
</nextsent>
<nextsent>in this study, we explore the possibility of using verb lexicon, rather than hand-labelled corpus, as the primary resource in the semantic role labelling task.
</nextsent>
<nextsent>perhaps because of the focus on what can be gleaned from labelled data, existing supervised approaches have made little use of the additional knowledge available in the predicate lexicon associated with the labelled corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1160">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>intelligent language technologies capable of full semantic interpretation of domain-general text remain an elusive goal.
</prevsent>
<prevsent>however, statistical advances have made it possible to address core pieces of the problem.
</prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
recent years have seen wealth of research on one important component of semantic interpretation automatic role labelling (e.g.,gildea and jurafsky, 2002; <papid> J02-3001 </papid>pradhan et al, 2004; <papid> N04-1030 </papid>hacioglu et al, 2004, <papid> W04-2416 </papid>and additional papers from carreras and marquez, 2004).</citsent>
<aftsection>
<nextsent>such work aims to annotate each constituent in clause with semantic tag indicating the role that the constituent plays with respect to the target predicate, as in (1): (1) [yuka]agent [whispered]pred to [dar]recipient semantic role labelling systems address crucial first step in the automatic extraction of semantic relations from domain-general text, taking us closer to the goal of comprehensive semantic mark-up.most work thus far on domain-general role labelling depends on supervised learning over statistical features extracted from hand-labelled corpus.
</nextsent>
<nextsent>the reliance on such resource one in which the arguments of each predicate are manually identified and assigned semantic role limits the portability of such methods to other languages or even to other genres of corpora.
</nextsent>
<nextsent>in this study, we explore the possibility of using verb lexicon, rather than hand-labelled corpus, as the primary resource in the semantic role labelling task.
</nextsent>
<nextsent>perhaps because of the focus on what can be gleaned from labelled data, existing supervised approaches have made little use of the additional knowledge available in the predicate lexicon associated with the labelled corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1161">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we face methodological challenge arising fromthe particular choice of verbnet for the prototyping of our method: the lexicon has no associated semantic role labelled corpus.
</prevsent>
<prevsent>while this underscores the need for approaches which do not relyon such resource, it also means that we lack labelled sample of data against which to evaluate our results.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
to address this, we use the existing labelled corpus of framenet (baker et al, 1998), <papid> P98-1013 </papid>and develop mapping for converting the framenet roles to corresponding verbnet roles.</citsent>
<aftsection>
<nextsent>our mapping method demonstrates the possibility of leveraging existing resources to support the development of role labelling systems based on verb lexicons that do not have an associated hand-labelled corpus.
</nextsent>
<nextsent>before describing our labelling algorithm, we first briefly introduce the semantic role information available in verbnet, and describe how we map framenet roles to verbnet roles.
</nextsent>
<nextsent>whisper frames: agent agent prep(+dest) recipient agent topic verbs in same (sub)class: [bark, croon, drone, grunt, holler, ...]
</nextsent>
<nextsent>figure 1: portion of verbnet entry.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1162">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> the frame matching process.  </section>
<citcontext>
<prevsection>
<prevsent>the final set consists of 16 roles: agent, amount, attribute, beneficiary,cause, destination, experiencer, instrument, location, material, predicate, recipient, source, stimulus, theme and time; plus the norole label.
</prevsent>
<prevsent>a main goal of our system is to demonstrate the usefulness of predicate lexicons for the role labelling task.
</prevsent>
</prevsection>
<citsent citstr=" W04-3213 ">
the primary way that we apply the knowledge in our lexicon is via process we call frame matching, adapted from swier and stevenson (2004).<papid> W04-3213 </papid></citsent>
<aftsection>
<nextsent>the automatic frame matcher aligns arguments extracted from an automatically parsed sentence with the frames in verbnet for the target verb in the sentence.
</nextsent>
<nextsent>the output of this process isa highly constrained set of candidate roles (possi bly of size one) for each potential argument.
</nextsent>
<nextsent>the resulting singleton sets constitute (noisy) role assignment for their corresponding arguments, forming our primary-labelled data.
</nextsent>
<nextsent>this data is then used to train probability model, described in section 4, which we employ to label the remaining arguments (those having more than one candidate role).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1176">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two exceptions are the subcategorization frame based work of atserias et al (2001) and the bootstrapping lab eller of swier and stevenson (2004), <papid> W04-3213 </papid>but both are evaluated on only asmall number of verbs and arguments.</prevsent>
<prevsent>in related unsupervised tasks, riloff and colleagues have learned case frames?</prevsent>
</prevsection>
<citsent citstr=" W98-1106 ">
for verbs (e.g., riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>while gildea (2002) <papid> C02-1132 </papid>has learned role slot mappings (but does not apply the knowledge for the labelling task).</citsent>
<aftsection>
<nextsent>other role labelling systems have also relied on the extraction of much more complex features or probability models than we adopt here.
</nextsent>
<nextsent>as point of comparison, we apply the iterative backoff model from swier and stevenson (2004), <papid> W04-3213 </papid>trained on 20% of the bnc, with our frame matcher and test data.</nextsent>
<nextsent>the backoff model achieves an f-measure of .63, slightly below the performance of .65 for our simplest probability model, which uses less training data and takes far less time to run (minutes rather than hours).in general, it is not possible to make direct comparisons between our work and most other role la bellers because of differences in corpora and role sets, and, perhaps more significantly, differences in the selection of target arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1177">
<title id=" H05-1111.xml">exploiting a verb lexicon in automatic semantic role labelling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>two exceptions are the subcategorization frame based work of atserias et al (2001) and the bootstrapping lab eller of swier and stevenson (2004), <papid> W04-3213 </papid>but both are evaluated on only asmall number of verbs and arguments.</prevsent>
<prevsent>in related unsupervised tasks, riloff and colleagues have learned case frames?</prevsent>
</prevsection>
<citsent citstr=" C02-1132 ">
for verbs (e.g., riloff and schmelzenbach, 1998), <papid> W98-1106 </papid>while gildea (2002) <papid> C02-1132 </papid>has learned role slot mappings (but does not apply the knowledge for the labelling task).</citsent>
<aftsection>
<nextsent>other role labelling systems have also relied on the extraction of much more complex features or probability models than we adopt here.
</nextsent>
<nextsent>as point of comparison, we apply the iterative backoff model from swier and stevenson (2004), <papid> W04-3213 </papid>trained on 20% of the bnc, with our frame matcher and test data.</nextsent>
<nextsent>the backoff model achieves an f-measure of .63, slightly below the performance of .65 for our simplest probability model, which uses less training data and takes far less time to run (minutes rather than hours).in general, it is not possible to make direct comparisons between our work and most other role la bellers because of differences in corpora and role sets, and, perhaps more significantly, differences in the selection of target arguments.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1183">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>once this is done, we can expect great progress to be made on natural language generation.
</prevsent>
<prevsent>in this paper, we propose novel automatic evaluation method for natural language generationtechnologies.
</prevsent>
</prevsection>
<citsent citstr=" C04-1077 ">
our method is based on the extended string sub sequence kernel (esk) (hiraoet al, 2004<papid> C04-1077 </papid>b) which is kind of convolution kernel (collins and duffy, 2001).</citsent>
<aftsection>
<nextsent>esk allows us to calculate the similarities between pair of texts taking account of word sequences, their word sense sequences and their combinations.
</nextsent>
<nextsent>we conducted an experimental evaluation using automatic summarization evaluation data developed for tsc-3 (hirao et al, 2004<papid> C04-1077 </papid>a).</nextsent>
<nextsent>the results of the comparison with rouge-n (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b), rouge-s(u) (lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004) <papid> P04-1077 </papid>and rouge-l (lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b) show that our method correlates more closely with human evaluations and is more robust.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1187">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>esk allows us to calculate the similarities between pair of texts taking account of word sequences, their word sense sequences and their combinations.
</prevsent>
<prevsent>we conducted an experimental evaluation using automatic summarization evaluation data developed for tsc-3 (hirao et al, 2004<papid> C04-1077 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
the results of the comparison with rouge-n (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b), rouge-s(u) (lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004) <papid> P04-1077 </papid>and rouge-l (lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b) show that our method correlates more closely with human evaluations and is more robust.</citsent>
<aftsection>
<nextsent>automatic evaluation methods for automatic summarization and machine translation are grouped intotwo classes.
</nextsent>
<nextsent>one is the longest common subse quence (lcs) based approach (hori et al, 2003; lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004).<papid> P04-1077 </papid></nextsent>
<nextsent>the other is the n-gram based approach (papineni et al, 145 table 1: components of vectors corresponding to s1 and s2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1189">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>esk allows us to calculate the similarities between pair of texts taking account of word sequences, their word sense sequences and their combinations.
</prevsent>
<prevsent>we conducted an experimental evaluation using automatic summarization evaluation data developed for tsc-3 (hirao et al, 2004<papid> C04-1077 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" W04-1013 ">
the results of the comparison with rouge-n (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b), rouge-s(u) (lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004) <papid> P04-1077 </papid>and rouge-l (lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b) show that our method correlates more closely with human evaluations and is more robust.</citsent>
<aftsection>
<nextsent>automatic evaluation methods for automatic summarization and machine translation are grouped intotwo classes.
</nextsent>
<nextsent>one is the longest common subse quence (lcs) based approach (hori et al, 2003; lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004).<papid> P04-1077 </papid></nextsent>
<nextsent>the other is the n-gram based approach (papineni et al, 145 table 1: components of vectors corresponding to s1 and s2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1231">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>esk allows us to calculate the similarities between pair of texts taking account of word sequences, their word sense sequences and their combinations.
</prevsent>
<prevsent>we conducted an experimental evaluation using automatic summarization evaluation data developed for tsc-3 (hirao et al, 2004<papid> C04-1077 </papid>a).</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
the results of the comparison with rouge-n (lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b), rouge-s(u) (lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004) <papid> P04-1077 </papid>and rouge-l (lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b) show that our method correlates more closely with human evaluations and is more robust.</citsent>
<aftsection>
<nextsent>automatic evaluation methods for automatic summarization and machine translation are grouped intotwo classes.
</nextsent>
<nextsent>one is the longest common subse quence (lcs) based approach (hori et al, 2003; lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b; lin and och, 2004).<papid> P04-1077 </papid></nextsent>
<nextsent>the other is the n-gram based approach (papineni et al, 145 table 1: components of vectors corresponding to s1 and s2.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1319">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>my-great 1 0 becoming-great
</prevsent>
<prevsent> 0 an-my 0
</prevsent>
</prevsection>
<citsent citstr=" P04-1078 ">
2002; lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b; soricut and brill, 2004).<papid> P04-1078 </papid>hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy.</citsent>
<aftsection>
<nextsent>they reported that their method is superior to bleu (papineni et al, 2002)<papid> P02-1040 </papid>in terms of the correlation between human assessment and automatic evaluation.</nextsent>
<nextsent>lin (2004<papid> W04-1013 </papid>a), lin (2004<papid> W04-1013 </papid>b)and lin and och (2004) <papid> P04-1077 </papid>proposed an lcs-based automatic evaluation measure called rouge-l.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1320">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent> 0 an-my 0
</prevsent>
<prevsent> 2002; lin and hovy, 2003; <papid> N03-1020 </papid>lin, 2004<papid> W04-1013 </papid>a; lin, 2004<papid> W04-1013 </papid>b; soricut and brill, 2004).<papid> P04-1078 </papid>hori et. al (2003) proposed an automatic evaluation method for speech summarization based on word recognition accuracy.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
they reported that their method is superior to bleu (papineni et al, 2002)<papid> P02-1040 </papid>in terms of the correlation between human assessment and automatic evaluation.</citsent>
<aftsection>
<nextsent>lin (2004<papid> W04-1013 </papid>a), lin (2004<papid> W04-1013 </papid>b)and lin and och (2004) <papid> P04-1077 </papid>proposed an lcs-based automatic evaluation measure called rouge-l.</nextsent>
<nextsent>they applied rouge-l to the evaluation of summarization and machine translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1329">
<title id=" H05-1019.xml">kernel based approach for automatic evaluation of natural language generation technologies application to automatic summarization </title>
<section> experimental evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>the lengths were about 5% and 10% of the total number of characters in the document set, respectively.
</prevsent>
<prevsent>thirty document sets were provided for the official run evaluation.
</prevsent>
</prevsection>
<citsent citstr=" W04-1003 ">
there were ten participant systems; one provided by the tsc organizers as baseline system.the evaluation metric follows ducs see evaluation scheme (harman and over, 2004).<papid> W04-1003 </papid></citsent>
<aftsection>
<nextsent>for each document set, one human subject makes reference summary and uses it as basis for evaluating ten system outputs.
</nextsent>
<nextsent>this human evaluation procedure consists of the following steps: step 1 for each reference sentence tufifl????$ , repeat steps 2 and 3.
</nextsent>
<nextsent>step 2 for ?t , the human assessor finds the most relevant sentence set ? from the system output.
</nextsent>
<nextsent>step 3 the assessor assigns score, vfint  ? $ , ? ?%?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1403">
<title id=" E95-1031.xml">a robust parser based on syntactic information </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, if we try to preserve robustness by adding such rules whenever we encounter an extra- grammatical sentence, the rule base will grow up rapidly, and thus processing and maintaining the excessive number of rules will become inefficient and impractical.
</prevsent>
<prevsent>therefore, extra grammatical sentences hould be handled by some recovery mechanism(s) rather than by set of additional rules.
</prevsent>
</prevsection>
<citsent citstr=" J81-4002 ">
many researchers have attempted several tech-niques to deal with extra grammatical sentences such as augmented transition network(atn) (kwasny and sondheimer, 1981), network-based semantic grammar (hendrix, 1977), partial pat-tern matching (hayes and mouradian, 1981), <papid> J81-4002 </papid>con-ceptual case frame (schank et al, 1980), <papid> J80-1002 </papid>and mul-tiple cooperating methods (hayes and carbonell, 1981).</citsent>
<aftsection>
<nextsent>abovementioned techniques take into ac-count various emantic factors depending on spe-cific domains on question in recovering extra gram matical sentences.
</nextsent>
<nextsent>w\]lereas they can provide ven better solutions intrinsically, they are usually ad- hoc and are lack of extensibility.
</nextsent>
<nextsent>therefore, it is 223 important recover extra grammatical sentences using syntactic factors only, which are indepen-dent of any particular system and any particular domain.
</nextsent>
<nextsent>mellish (mellish, 1989) <papid> P89-1013 </papid>introduced some chart- based techniques using only syntactic information for extra grammatical sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1404">
<title id=" E95-1031.xml">a robust parser based on syntactic information </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>how- ever, if we try to preserve robustness by adding such rules whenever we encounter an extra- grammatical sentence, the rule base will grow up rapidly, and thus processing and maintaining the excessive number of rules will become inefficient and impractical.
</prevsent>
<prevsent>therefore, extra grammatical sentences hould be handled by some recovery mechanism(s) rather than by set of additional rules.
</prevsent>
</prevsection>
<citsent citstr=" J80-1002 ">
many researchers have attempted several tech-niques to deal with extra grammatical sentences such as augmented transition network(atn) (kwasny and sondheimer, 1981), network-based semantic grammar (hendrix, 1977), partial pat-tern matching (hayes and mouradian, 1981), <papid> J81-4002 </papid>con-ceptual case frame (schank et al, 1980), <papid> J80-1002 </papid>and mul-tiple cooperating methods (hayes and carbonell, 1981).</citsent>
<aftsection>
<nextsent>abovementioned techniques take into ac-count various emantic factors depending on spe-cific domains on question in recovering extra gram matical sentences.
</nextsent>
<nextsent>w\]lereas they can provide ven better solutions intrinsically, they are usually ad- hoc and are lack of extensibility.
</nextsent>
<nextsent>therefore, it is 223 important recover extra grammatical sentences using syntactic factors only, which are indepen-dent of any particular system and any particular domain.
</nextsent>
<nextsent>mellish (mellish, 1989) <papid> P89-1013 </papid>introduced some chart- based techniques using only syntactic information for extra grammatical sentences.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1405">
<title id=" E95-1031.xml">a robust parser based on syntactic information </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>w\]lereas they can provide ven better solutions intrinsically, they are usually ad- hoc and are lack of extensibility.
</prevsent>
<prevsent>therefore, it is 223 important recover extra grammatical sentences using syntactic factors only, which are indepen-dent of any particular system and any particular domain.
</prevsent>
</prevsection>
<citsent citstr=" P89-1013 ">
mellish (mellish, 1989) <papid> P89-1013 </papid>introduced some chart- based techniques using only syntactic information for extra grammatical sentences.</citsent>
<aftsection>
<nextsent>this technique has an advantage that there is no repeating work for the chart to prevent he parser from generat-ing the same edge as the previously existed edge.
</nextsent>
<nextsent>also, because the recovery process runs when normal parser terminates unsuccessfully, the per-formance of the normal parser does not decrease in case of handling grammatical sentences.
</nextsent>
<nextsent>however, his experiment was not based on the errors in run-ning texts but on artificial ones which were ran-domly generated by human.
</nextsent>
<nextsent>moreover, only one word error was considered though several word er-rors can occur simultaneously in the running text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1406">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in reranking methods, baseline model is used to generate set of candidate output structures for each input intraining or test data.
</prevsent>
<prevsent>a second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
reranking approaches have given improvements inaccuracy on number of nlp problems including parsing (collins, 2000; charniak and johnson, 2005), <papid> P05-1022 </papid>machine translation(och and ney, 2002; <papid> P02-1038 </papid>shen et al, 2004), <papid> N04-1023 </papid>information extraction (collins, 2002), <papid> P02-1062 </papid>and natural language generation (walker et al, 2001).<papid> N01-1003 </papid></citsent>
<aftsection>
<nextsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</nextsent>
<nextsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</nextsent>
<nextsent>previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</nextsent>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1407">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in reranking methods, baseline model is used to generate set of candidate output structures for each input intraining or test data.
</prevsent>
<prevsent>a second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
reranking approaches have given improvements inaccuracy on number of nlp problems including parsing (collins, 2000; charniak and johnson, 2005), <papid> P05-1022 </papid>machine translation(och and ney, 2002; <papid> P02-1038 </papid>shen et al, 2004), <papid> N04-1023 </papid>information extraction (collins, 2002), <papid> P02-1062 </papid>and natural language generation (walker et al, 2001).<papid> N01-1003 </papid></citsent>
<aftsection>
<nextsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</nextsent>
<nextsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</nextsent>
<nextsent>previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</nextsent>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1408">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in reranking methods, baseline model is used to generate set of candidate output structures for each input intraining or test data.
</prevsent>
<prevsent>a second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline.
</prevsent>
</prevsection>
<citsent citstr=" N04-1023 ">
reranking approaches have given improvements inaccuracy on number of nlp problems including parsing (collins, 2000; charniak and johnson, 2005), <papid> P05-1022 </papid>machine translation(och and ney, 2002; <papid> P02-1038 </papid>shen et al, 2004), <papid> N04-1023 </papid>information extraction (collins, 2002), <papid> P02-1062 </papid>and natural language generation (walker et al, 2001).<papid> N01-1003 </papid></citsent>
<aftsection>
<nextsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</nextsent>
<nextsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</nextsent>
<nextsent>previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</nextsent>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1410">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in reranking methods, baseline model is used to generate set of candidate output structures for each input intraining or test data.
</prevsent>
<prevsent>a second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline.
</prevsent>
</prevsection>
<citsent citstr=" P02-1062 ">
reranking approaches have given improvements inaccuracy on number of nlp problems including parsing (collins, 2000; charniak and johnson, 2005), <papid> P05-1022 </papid>machine translation(och and ney, 2002; <papid> P02-1038 </papid>shen et al, 2004), <papid> N04-1023 </papid>information extraction (collins, 2002), <papid> P02-1062 </papid>and natural language generation (walker et al, 2001).<papid> N01-1003 </papid></citsent>
<aftsection>
<nextsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</nextsent>
<nextsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</nextsent>
<nextsent>previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</nextsent>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1411">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in reranking methods, baseline model is used to generate set of candidate output structures for each input intraining or test data.
</prevsent>
<prevsent>a second model, which typically makes use of more complex features than the baseline model, is then used to rerank the candidates proposed by the baseline.
</prevsent>
</prevsection>
<citsent citstr=" N01-1003 ">
reranking approaches have given improvements inaccuracy on number of nlp problems including parsing (collins, 2000; charniak and johnson, 2005), <papid> P05-1022 </papid>machine translation(och and ney, 2002; <papid> P02-1038 </papid>shen et al, 2004), <papid> N04-1023 </papid>information extraction (collins, 2002), <papid> P02-1062 </papid>and natural language generation (walker et al, 2001).<papid> N01-1003 </papid></citsent>
<aftsection>
<nextsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</nextsent>
<nextsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</nextsent>
<nextsent>previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</nextsent>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1413">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the success of reranking approaches depends critically on the choice of representation used by the reranking model.
</prevsent>
<prevsent>typically, each candidate structure (e.g., each parse tree in the case of parsing) is mapped to feature vector representation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1034 ">
previous work has generally relied on two approaches to rep resentation: explicitly handcrafted features (e.g., in charniak and johnson (2005)) <papid> P05-1022 </papid>or features defined through kernels (e.g., see collins and duffy (2002)).<papid> P02-1034 </papid>this paper describes new method for the representation of nlp structures within reranking ap proaches.</citsent>
<aftsection>
<nextsent>we build on the intuition that lexical items in natural language often fall into word clusters (for example, president and chairman might belong to the same cluster) or fall into distinct word senses (e.g., bank might have two distinct senses).
</nextsent>
<nextsent>our method involves hidden variable model, where the hidden variables correspond to an assignment of words to either clusters or wordsenses.
</nextsent>
<nextsent>lexical items are automatically assigned their hidden values using unsupervised learning within discriminative reranking approach.
</nextsent>
<nextsent>we make use of conditional loglinear model for our task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1414">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we discuss this point further in section 6.1.
</prevsent>
<prevsent>various machine learning methods have been used within reranking tasks, including conditional log?
</prevsent>
</prevsection>
<citsent citstr=" P99-1069 ">
linear models (ratnaparkhi et al, 1994; johnson etal., 1999), <papid> P99-1069 </papid>boosting methods (collins, 2000), variants of the perceptron algorithm (collins, 2002; <papid> P02-1062 </papid>shen et al, 2004), <papid> N04-1023 </papid>and generalizations of support?</citsent>
<aftsection>
<nextsent>vector machines (shen and joshi, 2003).<papid> W03-0402 </papid></nextsent>
<nextsent>there have been several previous approaches to parsing using loglinear models and hidden variables.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1418">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>various machine learning methods have been used within reranking tasks, including conditional log?
</prevsent>
<prevsent>linear models (ratnaparkhi et al, 1994; johnson etal., 1999), <papid> P99-1069 </papid>boosting methods (collins, 2000), variants of the perceptron algorithm (collins, 2002; <papid> P02-1062 </papid>shen et al, 2004), <papid> N04-1023 </papid>and generalizations of support?</prevsent>
</prevsection>
<citsent citstr=" W03-0402 ">
vector machines (shen and joshi, 2003).<papid> W03-0402 </papid></citsent>
<aftsection>
<nextsent>there have been several previous approaches to parsing using loglinear models and hidden variables.
</nextsent>
<nextsent>riezler et al (2002) <papid> P02-1035 </papid>describe discriminative lfg parsing model that is trained on standard (syntax only) treebank annotations by treating each tree as fulllfg analysis with an observed c-structure and hidden -structure.</nextsent>
<nextsent>clark and curran (2004) <papid> P04-1014 </papid>present an alternative ccg parsing approach that divides each ccg parse into dependency structure (observed) and derivation (hidden).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1419">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>vector machines (shen and joshi, 2003).<papid> W03-0402 </papid></prevsent>
<prevsent>there have been several previous approaches to parsing using loglinear models and hidden variables.</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
riezler et al (2002) <papid> P02-1035 </papid>describe discriminative lfg parsing model that is trained on standard (syntax only) treebank annotations by treating each tree as fulllfg analysis with an observed c-structure and hidden -structure.</citsent>
<aftsection>
<nextsent>clark and curran (2004) <papid> P04-1014 </papid>present an alternative ccg parsing approach that divides each ccg parse into dependency structure (observed) and derivation (hidden).</nextsent>
<nextsent>more recently, matsuzaki et al (2005) <papid> P05-1010 </papid>introduce probabilistic cfg augmented with hidden information at each nonterminal, which gives their model the ability to tailor it self to the task at hand.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1420">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there have been several previous approaches to parsing using loglinear models and hidden variables.
</prevsent>
<prevsent>riezler et al (2002) <papid> P02-1035 </papid>describe discriminative lfg parsing model that is trained on standard (syntax only) treebank annotations by treating each tree as fulllfg analysis with an observed c-structure and hidden -structure.</prevsent>
</prevsection>
<citsent citstr=" P04-1014 ">
clark and curran (2004) <papid> P04-1014 </papid>present an alternative ccg parsing approach that divides each ccg parse into dependency structure (observed) and derivation (hidden).</citsent>
<aftsection>
<nextsent>more recently, matsuzaki et al (2005) <papid> P05-1010 </papid>introduce probabilistic cfg augmented with hidden information at each nonterminal, which gives their model the ability to tailor it self to the task at hand.</nextsent>
<nextsent>the form of our model is closely related to that of quattoni et al (2005), who describe hidden variable model for object recognition in computer vision.the approaches of riezler et al, clark and curran, and matsuzaki et al are similar to our own work in that the hidden variables are exponential in number and must be handled with dynamic?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1421">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>riezler et al (2002) <papid> P02-1035 </papid>describe discriminative lfg parsing model that is trained on standard (syntax only) treebank annotations by treating each tree as fulllfg analysis with an observed c-structure and hidden -structure.</prevsent>
<prevsent>clark and curran (2004) <papid> P04-1014 </papid>present an alternative ccg parsing approach that divides each ccg parse into dependency structure (observed) and derivation (hidden).</prevsent>
</prevsection>
<citsent citstr=" P05-1010 ">
more recently, matsuzaki et al (2005) <papid> P05-1010 </papid>introduce probabilistic cfg augmented with hidden information at each nonterminal, which gives their model the ability to tailor it self to the task at hand.</citsent>
<aftsection>
<nextsent>the form of our model is closely related to that of quattoni et al (2005), who describe hidden variable model for object recognition in computer vision.the approaches of riezler et al, clark and curran, and matsuzaki et al are similar to our own work in that the hidden variables are exponential in number and must be handled with dynamic?
</nextsent>
<nextsent>programming techniques.
</nextsent>
<nextsent>however, they differ from our approach in the definition of the hidden variables (the matsuzaki et al model is the most similar).
</nextsent>
<nextsent>in addition, these three approaches dont use reranking, so their features must be restricted to local scope in order to allow dynamic programming approaches to training.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1422">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, these three approaches dont use reranking, so their features must be restricted to local scope in order to allow dynamic programming approaches to training.
</prevsent>
<prevsent>finally, these approaches use viterbi or other approximations during decoding, something our model can avoid (see section 6.2).in some instantiations, our model effectively clusters words into categories.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
our approach differs from standard word clustering in that the clustering criteria is directly linked to the reranking objective, whereas previous word clustering approaches (e.g. brown et al (1992) <papid> J92-4003 </papid>or pereira et al (1993)) <papid> P93-1024 </papid>have typically leveraged distributional similarity.</citsent>
<aftsection>
<nextsent>in other instantiations, our model establishes word?
</nextsent>
<nextsent>sense distinctions.
</nextsent>
<nextsent>bikel (2000) <papid> W00-1320 </papid>has done previous work on incorporating the wordnet hierarchy into generative parsing model; however, this approach requires data with word sense annotations whereas our model deals with word sense ambiguity through unsupervised discriminative training.</nextsent>
<nextsent>in this section we describe hidden variable model based on conditional loglinear models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1423">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in addition, these three approaches dont use reranking, so their features must be restricted to local scope in order to allow dynamic programming approaches to training.
</prevsent>
<prevsent>finally, these approaches use viterbi or other approximations during decoding, something our model can avoid (see section 6.2).in some instantiations, our model effectively clusters words into categories.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
our approach differs from standard word clustering in that the clustering criteria is directly linked to the reranking objective, whereas previous word clustering approaches (e.g. brown et al (1992) <papid> J92-4003 </papid>or pereira et al (1993)) <papid> P93-1024 </papid>have typically leveraged distributional similarity.</citsent>
<aftsection>
<nextsent>in other instantiations, our model establishes word?
</nextsent>
<nextsent>sense distinctions.
</nextsent>
<nextsent>bikel (2000) <papid> W00-1320 </papid>has done previous work on incorporating the wordnet hierarchy into generative parsing model; however, this approach requires data with word sense annotations whereas our model deals with word sense ambiguity through unsupervised discriminative training.</nextsent>
<nextsent>in this section we describe hidden variable model based on conditional loglinear models.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1424">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in other instantiations, our model establishes word?
</prevsent>
<prevsent>sense distinctions.
</prevsent>
</prevsection>
<citsent citstr=" W00-1320 ">
bikel (2000) <papid> W00-1320 </papid>has done previous work on incorporating the wordnet hierarchy into generative parsing model; however, this approach requires data with word sense annotations whereas our model deals with word sense ambiguity through unsupervised discriminative training.</citsent>
<aftsection>
<nextsent>in this section we describe hidden variable model based on conditional loglinear models.
</nextsent>
<nextsent>each sentence si for = 1 . . .
</nextsent>
<nextsent>n in our training data has set of ni candidate parse trees ti,1, . . .
</nextsent>
<nextsent>, ti,ni , which are the output of an nbest baseline parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1426">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> features for parse reranking.  </section>
<citcontext>
<prevsection>
<prevsent>, pp5}.
</prevsent>
<prevsent>super sense (prebuilt ontology) we borrow the idea of using wordnet lexicographer filenamesas broad supersenses?
</prevsent>
</prevsection>
<citsent citstr=" W03-1022 ">
from ciaramita and johnson (2003).<papid> W03-1022 </papid></citsent>
<aftsection>
<nextsent>for each word, we split each of itssupersenses into three subsupersenses.
</nextsent>
<nextsent>if no su per senses are available, we fall back to splitting the partofspeech into five subvalues.
</nextsent>
<nextsent>forex ample, shares has the super senses noun.possession,noun.act and noun.artifact, which yield the do main {noun.possession1, noun.act1, noun.artifact1, . . .
</nextsent>
<nextsent>noun.possession3, noun.act3, noun.artifact3}.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1428">
<title id=" H05-1064.xml">hidden variable models for discriminative reranking </title>
<section> conclusions and future research.  </section>
<citcontext>
<prevsection>
<prevsent>future work may consider the use of hidden value domains with mixed contents, such as domain that contains 3 refinement oriented lexical values and 3 clustering oriented partofspeech values.
</prevsent>
<prevsent>these mixed values would allow the hidden variable model to exploit interactions between clustering and refinement at the level of words and dependencies.
</prevsent>
</prevsection>
<citsent citstr=" N04-1043 ">
another area for future research is to investigate the use of unlabeled data within the approach, for example by making use of clusters derived from large amounts of unlabeled data (e.g., see miller et al (2004)).<papid> N04-1043 </papid></citsent>
<aftsection>
<nextsent>finally, future work may apply the models to nlp tasks other than parsing.
</nextsent>
<nextsent>acknowledgements we would like to thank regina barzilay, igor malioutov, and luke zettlemoyer for their many comments on the paper.
</nextsent>
<nextsent>we gratefully acknowledge the support of the national science foundation (under grants 0347631 and 0434222) and the darpa/sri calo project (through subcontract no. 03-000215).
</nextsent>

</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1429">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is huge amount of work about developing classification algorithms that have high generalization performance in the machine learning community.
</prevsent>
<prevsent>being able to incorporate such state-of-theart machine learning algorithms is important.
</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
indeed, sequential classification approaches with kernel support vector machines offer competitive performance in pos tagging and chunking (gimenez and marquez, 2003; kudo and matsumoto, 2001).<papid> N01-1025 </papid></citsent>
<aftsection>
<nextsent>one obvious way to improve the performance of sequential classification approaches is to enrich the information that the local classifiers can use.
</nextsent>
<nextsent>in standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the targetword.
</nextsent>
<nextsent>to make use of the information about future tags, toutanova et al proposed tagging algorithm based on bidirectional dependency networks 467(toutanova et al, 2003) <papid> N03-1033 </papid>and achieved the best accuracy on pos tagging on the wall street journal corpus.</nextsent>
<nextsent>as they pointed out in their paper, however,their method potentially suffers from collusion?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1430">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>one obvious way to improve the performance of sequential classification approaches is to enrich the information that the local classifiers can use.
</prevsent>
<prevsent>in standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the targetword.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
to make use of the information about future tags, toutanova et al proposed tagging algorithm based on bidirectional dependency networks 467(toutanova et al, 2003) <papid> N03-1033 </papid>and achieved the best accuracy on pos tagging on the wall street journal corpus.</citsent>
<aftsection>
<nextsent>as they pointed out in their paper, however,their method potentially suffers from collusion?
</nextsent>
<nextsent>effects which make the model lock onto conditionally consistent but jointly unlikely sequences.
</nextsent>
<nextsent>in their modeling, the local classifiers can always use the information about future tags, but that could cause double-counting effect of tag information.
</nextsent>
<nextsent>in this paper we propose an alternative way of making use of future tags.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1435">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> bidirectional inference.  </section>
<citcontext>
<prevsection>
<prevsent>i=1 p(ti|ti1o).
</prevsent>
<prevsent>(3) then we can employ probabilistic classifier trained with the preceding tag and observations in order to obtain p(ti|ti1o) for local classification.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
a common choice for the local probabilistic classifier is maximum entropy classifiers (berger et al, 1996).<papid> J96-1002 </papid></citsent>
<aftsection>
<nextsent>the best tag sequence can be efficiently computed by using viterbi decoding algorithm in polynomial time.
</nextsent>
<nextsent>t1 (a) t2 t3 t1 (b) t2 t3 t1 (c) t2 t3 t1 (d) t2 t3 o figure 1: different structures for decomposition.
</nextsent>
<nextsent>the right-to-left decomposition is (t1...tn|o) = n?
</nextsent>
<nextsent>i=1 p(ti|ti+1o).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1440">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> maximum entropy classifier.  </section>
<citcontext>
<prevsection>
<prevsent>for local classifiers, we used maximum entropy model which is common choice for incorporating various types of features for classification problems in natural language processing (berger et al, 1996).<papid> J96-1002 </papid></prevsent>
<prevsent>regularization is important in maximum entropy modeling to avoid over fitting to the training data.</prevsent>
</prevsection>
<citsent citstr=" W03-1018 ">
for this purpose, we use the maximum entropy modeling with inequality constraints (kazama andtsujii, 2003).<papid> W03-1018 </papid></citsent>
<aftsection>
<nextsent>the model gives equally good performance as the maximum entropy modeling with gaussian priors (chen and rosenfeld, 1999), and the size of the resulting model is much smaller than that of gaussian priors because most of the parameters become zero.
</nextsent>
<nextsent>this characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitively perform experiments.
</nextsent>
<nextsent>this modeling has one parameter to tune, which is called the width factor.
</nextsent>
<nextsent>we tuned this parameter using the development data in each type of experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1441">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>although achieving the best accuracy is not the primary purpose of this paper, we explored useful feature sets and parameter setting by using development data in order to make the experiments realistic.
</prevsent>
<prevsent>4.1 part-of-speech tagging experiments.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we split the penn treebank corpus (marcus et al, 1994) into training, development and test sets as in(collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>sections 0-18 are used as the training set.
</nextsent>
<nextsent>sections 19-21 are the development set, and sections 22-24 are used as the test set.
</nextsent>
<nextsent>all the experiments were carried out on the development set, except for the final accuracy report using the best setting.
</nextsent>
<nextsent>for features, we basically adopted the feature set method accuracy speed (%) (tokens/sec) left-to-right (viterbi) 96.92 844 right-to-left (viterbi) 96.89 902 dependency networks 97.06 1,446 easiest-last 96.58 2,360 easiest-first 97.13 2,461 full bidirectional 97.12 34table 2: pos tagging accuracy and speed on the development set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1450">
<title id=" H05-1059.xml">bidirectional inference with the easiest first strategy for tagging sequence data </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>for features, we basically adopted the feature set method accuracy speed (%) (tokens/sec) left-to-right (viterbi) 96.92 844 right-to-left (viterbi) 96.89 902 dependency networks 97.06 1,446 easiest-last 96.58 2,360 easiest-first 97.13 2,461 full bidirectional 97.12 34table 2: pos tagging accuracy and speed on the development set.
</prevsent>
<prevsent>method accuracy (%) dep.
</prevsent>
</prevsection>
<citsent citstr=" A00-1031 ">
networks (toutanova et al, 2003) <papid> N03-1033 </papid>97.24 perceptron (collins, 2002) <papid> W02-1001 </papid>97.11 svm (gimenez and marquez, 2003) 97.05 hmm (brants, 2000) <papid> A00-1031 </papid>96.48 easiest-first 97.10 full bidirectional 97.15table 3: pos tagging accuracy on the test set (sec tions 22-24 of the wsj, 5462 sentences).provided by (toutanova et al, 2003) <papid> N03-1033 </papid>except for complex features such as crude company-name detection features because they are specific to the penn tree bank and we could not find the exact implementation details.</citsent>
<aftsection>
<nextsent>table 1 lists the feature templates used in our experiments.
</nextsent>
<nextsent>we tested the proposed bidirectional methods,conventional unidirectional methods and the bidirectional dependency network proposed by toutanova (toutanova et al, 2003) <papid> N03-1033 </papid>for comparison.</nextsent>
<nextsent>1.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1471">
<title id=" E95-1012.xml">stochastic hpsg </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>this gives us clear account of the relationship between large class of feature structures and their probabilities, but does not treat re-entrancy.
</prevsent>
<prevsent>we conclude by sketching technique which does treat such struc-tures.
</prevsent>
</prevsection>
<citsent citstr=" P94-1044 ">
while we know of previous work which as-sociates cores with feature structures (kim, 1994) <papid> P94-1044 </papid>are not aware of any previous treatment which makes explicit the link to classical probability the- ory.</citsent>
<aftsection>
<nextsent>we take slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework of incremental description refinement (mellish, 1988) <papid> J88-1004 </papid>than to exploit the usual metaphors of constraint-based grammar.</nextsent>
<nextsent>in fact we can afford to remain entirely agnostic about the means by which the hpsg grammar associates igns with linguistic strings, because all that we need in or-der to train our stochastic procedures is corpus of signs which are known to be valid descriptions of strings.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1472">
<title id=" E95-1012.xml">stochastic hpsg </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>we conclude by sketching technique which does treat such struc-tures.
</prevsent>
<prevsent>while we know of previous work which as-sociates cores with feature structures (kim, 1994) <papid> P94-1044 </papid>are not aware of any previous treatment which makes explicit the link to classical probability the- ory.</prevsent>
</prevsection>
<citsent citstr=" J88-1004 ">
we take slightly unconventional perspective on feature structures, because it is easier to cast our theory within the more general framework of incremental description refinement (mellish, 1988) <papid> J88-1004 </papid>than to exploit the usual metaphors of constraint-based grammar.</citsent>
<aftsection>
<nextsent>in fact we can afford to remain entirely agnostic about the means by which the hpsg grammar associates igns with linguistic strings, because all that we need in or-der to train our stochastic procedures is corpus of signs which are known to be valid descriptions of strings.
</nextsent>
<nextsent>pcfgs we review the standard probabilistic interpreta-tion of pcfgs 1 pcfg is four-tuple   w,n, n1,r   , where is set of terminal symbols {wl, . . .
</nextsent>
<nextsent>, w~}, is set of non-terminal symbols {n1, . . .
</nextsent>
<nextsent>,n~}, n1 is the starting symbol and is set of rules of the form ~ ~ (j, where (j is string of terminals and non-terminals.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1473">
<title id=" E93-1008.xml">dis junctions and inheritance in the context feature structure system </title>
<section> the use of dis junctions and.  </section>
<citcontext>
<prevsection>
<prevsent>this way virtual copies of the structures are produced, and these are unified?
</prevsent>
<prevsent>it is essential for efficiency that virtual copy does not mean that the structure of the type has to be copied.
</prevsent>
</prevsection>
<citsent citstr=" C90-2039 ">
the lazy copying ap-proach (\[kogure, 1990\], <papid> C90-2039 </papid>and \[emele, 1991\] <papid> P91-1042 </papid>for lazy copying in tfs with historical backtracking) copies only overlapping parts of the structure.</citsent>
<aftsection>
<nextsent>cfs avoids even this by structure- and constraint-sharing.
</nextsent>
<nextsent>for common sentences in german, which tend to be rather long, lot of types will be generated?
</nextsent>
<nextsent>they supply only small part of structure themselves (just the path from the functor to the filler and simple slot-filler combination structure).
</nextsent>
<nextsent>the bulk of the 55 i: kinder _i : kinder open/sat 2: spielen ...i _2 : spielen _3 : spielen _2 _4 : spielen _2 open subje kinder _i open/sat trans kinder _i open 3: eine ...2 _s : eine open/sat 4: rolle ...3 _6 : rolle ...2 _7 : rolle _6 _ii: spielen _3 ...1 _14: spielen _2 open/sat refer eine _5 open/sat trans rolle _7 open/sat trans rolle _7 open 5: im ...4 _16: im open 6: theater ? ..5 _17: theater ...4 _18: im _16 caspntheater _17 ...3 _19: rolle _6 caspp im _18 ? ..2 _20: rolle _7 aspp im _18 _21: spielen _11 caspp im _18 ...i _22:spie len_14 caspp im_18 _26: spielen _3 trans rolle _20 ...i _29: spielen _2 trans rolle _20 open/sat open/sat open/sat open/sat open/sat open open/sat open 7: ? ??.6 _30: open _31: ? _30 praed spielen _26 sat _32: . _30 praed spielen _21 sat figure 4: chart for kinder spielen . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1474">
<title id=" E93-1008.xml">dis junctions and inheritance in the context feature structure system </title>
<section> the use of dis junctions and.  </section>
<citcontext>
<prevsection>
<prevsent>this way virtual copies of the structures are produced, and these are unified?
</prevsent>
<prevsent>it is essential for efficiency that virtual copy does not mean that the structure of the type has to be copied.
</prevsent>
</prevsection>
<citsent citstr=" P91-1042 ">
the lazy copying ap-proach (\[kogure, 1990\], <papid> C90-2039 </papid>and \[emele, 1991\] <papid> P91-1042 </papid>for lazy copying in tfs with historical backtracking) copies only overlapping parts of the structure.</citsent>
<aftsection>
<nextsent>cfs avoids even this by structure- and constraint-sharing.
</nextsent>
<nextsent>for common sentences in german, which tend to be rather long, lot of types will be generated?
</nextsent>
<nextsent>they supply only small part of structure themselves (just the path from the functor to the filler and simple slot-filler combination structure).
</nextsent>
<nextsent>the bulk of the 55 i: kinder _i : kinder open/sat 2: spielen ...i _2 : spielen _3 : spielen _2 _4 : spielen _2 open subje kinder _i open/sat trans kinder _i open 3: eine ...2 _s : eine open/sat 4: rolle ...3 _6 : rolle ...2 _7 : rolle _6 _ii: spielen _3 ...1 _14: spielen _2 open/sat refer eine _5 open/sat trans rolle _7 open/sat trans rolle _7 open 5: im ...4 _16: im open 6: theater ? ..5 _17: theater ...4 _18: im _16 caspntheater _17 ...3 _19: rolle _6 caspp im _18 ? ..2 _20: rolle _7 aspp im _18 _21: spielen _11 caspp im _18 ...i _22:spie len_14 caspp im_18 _26: spielen _3 trans rolle _20 ...i _29: spielen _2 trans rolle _20 open/sat open/sat open/sat open/sat open/sat open open/sat open 7: ? ??.6 _30: open _31: ? _30 praed spielen _26 sat _32: . _30 praed spielen _21 sat figure 4: chart for kinder spielen . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1475">
<title id=" E95-1024.xml">offline optimization for earley style hpsg processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>ex-tensive testing with large hpsg grammar revealed some important constraints on the form of the grammar.
</prevsent>
<prevsent>bidirectionality of grammar is research topic in natural anguage processing that is enjoying increas-ing attention (strzalkowski, 1993a).
</prevsent>
</prevsection>
<citsent citstr=" T87-1042 ">
this is mainly due to the clear theoretical and practical advantages of bidirectional grammar use (see, among others, appelt, 1987).<papid> T87-1042 </papid></citsent>
<aftsection>
<nextsent>we address this topic in describing novel approach to hpsg (pollard and sag, 1994) based language processing that uses an off-line com-piler to automatically prime declarative grammar for generation or parsing, and hands the primed grammar to an advanced earley processor.
</nextsent>
<nextsent>the de-veloped techniques are direction independent in the sense that they can be used for both generation and parsing with hpsg grammars.
</nextsent>
<nextsent>in this paper, we fo-cus on the application of the developed techniques in the context of the comparatively neglected area of hpsg generation.
</nextsent>
<nextsent>shieber (1988) <papid> C88-2128 </papid>gave the first use of earley al-gorithm for generation, but this algorithm does not *the presented research was sponsored by  l~eilpro - jekt b4  constraints on grammar for efficient genera-tion  of the sonderforschungsbereich 340  sprachtheo- retische grundlagen fiir die computerllnguistik  of the deutsche forschungsgemeinschaft.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1476">
<title id=" E95-1024.xml">offline optimization for earley style hpsg processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the de-veloped techniques are direction independent in the sense that they can be used for both generation and parsing with hpsg grammars.
</prevsent>
<prevsent>in this paper, we fo-cus on the application of the developed techniques in the context of the comparatively neglected area of hpsg generation.
</prevsent>
</prevsection>
<citsent citstr=" C88-2128 ">
shieber (1988) <papid> C88-2128 </papid>gave the first use of earley al-gorithm for generation, but this algorithm does not *the presented research was sponsored by  l~eilpro - jekt b4  constraints on grammar for efficient genera-tion  of the sonderforschungsbereich 340  sprachtheo- retische grundlagen fiir die computerllnguistik  of the deutsche forschungsgemeinschaft.</citsent>
<aftsection>
<nextsent>the authors wish to thank paul king, detmar meurers and shuly wintner for valuable comments and discussion.
</nextsent>
<nextsent>of course, the authors are responsible for all remaining errors.
</nextsent>
<nextsent>use the prediction step to restrict feature instantia- tions on the predicted phrases, and thus lacks goal- directedness.
</nextsent>
<nextsent>though gerdemann (1991) showed how to modify the restriction function to make top- down information available for the bottom-up com-pletion step, earley generation with top-down pre-diction still has problem in that generating the sub-parts of construction the wrong order might lead to massive nondeterminacy or even nontermination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1478">
<title id=" E95-1024.xml">offline optimization for earley style hpsg processing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the direct inversion approach (di,~) of minnen et al (1995) overcomes these problems by making the reordering process more goal-directed and developing reformulation technique that al-lows the successful treatment of rules which exhibit head-recursion.
</prevsent>
<prevsent>both the eaa and the dia were 173 presented as approaches to the inversion of parser- oriented grammars into grammars suitable for gen-eration.
</prevsent>
</prevsection>
<citsent citstr=" C90-3017 ">
however, both approaches can just as well take declarative grammar specification as input to produce generator and/or parser-oriented grammars as in dymetman et al (1990).<papid> C90-3017 </papid></citsent>
<aftsection>
<nextsent>in this paper we adopt the latter theoretically more interesting per-spective.
</nextsent>
<nextsent>we developed compiler for off-line optimization of phrase structure rule-based typed feature struc-ture grammars which generalizes the techniques de-veloped in the context of the dia, and we advanced typed extension of the earley-style generator of gerdemann (1991).
</nextsent>
<nextsent>off-line compilation (section 3) is used to produce grammars for the earley-style generator (section 2).
</nextsent>
<nextsent>we show that our use of off- line grammar optimization overcomes problems with empty or displaced heads.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1481">
<title id=" E95-1024.xml">offline optimization for earley style hpsg processing </title>
<section> advanced earley generation.  </section>
<citcontext>
<prevsection>
<prevsent>gerde-mann generator follows head-driven strategy in order to avoid inefficient evaluation orders.
</prevsent>
<prevsent>more specifically, the head of the right-hand side of each grammar ule is distinguished, and distinguished categories are scanned or predicted upon first.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
the resulting evaluation strategy is similar to that of the head-corner approach (shieber et al, 1990; <papid> J90-1004 </papid>gerde-mann and iiinrichs, in press): prediction follows the main flow of semantic information until lex-ical pivot is reached, and only then are the head- dependent subparts of the construction built up in bottom-up fashion.</citsent>
<aftsection>
<nextsent>this mixture of top-down and bottom-up information flow is crucial since the top- down semantic information from the goal category must be integrated with the bottom-up subcatego-rization information from the lexicon.
</nextsent>
<nextsent>a strict top- down evaluation strategy suffers from what may be called head-recursion, i.e. the generation analog of left recur sion in parsing.
</nextsent>
<nextsent>shieber et al (1990) <papid> J90-1004 </papid>show that top-down evaluation strategy will fail for rules such as vp --* vp x, irrespective of the order of eval-uation of the right-hand side categories in the rule.</nextsent>
<nextsent>by combining the off-line optimization process with mixed bottom-up/top-down evaluation strategy, we can refrain from complete reformulation of the grammar as, for example, in minnen et al (1995).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1486">
<title id=" E95-1024.xml">offline optimization for earley style hpsg processing </title>
<section> const ra in ts  on grammar.  </section>
<citcontext>
<prevsection>
<prevsent>our data flow analysis ignores the grammatical head, but identifies instead the  processing head , and (no less 176 importantly) the  first processing complement , the  second processing complement , and so on.
</prevsent>
<prevsent>our earley generator and the described compiler for off-line grammar optimization have been exten-sively tested with large hpsg grammar.
</prevsent>
</prevsection>
<citsent citstr=" C94-2154 ">
this test- grammar is based on the implementation an anal-ysis of partial vp topical ization german (hinrichs et al, 1994) in the troll system (gerdemann and king, 1994).<papid> C94-2154 </papid></citsent>
<aftsection>
<nextsent>testing the developed techniques un-covered important constraints on the form of the phrase structure rules in grammar imposed by the compiler.
</nextsent>
<nextsent>4.1 complement displacement.
</nextsent>
<nextsent>the compiler is not able to find an evaluation or-der such that the earley generator has sufficient re-stricting information to generate all subparts of the construction efficiently in particular cases of comple-ment displacement.
</nextsent>
<nextsent>more specifically, this problem arises when complement receives essential restrict-ing information from the head of the construction from which it has been extracted, while, at the same time, it provides essential restricting information for the complements hat stayed behind.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1487">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper refers to both anaphor and normal pronoun.of binding constraints.
</prevsent>
<prevsent>we will use the theory as guide line to help us design features in machine learning framework.
</prevsent>
</prevsection>
<citsent citstr=" J94-4002 ">
previous pronoun resolution work (hobbs, 1976; lappinand leass, 1994; <papid> J94-4002 </papid>ge et al, 1998; <papid> W98-1119 </papid>stuckardt, 2001) <papid> J01-4002 </papid>explicitly utilized syntactic information before.</citsent>
<aftsection>
<nextsent>but there are unique challenges in this study: (1) syntactic information is extracted from parse trees automatically generated.
</nextsent>
<nextsent>this is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (marcus et al, 1993; <papid> J93-2004 </papid>xia et al, 2000; maamouri and bies, 2004) <papid> W04-1602 </papid>for multiple languages; (2) the binding theory is used as guideline and syntactic structures are encoded as features in maximum entropy coreference system; (3) the syntactic features are evaluated on three languages: arabic, chinese and english (one goal is to see if features motivated by the english language canhelp coreference resolution in other languages).</nextsent>
<nextsent>all contrastive experiments are done on publicly-available data;(4) our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1488">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper refers to both anaphor and normal pronoun.of binding constraints.
</prevsent>
<prevsent>we will use the theory as guide line to help us design features in machine learning framework.
</prevsent>
</prevsection>
<citsent citstr=" W98-1119 ">
previous pronoun resolution work (hobbs, 1976; lappinand leass, 1994; <papid> J94-4002 </papid>ge et al, 1998; <papid> W98-1119 </papid>stuckardt, 2001) <papid> J01-4002 </papid>explicitly utilized syntactic information before.</citsent>
<aftsection>
<nextsent>but there are unique challenges in this study: (1) syntactic information is extracted from parse trees automatically generated.
</nextsent>
<nextsent>this is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (marcus et al, 1993; <papid> J93-2004 </papid>xia et al, 2000; maamouri and bies, 2004) <papid> W04-1602 </papid>for multiple languages; (2) the binding theory is used as guideline and syntactic structures are encoded as features in maximum entropy coreference system; (3) the syntactic features are evaluated on three languages: arabic, chinese and english (one goal is to see if features motivated by the english language canhelp coreference resolution in other languages).</nextsent>
<nextsent>all contrastive experiments are done on publicly-available data;(4) our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1490">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper refers to both anaphor and normal pronoun.of binding constraints.
</prevsent>
<prevsent>we will use the theory as guide line to help us design features in machine learning framework.
</prevsent>
</prevsection>
<citsent citstr=" J01-4002 ">
previous pronoun resolution work (hobbs, 1976; lappinand leass, 1994; <papid> J94-4002 </papid>ge et al, 1998; <papid> W98-1119 </papid>stuckardt, 2001) <papid> J01-4002 </papid>explicitly utilized syntactic information before.</citsent>
<aftsection>
<nextsent>but there are unique challenges in this study: (1) syntactic information is extracted from parse trees automatically generated.
</nextsent>
<nextsent>this is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (marcus et al, 1993; <papid> J93-2004 </papid>xia et al, 2000; maamouri and bies, 2004) <papid> W04-1602 </papid>for multiple languages; (2) the binding theory is used as guideline and syntactic structures are encoded as features in maximum entropy coreference system; (3) the syntactic features are evaluated on three languages: arabic, chinese and english (one goal is to see if features motivated by the english language canhelp coreference resolution in other languages).</nextsent>
<nextsent>all contrastive experiments are done on publicly-available data;(4) our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1492">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous pronoun resolution work (hobbs, 1976; lappinand leass, 1994; <papid> J94-4002 </papid>ge et al, 1998; <papid> W98-1119 </papid>stuckardt, 2001) <papid> J01-4002 </papid>explicitly utilized syntactic information before.</prevsent>
<prevsent>but there are unique challenges in this study: (1) syntactic information is extracted from parse trees automatically generated.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
this is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (marcus et al, 1993; <papid> J93-2004 </papid>xia et al, 2000; maamouri and bies, 2004) <papid> W04-1602 </papid>for multiple languages; (2) the binding theory is used as guideline and syntactic structures are encoded as features in maximum entropy coreference system; (3) the syntactic features are evaluated on three languages: arabic, chinese and english (one goal is to see if features motivated by the english language canhelp coreference resolution in other languages).</citsent>
<aftsection>
<nextsent>all contrastive experiments are done on publicly-available data;(4) our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.
</nextsent>
<nextsent>using machine-generated parse trees eliminates the needof hand-labeled trees in coreference system.
</nextsent>
<nextsent>however, it is major challenge to extract useful information from these noisy parse trees.
</nextsent>
<nextsent>our approach is encoding the structures contained in parse tree into set of computable features, each of which is associated with weight automatically determined by machine learning algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1493">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous pronoun resolution work (hobbs, 1976; lappinand leass, 1994; <papid> J94-4002 </papid>ge et al, 1998; <papid> W98-1119 </papid>stuckardt, 2001) <papid> J01-4002 </papid>explicitly utilized syntactic information before.</prevsent>
<prevsent>but there are unique challenges in this study: (1) syntactic information is extracted from parse trees automatically generated.</prevsent>
</prevsection>
<citsent citstr=" W04-1602 ">
this is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (marcus et al, 1993; <papid> J93-2004 </papid>xia et al, 2000; maamouri and bies, 2004) <papid> W04-1602 </papid>for multiple languages; (2) the binding theory is used as guideline and syntactic structures are encoded as features in maximum entropy coreference system; (3) the syntactic features are evaluated on three languages: arabic, chinese and english (one goal is to see if features motivated by the english language canhelp coreference resolution in other languages).</citsent>
<aftsection>
<nextsent>all contrastive experiments are done on publicly-available data;(4) our coreference system resolves coreferential relationships among all the annotated mentions, not just for pronouns.
</nextsent>
<nextsent>using machine-generated parse trees eliminates the needof hand-labeled trees in coreference system.
</nextsent>
<nextsent>however, it is major challenge to extract useful information from these noisy parse trees.
</nextsent>
<nextsent>our approach is encoding the structures contained in parse tree into set of computable features, each of which is associated with weight automatically determined by machine learning algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1496">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> statistical coreference resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>our coreference system uses binary entity-mention model pl(?|e, m) (henceforth link model?)
</prevsent>
<prevsent>to score the action of linking mention to an entity e. in our implementation, the link model is computed as pl(l = 1|e, m) ? max me pl(l = 1|e, m?, m), (1) where m?
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
is one mention in entity e, and the basic model building block pl(l = 1|e, m?, m) is an exponential or maximum entropy model (berger et al, 1996): <papid> J96-1002 </papid>pl(l|e, m?, m) = exp { ? igi(e, m?, m, l) } z(e, m?, m) , (2) where z(e, m?, m) is normalizing factor to ensure thatpl(?|e, m?, m) is probability, {gi(e, m?, m, l)} are features and {i} are feature weights.</citsent>
<aftsection>
<nextsent>another start model is used to score the action of creating new entity with the current mention m. since starting new entity depends on all the partial entities created in the history {ei}ti=1, we use the following approximation: ps(s = 1|e1, e2, ? ?
</nextsent>
<nextsent>, et, m) ? 1 ? max 1it pl(l = 1|ei, m) (3)in the maximum-entropy model (2), feature (typically binary) functions {gi(e, m?, m, ?)} provide us with flexible framework to encode useful information into the the system: it can be as simple as gi(e, m?, m, = 1) = 1 if m?
</nextsent>
<nextsent>and have the same surface string,?
</nextsent>
<nextsent>orgj(e, m?, m, = 0) = 1 if and differ in num ber,?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1499">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> statistical coreference resolution model.  </section>
<citcontext>
<prevsection>
<prevsent>c-commands and m?
</prevsent>
<prevsent>is name mention and is apronoun mention.?
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
these feature functions bear similarity to rules used in other coreference systems (lappin and leass, 1994; <papid> J94-4002 </papid>mitkov, 1998; <papid> P98-2143 </papid>stuckardt, 2001), <papid> J01-4002 </papid>except that the feature weights {i} are automatically trained over corpus with coreference information.</citsent>
<aftsection>
<nextsent>learning feature weights automatically eliminates the need of manually assigning the weights or precedence of rules, and opens the door for us to explore rich features extracted from parse trees, which is discussed in the next section.
</nextsent>
<nextsent>in this section, we present set of features extracted from syntactic parse trees.
</nextsent>
<nextsent>we discuss how we approximately compute linguistic concepts such as governing category (haegeman, 1994), apposition and dependency relationships from noisy syntactic parse trees.
</nextsent>
<nextsent>while parsing and parse trees depend on the target language, the automatic nature of feature extraction from parse trees makes the process language-independent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1504">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the three parsers are used to parse both ace training and test data.
</prevsent>
<prevsent>features described in section 3 are computed from machine-generated parse trees.apart from features extracted from parse trees, our coreference system also utilizes other features such as lexical features (e.g., string matching), distance features characterized as quant ized word and sentence distances, mention- and entity-level attribute information (e.g, ace distinguishes 4 types of mentions: nam(e), nom(inal), pre(modifier) and pro(noun)) found in the 2004 ace data.
</prevsent>
</prevsection>
<citsent citstr=" P04-1018 ">
details of these features can be found in (luo et al., 2004).<papid> P04-1018 </papid></citsent>
<aftsection>
<nextsent>4.2 performance metrics.
</nextsent>
<nextsent>the official performance metric in the ace task is ace value (nist, 2004).
</nextsent>
<nextsent>the ace-value is an entity-based metric computed by subtracting normalized cost from 1 (so it is unbounded below).
</nextsent>
<nextsent>the cost of system is weighted sum of costs associated with entity misses, false alarms and errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1505">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>the default weights in ace-value emphasize names, and severely discount pronouns: the relative importance of pronoun is two orders of magnitude less than that of aname.
</prevsent>
<prevsent>so the ace-value will not be able to accurately reflect systems improvement on pronouns2.
</prevsent>
</prevsection>
<citsent citstr=" H05-1004 ">
for this reason, we compute an unweighted entity-constrained mention f-measure (luo, 2005) <papid> H05-1004 </papid>and report all contrastive experiments with this metric.</citsent>
<aftsection>
<nextsent>the f-measure is computed by first aligning system and reference entities such that the number of common mentions is maximized and each system entity is constrained to align with atmost one reference entity, and vice versa.
</nextsent>
<nextsent>for example, suppose that reference document contains three entities: {[m1], [m2, m3], [m4]} while system outputs four entities: {[m1, m2], [m3], [m5], [m6]}, where {mi : = 1, 2, ? ?
</nextsent>
<nextsent>, 6} are mentions, then the best alignment from reference to system would be [m1] ? [m1, m2], [m2, m3] ? [m3] and other entities are not aligned.
</nextsent>
<nextsent>the number of common mentions of the best alignment is 2 2another possible choice is the muc f-measure (vilain et al., 1995).<papid> M95-1005 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1506">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>for example, suppose that reference document contains three entities: {[m1], [m2, m3], [m4]} while system outputs four entities: {[m1, m2], [m3], [m5], [m6]}, where {mi : = 1, 2, ? ?
</prevsent>
<prevsent>, 6} are mentions, then the best alignment from reference to system would be [m1] ? [m1, m2], [m2, m3] ? [m3] and other entities are not aligned.
</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
the number of common mentions of the best alignment is 2 2another possible choice is the muc f-measure (vilain et al., 1995).<papid> M95-1005 </papid></citsent>
<aftsection>
<nextsent>but the metric has systematic bias for systems generating fewer entities (bagga and baldwin, 1998) ? see luo (2005).<papid> H05-1004 </papid></nextsent>
<nextsent>another reason is that it cannot score single-mention entity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1512">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>but the metric has systematic bias for systems generating fewer entities (bagga and baldwin, 1998) ? see luo (2005).<papid> H05-1004 </papid></prevsent>
<prevsent>another reason is that it cannot score single-mention entity.</prevsent>
</prevsection>
<citsent citstr=" N04-1037 ">
663 (i.e., m1 and m3), thus the recall is 24 and precision is 2 5 . due to the one-to-one entity alignment constraint, thef-measure here is more stringent than the accuracy (ge et al, 1998; <papid> W98-1119 </papid>mitkov, 1998; <papid> P98-2143 </papid>kehler et al, 2004) <papid> N04-1037 </papid>computed on antecedent-pronoun pairs.</citsent>
<aftsection>
<nextsent>4.3 effect of syntactic features.
</nextsent>
<nextsent>we first present the contrastive experimental results on the devtest described in sub-section 4.1.
</nextsent>
<nextsent>two coreference systems are trained for each language:a baseline without syntactic features, and system including the syntactic features.
</nextsent>
<nextsent>the entity-constrained measures with mention-type breakdown are presented in table 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1515">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>many researchers have used the syntactic information in their coreference system before.
</prevsent>
<prevsent>for example, hobbs (1976) uses set of rules that are applied to parse trees to determine the antecedent of pronoun.
</prevsent>
</prevsection>
<citsent citstr=" H93-1025 ">
the rule precedence is determined heuristic ally and no weight is used.lappin and leass (1994) <papid> J94-4002 </papid>extracted rules from the out put of the english slot grammar (esg) (mccord, 1993).<papid> H93-1025 </papid>rule weights are assigned manually and the system resolves the third person pronouns and reflexive pronounsonly.</citsent>
<aftsection>
<nextsent>ge et al (1998) <papid> W98-1119 </papid>uses non-parametrized statistical model to find the antecedent from list of candidates generated by applying the hobbs algorithm to the english penn treebank.</nextsent>
<nextsent>kehler et al (2004) <papid> N04-1037 </papid>experiments making use of predicate-argument structure extracted from large tdt-corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1523">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in maximum-entropy coreference system, while (ge et al, 1998) <papid> W98-1119 </papid>assumes that correct parse trees are given.</prevsent>
<prevsent>feature weights are automatically trained in our system while (lappin and leass, 1994; <papid> J94-4002 </papid>stuckardt, 2001) <papid> J01-4002 </papid>assign weights manually.</prevsent>
</prevsection>
<citsent citstr=" P00-1023 ">
there are large amount of published work (morton, 2000; <papid> P00-1023 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al., 2003; <papid> P03-1023 </papid>luo et al, 2004; <papid> P04-1018 </papid>kehler et al, 2004) <papid> N04-1037 </papid>using machine-learning techniques in coreference resolution.but none of these work tried to compute complex linguistic concept such as governing category 3 . our work demonstrates how relevant linguistic knowledge can be derived automatically from system-generated parse trees and encoded into computable and trainable features in machine-learning framework.</citsent>
<aftsection>
<nextsent>in this paper, linguistic knowledge is used to guide us to design features in maximum-entropy-based coreference resolution systems.
</nextsent>
<nextsent>in particular, we show how to compute set of features to approximate the linguistic notions such as governing category and apposition, and how to compute the dependency features using syntactic parsetrees.
</nextsent>
<nextsent>while the features are motivated by examining english data, we see significant improvements on both english and arabic systems.
</nextsent>
<nextsent>due to the language idiosyncrasy (e.g., pro-drops), we do not see the syntactic features change the chinese system significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1524">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in maximum-entropy coreference system, while (ge et al, 1998) <papid> W98-1119 </papid>assumes that correct parse trees are given.</prevsent>
<prevsent>feature weights are automatically trained in our system while (lappin and leass, 1994; <papid> J94-4002 </papid>stuckardt, 2001) <papid> J01-4002 </papid>assign weights manually.</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
there are large amount of published work (morton, 2000; <papid> P00-1023 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al., 2003; <papid> P03-1023 </papid>luo et al, 2004; <papid> P04-1018 </papid>kehler et al, 2004) <papid> N04-1037 </papid>using machine-learning techniques in coreference resolution.but none of these work tried to compute complex linguistic concept such as governing category 3 . our work demonstrates how relevant linguistic knowledge can be derived automatically from system-generated parse trees and encoded into computable and trainable features in machine-learning framework.</citsent>
<aftsection>
<nextsent>in this paper, linguistic knowledge is used to guide us to design features in maximum-entropy-based coreference resolution systems.
</nextsent>
<nextsent>in particular, we show how to compute set of features to approximate the linguistic notions such as governing category and apposition, and how to compute the dependency features using syntactic parsetrees.
</nextsent>
<nextsent>while the features are motivated by examining english data, we see significant improvements on both english and arabic systems.
</nextsent>
<nextsent>due to the language idiosyncrasy (e.g., pro-drops), we do not see the syntactic features change the chinese system significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1525">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in maximum-entropy coreference system, while (ge et al, 1998) <papid> W98-1119 </papid>assumes that correct parse trees are given.</prevsent>
<prevsent>feature weights are automatically trained in our system while (lappin and leass, 1994; <papid> J94-4002 </papid>stuckardt, 2001) <papid> J01-4002 </papid>assign weights manually.</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
there are large amount of published work (morton, 2000; <papid> P00-1023 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al., 2003; <papid> P03-1023 </papid>luo et al, 2004; <papid> P04-1018 </papid>kehler et al, 2004) <papid> N04-1037 </papid>using machine-learning techniques in coreference resolution.but none of these work tried to compute complex linguistic concept such as governing category 3 . our work demonstrates how relevant linguistic knowledge can be derived automatically from system-generated parse trees and encoded into computable and trainable features in machine-learning framework.</citsent>
<aftsection>
<nextsent>in this paper, linguistic knowledge is used to guide us to design features in maximum-entropy-based coreference resolution systems.
</nextsent>
<nextsent>in particular, we show how to compute set of features to approximate the linguistic notions such as governing category and apposition, and how to compute the dependency features using syntactic parsetrees.
</nextsent>
<nextsent>while the features are motivated by examining english data, we see significant improvements on both english and arabic systems.
</nextsent>
<nextsent>due to the language idiosyncrasy (e.g., pro-drops), we do not see the syntactic features change the chinese system significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1526">
<title id=" H05-1083.xml">multilingual coreference resolution with syntactic features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in maximum-entropy coreference system, while (ge et al, 1998) <papid> W98-1119 </papid>assumes that correct parse trees are given.</prevsent>
<prevsent>feature weights are automatically trained in our system while (lappin and leass, 1994; <papid> J94-4002 </papid>stuckardt, 2001) <papid> J01-4002 </papid>assign weights manually.</prevsent>
</prevsection>
<citsent citstr=" P03-1023 ">
there are large amount of published work (morton, 2000; <papid> P00-1023 </papid>soon et al, 2001; <papid> J01-4004 </papid>ng and cardie, 2002; <papid> P02-1014 </papid>yang et al., 2003; <papid> P03-1023 </papid>luo et al, 2004; <papid> P04-1018 </papid>kehler et al, 2004) <papid> N04-1037 </papid>using machine-learning techniques in coreference resolution.but none of these work tried to compute complex linguistic concept such as governing category 3 . our work demonstrates how relevant linguistic knowledge can be derived automatically from system-generated parse trees and encoded into computable and trainable features in machine-learning framework.</citsent>
<aftsection>
<nextsent>in this paper, linguistic knowledge is used to guide us to design features in maximum-entropy-based coreference resolution systems.
</nextsent>
<nextsent>in particular, we show how to compute set of features to approximate the linguistic notions such as governing category and apposition, and how to compute the dependency features using syntactic parsetrees.
</nextsent>
<nextsent>while the features are motivated by examining english data, we see significant improvements on both english and arabic systems.
</nextsent>
<nextsent>due to the language idiosyncrasy (e.g., pro-drops), we do not see the syntactic features change the chinese system significantly.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1531">
<title id=" E93-1043.xml">coping with derivation in a morphological component </title>
<section> inheritance lexica.  </section>
<citcontext>
<prevsection>
<prevsent>the system described in this paper is combi-nation of feature-based hierarchical lexicon and word grammar with an extended two-level morphol-ogy.
</prevsent>
<prevsent>before desribing the system in more detail we will shortly discuss these two strands of research.
</prevsent>
</prevsection>
<citsent citstr=" J92-2004 ">
research directed at reducing redundancy in the lexi-con has come up with the idea of organizing the infor-mation hierarchically making use of inheritance (see, e.g. \[daelemans et al, 1992; <papid> J92-2004 </papid>russell et al, 1992\]).<papid> J92-3003 </papid></citsent>
<aftsection>
<nextsent>various formalisms up porting inheritance have been proposed that can be classified into two major approaches.
</nextsent>
<nextsent>one uses defaults, i.e., inherited data may be overwritten by more specific ones.
</nextsent>
<nextsent>the de-fault mechanism handles exceptions which are an in-herent phenomenon of the lexicon.
</nextsent>
<nextsent>a well-known formalism following this approach is datr \[evans and gazdar, 1989\].<papid> E89-1009 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1532">
<title id=" E93-1043.xml">coping with derivation in a morphological component </title>
<section> inheritance lexica.  </section>
<citcontext>
<prevsection>
<prevsent>the system described in this paper is combi-nation of feature-based hierarchical lexicon and word grammar with an extended two-level morphol-ogy.
</prevsent>
<prevsent>before desribing the system in more detail we will shortly discuss these two strands of research.
</prevsent>
</prevsection>
<citsent citstr=" J92-3003 ">
research directed at reducing redundancy in the lexi-con has come up with the idea of organizing the infor-mation hierarchically making use of inheritance (see, e.g. \[daelemans et al, 1992; <papid> J92-2004 </papid>russell et al, 1992\]).<papid> J92-3003 </papid></citsent>
<aftsection>
<nextsent>various formalisms up porting inheritance have been proposed that can be classified into two major approaches.
</nextsent>
<nextsent>one uses defaults, i.e., inherited data may be overwritten by more specific ones.
</nextsent>
<nextsent>the de-fault mechanism handles exceptions which are an in-herent phenomenon of the lexicon.
</nextsent>
<nextsent>a well-known formalism following this approach is datr \[evans and gazdar, 1989\].<papid> E89-1009 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1533">
<title id=" E93-1043.xml">coping with derivation in a morphological component </title>
<section> inheritance lexica.  </section>
<citcontext>
<prevsection>
<prevsent>one uses defaults, i.e., inherited data may be overwritten by more specific ones.
</prevsent>
<prevsent>the de-fault mechanism handles exceptions which are an in-herent phenomenon of the lexicon.
</prevsent>
</prevsection>
<citsent citstr=" E89-1009 ">
a well-known formalism following this approach is datr \[evans and gazdar, 1989\].<papid> E89-1009 </papid></citsent>
<aftsection>
<nextsent>the major advantage of defaults is the rather nat-ural hierarchy formation it supports where classes can be organized in tree instead of multiple- inheritance hierarchy.
</nextsent>
<nextsent>drawbacks are that defaults are computationally costly and one needs an inter-face to the sentence grammar which is usually writ-ten in default-free feature descriptions.
</nextsent>
<nextsent>although the term default is taken from knowledge representation should be aware of the quite dif-ferent usage.
</nextsent>
<nextsent>in knowledge representation defaults are used to describe uncertain facts which may or may not become xplicitly known later on.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1534">
<title id=" E93-1043.xml">coping with derivation in a morphological component </title>
<section> two-level morphology.  </section>
<citcontext>
<prevsection>
<prevsent>this way morphophonology can be treated in principled way while retaining the advantages of hierarchical lexica.
</prevsent>
<prevsent>for dealing with compositional syntax and seman-tics of derivatives one needs component that is capable of constructing arbitrary words from fi-nite set of morphs according to morpho tactic rules.
</prevsent>
</prevsection>
<citsent citstr=" P84-1038 ">
very successful in the domain of morphological nal- ysis/generation are finite-state approaches, notably two-level morphology \[koskenniemi, 1984\].<papid> P84-1038 </papid></citsent>
<aftsection>
<nextsent>two- level morphology deals with two aspects of word for- mation: morpho tact cs : the combination rules that gov-ern which morphs may be combined in what or-der to produce morphologically correct words.
</nextsent>
<nextsent>morphophono logy : phonological alterations oc-curing in the process of combination.
</nextsent>
<nextsent>morpho tactics dealt with by so-called continua-tion lexicon.
</nextsent>
<nextsent>in expressiveness that is equivalent to finite state automaton consuming morphs.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1536">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> data-oriented parsing.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" C92-3126 ">
a data-oriented parsing model (scha, 1990; bod, 1992, <papid> C92-3126 </papid>1993a) is characterized by corpus of analyzed language utterances, together with set of operations that combine sub-analyses from the corpus into new analyses.</citsent>
<aftsection>
<nextsent>we will limit ourselves in this paper to corpora with purely syntactic annotations.
</nextsent>
<nextsent>for the semantic dimension of dop, the reader is referred to (van den berg et al, 1994).
</nextsent>
<nextsent>consider the imaginary example corpus consisting of only two trees in figure 1.
</nextsent>
<nextsent>we will assume one operation for combining.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1537">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> dop as stochastic tree-.  </section>
<citcontext>
<prevsection>
<prevsent>subst tut ion grammar in order to deal with the problem of computing the most probable parse tree of string, it is convenient to describe dop as  stochastic tree-substitution grammar  (stsg).
</prevsent>
<prevsent>stsg can be seen as generalization over dop, where the elementary tree of stsg are the subtrees of dop, and the probabilities of the elementary trees are the 1very small frequencies are smoothed by good-turing.
</prevsent>
</prevsection>
<citsent citstr=" E93-1006 ">
substitution-probabilities of the corresponding subtrees ofdop (bod, 1993<papid> E93-1006 </papid>c).</citsent>
<aftsection>
<nextsent>a stochastic tree-substitution grammar is five- tuple   vn, vt-, s, r,  where vu is finite set of nonterminal symbols.
</nextsent>
<nextsent>vr is finite set of terminal symbols.
</nextsent>
<nextsent>s ~ vn is the distinguished symbol.
</nextsent>
<nextsent>r is finite set of elementary trees whose top nodes and interior nodes are labeled by nonterminal symbols and whose yield nodes are labeled by terminal or nonterminal symbols.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1538">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> dop as stochastic tree-.  </section>
<citcontext>
<prevsection>
<prevsent>we hypothesize that monte carlo disambiguation is also relevant for other stochastic grammars.
</prevsent>
<prevsent>it turns out that all stochastic extensions of cfgs that are stochastic ally richer than scfg need exponential time algorithms for finding most probable parse tree (cf.
</prevsent>
</prevsection>
<citsent citstr=" P92-1006 ">
briscoe &amp; carroll, 1992; black et al, 1993; magerman &amp; weir, 1992; <papid> P92-1006 </papid>schabes &amp; waters, 1993).</citsent>
<aftsection>
<nextsent>to our knowledge, it has never been studied whether there exist bpp-algorithms for these models.
</nextsent>
<nextsent>alhough it is beyond the scope of our research, we conjecture that there exists monte carlo disambiguation algorithm for at least stochastic tree-adjoining grammar (schabes, 1992).<papid> C92-2066 </papid></nextsent>
<nextsent>in our opinion, this should not be seen as disadvantage.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1539">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> dop as stochastic tree-.  </section>
<citcontext>
<prevsection>
<prevsent>briscoe &amp; carroll, 1992; black et al, 1993; magerman &amp; weir, 1992; <papid> P92-1006 </papid>schabes &amp; waters, 1993).</prevsent>
<prevsent>to our knowledge, it has never been studied whether there exist bpp-algorithms for these models.</prevsent>
</prevsection>
<citsent citstr=" C92-2066 ">
alhough it is beyond the scope of our research, we conjecture that there exists monte carlo disambiguation algorithm for at least stochastic tree-adjoining grammar (schabes, 1992).<papid> C92-2066 </papid></citsent>
<aftsection>
<nextsent>in our opinion, this should not be seen as disadvantage.
</nextsent>
<nextsent>in fact, absolute confidence about the most probable parse does not have any significance, as the probability assigned to p~se is already an estimation of its actual probability.
</nextsent>
<nextsent>one may ask as to whether monte carlo is appropriate for modeling 108 human sentence perception.
</nextsent>
<nextsent>the following lists some properties of monte carlo disambiguation that may be of psychological interest: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1540">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> dop as stochastic tree-.  </section>
<citcontext>
<prevsection>
<prevsent>in (bod, 1995) it is shown how dop is extended to parse word strings that possibly contain unknown words.
</prevsent>
<prevsent>4.1 the test environment.
</prevsent>
</prevsection>
<citsent citstr=" H90-1021 ">
for our experiments, we used manually corrected version of the air travel information system (atis) spoken language corpus (hemphill et al, 1990) <papid> H90-1021 </papid>annotated in the pennsylvania treebank (marcus et al., 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>we employed the  blind testing  method, dividing the corpus into 90% training set and 10% test set by randomly selecting sentences.
</nextsent>
<nextsent>the 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4 105 different subtrees.
</nextsent>
<nextsent>the 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set.
</nextsent>
<nextsent>as motivated in (bed, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1541">
<title id=" E95-1015.xml">the problem of computing the most probable tree in data oriented parsing and stochastic tree grammars </title>
<section> dop as stochastic tree-.  </section>
<citcontext>
<prevsection>
<prevsent>in (bod, 1995) it is shown how dop is extended to parse word strings that possibly contain unknown words.
</prevsent>
<prevsent>4.1 the test environment.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
for our experiments, we used manually corrected version of the air travel information system (atis) spoken language corpus (hemphill et al, 1990) <papid> H90-1021 </papid>annotated in the pennsylvania treebank (marcus et al., 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>we employed the  blind testing  method, dividing the corpus into 90% training set and 10% test set by randomly selecting sentences.
</nextsent>
<nextsent>the 675 trees from the training set were converted into their subtrees together with their relative frequencies, yielding roughly 4 105 different subtrees.
</nextsent>
<nextsent>the 75 part-of-speech sequences from the test set served as input strings that were parsed and disambiguated using the subtrees from the training set.
</nextsent>
<nextsent>as motivated in (bed, 1993b), we use the notion of parse accuracy as our accuracy metric, defined as the percentage of the test strings for which the most probable parse is identical to the parse in the test set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1542">
<title id=" E99-1022.xml">selective magic hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>magic is compilation technique originally de-veloped for goal-directed bottom-up rocessing of logic programs.
</prevsent>
<prevsent>see, among others, (ramakrish- nan et al 1992).
</prevsent>
</prevsection>
<citsent citstr=" P96-1033 ">
as shown in (minnen, 1996) *<papid> P96-1033 </papid>the presented research was carried out at the uni-versity of tfibingen, germany, as part of the sonder- forschungsbereich 340.</citsent>
<aftsection>
<nextsent>1a more detailed iscussion of various aspects of the proposed parser can be found in (minnen, 1998).
</nextsent>
<nextsent>magic is an interesting technique with respect natural language processing as it incorporates fil-tering into the logic underlying the grammar and enables elegant control independent filtering im-provements.
</nextsent>
<nextsent>in this paper we investigate these- lective application of magic to typed feature gram-mars type of constraint-logic grammar based on typed feature logic (tgv?:; gstz, 1995).
</nextsent>
<nextsent>typed feature grammars can be used as the basis for implementations of head-driven phrase structure grammar (hpsg; pollard and sag, 1994) as dis-cussed in (gstz and meurers, 1997a) and (meur- ers and minnen, 1997).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1543">
<title id=" E99-1022.xml">selective magic hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for expository reasons we represent he argn features of the append relation as separate argu-ments.
</prevsent>
<prevsent>typed feature grammars can be used as the basis for implementations of head-driven phrase structure grammar (pollard and sag, 1994).
</prevsent>
</prevsection>
<citsent citstr=" J97-4003 ">
3 (meurers and minnen, 1997) <papid> J97-4003 </papid>propose compi-lation of lexical rules into t~r/: definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as mod- ehng partial information as in (carpenter, 1992).</citsent>
<aftsection>
<nextsent>typed feature structures as normal form ir~ ~e terms are merely syntactic objects.
</nextsent>
<nextsent>asee (king, 1994) <papid> C94-2204 </papid>for discussion of the appro-priateness of t~-?: for hpsg and comparison with other feature logic approaches designed for hpsg.</nextsent>
<nextsent>(1) constituent( \[phon ):- lsem phon constituent( \[ agr )  i_sr~m teat?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1544">
<title id=" E99-1022.xml">selective magic hpsg parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>3 (meurers and minnen, 1997) <papid> J97-4003 </papid>propose compi-lation of lexical rules into t~r/: definite clauses 2this view of typed feature structures differs from the perspective on typed feature structures as mod- ehng partial information as in (carpenter, 1992).</prevsent>
<prevsent>typed feature structures as normal form ir~ ~e terms are merely syntactic objects.</prevsent>
</prevsection>
<citsent citstr=" C94-2204 ">
asee (king, 1994) <papid> C94-2204 </papid>for discussion of the appro-priateness of t~-?: for hpsg and comparison with other feature logic approaches designed for hpsg.</citsent>
<aftsection>
<nextsent>(1) constituent( \[phon ):- lsem phon constituent( \[ agr )  i_sr~m teat?
</nextsent>
<nextsent>1 constituent( |agr )  append(\[~,\[~,\[~).
</nextsent>
<nextsent>rcat ?, \] (2) constituent( \[phon ( ,,,,y ) /xgr ,h.~-,,.~\] )  (3) constituent( |phon (,lecp,) /agr ,h,.~-.,.~ ).
</nextsent>
<nextsent>lsem sleep (4) append((), ~  ~)  (5) append( 3 | a.ppend(f x- ~,~, ~y s\])- figure 3: example of set of t:7:?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1545">
<title id=" E95-1010.xml">text alignment in the real world improving alignments of noisy translations using common lexical features string matching strategies and ngram comparisons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a method based on dynamic programming framework, but using decision criterion derived from combination of byte-length ratio measures, hard matching of numbers, string comparisons and n-gram co-occur- rence matching substantially improves the performance of the alignment process.
</prevsent>
<prevsent>given texts in two languages that are to some degree translations of one another, an alignment of the texts associates ente nces, paragraphs or phrases in one document with their translations in the other.
</prevsent>
</prevsection>
<citsent citstr=" P94-1012 ">
success-ful approaches to alignment can be divided into two primary types: those that use comparisons of lexical elements between the documents (wu, 1994; <papid> P94-1012 </papid>chen 1993; <papid> P93-1002 </papid>catizone, russell and warwick, 1989), and ithis research was funded under dod contract #mda 904-94-c-e086 those that use statistical decision process derived from byte-length ratios between alignment blocks (wu, 1994; <papid> P94-1012 </papid>church, 1993; <papid> P93-1001 </papid>gale and church, 1991).<papid> P91-1023 </papid></citsent>
<aftsection>
<nextsent>methods vary for the former approach, hut in the lat-ter approach, dynamic programming framework is used to sequentially align blocks as the alignment pro-cess proceeds.
</nextsent>
<nextsent>under this model, blocks are compared only with nearby blocks as the alignment proceeds, substantially reducing the computational overhead \[o(n 2) ~ o(n!)\] of the alignment process.
</nextsent>
<nextsent>in the primary literature on alignment, he texts are typically well-behaved.
</nextsent>
<nextsent>in byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augment-ing the byte-length measure by scores derived from lexical feature matching (wu, 1994).<papid> P94-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1547">
<title id=" E95-1010.xml">text alignment in the real world improving alignments of noisy translations using common lexical features string matching strategies and ngram comparisons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a method based on dynamic programming framework, but using decision criterion derived from combination of byte-length ratio measures, hard matching of numbers, string comparisons and n-gram co-occur- rence matching substantially improves the performance of the alignment process.
</prevsent>
<prevsent>given texts in two languages that are to some degree translations of one another, an alignment of the texts associates ente nces, paragraphs or phrases in one document with their translations in the other.
</prevsent>
</prevsection>
<citsent citstr=" P93-1002 ">
success-ful approaches to alignment can be divided into two primary types: those that use comparisons of lexical elements between the documents (wu, 1994; <papid> P94-1012 </papid>chen 1993; <papid> P93-1002 </papid>catizone, russell and warwick, 1989), and ithis research was funded under dod contract #mda 904-94-c-e086 those that use statistical decision process derived from byte-length ratios between alignment blocks (wu, 1994; <papid> P94-1012 </papid>church, 1993; <papid> P93-1001 </papid>gale and church, 1991).<papid> P91-1023 </papid></citsent>
<aftsection>
<nextsent>methods vary for the former approach, hut in the lat-ter approach, dynamic programming framework is used to sequentially align blocks as the alignment pro-cess proceeds.
</nextsent>
<nextsent>under this model, blocks are compared only with nearby blocks as the alignment proceeds, substantially reducing the computational overhead \[o(n 2) ~ o(n!)\] of the alignment process.
</nextsent>
<nextsent>in the primary literature on alignment, he texts are typically well-behaved.
</nextsent>
<nextsent>in byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augment-ing the byte-length measure by scores derived from lexical feature matching (wu, 1994).<papid> P94-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1550">
<title id=" E95-1010.xml">text alignment in the real world improving alignments of noisy translations using common lexical features string matching strategies and ngram comparisons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a method based on dynamic programming framework, but using decision criterion derived from combination of byte-length ratio measures, hard matching of numbers, string comparisons and n-gram co-occur- rence matching substantially improves the performance of the alignment process.
</prevsent>
<prevsent>given texts in two languages that are to some degree translations of one another, an alignment of the texts associates ente nces, paragraphs or phrases in one document with their translations in the other.
</prevsent>
</prevsection>
<citsent citstr=" P93-1001 ">
success-ful approaches to alignment can be divided into two primary types: those that use comparisons of lexical elements between the documents (wu, 1994; <papid> P94-1012 </papid>chen 1993; <papid> P93-1002 </papid>catizone, russell and warwick, 1989), and ithis research was funded under dod contract #mda 904-94-c-e086 those that use statistical decision process derived from byte-length ratios between alignment blocks (wu, 1994; <papid> P94-1012 </papid>church, 1993; <papid> P93-1001 </papid>gale and church, 1991).<papid> P91-1023 </papid></citsent>
<aftsection>
<nextsent>methods vary for the former approach, hut in the lat-ter approach, dynamic programming framework is used to sequentially align blocks as the alignment pro-cess proceeds.
</nextsent>
<nextsent>under this model, blocks are compared only with nearby blocks as the alignment proceeds, substantially reducing the computational overhead \[o(n 2) ~ o(n!)\] of the alignment process.
</nextsent>
<nextsent>in the primary literature on alignment, he texts are typically well-behaved.
</nextsent>
<nextsent>in byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augment-ing the byte-length measure by scores derived from lexical feature matching (wu, 1994).<papid> P94-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1551">
<title id=" E95-1010.xml">text alignment in the real world improving alignments of noisy translations using common lexical features string matching strategies and ngram comparisons </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a method based on dynamic programming framework, but using decision criterion derived from combination of byte-length ratio measures, hard matching of numbers, string comparisons and n-gram co-occur- rence matching substantially improves the performance of the alignment process.
</prevsent>
<prevsent>given texts in two languages that are to some degree translations of one another, an alignment of the texts associates ente nces, paragraphs or phrases in one document with their translations in the other.
</prevsent>
</prevsection>
<citsent citstr=" P91-1023 ">
success-ful approaches to alignment can be divided into two primary types: those that use comparisons of lexical elements between the documents (wu, 1994; <papid> P94-1012 </papid>chen 1993; <papid> P93-1002 </papid>catizone, russell and warwick, 1989), and ithis research was funded under dod contract #mda 904-94-c-e086 those that use statistical decision process derived from byte-length ratios between alignment blocks (wu, 1994; <papid> P94-1012 </papid>church, 1993; <papid> P93-1001 </papid>gale and church, 1991).<papid> P91-1023 </papid></citsent>
<aftsection>
<nextsent>methods vary for the former approach, hut in the lat-ter approach, dynamic programming framework is used to sequentially align blocks as the alignment pro-cess proceeds.
</nextsent>
<nextsent>under this model, blocks are compared only with nearby blocks as the alignment proceeds, substantially reducing the computational overhead \[o(n 2) ~ o(n!)\] of the alignment process.
</nextsent>
<nextsent>in the primary literature on alignment, he texts are typically well-behaved.
</nextsent>
<nextsent>in byte-length ratio approaches, the presence of long stretches of blocks that have roughly similar lengths can be problematic, and some improvement can be achieved by augment-ing the byte-length measure by scores derived from lexical feature matching (wu, 1994).<papid> P94-1012 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1556">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, we also show that how anaphoric information is used is crucial: whereas using this information to add new terms does result in improved performance, simple substitution makes the performance worse.
</prevsent>
<prevsent>many approaches to summarization can be very broadly characterized as term-based: they attempt to identify the main topics,?
</prevsent>
</prevsection>
<citsent citstr=" W97-0704 ">
which generally are terms, and then to extract from the document the most important information about these terms (hovy and lin, 1997).<papid> W97-0704 </papid></citsent>
<aftsection>
<nextsent>these approaches can be divided again very broadly in lex ical?
</nextsent>
<nextsent>approaches, among which we would includelsa-based approaches, and coreference-based?
</nextsent>
<nextsent>approaches . lexical approaches to term-based summarization use lexical relations to identify central terms (barzilay and elhadad, 1997; <papid> W97-0703 </papid>gong andliu, 2002); coreference- (or anaphora-) based approaches (baldwin and morton, 1998; boguraev and kennedy, 1999; azzam et al, 1999; <papid> W99-0211 </papid>bergler et al,2003; stuckardt, 2003) identify these terms by running coreference- or anaphoric re solver over the text.1 we are not aware, however, of any attempt touse both lexical and anaphoric information to identify the main terms.</nextsent>
<nextsent>in addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to summarizer significantly improves the performance of summarizer using standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures).in this paper we compare two sentence extraction based summarizers.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1557">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches can be divided again very broadly in lex ical?
</prevsent>
<prevsent>approaches, among which we would includelsa-based approaches, and coreference-based?
</prevsent>
</prevsection>
<citsent citstr=" W97-0703 ">
approaches . lexical approaches to term-based summarization use lexical relations to identify central terms (barzilay and elhadad, 1997; <papid> W97-0703 </papid>gong andliu, 2002); coreference- (or anaphora-) based approaches (baldwin and morton, 1998; boguraev and kennedy, 1999; azzam et al, 1999; <papid> W99-0211 </papid>bergler et al,2003; stuckardt, 2003) identify these terms by running coreference- or anaphoric re solver over the text.1 we are not aware, however, of any attempt touse both lexical and anaphoric information to identify the main terms.</citsent>
<aftsection>
<nextsent>in addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to summarizer significantly improves the performance of summarizer using standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures).in this paper we compare two sentence extraction based summarizers.
</nextsent>
<nextsent>both use latent semantic analysis (lsa) (landauer, 1997) to identify the main terms of text for summarization; however,the first system (steinberger and jezek, 2004), discussed in section 2, only uses lexical information to identify the main topics, whereas the second system exploits both lexical and anaphoric information.this second system uses an existing anaphora resolution system to resolve anaphoric expressions, guitar (poesio and kabadjov, 2004); but, crucially, two different ways of using this information for summarization were tested.
</nextsent>
<nextsent>(section 3.)
</nextsent>
<nextsent>both sum marizers were tested over the cast corpus (orasanet al, 2003), as discussed in section 4, and sig1the terms anaphora resolution?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1558">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these approaches can be divided again very broadly in lex ical?
</prevsent>
<prevsent>approaches, among which we would includelsa-based approaches, and coreference-based?
</prevsent>
</prevsection>
<citsent citstr=" W99-0211 ">
approaches . lexical approaches to term-based summarization use lexical relations to identify central terms (barzilay and elhadad, 1997; <papid> W97-0703 </papid>gong andliu, 2002); coreference- (or anaphora-) based approaches (baldwin and morton, 1998; boguraev and kennedy, 1999; azzam et al, 1999; <papid> W99-0211 </papid>bergler et al,2003; stuckardt, 2003) identify these terms by running coreference- or anaphoric re solver over the text.1 we are not aware, however, of any attempt touse both lexical and anaphoric information to identify the main terms.</citsent>
<aftsection>
<nextsent>in addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to summarizer significantly improves the performance of summarizer using standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures).in this paper we compare two sentence extraction based summarizers.
</nextsent>
<nextsent>both use latent semantic analysis (lsa) (landauer, 1997) to identify the main terms of text for summarization; however,the first system (steinberger and jezek, 2004), discussed in section 2, only uses lexical information to identify the main topics, whereas the second system exploits both lexical and anaphoric information.this second system uses an existing anaphora resolution system to resolve anaphoric expressions, guitar (poesio and kabadjov, 2004); but, crucially, two different ways of using this information for summarization were tested.
</nextsent>
<nextsent>(section 3.)
</nextsent>
<nextsent>both sum marizers were tested over the cast corpus (orasanet al, 2003), as discussed in section 4, and sig1the terms anaphora resolution?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1559">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> an lsa-based summarizer using.  </section>
<citcontext>
<prevsection>
<prevsent>lexical information only lsa (landauer, 1997) is technique for extracting the hidden?
</prevsent>
<prevsent>dimensions of the semantic representation of terms, sentences, or documents, on the basisof their contextual use.
</prevsent>
</prevsection>
<citsent citstr=" W01-0514 ">
it is very powerful technique already used for nlp applications such as information retrieval (berry et al, 1995) and text segmentation (choi et al, 2001) <papid> W01-0514 </papid>and, more recently, multi- and single-document summarization.the approach to using lsa in text summarization we followed in this paper was proposed in (gong and liu, 2002).</citsent>
<aftsection>
<nextsent>gong and liu propose to start by creating term by sentences matrix =[a1, a2, . . .
</nextsent>
<nextsent>, an], where each column vector ai represents the weighted term-frequency vector of sentence in the document under consideration.
</nextsent>
<nextsent>if there are total of terms and sentences in the document, then we will have an ? matrix for the document.
</nextsent>
<nextsent>the next step is to apply singular value decomposition (svd) to matrix a. given an m?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1560">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> using anaphora resolution for.  </section>
<citcontext>
<prevsection>
<prevsent>as example (3) shows, stylistic conventions forbid verbatim repetition, hence the six mentions of fernandez in the text above contain only one lexical repetition, fernandez?.
</prevsent>
<prevsent>the main problem are pronouns, that tend to share the least lexical similarity with the form used to express the antecedent (and anyway are usually removed by stop word lists, therefore do not 2it should be noted that for many newspaper articles, indeed many non-educational texts, only entity-centered?
</prevsent>
</prevsection>
<citsent citstr=" J04-3003 ">
structure can be clearly identified, as opposed to relation-centeredstructure of the type hypothesized in rhetorical structures theory (knott et al, 2001; poesio et al, 2004).<papid> J04-3003 </papid>get included in the svd matrix).</citsent>
<aftsection>
<nextsent>the form of definite descriptions (the spaniard) doesnt always overlap with that of their antecedent, either, especially when the antecedent was expressed with propername.
</nextsent>
<nextsent>the form of mention which more often overlaps to degree with previous mentions is proper nouns, and even then at least some way of dealing with acronyms is necessary (cfr.
</nextsent>
<nextsent>european union / e.u.).
</nextsent>
<nextsent>the motivation for anaphora resolution is that it should tell us which entities are repeatedly mentioned.in this work, we tested mixed approach to integrate anaphoric and word information: using the output of the anaphoric re solver guitar to modify the svd matrix used to determine the sentences toextract.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1561">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> using anaphora resolution for.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 guitar: general-purpose anaphoric.
</prevsent>
<prevsent>re solver the system we used in these experiments, guitar(poesio and kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of nlprocessing pipeline.
</prevsent>
</prevsection>
<citsent citstr=" P98-2143 ">
the current version of the system includes an implementation of the mars pronoun resolution algorithm (mitkov, 1998) <papid> P98-2143 </papid>and partial implementation of the algorithm for resolving definite descriptions proposed by vieira and poe sio (2000).<papid> J00-4003 </papid></citsent>
<aftsection>
<nextsent>the current version of guitar does not include methods for resolving proper nouns.
</nextsent>
<nextsent>3.2.1 pronoun resolution mitkov (1998) <papid> P98-2143 </papid>developed robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to beidentified.</nextsent>
<nextsent>mitkovs algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as antecedent indicators?).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1562">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> using anaphora resolution for.  </section>
<citcontext>
<prevsection>
<prevsent>3.2 guitar: general-purpose anaphoric.
</prevsent>
<prevsent>re solver the system we used in these experiments, guitar(poesio and kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of nlprocessing pipeline.
</prevsent>
</prevsection>
<citsent citstr=" J00-4003 ">
the current version of the system includes an implementation of the mars pronoun resolution algorithm (mitkov, 1998) <papid> P98-2143 </papid>and partial implementation of the algorithm for resolving definite descriptions proposed by vieira and poe sio (2000).<papid> J00-4003 </papid></citsent>
<aftsection>
<nextsent>the current version of guitar does not include methods for resolving proper nouns.
</nextsent>
<nextsent>3.2.1 pronoun resolution mitkov (1998) <papid> P98-2143 </papid>developed robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to beidentified.</nextsent>
<nextsent>mitkovs algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as antecedent indicators?).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1566">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>using p&r;, system will be ranked much higher than system b. it is quite possible that sentences 2 and 3 are equally important, in which case the two systems should get the same score.
</prevsent>
<prevsent>to address the problem with precision and recall we used combination of evaluation measures.
</prevsent>
</prevsection>
<citsent citstr=" W00-0403 ">
the first of these, relative utility (ru) (radev et al,2000) <papid> W00-0403 </papid>allows model summaries to consist of sentences with variable ranking.</citsent>
<aftsection>
<nextsent>with ru, the model summary represents all sentences of the input document with confidence values for their inclusion in the summary.
</nextsent>
<nextsent>for example, document with five sentences [1 2 3 4 5] is represented as [1/5 2/4 3/4 5 evaluation lexical lsa manual manual method substitution additition relative utility 0.595 0.573 0.662 f-score 0.420 0.410 0.489 cosine similarity 0.774 0.806 0.823 main topic similarity 0.686 0.682 0.747 table 1: evaluation of the manual annotation improvement - summarization ratio: 15%.
</nextsent>
<nextsent>evaluation lexical lsa manual manual method substitution addition relative utility 0.645 0.662 0.688 f-score 0.557 0.549 0.583 cosine similarity 0.863 0.878 0.886 main topic similarity 0.836 0.829 0.866 table 2: evaluation of the manual annotation improvement - summarization ratio: 30%.
</nextsent>
<nextsent>4/1 5/2].
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1568">
<title id=" H05-1001.xml">improving lsa based summarization with anaphora resolution </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>both of these improvements were significant by t-test at 95% confidence.
</prevsent>
<prevsent>4.4 results with guitar.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
to use guitar, we first parsed the texts using char niaks parser (charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>the output of the parser was then converted into the mas-xml format expected by guitar by one of the pre proces sors that come with the system.
</nextsent>
<nextsent>(this step includes heuristic methods for guessing agreement features.)finally, guitar was ran to add anaphoric information to the files.
</nextsent>
<nextsent>the resulting files were then processed by the summarizer.
</nextsent>
<nextsent>guitar achieved precision of 56% and recall of 51% over the 37 documents.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1569">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extensive experimental evaluation in the conll 2005 shared task framework supports our previous claims.
</prevsent>
<prevsent>the proposed architecture outperforms the best results reported in that evaluation exercise.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the task of semantic role labeling (srl), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2005<papid> W05-0634 </papid>a; carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>it was shown thatthe identification of such event frames has significant contribution for many natural language processing (nlp) applications such as information extraction (surdeanu et al, 2003) <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></nextsent>
<nextsent>most current srl approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>and approaches that use partial syntactic analysis, championed by previous evaluations performed within the conference on computational natural language learning (conll) (carreras and ma`rquez, 2004).the wisdom extracted from this volume of work indicates that full syntactic analysis has significant contribution to the srl performance, when using hand-corrected syntactic information.on the other hand, when only automatically generated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1570">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extensive experimental evaluation in the conll 2005 shared task framework supports our previous claims.
</prevsent>
<prevsent>the proposed architecture outperforms the best results reported in that evaluation exercise.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
the task of semantic role labeling (srl), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2005<papid> W05-0634 </papid>a; carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>it was shown thatthe identification of such event frames has significant contribution for many natural language processing (nlp) applications such as information extraction (surdeanu et al, 2003) <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></nextsent>
<nextsent>most current srl approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>and approaches that use partial syntactic analysis, championed by previous evaluations performed within the conference on computational natural language learning (conll) (carreras and ma`rquez, 2004).the wisdom extracted from this volume of work indicates that full syntactic analysis has significant contribution to the srl performance, when using hand-corrected syntactic information.on the other hand, when only automatically generated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1571">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extensive experimental evaluation in the conll 2005 shared task framework supports our previous claims.
</prevsent>
<prevsent>the proposed architecture outperforms the best results reported in that evaluation exercise.
</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
the task of semantic role labeling (srl), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2005<papid> W05-0634 </papid>a; carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>it was shown thatthe identification of such event frames has significant contribution for many natural language processing (nlp) applications such as information extraction (surdeanu et al, 2003) <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></nextsent>
<nextsent>most current srl approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>and approaches that use partial syntactic analysis, championed by previous evaluations performed within the conference on computational natural language learning (conll) (carreras and ma`rquez, 2004).the wisdom extracted from this volume of work indicates that full syntactic analysis has significant contribution to the srl performance, when using hand-corrected syntactic information.on the other hand, when only automatically generated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1572">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>extensive experimental evaluation in the conll 2005 shared task framework supports our previous claims.
</prevsent>
<prevsent>the proposed architecture outperforms the best results reported in that evaluation exercise.
</prevsent>
</prevsection>
<citsent citstr=" W05-0634 ">
the task of semantic role labeling (srl), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2005<papid> W05-0634 </papid>a; carreras and ma`rquez, 2005).</citsent>
<aftsection>
<nextsent>it was shown thatthe identification of such event frames has significant contribution for many natural language processing (nlp) applications such as information extraction (surdeanu et al, 2003) <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></nextsent>
<nextsent>most current srl approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>and approaches that use partial syntactic analysis, championed by previous evaluations performed within the conference on computational natural language learning (conll) (carreras and ma`rquez, 2004).the wisdom extracted from this volume of work indicates that full syntactic analysis has significant contribution to the srl performance, when using hand-corrected syntactic information.on the other hand, when only automatically generated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1574">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the proposed architecture outperforms the best results reported in that evaluation exercise.
</prevsent>
<prevsent>the task of semantic role labeling (srl), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (gildea and jurafsky, 2002; <papid> J02-3001 </papid>surdeanu et al, 2003; <papid> P03-1002 </papid>xue and palmer, 2004; <papid> W04-3212 </papid>pradhan et al, 2005<papid> W05-0634 </papid>a; carreras and ma`rquez, 2005).</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
it was shown thatthe identification of such event frames has significant contribution for many natural language processing (nlp) applications such as information extraction (surdeanu et al, 2003) <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid></citsent>
<aftsection>
<nextsent>most current srl approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by gildea and jurafsky (2002), <papid> J02-3001 </papid>and approaches that use partial syntactic analysis, championed by previous evaluations performed within the conference on computational natural language learning (conll) (carreras and ma`rquez, 2004).the wisdom extracted from this volume of work indicates that full syntactic analysis has significant contribution to the srl performance, when using hand-corrected syntactic information.on the other hand, when only automatically generated syntax is available, the quality of the information provided through full syntax decreases because the state-of-the-art of full parsing is less robust and performs worse than the tools used for partial syntactic analysis.</nextsent>
<nextsent>under such real-worldconditions, the difference between the two srl approaches (with full or partial syntax) is not that high.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1576">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we do not use any attributes specific to the individual models, not even the confidence assigned by the individual classifiers.
</prevsent>
<prevsent>besides simplicity, the consequence of this decision is that our approach does not impose any restrictions on the individual srl strategies, as long as one solution is provided for each predicate.
</prevsent>
</prevsection>
<citsent citstr=" W05-0625 ">
on the other hand, probabilistic inference processes, which have been successfully used for srl (koomen et al, 2005), <papid> W05-0625 </papid>mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model.</citsent>
<aftsection>
<nextsent>however, this information is not directly available in two out of three of our individual models, which classify argument chunks and not entire arguments.despite its simplicity, our approach obtains encouraging results: the combined system outperforms any of the individual systems and, using exactly the same data, it is also competitive with the best srl systems that participated in the latest conll shared task evaluation (carreras and ma`rquez, 2005).
</nextsent>
<nextsent>in this paper we report results using propbank, an approximately one-million-word corpus annotated with predicate-argument structures (kingsbury et al., 2002).
</nextsent>
<nextsent>to date, propbank addresses mainly predicates lexicalized by verbs and small number of predicates lexicalized by verb nominalizations and adjectives.the arguments of each predicate are numbered sequentially from arg0 to arg5.
</nextsent>
<nextsent>generally, arg0stands for agent, arg1 for theme or direct object, and arg2 for indirect object, benefactive or instrument, but mnemonics tend to be verb specific.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1579">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> individual srl models.  </section>
<citcontext>
<prevsection>
<prevsent>646 relative position, distance in words and chunks, and level of embedding (in #clause-levels) with respect to the constituent.
</prevsent>
<prevsent>constituent path as described in (gildea and jurafsky, 2002) <papid> J02-3001 </papid>and all 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent.</prevsent>
</prevsection>
<citsent citstr=" W04-2415 ">
partial parsing path as described in (carreras et al, 2004) <papid> W04-2415 </papid>and all 3/4/5-grams of path elements beginning at the verb predicate or ending at the constituent.</citsent>
<aftsection>
<nextsent>syntactic frame as described by xue and palmer (2004) <papid> W04-3212 </papid>table 3: predicate constituent features: models 1/2 the syntactic label of the candidate constituent.</nextsent>
<nextsent>the constituent head word, suffixes of length 2, 3, and 4, lemma, and pos tag.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1583">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> individual srl models.  </section>
<citcontext>
<prevsection>
<prevsent>a binary feature to indicate if the argument starts with predicate particle, i.e. token seen with the rp* pos tag and directly attached to the predicate in training.
</prevsent>
<prevsent>table 6: predicate constituent features: model 3 once the mapping process completes, model 3extracts rich set of lexical, syntactic, and semantic features.
</prevsent>
</prevsection>
<citsent citstr=" W05-0635 ">
tables 4, 5, and 6 present these features organized in the same three categories as the previous models 1 and 2 ? see (surdeanu and turmo, 2005) <papid> W05-0635 </papid>for more details.</citsent>
<aftsection>
<nextsent>similarly with models 1 and 2, model 3 trains one-vs-all classifiers using ada boost for the most common argument labels.
</nextsent>
<nextsent>to reduce the sample space, model 3 selects training examples (both positive and negative) only from: (a) the first clause that includes the predicate, or (b) from phrases that appear to the left of the predicate in the sentence.
</nextsent>
<nextsent>more than 98% of the argument constituents fall into one of these classes.at prediction time the classifiers are combined using simple greedy technique that iteratively assigns 647 to each predicate the argument classified with the highest confidence.
</nextsent>
<nextsent>for each predicate we consider as candidates all am attributes, but only numbered attributes indicated in the corresponding propbank frame.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1585">
<title id=" H05-1081.xml">a robust combination strategy for semantic role labeling </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>differing from the strategy presented in this paper, their inference layer does not include learning.
</prevsent>
<prevsent>also, they require confidence values from individual classifiers.
</prevsent>
</prevsection>
<citsent citstr=" W05-0623 ">
this is the best performing system at conll-2005.haghighi et al (2005) <papid> W05-0623 </papid>implemented double reranking model on top of the base srl models to select the most probable solution among set of can didates.</citsent>
<aftsection>
<nextsent>the re-ranking is performed, first, on set of n-best solutions obtained by the base system runon single parse tree, and, then, on the set of best candidates coming from the n-best parse trees.
</nextsent>
<nextsent>the 650 re-ranking approach allows to define global complex features applying to complete candidate solutions to train the rankers.
</nextsent>
<nextsent>the main drawback, compared to our approach, is that re-ranking does not permit to combine different solutions since it is forced to select complete candidate solution.
</nextsent>
<nextsent>this fact implies that the performance upper limit strongly depend son the ability of the base model to generate the complete correct solution in the set of n-best candidates.finally, pradhan et al (2005<papid> W05-0634 </papid>b) followed stacking approach by learning two individual systems based on full syntax, whose outputs are used to generate features to feed the training stage of final chunk-by-chunk srl system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1587">
<title id=" E93-1034.xml">tuples discontinuity and gapping in categorial grammar </title>
<section> prosodic labelling.  </section>
<citcontext>
<prevsection>
<prevsent>289 the semantic interpretation with respect func-tion and cartesian product formation can also be ap-plied uniformly, with systematic labelling as in the previous ection.
</prevsent>
<prevsent>4 iscont inu ty elegant as such categorial grammar is, it is more suggestive of an approach to computational linguis-tic grammar formalism, than actually representa-tive of such.
</prevsent>
</prevsection>
<citsent citstr=" E91-1035 ">
amongst he various enrichments that have been proposed (see e.g. \[van benthem, 1989; morrill et al, 1990; barry et al, 1991; <papid> E91-1035 </papid>morrill, 1990a; morrill, 1990b; moortgat and morrill, 1991; morrill, 1992a; morrill, 1992b\]), \[moortgat, 1988\] advanced earlier discussion of discontinuity in e.g. \[bach, 1981; bach, 1984\] with proposal for in fixing and wrap-ping operators.</citsent>
<aftsection>
<nextsent>the operators not only provide scope over these particular phenomena but also, as indi-cated in e.g. \[moortgat, 1990\], seem to provide an underlying basis in terms of which operators for bind-ing phenomena such as quantification and reflexivisa- tion should be definable.
</nextsent>
<nextsent>the coverage of pied piping in \[morrill, 1992b\] would also be definable in terms of these primitives, but all this depends on the reso-lution of certain technical issues which have been to date outstanding.
</nextsent>
<nextsent>amongst he examples we shall be able to treat by means of our present proposals are the following.
</nextsent>
<nextsent>a. mary rang john up.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1588">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the next requirement is that exceptions and sub- regularities can be expressed.
</prevsent>
<prevsent>it must be possible to describe concisely the situation where word or class of words are members of some super class, and share the regular characteristics of the super class in most respects, but have different values for some feature or cluster of features.
</prevsent>
</prevsection>
<citsent citstr=" E89-1009 ">
severm lexical representation for-malisms addressing these desiderata have been pro-posed, e.g. datr \[evans and gazdar 1989<papid> E89-1009 </papid>a, 1989b, 1990\]; lrl \[copestake, 1992\]; \[<papid> A92-1012 </papid>russell et al 1991\].</citsent>
<aftsection>
<nextsent>the work described here uses datr.
</nextsent>
<nextsent>datr has certain desirable formm and computa-tional properties.
</nextsent>
<nextsent>it is formal language with declarative semantics.
</nextsent>
<nextsent>retrieving values for queries involves no search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1589">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>the next requirement is that exceptions and sub- regularities can be expressed.
</prevsent>
<prevsent>it must be possible to describe concisely the situation where word or class of words are members of some super class, and share the regular characteristics of the super class in most respects, but have different values for some feature or cluster of features.
</prevsent>
</prevsection>
<citsent citstr=" A92-1012 ">
severm lexical representation for-malisms addressing these desiderata have been pro-posed, e.g. datr \[evans and gazdar 1989<papid> E89-1009 </papid>a, 1989b, 1990\]; lrl \[copestake, 1992\]; \[<papid> A92-1012 </papid>russell et al 1991\].</citsent>
<aftsection>
<nextsent>the work described here uses datr.
</nextsent>
<nextsent>datr has certain desirable formm and computa-tional properties.
</nextsent>
<nextsent>it is formal language with declarative semantics.
</nextsent>
<nextsent>retrieving values for queries involves no search.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1590">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>multiple inheritance specifica-tions are always orthogonal, so word may inherit from more than one place, but any fact about that word has the place it is to be inherited from uniquely specified.
</prevsent>
<prevsent>the problem of different ancestors provid-ing contradictory values, often associated with mul-tiple default inheritance, is thereby avoided, yet the kinds of generalisation most often associated with the lexicon can still be simply stated.
</prevsent>
</prevsection>
<citsent citstr=" E93-1012 ">
to date it has been used to express yntactic, morphologi-cal, phonological and limited amount of seman-tic lexical information \[evans and gazdar, 1990; cahill and evans, 1990; gibbon, 1990; cahill, 1993\].<papid> E93-1012 </papid></citsent>
<aftsection>
<nextsent>verb alternations have not previously received datr treatment.
</nextsent>
<nextsent>1.2 related work.
</nextsent>
<nextsent>the work described here is at the meeting-point of lexical representation la guages (as discussed above), lexical semantics (as in atkins et aland levin and rappoport hovav; see also \[levin, 1991\]) and for-mal accounts of alternations ( ee particularly \[dowty, 1979\]).
</nextsent>
<nextsent>recent work which aims to bring these three threads together in relation to the lexical repre-sentation of nouns includes \[briscoe et ai., 1990; pustejovsky, 1991; <papid> J91-4003 </papid>copestake and briscoe, 1991; <papid> W91-0209 </papid>kilgarriff, 1993 forthcoming; kilgarriff and gazdar, 1993 forthcoming\].</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1591">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 related work.
</prevsent>
<prevsent>the work described here is at the meeting-point of lexical representation la guages (as discussed above), lexical semantics (as in atkins et aland levin and rappoport hovav; see also \[levin, 1991\]) and for-mal accounts of alternations ( ee particularly \[dowty, 1979\]).
</prevsent>
</prevsection>
<citsent citstr=" J91-4003 ">
recent work which aims to bring these three threads together in relation to the lexical repre-sentation of nouns includes \[briscoe et ai., 1990; pustejovsky, 1991; <papid> J91-4003 </papid>copestake and briscoe, 1991; <papid> W91-0209 </papid>kilgarriff, 1993 forthcoming; kilgarriff and gazdar, 1993 forthcoming\].</citsent>
<aftsection>
<nextsent>(the latter two are companion papers to this, also using datr in similar ways.)
</nextsent>
<nextsent>a paper addressing verbs is \[sanfilippo and poznanski, 1992\].<papid> A92-1011 </papid></nextsent>
<nextsent>this covers some of the same alternations as this 214 paper, and has similar goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1592">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>1.2 related work.
</prevsent>
<prevsent>the work described here is at the meeting-point of lexical representation la guages (as discussed above), lexical semantics (as in atkins et aland levin and rappoport hovav; see also \[levin, 1991\]) and for-mal accounts of alternations ( ee particularly \[dowty, 1979\]).
</prevsent>
</prevsection>
<citsent citstr=" W91-0209 ">
recent work which aims to bring these three threads together in relation to the lexical repre-sentation of nouns includes \[briscoe et ai., 1990; pustejovsky, 1991; <papid> J91-4003 </papid>copestake and briscoe, 1991; <papid> W91-0209 </papid>kilgarriff, 1993 forthcoming; kilgarriff and gazdar, 1993 forthcoming\].</citsent>
<aftsection>
<nextsent>(the latter two are companion papers to this, also using datr in similar ways.)
</nextsent>
<nextsent>a paper addressing verbs is \[sanfilippo and poznanski, 1992\].<papid> A92-1011 </papid></nextsent>
<nextsent>this covers some of the same alternations as this 214 paper, and has similar goals.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1593">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>recent work which aims to bring these three threads together in relation to the lexical repre-sentation of nouns includes \[briscoe et ai., 1990; pustejovsky, 1991; <papid> J91-4003 </papid>copestake and briscoe, 1991; <papid> W91-0209 </papid>kilgarriff, 1993 forthcoming; kilgarriff and gazdar, 1993 forthcoming\].</prevsent>
<prevsent>(the latter two are companion papers to this, also using datr in similar ways.)</prevsent>
</prevsection>
<citsent citstr=" A92-1011 ">
a paper addressing verbs is \[sanfilippo and poznanski, 1992\].<papid> A92-1011 </papid></citsent>
<aftsection>
<nextsent>this covers some of the same alternations as this 214 paper, and has similar goals.
</nextsent>
<nextsent>the formalism it uses is lrl, the typed default unification formalism of \[copestake, 1992\].<papid> A92-1012 </papid></nextsent>
<nextsent>unlike datr, this is both lex-ical representation language and grammar formal- ism.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1595">
<title id=" E93-1026.xml">inheriting verb alternations </title>
<section> an  hpsg-s ty le  lexicon.  </section>
<citcontext>
<prevsection>
<prevsent>5 we have causative melt as shown in fig.
</prevsent>
<prevsent>6.
</prevsent>
</prevsection>
<citsent citstr=" P89-1005 ">
(the relation between lambda expressions and feature structures discussed in \[moore, 1989; <papid> P89-1005 </papid>kilgarriff, 1992\].)</citsent>
<aftsection>
<nextsent>word syn sem melt maj \] subcat (np\[nom\] sem ~\] ) reln melt/i \] melted e\] figure 5: avm for in transitive melt.
</nextsent>
<nextsent>word syn sem melt subcat (np\[nom\] sem , np\[acc\] sem ) oa ser \]rel melt/, used mel edf1 figure 6: avm for causative melt.
</nextsent>
<nextsent>a simple datr equation has, on its lhs, node and path, and, on its rhs, either value: node : b  iffi value . or an inheritance specification.
</nextsent>
<nextsent>nodes start with cap-ital letters, paths are sequences enclosed in angle- brackets, anything on the rhs that is not node or path is value.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1596">
<title id=" E95-1030.xml">a fast partial parse of natural language sentences using a connection ist method </title>
<section> using negative information.  </section>
<citcontext>
<prevsection>
<prevsent>now, in natural anguage negative correlations are an important source of information: the occur-rence of some words or groups of words inhibit others from following.
</prevsent>
<prevsent>we wish to exploit these constraints.
</prevsent>
</prevsection>
<citsent citstr=" H90-1055 ">
(brill et al  , 1990) <papid> H90-1055 </papid>recognised this, and introduced the idea of distituents.</citsent>
<aftsection>
<nextsent>these are elements of sentence that should be separated, as opposed to elements of const tuents that cling together.
</nextsent>
<nextsent>brill addresses the problem of finding valid metric for distituency by using gener-alized mutual information statistic.
</nextsent>
<nextsent>distituency is marked by mutual information minima.
</nextsent>
<nextsent>his method is supported by small 4 rule grammar.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1597">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the switchboard corpus of conversational speech, the system achieves improved parse accuracy over state-of-the-art system which uses only lexical and syntactic features.
</prevsent>
<prevsent>since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that pcfgs are not competitive with more specialized techniques.
</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
for more than decade, the penn treebanks wall street journal corpus has served as benchmark for developing and evaluating statistical parsing techniques (collins, 2000; charniak and johnson, 2005).<papid> P05-1022 </papid>while this common benchmark has served as valuable shared task for focusing community effort, it has unfortunately led to the relative neglect of other genres, particularly speech.</citsent>
<aftsection>
<nextsent>parsed speech stands to benefit from practically every application envisioned for parsed text, including machine translation, information extraction, and language modeling.
</nextsent>
<nextsent>in contrast to text, however, speech (in particular, conversational speech) presents distinct set of opportunities and challenges.
</nextsent>
<nextsent>while new obstacles arise from the presence of speech repairs, the possibility of word errors, and the absence of punctuation and sentence boundaries, speech also presents tremendous opportunity to leverage multi-modal input, in the form of acoustic or even visual cues.
</nextsent>
<nextsent>as step in this direction, this paper identifies set of useful prosodic features and describes howthey can be effectively incorporated into statistical parsing model, ignoring for now the problem of word errors.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1599">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as step in this direction, this paper identifies set of useful prosodic features and describes howthey can be effectively incorporated into statistical parsing model, ignoring for now the problem of word errors.
</prevsent>
<prevsent>evaluated on the switchboard corpus of conversational telephone speech (graff and bird, 2000), our prosody-aware parser out-performsa state-of-the-art system that uses lexical and syntactic features only.
</prevsent>
</prevsection>
<citsent citstr=" N04-1011 ">
while we are not the first to employ prosodic cues in statistical parsing model, previous efforts (gregory et al , 2004; <papid> N04-1011 </papid>kahn et al , 2004) <papid> N04-4032 </papid>incorporated these features as word tokens and thereby suffered from the side-effect of displacing words inthe n-gram models by the parser.</citsent>
<aftsection>
<nextsent>to avoid this problem, we generate set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses.
</nextsent>
<nextsent>our system architecture combines earlier models proposed for parse reranking (collins, 2000) and filtering out edit regions (charniak and johnson, 2001).<papid> N01-1016 </papid></nextsent>
<nextsent>detecting and removing edits prior to parsing is motivated by the claim that probabilistic context free grammars (pcfgs) perform poorly at detecting edit regions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1600">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as step in this direction, this paper identifies set of useful prosodic features and describes howthey can be effectively incorporated into statistical parsing model, ignoring for now the problem of word errors.
</prevsent>
<prevsent>evaluated on the switchboard corpus of conversational telephone speech (graff and bird, 2000), our prosody-aware parser out-performsa state-of-the-art system that uses lexical and syntactic features only.
</prevsent>
</prevsection>
<citsent citstr=" N04-4032 ">
while we are not the first to employ prosodic cues in statistical parsing model, previous efforts (gregory et al , 2004; <papid> N04-1011 </papid>kahn et al , 2004) <papid> N04-4032 </papid>incorporated these features as word tokens and thereby suffered from the side-effect of displacing words inthe n-gram models by the parser.</citsent>
<aftsection>
<nextsent>to avoid this problem, we generate set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses.
</nextsent>
<nextsent>our system architecture combines earlier models proposed for parse reranking (collins, 2000) and filtering out edit regions (charniak and johnson, 2001).<papid> N01-1016 </papid></nextsent>
<nextsent>detecting and removing edits prior to parsing is motivated by the claim that probabilistic context free grammars (pcfgs) perform poorly at detecting edit regions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1601">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while we are not the first to employ prosodic cues in statistical parsing model, previous efforts (gregory et al , 2004; <papid> N04-1011 </papid>kahn et al , 2004) <papid> N04-4032 </papid>incorporated these features as word tokens and thereby suffered from the side-effect of displacing words inthe n-gram models by the parser.</prevsent>
<prevsent>to avoid this problem, we generate set of candidate parses using an off-the-shelf, k-best parser, and use prosodic (and other) features to rescore the candidate parses.</prevsent>
</prevsection>
<citsent citstr=" N01-1016 ">
our system architecture combines earlier models proposed for parse reranking (collins, 2000) and filtering out edit regions (charniak and johnson, 2001).<papid> N01-1016 </papid></citsent>
<aftsection>
<nextsent>detecting and removing edits prior to parsing is motivated by the claim that probabilistic context free grammars (pcfgs) perform poorly at detecting edit regions.
</nextsent>
<nextsent>we validate this claim empirically: two state-of-the-art pcfgs (bikel, 2004; charniak and johnson, 2005) <papid> P05-1022 </papid>are both shown to perform significantly below state-of-the-art edit detection system (johnson et al , 2004).</nextsent>
<nextsent>as mentioned earlier, conversational speech presents different set of challenges and opportunities than encountered in parsing text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1605">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the relationship between reparandum and repair is quite different: the repair is often rough copy?
</prevsent>
<prevsent>of the reparandum, using the same or very similar wordsin roughly the same order.
</prevsent>
</prevsection>
<citsent citstr=" J99-4003 ">
a language model characterizing this dependency with hidden stack operations is proposed in (heeman and allen, 1999).<papid> J99-4003 </papid></citsent>
<aftsection>
<nextsent>several parsing models have been proposed which accord special treatment to speech repairs.
</nextsent>
<nextsent>most prior work has focused on handling disfluencies and continued to relyon hand-annotated transcripts that include punctuation, case, and known sentence boundaries (hindle, 1983; <papid> P83-1019 </papid>core and schubert, 1999; <papid> P99-1053 </papid>charniak and johnson, 2001; <papid> N01-1016 </papid>engel et al , 2002).<papid> W02-1007 </papid>of particular mention is the analysis of the relationship between speech repairs and parsing accuracy presented by charniak and johnson (2001), <papid> N01-1016 </papid>as this directly influenced our work.</nextsent>
<nextsent>they presented evidence that improved edit detection (i.e. detecting the reparandum and edit phrase) leads to better parsing accuracy, showing relative reduction in score error of 14% (2% absolute) between oracle and automatic edit removal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1606">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a language model characterizing this dependency with hidden stack operations is proposed in (heeman and allen, 1999).<papid> J99-4003 </papid></prevsent>
<prevsent>several parsing models have been proposed which accord special treatment to speech repairs.</prevsent>
</prevsection>
<citsent citstr=" P83-1019 ">
most prior work has focused on handling disfluencies and continued to relyon hand-annotated transcripts that include punctuation, case, and known sentence boundaries (hindle, 1983; <papid> P83-1019 </papid>core and schubert, 1999; <papid> P99-1053 </papid>charniak and johnson, 2001; <papid> N01-1016 </papid>engel et al , 2002).<papid> W02-1007 </papid>of particular mention is the analysis of the relationship between speech repairs and parsing accuracy presented by charniak and johnson (2001), <papid> N01-1016 </papid>as this directly influenced our work.</citsent>
<aftsection>
<nextsent>they presented evidence that improved edit detection (i.e. detecting the reparandum and edit phrase) leads to better parsing accuracy, showing relative reduction in score error of 14% (2% absolute) between oracle and automatic edit removal.
</nextsent>
<nextsent>thus, this work adopts their edit detection preprocessing approach.
</nextsent>
<nextsent>they have subsequently presented an improved model forde tecting edits (johnson et al , 2004), and our results here complement their analysis of the edit detection and parsing relationship, particularly with respect to the limitations of pcfgs in edit detection.
</nextsent>
<nextsent>2.2 prosody and parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1607">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a language model characterizing this dependency with hidden stack operations is proposed in (heeman and allen, 1999).<papid> J99-4003 </papid></prevsent>
<prevsent>several parsing models have been proposed which accord special treatment to speech repairs.</prevsent>
</prevsection>
<citsent citstr=" P99-1053 ">
most prior work has focused on handling disfluencies and continued to relyon hand-annotated transcripts that include punctuation, case, and known sentence boundaries (hindle, 1983; <papid> P83-1019 </papid>core and schubert, 1999; <papid> P99-1053 </papid>charniak and johnson, 2001; <papid> N01-1016 </papid>engel et al , 2002).<papid> W02-1007 </papid>of particular mention is the analysis of the relationship between speech repairs and parsing accuracy presented by charniak and johnson (2001), <papid> N01-1016 </papid>as this directly influenced our work.</citsent>
<aftsection>
<nextsent>they presented evidence that improved edit detection (i.e. detecting the reparandum and edit phrase) leads to better parsing accuracy, showing relative reduction in score error of 14% (2% absolute) between oracle and automatic edit removal.
</nextsent>
<nextsent>thus, this work adopts their edit detection preprocessing approach.
</nextsent>
<nextsent>they have subsequently presented an improved model forde tecting edits (johnson et al , 2004), and our results here complement their analysis of the edit detection and parsing relationship, particularly with respect to the limitations of pcfgs in edit detection.
</nextsent>
<nextsent>2.2 prosody and parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1609">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>a language model characterizing this dependency with hidden stack operations is proposed in (heeman and allen, 1999).<papid> J99-4003 </papid></prevsent>
<prevsent>several parsing models have been proposed which accord special treatment to speech repairs.</prevsent>
</prevsection>
<citsent citstr=" W02-1007 ">
most prior work has focused on handling disfluencies and continued to relyon hand-annotated transcripts that include punctuation, case, and known sentence boundaries (hindle, 1983; <papid> P83-1019 </papid>core and schubert, 1999; <papid> P99-1053 </papid>charniak and johnson, 2001; <papid> N01-1016 </papid>engel et al , 2002).<papid> W02-1007 </papid>of particular mention is the analysis of the relationship between speech repairs and parsing accuracy presented by charniak and johnson (2001), <papid> N01-1016 </papid>as this directly influenced our work.</citsent>
<aftsection>
<nextsent>they presented evidence that improved edit detection (i.e. detecting the reparandum and edit phrase) leads to better parsing accuracy, showing relative reduction in score error of 14% (2% absolute) between oracle and automatic edit removal.
</nextsent>
<nextsent>thus, this work adopts their edit detection preprocessing approach.
</nextsent>
<nextsent>they have subsequently presented an improved model forde tecting edits (johnson et al , 2004), and our results here complement their analysis of the edit detection and parsing relationship, particularly with respect to the limitations of pcfgs in edit detection.
</nextsent>
<nextsent>2.2 prosody and parsing.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1612">
<title id=" H05-1030.xml">effective use of prosody in parsing conversational speech </title>
<section> previous work.  </section>
<citcontext>
<prevsection>
<prevsent>however, the utility of sentence-internal prosody in parsing conversational speech is not well established.
</prevsent>
<prevsent>most early work on integrating prosody in parsing was in the context of human-computer dialog systems, where parsers typically operated on isolated utterances.
</prevsent>
</prevsection>
<citsent citstr=" P90-1003 ">
the primary use of prosody was to rule out candidate parses (bear and price, 1990; <papid> P90-1003 </papid>batliner et al , 1996).</citsent>
<aftsection>
<nextsent>since then, parsing has advanced considerably, and the use of statistical parsers makes the candidate pruning benefits of prosody less important.
</nextsent>
<nextsent>this raises the question of whether prosody is useful for improving parsing accuracy for conversational speech, apart from its use in sentence 234 figure 2: system architecture boundary detection.
</nextsent>
<nextsent>extensions of charniak and johnson (2001) <papid> N01-1016 </papid>look at using quant ized combinations of prosodic features as additional words?, similar to the use of punctuation in parsing written text (gregory et al , 2004), <papid> N04-1011 </papid>but do not find that the prosodic features are useful.</nextsent>
<nextsent>it may be that with the short sentences?</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1627">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the web contains wealth of opinions about products, politicians, and more, which are expressed in newsgroupposts, review sites, and elsewhere.
</prevsent>
<prevsent>as result, the problem of opinion mining?
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
has seen increasing attention over the last three years from (turney, 2002; <papid> P02-1053 </papid>hu and liu, 2004) and many others.</citsent>
<aftsection>
<nextsent>this paper focuses on product reviews, though our methods apply to broader range of opinions.
</nextsent>
<nextsent>product reviews on web sites such as amazon.com and elsewhere often associate meta-data with each review indicating how positive (or negative) it is using 5-starscale, and also rank products by how they fare in there views at the site.
</nextsent>
<nextsent>however, the readers taste may differ from the reviewers?.
</nextsent>
<nextsent>for example, the reader may feel strongly about the quality of the gym in hotel, whereas many reviewers may focus on other aspects of the hotel, such as the decor or the location.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1629">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> opine overview.  </section>
<citcontext>
<prevsection>
<prevsent>3.3.1 extracting potential opinion phrases opine uses explicit features to identify potential opinion phrases.
</prevsent>
<prevsent>our intuition is that an opinion phrase associated with product feature will occur in its vicinity.
</prevsent>
</prevsection>
<citsent citstr=" C04-1200 ">
this idea is similar to that of (kim and hovy, 2004) <papid> C04-1200 </papid>and (hu and liu, 2004), but instead of using window of size or the output of noun phrase chunker, opine takes advantage of the syntactic dependencies computed by theminipar parser.</citsent>
<aftsection>
<nextsent>our intuition is embodied by 10 extraction rules, some of which are shown in table 4.
</nextsent>
<nextsent>if an explicit feature is found in sentence, opine applies the extraction rules in order to find the heads of potential opinion phrases.
</nextsent>
<nextsent>each head word together with its modi 341 fiers is returned as potential opinion phrase1.
</nextsent>
<nextsent>extraction rules examples if ?(m,np = f)?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1631">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> opine overview.  </section>
<citcontext>
<prevsection>
<prevsent>finds so label for each (w, , s) input tuple.each of these subtasks is cast as an unsupervised collective classification problem and solved using the samemechanism.
</prevsent>
<prevsent>in each case, opine is given set of objects (words, pairs or tuples) and set of labels (so labels); opine then searches for global assignment of labels to objects.
</prevsent>
</prevsection>
<citsent citstr=" P97-1023 ">
in each case, opine makes use of local constraints on label assignments (e.g., conjunctions and dis junctions constraining the assignment of so labels to words (hatzivassiloglou and mckeown, 1997)).<papid> P97-1023 </papid></citsent>
<aftsection>
<nextsent>a key insight in opine is that the problem of searching for global so label assignment to words, pairs or tupleswhile trying to satisfy as many local constraints on assignments as possible is analogous to labeling problems in computervision (e.g., model-based matching).
</nextsent>
<nextsent>opine uses well-known computervision technique, relaxation labeling (hummel and zucker, 1983), in order to solve the three subtasks described above.
</nextsent>
<nextsent>3.3.3 relaxation labeling overview relaxation labeling is an unsupervised classification technique which takes as input: a) set of objects (e.g., words) b) set of labels (e.g., so labels) c) initial probabilities for each objects possible labels d) the definition of an object os neighborhood (a set of other objects which influence the choice of os label) e) the definition of neighborhood features f) the definition of support function for an object label the influence of an object os neighborhood on its label is quantified using the support function.
</nextsent>
<nextsent>the support function computes the probability of the label being assigned to as function of os neighborhood features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1634">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>neither model explicitly addresses composite (feature of feature) or implicit features.
</prevsent>
<prevsent>other systems (morinaga et al, 2002; kushal et al, 2003) also look at web product reviews but they do not extract 345 opinions about particular product features.
</prevsent>
</prevsection>
<citsent citstr=" P99-1008 ">
opines use of meronymy lexico-syntactic patterns is similar to that of many others, from (berland and charniak, 1999) <papid> P99-1008 </papid>to (almuhareb and poesio, 2004).<papid> W04-3221 </papid></citsent>
<aftsection>
<nextsent>recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (turney, 2003; riloff et al, 2003; <papid> W03-0404 </papid>wiebe, 2000; hatzivassiloglou and mckeown, 1997).<papid> P97-1023 </papid></nextsent>
<nextsent>most recently, (takamura et al, 2005) <papid> P05-1017 </papid>reports on the use of spin models to infer the semantic orientation of words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1635">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>neither model explicitly addresses composite (feature of feature) or implicit features.
</prevsent>
<prevsent>other systems (morinaga et al, 2002; kushal et al, 2003) also look at web product reviews but they do not extract 345 opinions about particular product features.
</prevsent>
</prevsection>
<citsent citstr=" W04-3221 ">
opines use of meronymy lexico-syntactic patterns is similar to that of many others, from (berland and charniak, 1999) <papid> P99-1008 </papid>to (almuhareb and poesio, 2004).<papid> W04-3221 </papid></citsent>
<aftsection>
<nextsent>recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (turney, 2003; riloff et al, 2003; <papid> W03-0404 </papid>wiebe, 2000; hatzivassiloglou and mckeown, 1997).<papid> P97-1023 </papid></nextsent>
<nextsent>most recently, (takamura et al, 2005) <papid> P05-1017 </papid>reports on the use of spin models to infer the semantic orientation of words.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1636">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other systems (morinaga et al, 2002; kushal et al, 2003) also look at web product reviews but they do not extract 345 opinions about particular product features.
</prevsent>
<prevsent>opines use of meronymy lexico-syntactic patterns is similar to that of many others, from (berland and charniak, 1999) <papid> P99-1008 </papid>to (almuhareb and poesio, 2004).<papid> W04-3221 </papid></prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (turney, 2003; riloff et al, 2003; <papid> W03-0404 </papid>wiebe, 2000; hatzivassiloglou and mckeown, 1997).<papid> P97-1023 </papid></citsent>
<aftsection>
<nextsent>most recently, (takamura et al, 2005) <papid> P05-1017 </papid>reports on the use of spin models to infer the semantic orientation of words.</nextsent>
<nextsent>the papers global optimization approach and use of multiple sources of constraints on words semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1638">
<title id=" H05-1043.xml">extracting product features and opinions from reviews </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>opines use of meronymy lexico-syntactic patterns is similar to that of many others, from (berland and charniak, 1999) <papid> P99-1008 </papid>to (almuhareb and poesio, 2004).<papid> W04-3221 </papid></prevsent>
<prevsent>recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (turney, 2003; riloff et al, 2003; <papid> W03-0404 </papid>wiebe, 2000; hatzivassiloglou and mckeown, 1997).<papid> P97-1023 </papid></prevsent>
</prevsection>
<citsent citstr=" P05-1017 ">
most recently, (takamura et al, 2005) <papid> P05-1017 </papid>reports on the use of spin models to infer the semantic orientation of words.</citsent>
<aftsection>
<nextsent>the papers global optimization approach and use of multiple sources of constraints on words semantic orientation is similar to ours, but the mechanism differs and they currently omit the use of syntactic information.
</nextsent>
<nextsent>subjective phrases are used by (turney, 2002; <papid> P02-1053 </papid>pang and vaithyanathan, 2002; kushal et al, 2003; kim and hovy, 2004) <papid> C04-1200 </papid>and others in order to classify reviews or sentences as positive or negative.</nextsent>
<nextsent>so far, opines focus has been on extracting and analyzing opinion phrases corresponding to specific features in specific sentences, rather than on determining sentence or review polarity.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1643">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there is large body of psycho linguistic evidence which suggests that meaning can be extracted be-fore the end of sentence, and before the end of phrasal constituents (e.g. marslen-wilson 1973, tanenhaus et al 1990).
</prevsent>
<prevsent>there is also recent evi-dence suggesting that, during speech processing, partial interpretations can be built extremely ra-pidly, even before words are completed (spivey- knowlton et al 1994) 1.
</prevsent>
</prevsection>
<citsent citstr=" C94-2120 ">
there are also potential computational pplications for incremental inter-pretation, including early parse filtering using sta-tistics based on logical form plausibility, and in-terpretation of fragments of dialogues (a survey is provided by milward and cooper, 1994, <papid> C94-2120 </papid>hence-forth referred to as m&amp;:c).</citsent>
<aftsection>
<nextsent>in the current computational nd psycholingui- stic literature there are two main approaches to the incremental construction of logical forms.
</nextsent>
<nextsent>one approach is to use grammar with  non-standard  *this research was supported by the u.k. science and engineering research council, grant rr30718.
</nextsent>
<nextsent>i am grateful to patrick sturt, carl vogel, and the reviewers for comments on an earlier version.
</nextsent>
<nextsent>1spivey-knowlton et al reported 3 experiments.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1644">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>another showed on-line effects from adjectives and determiners during noun phrase processing.
</prevsent>
<prevsent>constituency, so that an initial fragment of sen-tence, such as john likes, can be treated as con-stituent, and hence be assigned type and se-mantics.
</prevsent>
</prevsection>
<citsent citstr=" P91-1010 ">
this approach is exemplified by com- bina tory categorial grammar, ccg (steedman 1991), <papid> P91-1010 </papid>which takes basic cg with just applica-tion, and adds various new ways of combining ele-ments together 2.</citsent>
<aftsection>
<nextsent>incremental interpretation can then be achieved using standard bottom-up shift reduce parser, working from left to right along the sentence.
</nextsent>
<nextsent>the alternative approach, exempli-fied by the work of stabler on top-down parsing (stabler 1991), and pulman on left-corner parsing (pulman 1986) is to associate semantics directly with the partial structures formed during top- down or left-corner parse.
</nextsent>
<nextsent>for example, syntax tree missing noun phrase, such as the following / \ np vp john / \ np  likes can be given semantics as function from enti-ties to truth values i.e. ax.
</nextsent>
<nextsent>l ikes( john,x) , with- out having to say that john likes is constituent.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1645">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, there are actually any number of such syntax trees corresponding to, for example, the first semantic representation, since the np and the can be arbitrarily far apart.
</prevsent>
<prevsent>the following tree is suitable for the sentence mary thinks john shaves but not for e.g. mary thinks john coming here was mistake.
</prevsent>
</prevsection>
<citsent citstr=" P83-1020 ">
s / \ np vp mary / \ s thinks / \ np vp^ john m&c; suggest various possibilities for packing the partial syntax trees, including using tree adjoi-ning grammar (joshi 1987) or description theory (marcus et al 1983).<papid> P83-1020 </papid></citsent>
<aftsection>
<nextsent>one further possibility is to choose single syntax tree, and to use destructive tree operations later in the parse a. the approach which we will adopt here is based on milward (1992), <papid> C92-4171 </papid>based on milward (1994).</nextsent>
<nextsent>partial syntax trees can be regarded as performing two main roles.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1646">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the following tree is suitable for the sentence mary thinks john shaves but not for e.g. mary thinks john coming here was mistake.
</prevsent>
<prevsent>s / \ np vp mary / \ s thinks / \ np vp^ john m&c; suggest various possibilities for packing the partial syntax trees, including using tree adjoi-ning grammar (joshi 1987) or description theory (marcus et al 1983).<papid> P83-1020 </papid></prevsent>
</prevsection>
<citsent citstr=" C92-4171 ">
one further possibility is to choose single syntax tree, and to use destructive tree operations later in the parse a. the approach which we will adopt here is based on milward (1992), <papid> C92-4171 </papid>based on milward (1994).</citsent>
<aftsection>
<nextsent>partial syntax trees can be regarded as performing two main roles.
</nextsent>
<nextsent>the first is to provide syntactic information which gui-des how the rest of the sentence can be integrated into the tree.
</nextsent>
<nextsent>the second is to provide basis for semantic representation.
</nextsent>
<nextsent>the first role can be cap-tured using syntactic types, where each type corre-sponds to potentially infinite number of partial syntax trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1648">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the second role can be captured by the parser constructing semantic representations directly.
</prevsent>
<prevsent>the general processing model therefore consists of transitions of the form: syntactic type -+ syntactic typei+ 1 semantic repi semantic repi+ 1 3this might turn out to be similar to one view of tree adjoining grammar, where adjunction adds into pre-existing well-formed tree structure.
</prevsent>
</prevsection>
<citsent citstr=" P84-1085 ">
it is also closer to some methods for incremental dap tation of discourse structures, where additions are allowed to the right-frontier of tree structure (e.g. polanyi and scha 1984).<papid> P84-1085 </papid></citsent>
<aftsection>
<nextsent>there are however problems with this kind of approach when features are considered (see e.g. vijay-shanker 1992).
</nextsent>
<nextsent>this provides state-transition dynamic model of processing, with each state being pair of syntactic type and semantic value.
</nextsent>
<nextsent>the main difference between our approach and that of milward (1992), <papid> C92-4171 </papid>that of milward (1994) is that it is based on more expressive grammar formalism, appli- cative categorial grammar, as opposed to lexi-calised dependency grammar.</nextsent>
<nextsent>applicative cate-gorial grammars allow categories to have argu-ments which are themselves functions (e.g. very can be treated as function of function, and gi-ven the type (n /n ) / (n /n ) when used as an adjec- tival modifier).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1651">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> applicative categorial grammar.  </section>
<citcontext>
<prevsection>
<prevsent>applicative categorial grammar is the most ba-sic form of categorial grammar, with just single combination rule corresponding to function appli-cation.
</prevsent>
<prevsent>it was first applied to linguistic descrip-tion by adjukiewicz and bar-hillel in the 1950s.
</prevsent>
</prevsection>
<citsent citstr=" P94-1021 ">
although it is still used for linguistic description (e.g. bouma and van noord, 1994), <papid> P94-1021 </papid>it has been somewhat overshadowed in recent years by hpsg (pollard and sag 1994), and by lambek cate-gorial grammars (lambek 1958).</citsent>
<aftsection>
<nextsent>it is therefore worth giving some brief indications of how it fits in with these developments.
</nextsent>
<nextsent>the first directed applicative cg was proposed by bar-hillel (1953).
</nextsent>
<nextsent>functional types included list of arguments to the left, and list of argu-ments to the right.
</nextsent>
<nextsent>translating bar-hillel nota-tion into feature based notation similar to that in hpsg (pollard and sag 1994), we obtain the following category for ditransitive verb such as put: s \] unp  r(np, pp  the list of arguments to the left are gathered un-der the feature, l, and those to the right, an np and pp in that order, under the feature r. bar-hillel employed single application rule, which corresponds to the following: 120 ix 1 l~ ..
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1652">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> applicative categorial grammar.  </section>
<citcontext>
<prevsection>
<prevsent>if is syntactic type, and and are lists.
</prevsent>
<prevsent>of categories, then application to the right is defined by the rules: 6one area where application based approaches to semantic ombination gain in simplicity over unifica-tion based approaches in providing semantics for functions of functions.
</prevsent>
</prevsection>
<citsent citstr=" P89-1005 ">
moore (1989) <papid> P89-1005 </papid>provides treat-ment of functions of functions in unification based approach, but only by explicitly incorporating lambda expressions.</citsent>
<aftsection>
<nextsent>pollard and sag (1994) deal with some functions of functions, such as non-intersective adjec-tives, by explicit set construction.
</nextsent>
<nextsent>7as discussed above, wh-movement requires ome- thing more like composition than application.
</nextsent>
<nextsent>a sim-ple syntax semantics interface can be retained if the same operation is used in both syntax and semantics.
</nextsent>
<nextsent>wh-arguments can be treated as similar to other ar-guments i.e. as lambda abstracted in the semantics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1653">
<title id=" E95-1017.xml">incremental interpretation of categorial grammar </title>
<section> applicative categorial grammar.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, packed recogniser based on similar, but much simpler, incremental parser for lexicalised de-pendency grammar has o(n a) time complexity (mil- ward 1994) and good practical performance, taking couple of seconds on 30 word sentences.
</prevsent>
<prevsent>14the usage of the term language tuning is perhaps broader here than its use in the psycho linguistic te- rature to refer to different structural preferences bet-ween languages e.g. for high versus low attachment (mitchell et al 1992).
</prevsent>
</prevsection>
<citsent citstr=" E95-1038 ">
125 there has already been some early work done on providing statistically based parsing using transi-tions between recursively structured syntactic a- tegories (tugwell 1995) <papid> E95-1038 </papid>15.</citsent>
<aftsection>
<nextsent>unlike simple markov process, there are potentially infinite number of states, so there is inevitably problem of sparse data.
</nextsent>
<nextsent>it is therefore necessary to make various generalisations over the states, for example by ig-noring the r2 lists.
</nextsent>
<nextsent>the full processing model can then be either serial, exploring the most highly ranked transiti-ons first (but allowing backtracking if the seman-tic plausibility of the current interpretation drops too low), or ranked parallel, exploring just the paths ranked highest according to the transition probabilities and semantic plausibility.
</nextsent>
<nextsent>6 conc lus ion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1654">
<title id=" E93-1019.xml">rule based acquisition and maintenance of lexical and semantic knowledge </title>
<section> re la ted  work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the effort in developing software tools for nlp has focused on user interfaces and acquisition of lexical databases from text corpora, but there are very few rule-based systems for knowledge mainte-nance.
</prevsent>
<prevsent>\[pin-ngern et al, 1989\] go beyond corpus analysis by augmenting the lexicm databases with knowledge supplied by human editors.
</prevsent>
</prevsection>
<citsent citstr=" C88-1032 ">
the word manager \[domenig, 1988\] <papid> C88-1032 </papid>is system for both acqui-sition and maintenance of morphological knowledge, but its main strength is its user-interface.</citsent>
<aftsection>
<nextsent>luke \[knight, 1991\] is an interactive system which uses several heuristics exploiting the relationship between linguistic and world knowledge to partially automate the acquisition process.
</nextsent>
<nextsent>more effort has gone into the acquisition and main-tenance of knowledge for expert-systems.
</nextsent>
<nextsent>4 the fo-cus of such efforts is to acquire smaller amounts of problem-solving knowledge, which is more complex than the semantic and lexicm knowledge used in es- trato.
</nextsent>
<nextsent>we intend to extend cool in three directions: by supporting the acquisition and maintenance of lexi-cal and semantic information for new languages, by adding rules for completely automating the acquis- tion of semantic lasses and lexical argument alter-nations \[bresnan, 1982; perlmutter, 1983\], and by 4for example, \[michalski, 1989\] contains everal arti-cles on these efforts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1655">
<title id=" H05-1006.xml">error detection using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a common observation is that the combination of sr-dependent features can only marginally improve the performance achieved by using only the best single feature (zhang and rudnicky, 2001; sarikaya et al, 2003).
</prevsent>
<prevsent>hence information sources beyond the sr system are desired in error detection.high-level linguistic knowledge is good candidate for additional information sources.
</prevsent>
</prevsection>
<citsent citstr=" P98-1028 ">
it can be extracted from the sr output via natural language processing, which compensates for the lack of high 41 level linguistic knowledge in typical sr system.a user study (brill et al, 1998) <papid> P98-1028 </papid>showed that humans can utilize linguistic knowledge at various levels to improve the sr output by selecting the best utterance hypotheses from n-best lists.</citsent>
<aftsection>
<nextsent>linguistic features from syntactic, semantic, and dialogue discourse analyses have proven their values in error detection in domain specific spoken dialogue systems, e.g.
</nextsent>
<nextsent>(rayner et al, 1994; <papid> H94-1040 </papid>carpenter et al, 2001;sarikaya et al, 2003).</nextsent>
<nextsent>however, few studies have investigated the merit of linguistic knowledge for error detection in dictation, domain-independent appli cation.transformation-based learning (tbl) is rule based learning method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1656">
<title id=" H05-1006.xml">error detection using linguistic features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it can be extracted from the sr output via natural language processing, which compensates for the lack of high 41 level linguistic knowledge in typical sr system.a user study (brill et al, 1998) <papid> P98-1028 </papid>showed that humans can utilize linguistic knowledge at various levels to improve the sr output by selecting the best utterance hypotheses from n-best lists.</prevsent>
<prevsent>linguistic features from syntactic, semantic, and dialogue discourse analyses have proven their values in error detection in domain specific spoken dialogue systems, e.g.</prevsent>
</prevsection>
<citsent citstr=" H94-1040 ">
(rayner et al, 1994; <papid> H94-1040 </papid>carpenter et al, 2001;sarikaya et al, 2003).</citsent>
<aftsection>
<nextsent>however, few studies have investigated the merit of linguistic knowledge for error detection in dictation, domain-independent appli cation.transformation-based learning (tbl) is rule based learning method.
</nextsent>
<nextsent>it has been used in error correction (mangu and padmanabhan, 2001) and error detection (skantze and edlund, 2004).
</nextsent>
<nextsent>the rules learned by tbl show good interpret ability as well as good performance.
</nextsent>
<nextsent>although statistical learning methods have been widely used in confidence annotation (carpenter et al, 2001; pao et al, 1998; chase, 1997), their results are difficult to interpret.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1661">
<title id=" H05-1006.xml">error detection using linguistic features </title>
<section> linguistic features.  </section>
<citcontext>
<prevsection>
<prevsent>for each output word, two sets of linguistic features are extracted: lexical features and syntactic features.
</prevsent>
<prevsent>3.1 lexical features.
</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
for each word w, the following lexical features are extracted: ? word: itself ? pos: part-of-speech tag from brills tagger (brill, 1995) ? <papid> J95-4004 </papid>syllables: number of syllables in w, estimated based on the distribution patterns of vowels and consonants?</citsent>
<aftsection>
<nextsent>position: the position of in the sentence: beginning, end, and middle 3.2 syntactic features.
</nextsent>
<nextsent>speech recognition errors may result in ungrammatical sentences under the assumption that the speaker follows grammar rules while speaking.
</nextsent>
<nextsent>such an assumption holds true especially for dictation application because the general purpose of dictation isto create understandable documents for communi cation.syntactic parsers are considered as the closest approximation to this intuition since there is still lackof semantic parsers for the general domain.
</nextsent>
<nextsent>more over, robust parsers are preferred so that an error in recognized sentence does not lead to failure in parsing the entire sentence.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1663">
<title id=" H05-1006.xml">error detection using linguistic features </title>
<section> error detection based on.  </section>
<citcontext>
<prevsection>
<prevsent>ci, ti, si, pi, li, di,hi, hli, and lli are possible values of the corresponding features.
</prevsent>
<prevsent>the initial state annotator initializes all the wordsas correct words.
</prevsent>
</prevsection>
<citsent citstr=" W99-0705 ">
a prolog based tbl tool, tbl (lager, 1999) <papid> W99-0705 </papid>2 is used in this study.</citsent>
<aftsection>
<nextsent>classification accuracy is adopted as the objective function.
</nextsent>
<nextsent>for each transformation, its positive effect(pe) is the number of words whose labels are correctly updated by applying it, and its negative effect (ne) is the number of words wrongly updated.two cut-off thresholds are used to select transformations with strong positive effects: net positive effect (pe ? ne), and the ratio of positive effect (pe/(pe +ne)).
</nextsent>
<nextsent>experiments were conducted at several levels.
</nextsent>
<nextsent>starting with transformation rules with word alone conditions, additional rules with local context and sentence context conditions were incorporated incrementally by enlarging the scope of the context.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1664">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" J98-1006 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1665">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1666">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" J04-1001 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1667">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" W04-2405 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1668">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" P05-1049 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1669">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" P00-1069 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1670">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1671">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" W97-0322 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1672">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, when combined with semi-supervised wsd algorithm, semi-supervised feature clustering outperforms other dimensionality reduction techniques, which indicates that using unlabeled data in learning process helps to improve the performance of feature clustering and sense disambiguation.
</prevsent>
<prevsent>this paper deals with word sense disambiguation (wsd) problem, which is to assign an appropriate sense to an occurrence of word in given context.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (leacock et al, 1998; <papid> J98-1006 </papid>towel andvoorheest, 1998), weakly supervised learning algorithms (dagan and itai, 1994; <papid> J94-4003 </papid>li and li, 2004; <papid> J04-1001 </papid>mihalcea, 2004; <papid> W04-2405 </papid>niu et al, 2005; <papid> P05-1049 </papid>park et al, 2000; <papid> P00-1069 </papid>yarowsky, 1995), <papid> P95-1026 </papid>unsupervised learning algorithms (or word sense discrimination) (pedersen and bruce,1997; <papid> W97-0322 </papid>schutze, 1998), and knowledge based algorithms (lesk, 1986; mccarthy et al, 2004).<papid> P04-1036 </papid></citsent>
<aftsection>
<nextsent>in general, the most common approaches start by evaluating the co-occurrence matrix of features versus contexts of instances of ambiguous word, given sense-tagged training data for this target word.
</nextsent>
<nextsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</nextsent>
<nextsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</nextsent>
<nextsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1673">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</prevsent>
<prevsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</prevsent>
</prevsection>
<citsent citstr=" C02-1039 ">
supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</citsent>
<aftsection>
<nextsent>latent semantic indexing (lsi) studied in (schutze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (niu et al,2004).<papid> P04-1080 </papid></nextsent>
<nextsent>but little work is done on using feature clustering to conduct dimensionality reduction for wsd.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1674">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>asa result, contexts are usually represented in high dimensional sparse feature space, which is far from optimal for many classification algorithms.
</prevsent>
<prevsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</prevsent>
</prevsection>
<citsent citstr=" W02-1006 ">
supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</citsent>
<aftsection>
<nextsent>latent semantic indexing (lsi) studied in (schutze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (niu et al,2004).<papid> P04-1080 </papid></nextsent>
<nextsent>but little work is done on using feature clustering to conduct dimensionality reduction for wsd.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1677">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, processing data lying in high-dimensional feature space requires large amount of memory and cpu time, which limits the scala bility of wsd model to very large datasets or incorporation ofwsd model into natural language processing sys tems.standard dimentionality reduction techniques include (1) supervised feature selection and supervised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic indexing, and unsupervised feature clustering when only unlabeled data is available.
</prevsent>
<prevsent>supervised feature selection improves the performance of an examplar based learning algorithm over senseval2 data (mihalcea, 2002), <papid> C02-1039 </papid>naive bayes and decision tree over senseval-1 and senseval-2 data (lee and ng, 2002), but feature selection does not improve svm and ada boost over senseval-1 and senseval-2 data (lee and ng, 2002) <papid> W02-1006 </papid>forword sense disambiguation.</prevsent>
</prevsection>
<citsent citstr=" P04-1080 ">
latent semantic indexing (lsi) studied in (schutze, 1998) improves the performance of sense discrimination, while unsupervised feature selection also improves the performance of word sense discrimination (niu et al,2004).<papid> P04-1080 </papid></citsent>
<aftsection>
<nextsent>but little work is done on using feature clustering to conduct dimensionality reduction for wsd.
</nextsent>
<nextsent>this paper will describe an application of feature 907 clustering technique to wsd task.
</nextsent>
<nextsent>feature clustering has been extensively studied for the benefit of text categorization and documentclustering.
</nextsent>
<nextsent>in the context of text categorization, supervised feature clustering algorithms (baker and mccallum, 1998; bekkerman et al, 2003; slonim and tishby, 2001) usually cluster words into groups based on the distribution of class labels over features, which can compress the feature space much more aggressively while still maintaining state of the art classification accuracy.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1682">
<title id=" H05-1114.xml">a semi supervised feature clustering algorithm with application to word sense disambiguation </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>latent semantic indexing technique(lsi) is used to perform factor analysis in mf,x before calculating the distance between features in step 2.
</prevsent>
<prevsent>4.1 experiment design.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
for empirical study of dimensionality reduction techniques on wsd task, we evaluated five dimensionality reduction algorithms on the data in english lexical sample (els) task of senseval-3 (mihal cea et al, 2004)(<papid> W04-0807 </papid>including all the 57 english words ) 1: supervised feature clustering (sufc) (baker and mccallum, 1998; bekkerman et al, 2003; slonim 1available at http://www.senseval.org/senseval3 909 and tishby, 2001), iterative double clustering (idc) (el-yaniv and souroujon, 2001), semi-supervisedfeature clustering (semifc) (our algorithm), supervised feature selection (sufs) (forman, 2003), and latent semantic indexing (lsi) (deerwester et. al., 1990) 2.</citsent>
<aftsection>
<nextsent>we used sib algorithm 3 to cluster features infl into groups based on the distribution of class labels associated with each feature.
</nextsent>
<nextsent>this procedure can be considered as our re-implementation of supervised feature clustering.
</nextsent>
<nextsent>after feature clustering, examples can be represented as vectors over feature clusters.
</nextsent>
<nextsent>idc is an extension of double clustering method(dc) (slonim and tishby, 2000), which performs iterations of dc.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1687">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, chinese ner is far from mature.
</prevsent>
<prevsent>for example, the performance (precision, recall) of the best chinese ner system in met-2 is (66%, 92%), (89%, 91%), (89%, 88%) for pn, ln and on respectively.
</prevsent>
</prevsection>
<citsent citstr=" M98-1014 ">
recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.
</nextsent>
<nextsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</nextsent>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1688">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, chinese ner is far from mature.
</prevsent>
<prevsent>for example, the performance (precision, recall) of the best chinese ner system in met-2 is (66%, 92%), (89%, 91%), (89%, 88%) for pn, ln and on respectively.
</prevsent>
</prevsection>
<citsent citstr=" M95-1012 ">
recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.
</nextsent>
<nextsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</nextsent>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1689">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, chinese ner is far from mature.
</prevsent>
<prevsent>for example, the performance (precision, recall) of the best chinese ner system in met-2 is (66%, 92%), (89%, 91%), (89%, 88%) for pn, ln and on respectively.
</prevsent>
</prevsection>
<citsent citstr=" W98-1120 ">
recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.
</nextsent>
<nextsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</nextsent>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1690">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, chinese ner is far from mature.
</prevsent>
<prevsent>for example, the performance (precision, recall) of the best chinese ner system in met-2 is (66%, 92%), (89%, 91%), (89%, 88%) for pn, ln and on respectively.
</prevsent>
</prevsection>
<citsent citstr=" A97-1029 ">
recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.
</nextsent>
<nextsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</nextsent>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1691">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>however, chinese ner is far from mature.
</prevsent>
<prevsent>for example, the performance (precision, recall) of the best chinese ner system in met-2 is (66%, 92%), (89%, 91%), (89%, 88%) for pn, ln and on respectively.
</prevsent>
</prevsection>
<citsent citstr=" M98-1021 ">
recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></citsent>
<aftsection>
<nextsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.
</nextsent>
<nextsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</nextsent>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1692">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>recently, approaches for ner are shift away from handcrafted rules[grishman, et al 1995] [krupka, et al 1998][black et al 1998] <papid> M98-1014 </papid>towards machine learning algorithms, i.e. unsupervised model like dl-cotrain, coboost[collins, 1999, 2002], supervised learning like error-driven [ab erdeen, et al 1995], <papid> M95-1012 </papid>decision tree [sekine, et al 1998], <papid> W98-1120 </papid>hmm[bikel, et al 1997] <papid> A97-1029 </papid>and maximum entropy[borthwick, et al 1999][mikheev, et al.1998].<papid> M98-1021 </papid></prevsent>
<prevsent>similarly, the models for chinese ner can also be divided into two categories: individual model and integrated model.</prevsent>
</prevsection>
<citsent citstr=" M98-1017 ">
individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</citsent>
<aftsection>
<nextsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></nextsent>
<nextsent>integrated model[sun, et al 2002] [<papid> C02-1012 </papid>zhang, et al 2003][yu, et al. 1998][<papid> M98-1016 </papid>chua, et al 2002] deals with all kinds of entities in unified statistical framework.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1694">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</prevsent>
<prevsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" C02-1012 ">
integrated model[sun, et al 2002] [<papid> C02-1012 </papid>zhang, et al 2003][yu, et al. 1998][<papid> M98-1016 </papid>chua, et al 2002] deals with all kinds of entities in unified statistical framework.</citsent>
<aftsection>
<nextsent>most of these integrated models can be viewed as hmm model.
</nextsent>
<nextsent>the differences among them are the definition of state and the features used in entity model and context model.
</nextsent>
<nextsent>in fact, ner model recognizes named entities through mining the intrinsic features in the entities and the contextual features around the entities.
</nextsent>
<nextsent>most of existing approaches employ either coarse particle features, like pos and role[zhang, et al 2003], or fine particle features like word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1695">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>individual model[chen, et al 1998][<papid> M98-1017 </papid>sun, et al 1994][zheng, et al 2000] consists of several sub models, each of them deals with kind of entities.</prevsent>
<prevsent>for example, the recognition of pn may be statistical-based model, while ln and on may be rule based model like [chen, et al 1998].<papid> M98-1017 </papid></prevsent>
</prevsection>
<citsent citstr=" M98-1016 ">
integrated model[sun, et al 2002] [<papid> C02-1012 </papid>zhang, et al 2003][yu, et al. 1998][<papid> M98-1016 </papid>chua, et al 2002] deals with all kinds of entities in unified statistical framework.</citsent>
<aftsection>
<nextsent>most of these integrated models can be viewed as hmm model.
</nextsent>
<nextsent>the differences among them are the definition of state and the features used in entity model and context model.
</nextsent>
<nextsent>in fact, ner model recognizes named entities through mining the intrinsic features in the entities and the contextual features around the entities.
</nextsent>
<nextsent>most of existing approaches employ either coarse particle features, like pos and role[zhang, et al 2003], or fine particle features like word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1696">
<title id=" H05-1054.xml">chinese named entity recognition with multiple features </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>our idea is that coarse particle features should be integrated into fine particle features to overcome the disadvantages of them.
</prevsent>
<prevsent>however, most systems do not combine them and especially ignore the impact of pos.
</prevsent>
</prevsection>
<citsent citstr=" P00-1015 ">
inspired by the algorithms of identifying basenp and chunk[xun, et al 2000], <papid> P00-1015 </papid>we propose hybrid ner model which emphasizes on combining coarse particle features (pos model) with fine particle features (word model).</citsent>
<aftsection>
<nextsent>though the hybrid model can overcome the disadvantages of the word model and the pos model, there are still some problems in such framework.
</nextsent>
<nextsent>data sparseness still exists and very large searching space in decoding will influence efficiency.
</nextsent>
<nextsent>our idea is that heuristic human knowledge can not only improve the time efficiency, but also solve the data sparseness problem to some extent by restricting the generation of entity candidates.
</nextsent>
<nextsent>so we intend to incorporate human knowledge into the statistical model to improve efficiency and effectivity of the hybrid model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1699">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>when the lexicon can assure high coverage, unknown word guessing can be viewed as decision taken upon the pos of open-class words (i.e., noun, verb, adjective, adverb or participle).
</prevsent>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
</prevsection>
<citsent citstr=" E95-1022 ">
on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></citsent>
<aftsection>
<nextsent>on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></nextsent>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1700">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" A88-1019 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1701">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" A92-1018 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1702">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" P89-1015 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1703">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1704">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" W96-0102 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1705">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>towards the disambiguation of pos tags, two main approaches have been followed.
</prevsent>
<prevsent>on one hand, according to the linguistic approach, experts encode handcrafted rules or constraints based on abstractions derived from language paradigms (usually with the aid of corpora) (green and rubin, 1971; voutilainen 1995).<papid> E95-1022 </papid></prevsent>
</prevsection>
<citsent citstr=" C94-1027 ">
on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></citsent>
<aftsection>
<nextsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</nextsent>
<nextsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></nextsent>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1707">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, according to the data-driven approach, frequency-based language model is acquired from corpora and has the forms of n- grams (church, 1988; <papid> A88-1019 </papid>cutting et al, 1992), <papid> A92-1018 </papid>rules (hindle, 1989; <papid> P89-1015 </papid>brill, 1995), <papid> J95-4004 </papid>decision trees (cardie, 1994; daelemans et al, 1996) <papid> W96-0102 </papid>or neural networks (schmid, 1994).<papid> C94-1027 </papid></prevsent>
<prevsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></citsent>
<aftsection>
<nextsent>more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></nextsent>
<nextsent>this paper presents pos tagger for modem greek (m. greek), highly inflectional language, and focuses on data-driven approach for the induction of decision trees used as disambiguation/guessing devices.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1708">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in order to increase their robusmess, most pos taggers include guesser, which tries to extract the pos of words not present in the lexicon.
</prevsent>
<prevsent>as common strategy, pos guess ers examine the endings of unknown words (cutting et al 1992) <papid> A92-1018 </papid>along with their capitalization, or consider the distribution of unknown words over specific parts-of-speech (weischedel et al, 1993).<papid> J93-2006 </papid></prevsent>
</prevsection>
<citsent citstr=" C96-2130 ">
more sophisticated guess ers further examine the prefixes of unknown words (mikheev, 1996) <papid> C96-2130 </papid>and the categories of contextual tokens (brill, 1995; <papid> J95-4004 </papid>daelemans et al, 1996).<papid> W96-0102 </papid></citsent>
<aftsection>
<nextsent>this paper presents pos tagger for modem greek (m. greek), highly inflectional language, and focuses on data-driven approach for the induction of decision trees used as disambiguation/guessing devices.
</nextsent>
<nextsent>based on high-coverage 1 lexicon, we prepared tagged corpus capable of showing off the behavior of all pos ambiguity schemes present in m. greek (e.g., pronoun-clitic-article, pronoun-clitic, adjective-adverb, verb-noun, etc.), as well as the characteristics of unknown words.
</nextsent>
<nextsent>consequently, we used the corpus for the induction of decision trees, which, along with 1 at present, he lexicon is capable of assigning full.
</nextsent>
<nextsent>morphosyntactic attributes (i.e., pos, number, gender, case, person, tense, voice, mood) to -870.000 greek word-forms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1719">
<title id=" E99-1018.xml">pos disambiguation and unknown word guessing with decision trees </title>
<section> discussion and future goals.  </section>
<citcontext>
<prevsection>
<prevsent>meet he needs of the specific language problem.
</prevsent>
<prevsent>based on these statements, we combined high- coverage lexicon and set of empirically induced decision trees into pos tagger achieving ~5,5% error rate for pos disambiguation and ~16% error rate for unknown word guessing.
</prevsent>
</prevsection>
<citsent citstr=" J95-2001 ">
the decision-tree approach outperforms both the naive approach of assigning the most frequent pos, as well as the ~20% error rate obtained by the n-gram tagger for m. greek presented in (dermatas and kokkinakis, 1995).<papid> J95-2001 </papid></citsent>
<aftsection>
<nextsent>comparing our tree-induction algorithm and igtree, the algorithm used in mbt (daelemans et al, 1996), <papid> W96-0102 </papid>their main difference is that igtree produces oblivious decision trees by supplying an priori ordered list of best features instead of re-computing the best feature during each branching, which is our case.</nextsent>
<nextsent>after applying igtree to the datasets described in section 3, we measured similar performance (-7% error rate for disambiguation and -17% for guessing).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1721">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this salient part of the physical world will prime what users tend to communicate in speech and in turn can be used to constrain hypotheses for spoken language understanding, thus improving overall input interpretation.
</prevsent>
<prevsent>our experimental results have indicated the potential of this approach in reducing word error rate and improving concept identification in multimodal conversation.
</prevsent>
</prevsection>
<citsent citstr=" P02-1048 ">
multimodal conversational systems promote more natural and effective human machine communication by allowing users to interact with systems through multiple modalities such as speech and gesture (cohen et al, 1996; johnston et al, 2002; <papid> P02-1048 </papid>pieraccini et al, 2004).</citsent>
<aftsection>
<nextsent>despite recent advances, interpreting what users communicate to the system is still significant challenge due to insufficient recognition (e.g., speech recognition) and understanding (e.g., language understanding) performance.
</nextsent>
<nextsent>significant improvement in the robustness of multimodal interpretation is crucial if multimodal systems are to be effective and practical for real world applications.
</nextsent>
<nextsent>previous studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (cassell et al 1994).
</nextsent>
<nextsent>fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (oviatt 1999).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1722">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (cassell et al 1994).
</prevsent>
<prevsent>fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (oviatt 1999).
</prevsent>
</prevsection>
<citsent citstr=" P98-1102 ">
for semantically-rich modalities such as speech and pen based gesture, mutual disambiguation usually happens at the fusion stage where partial semantic representations from individual modalities are disambiguated and combined into an overall interpretation (johnston 1998, <papid> P98-1102 </papid>chai et al, 2004<papid> P04-1001 </papid>a).</citsent>
<aftsection>
<nextsent>one problem is that some critical but low probability information from individual modalities (e.g., recognized alternatives with low probabilities) may never reach the fusion stage.
</nextsent>
<nextsent>therefore, this paper addresses how to use information from one modality (e.g., deictic gesture) to directly influence the semantic processing of another modality (e.g., spoken language understanding) even before the fusion stage.
</nextsent>
<nextsent>in particular we present new salience driven approach that uses gesture to influence spoken language understanding.
</nextsent>
<nextsent>this approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on graphical interface can signal part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1723">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>previous studies have shown that, in multimodal conversation, multiple modalities tend to complement each other (cassell et al 1994).
</prevsent>
<prevsent>fusing two or more modalities can be an effective means of reducing recognition uncertainties, for example, through mutual disambiguation (oviatt 1999).
</prevsent>
</prevsection>
<citsent citstr=" P04-1001 ">
for semantically-rich modalities such as speech and pen based gesture, mutual disambiguation usually happens at the fusion stage where partial semantic representations from individual modalities are disambiguated and combined into an overall interpretation (johnston 1998, <papid> P98-1102 </papid>chai et al, 2004<papid> P04-1001 </papid>a).</citsent>
<aftsection>
<nextsent>one problem is that some critical but low probability information from individual modalities (e.g., recognized alternatives with low probabilities) may never reach the fusion stage.
</nextsent>
<nextsent>therefore, this paper addresses how to use information from one modality (e.g., deictic gesture) to directly influence the semantic processing of another modality (e.g., spoken language understanding) even before the fusion stage.
</nextsent>
<nextsent>in particular we present new salience driven approach that uses gesture to influence spoken language understanding.
</nextsent>
<nextsent>this approach is based on the observation that, during multimodal conversation, information from deictic gestures (e.g., point or circle) on graphical interface can signal part of the physical world (i.e., representation of the domain and task) of the application which is salient during the communication.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1729">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>related work on salience modeling we first give brief overview on the notion of salience and how salience modeling is applied in earlier work on natural language and multimodal language processing.
</prevsent>
<prevsent>linguistic salience describes the accessibility of entities in speaker/hearers memory and its implication in language production and interpretation.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
many theories on linguistic salience have been developed, including how the salience of entities affects the form of referring expressions as in the givenness hierarchy (gundel et al, 1993) and the local coherence of discourse as in the centering theory (grosz et al, 1995).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>salience modeling is used for both language generation and language interpretation; the latter is more relevant to our work.
</nextsent>
<nextsent>most salience-based interpretation has focused on reference resolution for both linguistic referring expressions (e.g., pronouns) (lappin and leass 1995) and multimodal expressions (hul et al 1995; eisenstein and christoudias 2004).<papid> N04-1004 </papid></nextsent>
<nextsent>speech input gesture input speech recognition language understanding gesture recognizer multimodal fusion semantic representation gesture understanding semantic representation semantic representation what is the price of this painting point to position on the screen intent: ask type: painting aspect: price type: painting id: p23 intent: ask type: painting aspect: price id: p23 type: wall id: w1 (a) (b) (c) figure 1: semantics-based multimodal interpretation visual salience considers an object salient when it attracts users visual attention more than others.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1730">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>many theories on linguistic salience have been developed, including how the salience of entities affects the form of referring expressions as in the givenness hierarchy (gundel et al, 1993) and the local coherence of discourse as in the centering theory (grosz et al, 1995).<papid> J95-2003 </papid></prevsent>
<prevsent>salience modeling is used for both language generation and language interpretation; the latter is more relevant to our work.</prevsent>
</prevsection>
<citsent citstr=" N04-1004 ">
most salience-based interpretation has focused on reference resolution for both linguistic referring expressions (e.g., pronouns) (lappin and leass 1995) and multimodal expressions (hul et al 1995; eisenstein and christoudias 2004).<papid> N04-1004 </papid></citsent>
<aftsection>
<nextsent>speech input gesture input speech recognition language understanding gesture recognizer multimodal fusion semantic representation gesture understanding semantic representation semantic representation what is the price of this painting point to position on the screen intent: ask type: painting aspect: price type: painting id: p23 intent: ask type: painting aspect: price id: p23 type: wall id: w1 (a) (b) (c) figure 1: semantics-based multimodal interpretation visual salience considers an object salient when it attracts users visual attention more than others.
</nextsent>
<nextsent>the cause of such attention depends on many factors including user intention, familiarity, and physical characteristics of objects.
</nextsent>
<nextsent>for example, an object may be salient when it has some properties the others do not have, such as it is the only one that is highlighted, or the only one of certain size, category, or color 218 (landragin et al, 2001).
</nextsent>
<nextsent>visual salience can also be useful in input interpretation, for example, for multimodal reference resolution (kehler 2000) and cross-modal coreference interpretation (byron et al, 2005).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1749">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>given an observed speech utterance o, the goal of speech recognition is to find sequence of words w* so that p , where p(o|w) is the acoustic model and p(w) is the language model.
</prevsent>
<prevsent>in traditional speech recognition systems, the acoustic model provides the probability of observing the acoustic features given hypothesized word sequences and the language model provides the probability of sequence of words.
</prevsent>
</prevsection>
<citsent citstr=" A00-2014 ">
the language model is computed as follows: * arg max ( | ) ( )o w= )|()...|()|()()( 112131211 ?= nnn wwpwwwpwwpwpwp using the markov assumption, the language model can be approximated by bigram model as in: ? = ?= i ii wwpwp 1 11 )|()( to improve the speech understanding results for spoken language interfaces, many systems have applied loosely-integrated approach which decouples the language model from the acoustic model (zue et al, 1991, harper et al, 2000).<papid> A00-2014 </papid></citsent>
<aftsection>
<nextsent>this allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (gildea and hofmann 1999), syntactic or semantic labels (heeman 1999), <papid> W99-0617 </papid>and linguistic structures (chelba and jelinek 2000, wang and harper 2002).<papid> W02-1031 </papid></nextsent>
<nextsent>recently, we have seen work on language understanding based on environment (schuler 2003) <papid> P03-1067 </papid>and language modeling using visual context (roy and mukherjee 2005).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1750">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in traditional speech recognition systems, the acoustic model provides the probability of observing the acoustic features given hypothesized word sequences and the language model provides the probability of sequence of words.
</prevsent>
<prevsent>the language model is computed as follows: * arg max ( | ) ( )o w= )|()...|()|()()( 112131211 ?= nnn wwpwwwpwwpwpwp using the markov assumption, the language model can be approximated by bigram model as in: ? = ?= i ii wwpwp 1 11 )|()( to improve the speech understanding results for spoken language interfaces, many systems have applied loosely-integrated approach which decouples the language model from the acoustic model (zue et al, 1991, harper et al, 2000).<papid> A00-2014 </papid></prevsent>
</prevsection>
<citsent citstr=" W99-0617 ">
this allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (gildea and hofmann 1999), syntactic or semantic labels (heeman 1999), <papid> W99-0617 </papid>and linguistic structures (chelba and jelinek 2000, wang and harper 2002).<papid> W02-1031 </papid></citsent>
<aftsection>
<nextsent>recently, we have seen work on language understanding based on environment (schuler 2003) <papid> P03-1067 </papid>and language modeling using visual context (roy and mukherjee 2005).</nextsent>
<nextsent>our salience driven approach is inspired by this earlier work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1751">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in traditional speech recognition systems, the acoustic model provides the probability of observing the acoustic features given hypothesized word sequences and the language model provides the probability of sequence of words.
</prevsent>
<prevsent>the language model is computed as follows: * arg max ( | ) ( )o w= )|()...|()|()()( 112131211 ?= nnn wwpwwwpwwpwpwp using the markov assumption, the language model can be approximated by bigram model as in: ? = ?= i ii wwpwp 1 11 )|()( to improve the speech understanding results for spoken language interfaces, many systems have applied loosely-integrated approach which decouples the language model from the acoustic model (zue et al, 1991, harper et al, 2000).<papid> A00-2014 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1031 ">
this allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (gildea and hofmann 1999), syntactic or semantic labels (heeman 1999), <papid> W99-0617 </papid>and linguistic structures (chelba and jelinek 2000, wang and harper 2002).<papid> W02-1031 </papid></citsent>
<aftsection>
<nextsent>recently, we have seen work on language understanding based on environment (schuler 2003) <papid> P03-1067 </papid>and language modeling using visual context (roy and mukherjee 2005).</nextsent>
<nextsent>our salience driven approach is inspired by this earlier work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1752">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the language model is computed as follows: * arg max ( | ) ( )o w= )|()...|()|()()( 112131211 ?= nnn wwpwwwpwwpwpwp using the markov assumption, the language model can be approximated by bigram model as in: ? = ?= i ii wwpwp 1 11 )|()( to improve the speech understanding results for spoken language interfaces, many systems have applied loosely-integrated approach which decouples the language model from the acoustic model (zue et al, 1991, harper et al, 2000).<papid> A00-2014 </papid></prevsent>
<prevsent>this allows the development of powerful language models independent of the acoustic model, for example, utilizing topics of the utterances (gildea and hofmann 1999), syntactic or semantic labels (heeman 1999), <papid> W99-0617 </papid>and linguistic structures (chelba and jelinek 2000, wang and harper 2002).<papid> W02-1031 </papid></prevsent>
</prevsection>
<citsent citstr=" P03-1067 ">
recently, we have seen work on language understanding based on environment (schuler 2003) <papid> P03-1067 </papid>and language modeling using visual context (roy and mukherjee 2005).</citsent>
<aftsection>
<nextsent>our salience driven approach is inspired by this earlier work.
</nextsent>
<nextsent>here, we do not address the acoustic model of speech recognition, but rather incorporate the salience distribution for language modeling.
</nextsent>
<nextsent>in particular, our focus is on investigating the effect of incorporating additional information from other modalities (e.g., gesture) with traditional language models.
</nextsent>
<nextsent>primed language model the calculated salience distribution is used to prime the language model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1753">
<title id=" H05-1028.xml">a salience driven approach to robust input interpretation in multimodal conversational systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in particular, our focus is on investigating the effect of incorporating additional information from other modalities (e.g., gesture) with traditional language models.
</prevsent>
<prevsent>primed language model the calculated salience distribution is used to prime the language model.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
more specifically, we use class-based bigram model from (brown et al 1992): )|()|()|( <papid> J92-4003 </papid>11 ??</citsent>
<aftsection>
<nextsent>= iiiiii ccpcwpwwp (3) in equation (3), ci is the class of the word wi, which could be syntactic class or semantic class.
</nextsent>
<nextsent>is the class transition probability, which reflects the grammatical formation of utterances.
</nextsent>
<nextsent>is the word class probability which measures the probability of seeing word wi given class ci.
</nextsent>
<nextsent>the class-based n-gram model can make better use of limited training data by clustering words into classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1767">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>event recognition is also at the core of question answering, 1this work was supported by grant from the advanced research and development activity in information technology(arda), u.s. government entity which sponsors and promotes research of import to the intelligence community which includes but is not limited to the cia, dia, nsa, nima, and nro.
</prevsent>
<prevsent>since input questions touch on events and situations in the world (states, actions, properties, etc.), as they are reported in the text.
</prevsent>
</prevsection>
<citsent citstr=" W02-1033 ">
in this field as well, the use of pre-defined sets of relation patterns has proved fairly reliable, particularly in the case of factoid type queries (brill et al , 2002; <papid> W02-1033 </papid>ravichandran and hovy, 2002; <papid> P02-1006 </papid>hovy et al , 2002; soubbotin and soubbotin, 2002).</citsent>
<aftsection>
<nextsent>nonetheless, such an approach is not sensitive to certain contextual elements that may be fundamental for returning the appropriate answer.
</nextsent>
<nextsent>this is for instance the case in reporting or attempting contexts.given the passage in (1a), pattern-generated answer to question (1b) would be (1c).
</nextsent>
<nextsent>similarly, disregarding the reporting context in example (2) could erroneously lead to concluding that no one from the white house was involved in the watergate affair.
</nextsent>
<nextsent>(1) a. of the 14 known ways to reach the summit, only the east ridge route has never been successfully climbed since george mallory and andrew sandy?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1768">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>event recognition is also at the core of question answering, 1this work was supported by grant from the advanced research and development activity in information technology(arda), u.s. government entity which sponsors and promotes research of import to the intelligence community which includes but is not limited to the cia, dia, nsa, nima, and nro.
</prevsent>
<prevsent>since input questions touch on events and situations in the world (states, actions, properties, etc.), as they are reported in the text.
</prevsent>
</prevsection>
<citsent citstr=" P02-1006 ">
in this field as well, the use of pre-defined sets of relation patterns has proved fairly reliable, particularly in the case of factoid type queries (brill et al , 2002; <papid> W02-1033 </papid>ravichandran and hovy, 2002; <papid> P02-1006 </papid>hovy et al , 2002; soubbotin and soubbotin, 2002).</citsent>
<aftsection>
<nextsent>nonetheless, such an approach is not sensitive to certain contextual elements that may be fundamental for returning the appropriate answer.
</nextsent>
<nextsent>this is for instance the case in reporting or attempting contexts.given the passage in (1a), pattern-generated answer to question (1b) would be (1c).
</nextsent>
<nextsent>similarly, disregarding the reporting context in example (2) could erroneously lead to concluding that no one from the white house was involved in the watergate affair.
</nextsent>
<nextsent>(1) a. of the 14 known ways to reach the summit, only the east ridge route has never been successfully climbed since george mallory and andrew sandy?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1769">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>non-factoid questions introduce an even higher level of difficulty.
</prevsent>
<prevsent>unlike factoid questions, thereis no simple or unique answer, but more or less satisfactory ones instead.
</prevsent>
</prevsection>
<citsent citstr=" W03-1206 ">
in many cases, they involve dealing with several events, or identifying andrea soning about certain relations among events which are only partially stated in the source documents (such as temporal and causal ones), all of which makes the pattern-based approach less suitable for the task (small et al , 2003, <papid> W03-1206 </papid>soricut and brill, 2004).<papid> N04-1008 </papid>temporal information in particular plays significant role in the context of question answering systems (pustejovsky et al , forthcoming).</citsent>
<aftsection>
<nextsent>the question in (3), for instance, requires identifying set of events related to the referred killing of peasants in mexico, and subsequently ordering them along temporal axis.
</nextsent>
<nextsent>(3) what happened in chiapas, mexico, after the killing of 45 peasants in acteal?
</nextsent>
<nextsent>reasoning about events in intensional contexts, or with event-ordering relations such as temporality and causality, is requisite for any open-domain qasystem aiming at both factoid and non-factoid questions.
</nextsent>
<nextsent>as first step, this involves the identification of all relevant events reported in the source documents, so that later processing stages can locate in tens ional context boundaries and temporal relations among these events.in this article, we present evita, tool for recognizing events in natural language texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1770">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>non-factoid questions introduce an even higher level of difficulty.
</prevsent>
<prevsent>unlike factoid questions, thereis no simple or unique answer, but more or less satisfactory ones instead.
</prevsent>
</prevsection>
<citsent citstr=" N04-1008 ">
in many cases, they involve dealing with several events, or identifying andrea soning about certain relations among events which are only partially stated in the source documents (such as temporal and causal ones), all of which makes the pattern-based approach less suitable for the task (small et al , 2003, <papid> W03-1206 </papid>soricut and brill, 2004).<papid> N04-1008 </papid>temporal information in particular plays significant role in the context of question answering systems (pustejovsky et al , forthcoming).</citsent>
<aftsection>
<nextsent>the question in (3), for instance, requires identifying set of events related to the referred killing of peasants in mexico, and subsequently ordering them along temporal axis.
</nextsent>
<nextsent>(3) what happened in chiapas, mexico, after the killing of 45 peasants in acteal?
</nextsent>
<nextsent>reasoning about events in intensional contexts, or with event-ordering relations such as temporality and causality, is requisite for any open-domain qasystem aiming at both factoid and non-factoid questions.
</nextsent>
<nextsent>as first step, this involves the identification of all relevant events reported in the source documents, so that later processing stages can locate in tens ional context boundaries and temporal relations among these events.in this article, we present evita, tool for recognizing events in natural language texts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1771">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> evita, an event recognition tool.  </section>
<citcontext>
<prevsection>
<prevsent>evita (events in text analyzer?)
</prevsent>
<prevsent>is an event recognition system developed under the arda-funded tarsqi research framework.
</prevsent>
</prevsection>
<citsent citstr=" P05-3021 ">
tarsqi is devoted to two complementary lines of work: (1) establishing specification language, timeml, aimedat capturing the richness of temporal and event related information in language (pustejovsky et al , 2003a, forthcoming), and (2) the construction of aset of tools that perform tasks of identifying, tagging, and reasoning about even tive and temporal information in natural language texts (pustejovsky and gaizauskas, forthcoming, mani, 2005; mani and schiffman, forthcoming; verhagen, 2004; verhagen et al , 2005; <papid> P05-3021 </papid>verhagen and knippen, forthcoming).within tarsqis framework, evitas role is locating and tagging all event-referring expressions in the input text that can be temporally ordered.</citsent>
<aftsection>
<nextsent>evita combines linguistic- and statistically-based techniques to better address all subtasks of event recognition.
</nextsent>
<nextsent>for example, the module devoted to recognizing temporal information that is expressed through the morphology of certain event expressions(such as tense and aspect) uses grammatical information (see section 2.4), whereas disambiguating nouns that can have both even tive and non-eventive interpretations is carried out by statistical module (section 2.3).
</nextsent>
<nextsent>the functionality of evita breaks down into twoparts: event identification and analysis of the event based grammatical features that are relevant for temporal reasoning purposes.
</nextsent>
<nextsent>both tasks relyon preprocessing step which performs part-of-speech tag 701ging and chunking, and on module for clustering together chunks that refer to the same event.in the following subsection we provide the linguistic assumptions informing evita.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1772">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> evita, an event recognition tool.  </section>
<citcontext>
<prevsection>
<prevsent>event-denoting expressions are found in wide range of syntactic expressions, such as finite clauses (that no-one from the white house was involved), non finite clauses (to climb everest), noun phrases headed by nominalizations (the young industrys rapid growth, several anti-war demonstrations) or event-referring nouns (the controversial war), and adjective phrases (fully prepared).
</prevsent>
<prevsent>in addition to identifying the textual extent of events, evita also analyzes certain grammatical features associated with them.
</prevsent>
</prevsection>
<citsent citstr=" N04-1020 ">
these include:  the polarity (positive or negative) of the expression tells whether the referred event has happened or not;   modality (as marked by modal auxiliaries may, can, might, could, should, etc., or adverbials like probably, likely, etc.) qualifies the denoted event with modal information (irrealis, necessity, possibility), and therefore has implications for the suitability of statements as answers to questions, in parallel way to other intensional contexts exemplified in (1-2);   tense and aspect provide crucial information for the temporal ordering of the events;   similarly, the non-finite morphology of certain verbal expressions (infinitival, present partici ple, or past participle) has been shown as useful in predicting temporal relations between events(lapata and lascarides, 2004).<papid> N04-1020 </papid></citsent>
<aftsection>
<nextsent>we also consider as possible values here the categories of noun and adjective.
</nextsent>
<nextsent>  event class distinguishes among states (e.g., be the director of), general occurrences (walk),reporting (tell), intensional (attempt), and perception (observe) events.
</nextsent>
<nextsent>this classification is relevant for characterizing the nature of the event as irrealis, factual, possible, reported, etc.
</nextsent>
<nextsent>(recall examples (1-2) above).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1773">
<title id=" H05-1088.xml">evita a robust event recognizer for qa systems </title>
<section> evita, an event recognition tool.  </section>
<citcontext>
<prevsection>
<prevsent>part of speech tags and phrase chunks are also valuable for the identification of certain grammatical features such as tense, non-finite morphology, or polarity.
</prevsent>
<prevsent>finally, lexical stems are necessary for those tasks involving lexical look-up.
</prevsent>
</prevsection>
<citsent citstr=" A97-1051 ">
we obtain all such grammatical information by first preprocessing the input file using the alembic workbench tagger, lemmatizer, and chunker (day et al , 1997).<papid> A97-1051 </papid></citsent>
<aftsection>
<nextsent>evitas in put must be xml-compliant, but need not conform to the timeml dtd.
</nextsent>
<nextsent>2.3 event recognition.
</nextsent>
<nextsent>event identification in evita is based on the notion of event as defined in the previous section.
</nextsent>
<nextsent>only lexical items tagged by the preprocessing stage as either verbs, nouns, or adjectives are considered event candidates.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1775">
<title id=" H05-1118.xml">integrating linguistic knowledge in passage retrieval for question answering </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>improving information retrieval (ir) through natural language processing (nlp) has been the goal for many researchers.
</prevsent>
<prevsent>nlp techniques such as lemmatization and compound splitting have been used in several studies (krovetz, 1993; hollink et al, 2003).
</prevsent>
</prevsection>
<citsent citstr=" A97-1046 ">
linguistically motivated syntactic units such as noun phrases (zhai, 1997), <papid> A97-1046 </papid>head-modifier pairs(fagan, 1987; strzalkowski et al, 1996) and subject verb-object triples (katz and lin, 2003) have also been integrated in information retrieval.</citsent>
<aftsection>
<nextsent>however, most of these studies resulted in only little success or even decreasing performance.
</nextsent>
<nextsent>it has been argued that nlp and especially deep syntactic analysis is still too brittle and ineffective (katz and lin, 2003).
</nextsent>
<nextsent>integrating nlp in information retrieval seems to be very hard because the task here is to match plain text keywords to natural language documents.
</nextsent>
<nextsent>in question answering (qa), however, the task isto match natural language questions to relevant answers within document collections.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1776">
<title id=" H05-1118.xml">integrating linguistic knowledge in passage retrieval for question answering </title>
<section> question answering with dependency.  </section>
<citcontext>
<prevsection>
<prevsent>relations our dutch question answering system, joost (bouma et al, 2005), consists of two streams: tablelook-up strategy using off-line information extraction and an on-line strategy using passage retrieval and on-the-fly answer extraction.
</prevsent>
<prevsent>in both strategies we use syntactic information produced by wide-coverage dependency parser for dutch, alpino (bouma et al, 2001).
</prevsent>
</prevsection>
<citsent citstr=" C04-1188 ">
in the off-line strategy we use syntactic patterns to extract information from unrestricted text to be stored in fact tables (jijkoun et al., 2004).<papid> C04-1188 </papid></citsent>
<aftsection>
<nextsent>for the on-line strategy, we assume that there is certain overlap between syntactic relation sin the question and in passages containing the answers.
</nextsent>
<nextsent>furthermore, we also use strategies for reasoning over dependency rules to capture semantic relationships that are expressed by different syntactic patterns (bouma et al, 2005).our focus is set on open-domain question answering using data from the clef competition ondutch qa.
</nextsent>
<nextsent>we have parsed the entire corpus provided by clef with about 4,000,000 sentences in about 190,000 documents.
</nextsent>
<nextsent>the dependency trees are stored in xml and are directly accessible from theqa system.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1777">
<title id=" H05-1071.xml">knowitnow fast scalable information extraction from the web </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>knowitall has generate and-test architecture that extracts information in twostages.
</prevsent>
<prevsent>first, knowitall utilizes small set of domain independent extraction patterns to generate candidate facts (cf.
</prevsent>
</prevsection>
<citsent citstr=" C92-2082 ">
(hearst, 1992)).<papid> C92-2082 </papid></citsent>
<aftsection>
<nextsent>for example, the generic pattern np1 such as nplist2?
</nextsent>
<nextsent>indicates that the head of each simple noun phrase (np) in nplist2 is member of the class named in np1.
</nextsent>
<nextsent>by instantiating the pattern for class city, knowitall extracts three candidate cities from the sentence: we provide tours to cities such as paris, london, and berlin.?
</nextsent>
<nextsent>note that it must also fetch each document that contains potential candidate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1778">
<title id=" H05-1071.xml">knowitnow fast scalable information extraction from the web </title>
<section> experimental results.  </section>
<citcontext>
<prevsection>
<prevsent>and so forth.both knowitnow and knowitall merge extractions with slight variants in the name, such as those differing only in punctuation or whit espace, or in the presence or absence of corporate designator.
</prevsent>
<prevsent>for binary extractions, ceos with the same last name and same company were also merged.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
both systems relyon the opennlp maximum-entropy part-of-speech tagger and chunker (ratnaparkhi, 1996), <papid> W96-0213 </papid>but knowitall applies them to pages downloaded from the web based on there sults of google queries, whereas knowitnow applies them once to crawled and indexed pages.6 overall, eachof the above elements of knowitall and knowit now are the same to allow for controlled experiments.whereas knowitnow runs small number of vari abilized queries (one for each extraction pattern, foreach relation), knowitall requires stopping crite rion.</citsent>
<aftsection>
<nextsent>otherwise, knowitall will continue to query google and download urls found in its result pages over many days and even weeks.
</nextsent>
<nextsent>we allowed total of 6 days of search time for knowitall, allocating more search for the relations that continued to be most productive.
</nextsent>
<nextsent>for ceoof knowitnow returned all pairs of corp,ceo 6our time measurements for knowitall are not affected by the tagging and chunking time because it is dominated by time required to query google, waiting second between queries.
</nextsent>
<nextsent>0.75 0.8 0.85 0.9 0.95 1 0 5,000 10,000 15,000 20,000 25,000 correct extractions pr ec is io knowitnow-freq knowitnow-urns knowitall-pmifigure 5: corp: knowitalls pmi assessment maintains high precision.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1779">
<title id=" H05-1071.xml">knowitnow fast scalable information extraction from the web </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>there has been very little work published on how to make nlp computations such as pmi-ir and ie fast for large corpora.
</prevsent>
<prevsent>indeed, extraction rate is not metric typically used to evaluate ie systems, but we believe it is an important metric if ie is to scale.
</prevsent>
</prevsection>
<citsent citstr=" M92-1036 ">
hobbs et al point out the advantage of fast text processing for rapid system development (hobbs et al, 1992).<papid> M92-1036 </papid></citsent>
<aftsection>
<nextsent>they could test each change to system parameters 569and domain-specific patterns on large sample of documents, having moved from system that took 36 hours to process 100 documents to fastus, which took only 11 minutes.
</nextsent>
<nextsent>this allowed them to develop one of the highest performing muc-4 systems in only one month.
</nextsent>
<nextsent>while there has been extensive work in their andweb communities on improvements to the standard inverted index scheme, there has been little work on efficient large-scale search to support natural language applications.
</nextsent>
<nextsent>one exception is resniks linguists search engine (elkiss and resnik, 2004), tool for searching large corpora of parse trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1780">
<title id=" H05-1071.xml">knowitnow fast scalable information extraction from the web </title>
<section> conclusions and future directions.  </section>
<citcontext>
<prevsection>
<prevsent>in contrast, the be system has single index, constructed just once, that serves allqueries.
</prevsent>
<prevsent>there is no published performance data available for resniks system.
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
in previous work, statistical nlp computation over large corpora has been slow, offline process, as in knowitall (etzioni et al, 2005) and also in pmi-ir applications such as sentiment classification (turney, 2002).<papid> P02-1053 </papid></citsent>
<aftsection>
<nextsent>technology trends, and open source search engines such as nutch, have made it feasible to create private?
</nextsent>
<nextsent>search engines that index large collections of documents; but asshown in figure 2, firing large numbers of queries at private search engines is still slow.
</nextsent>
<nextsent>this paper described novel and practical approach towards substantially speeding up ie.
</nextsent>
<nextsent>we describedknowitnow, which extracts thousands of facts in minutes instead of days.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1781">
<title id=" E99-1003.xml">term extraction i term clustering an integrated platform for compute raided terminology </title>
<section> computat iona  terminology.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" E93-1011 ">
in the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (bourigault, 1993; <papid> E93-1011 </papid>justeson and katz, 1995; daille, 1996; brun, 1998) <papid> P98-1030 </papid>and tools for automatic thesaurus construc-tion (grefenstette, 1994).</citsent>
<aftsection>
<nextsent>these tools are ex-pected to be complementary in the sense that the links and clusters proposed in automatic the-saurus construction can be exploited for structur-ing the term candidates produced by the auto-matic term extractors.
</nextsent>
<nextsent>in fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic the-saurus construction yield clusters of single-word terms.
</nextsent>
<nextsent>on the one hand, term extractors focus on multi-word terms for onto logical motivations: single-word terms are too polysemous and too generic and it is therefore necessary to provide the user with multi-word terms that represent finer concepts in domain.
</nextsent>
<nextsent>the counterpart of this focus is that automatic term extractors yield important volumes of data that require structur-ing through postprocessor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1782">
<title id=" E99-1003.xml">term extraction i term clustering an integrated platform for compute raided terminology </title>
<section> computat iona  terminology.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" P98-1030 ">
in the domain of corpus-based terminology two types of tools are currently developed: tools for automatic term extraction (bourigault, 1993; <papid> E93-1011 </papid>justeson and katz, 1995; daille, 1996; brun, 1998) <papid> P98-1030 </papid>and tools for automatic thesaurus construc-tion (grefenstette, 1994).</citsent>
<aftsection>
<nextsent>these tools are ex-pected to be complementary in the sense that the links and clusters proposed in automatic the-saurus construction can be exploited for structur-ing the term candidates produced by the auto-matic term extractors.
</nextsent>
<nextsent>in fact, complementarity is difficult because term extractors provide mainly multi-word terms, while tools for automatic the-saurus construction yield clusters of single-word terms.
</nextsent>
<nextsent>on the one hand, term extractors focus on multi-word terms for onto logical motivations: single-word terms are too polysemous and too generic and it is therefore necessary to provide the user with multi-word terms that represent finer concepts in domain.
</nextsent>
<nextsent>the counterpart of this focus is that automatic term extractors yield important volumes of data that require structur-ing through postprocessor.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1783">
<title id=" E99-1003.xml">term extraction i term clustering an integrated platform for compute raided terminology </title>
<section> computat iona  terminology.  </section>
<citcontext>
<prevsection>
<prevsent>since they cluster terms through statistical measures of context similarities, these tools exploit recur-ring situations.
</prevsent>
<prevsent>since single-word terms denote broader concepts than multi-word terms, they ap-pear more frequently in corpora and are therefore more appropriate for statistical clustering.
</prevsent>
</prevsection>
<citsent citstr=" P97-1004 ">
the contribution of this paper is to propose an integrated platform for computer-aided term extraction and structuring that results from the combination of lexter, term extraction tool (bouriganlt et al, 1996), and fastr 1, term normalization tool (jacquemin et al, 1997).<papid> P97-1004 </papid></citsent>
<aftsection>
<nextsent>computer-aided terminology the platform for computer-aided terminology is organized as chain of four modules and the cor-responding flow chart is given by figure 1.
</nextsent>
<nextsent>the modules are: pos tagging first the corpus is processed by sylex, part-of-speech tagger.
</nextsent>
<nextsent>each word is unambiguously tagged and receives single lemma.
</nextsent>
<nextsent>term extract ion lexter, the term extrac-tion tool acquires term candidates from the tagged corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1784">
<title id=" E99-1003.xml">term extraction i term clustering an integrated platform for compute raided terminology </title>
<section> term extract ion.  </section>
<citcontext>
<prevsection>
<prevsent>most of mlnp se-quences are ambiguous.
</prevsent>
<prevsent>two (or more) binary decompositions compete, corresponding to several possibilities of prepositional phrase or adjective attachment.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
the disambiguation is performed by corpus-based method which relies on endoge-nous learning procedures (bouriganlt, 1993; rat-naparkhi, 1998).<papid> P98-2177 </papid></citsent>
<aftsection>
<nextsent>an example of such procedure is given in figure 2.
</nextsent>
<nextsent>3.4 network of term candidates.
</nextsent>
<nextsent>the sub-groups generated by the parsing module, together with the maximal-length noun phrases extracted by the splitting module, are the term candidates produced by the term extraction tool.
</nextsent>
<nextsent>this set of term candidates is represented as network: each multi-word term candidate is con-nected to its head constituent and to its expansion constituent by syntactic decomposition links.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1786">
<title id=" E99-1052.xml">determination of syntactic functions in estonian constraint grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the main goal of this work is to elaborate an efficient parser for estonian and annotate the corpus of estonian written texts syn-tactically.
</prevsent>
<prevsent>it is the first attempt to write parser for estonian.
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
the main idea of the constraint grammar (karls- son, 1990) <papid> C90-3030 </papid>is that it determines the surface-level syntactic analysis of the text which has gone through prior morphological nalysis.</citsent>
<aftsection>
<nextsent>the process of syntactic analysis consists of three stages: mor-phological disambiguation, identification of clause boundaries, and identification of syntactic func-tions of words.
</nextsent>
<nextsent>this article focuses on the last module in detail.
</nextsent>
<nextsent>grammatical features of words are presented in the forms of tags which are at-tached to words.
</nextsent>
<nextsent>the tags indicate the inflectional and derivational properties of the word and the word class membership, the tags attached uring the last stage of the analysis indicate its syntactic functions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1787">
<title id=" E99-1032.xml">result stages and the lexicon the proper treatment of event structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>telicity telic events are generally viewed as an opposition between previously holding state and new one, called result state (e.g. dead(y) for kills y).
</prevsent>
<prevsent>they trigger change-of-state (cos, henceforth), result states (rss, henceforth) being ent ailments of coss.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
moens and steedman (1988), <papid> J88-2003 </papid>smith (1991), pustejovsky (1995), and others argue that it is defining property of telic events.</citsent>
<aftsection>
<nextsent>they should therefore include an  under goer  argument, whose cos determines the telicity of the event (i.e., it acts as measur ing-out argument).
</nextsent>
<nextsent>tenny (1987) thus claims that telic events require such an argument, which she calls an affected argument.
</nextsent>
<nextsent>consider for instance john reviewed the paper: as the event reaches its culimation, the affected argument undergoes cos (from --,reviewed(paper) to reviewed(paper)), producing rs.
</nextsent>
<nextsent>to put it short, the standard theory of event structure says that telicity implies affected ness (and conversely), and that affected ness implies cos/rs (and conversely), associating tightly all those notions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1788">
<title id=" E99-1032.xml">result stages and the lexicon the proper treatment of event structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>it can be adapted to any type of feature-structure-based computational lexicon, though.
</prevsent>
<prevsent>note that the m- inc and - inc functions are homomorphi?
</prevsent>
</prevsection>
<citsent citstr=" P99-1064 ">
aspect ual roles relating events to the individual vs. material subparts of objects (see caudal (1999) <papid> P99-1064 </papid>for further details).</citsent>
<aftsection>
<nextsent>235 proceedings of eacl  99 figure 1.
</nextsent>
<nextsent>lexical representation of drink within the generative lexicon framework  -drink argstr = event str = qualia = - -argi = arg2 = restr = -s tage = r-stage = formal = onstitut ive = gentive = : animate_ind ^ i-inc(x,e i) : beverage ^ m-inc(y, ea)  * x : ~ drinking_act(el,x,y) 2 : ~ binary--m-inc--rstage(e2,y) ~ ^ delimited(e2) z-stage / -stage describe the inner and result stages.
</nextsent>
<nextsent>the del imi ted sort indicates delimited events, while the binary_m-inc_rstage sort bears the transition function (i.e., the binary sortal domain) attached to drink, thus allowing it to be read as an incremental telic event ; cf.
</nextsent>
<nextsent>(9).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1789">
<title id=" H05-1038.xml">using question series to evaluate question answering system effectiveness </title>
<section> hale bopp comet.  </section>
<citcontext>
<prevsection>
<prevsent>f scores.
</prevsent>
<prevsent>2.3 other questions.
</prevsent>
</prevsection>
<citsent citstr=" N03-2037 ">
the other questions were evaluated using the same methodology as the trec 2003 definition questions (voorhees, 2003).<papid> N03-2037 </papid></citsent>
<aftsection>
<nextsent>a systems response for an other question is an unordered set of [doc-id, answer-string] pairs as for list questions.
</nextsent>
<nextsent>each string is presumed to be facet in the definition of theseries?
</nextsent>
<nextsent>target that had not yet been covered by earlier questions in the series.
</nextsent>
<nextsent>the requirement to not repeat information already covered by earlier questions in the series made answering other questions somewhat more difficult than answering trec 2003 definition questions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1790">
<title id=" H05-1038.xml">using question series to evaluate question answering system effectiveness </title>
<section> hale bopp comet.  </section>
<citcontext>
<prevsection>
<prevsent>in the trec 2003 combined task there were no series but there were different question types, so question scores were first averaged by question type and then those averages were combined.
</prevsent>
<prevsent>this strategy emphasizes question-type analysis in that it is easy to compare different systems abilities for the different question types.
</prevsent>
</prevsection>
<citsent citstr=" W04-2509 ">
the qa ciad challenge contained only single question type but introduced series structure into the test set (kato et al, 2004).<papid> W04-2509 </papid></citsent>
<aftsection>
<nextsent>in qaciad, the scores were aggregated over the series and the series scores averaged.
</nextsent>
<nextsent>the qaciad series were specifically constructed to be an abstraction of an information seekers dialogue, and the aggregation of scores over series supports comparing different series types.
</nextsent>
<nextsent>for example, qaciad results show browsing series to be more difficult than gathering series.
</nextsent>
<nextsent>the trec 2004 qa track contained both series structure and different question types, so individual question scores could be aggregated either by series or by question type.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1791">
<title id=" E93-1010.xml">head driven parsing for lexical ist grammars experimental results </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>is simply too general.
</prevsent>
<prevsent>for some grammars, naive top-down prediction may even fail to termi-nate.
</prevsent>
</prevsection>
<citsent citstr=" P85-1018 ">
\[shieber, 1985\] <papid> P85-1018 </papid>therefore proposes modified version of the earley-parser, using restricted top- down prediction.</citsent>
<aftsection>
<nextsent>while this modification leads to termination ofthe prediction step, in practice it eas-ily leads to trivial top-down prediction step, thus leading to inferior performance.
</nextsent>
<nextsent>bottom-up arsing is far more attractive for lexi- calist formalisms, as it is driven by the syntactic in-formation associated with lexical elements.
</nextsent>
<nextsent>certain inadequacies remain, however.
</nextsent>
<nextsent>most importantly, the selection of rules to be considered for application may not be very efficient.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1792">
<title id=" E93-1010.xml">head driven parsing for lexical ist grammars experimental results </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>it is shown that, inspite of the fact that bidirectional parsing seemingly leads to more overhead than left-to-right parsing, the worst-case complexity of head-corner parser does not ex-ceed that of an earley parser.
</prevsent>
<prevsent>\[van noord, 1991; van noord, 1993\] argues that head-corner parsing is especially useful for parsing with non-concatenative grammar formalisms.
</prevsent>
</prevsection>
<citsent citstr=" E91-1006 ">
in \[lavelli and satta, 1991\] <papid> E91-1006 </papid>head-driven parsing strategy for lexicalized tree adjoining grammars presented.</citsent>
<aftsection>
<nextsent>although it has been suggested that head-driven parsing has benefits for lexical ist grammars, this has not been established in practice.
</nextsent>
<nextsent>the poten-tial efficiency gains of head-driven parser are of- ten out balanced by the cost of additional overhead.
</nextsent>
<nextsent>this is particularly true for the (bidirectional) head- corner parser.
</nextsent>
<nextsent>the results of the experiment we describe in section 3 establish that efficient head- driven parsing is possible.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1793">
<title id=" E93-1010.xml">head driven parsing for lexical ist grammars experimental results </title>
<section> two head-dr iven  parsers.  </section>
<citcontext>
<prevsection>
<prevsent>head-dr iven vs. functor-dr lven parsing.
</prevsent>
<prevsent>for categorial unification grammars in which we choose the functor as the head of rule, the head-corner table is not going to be discriminating, because the grammar ules in such grammar may simply be (in dcg notation, given appropriate operator defi- nitions): 1 val ---* val/ arg, arg pal --* arg, arg\ val (5) as no information about word-class or morphology is stated in the rules, such information will not be found in the head-corner table.
</prevsent>
</prevsection>
<citsent citstr=" E91-1031 ">
a possibly useful approach ere is to compile some lexical information to the rule set, along the lines proposed in \[bouma, 1991\].<papid> E91-1031 </papid></citsent>
<aftsection>
<nextsent>in that paper it is pro-posed to compile lexical information to the rule-set, and parse with this  enriched  rule-set.
</nextsent>
<nextsent>what seems to be most useful here, is to use this enriched gram-mar only for the compilation of the head-corner ta-ble.
</nextsent>
<nextsent>the parser then uses the general rule schemata themselves.
</nextsent>
<nextsent>however, given the usual analysis of modifiers as functors, even this approach may fail to yield an in-teresting head-corner table.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1794">
<title id=" H05-1104.xml">parallelism in coordination as an instance of syntactic priming evidence from corpus based modeling </title>
<section> adaptation.  </section>
<citcontext>
<prevsection>
<prevsent>the importance of comprehension priming has also been noted by the speech recognition community (kuhn and de mori, 1990), who use so-called caching language models to improve the performance of speech comprehension software.
</prevsent>
<prevsent>the concept of caching language models is quite simple: cache of recently seen words is maintained, and the probability of words in the cache is higher than those outside the cache.while the performance of caching language models is judged by their success in improving speech recognition accuracy, it is also possible to use an abstract measure to diagnose their efficacy more closely.
</prevsent>
</prevsection>
<citsent citstr=" C00-1027 ">
church (2000) <papid> C00-1027 </papid>introduces such diagnostic for lexical priming: adaptation probabilities.</citsent>
<aftsection>
<nextsent>adaptation probabilities provide method to separate the general problem of priming from particular implementation (i.e., caching models).
</nextsent>
<nextsent>they measure the amount of priming that occurs forgiven construction, and therefore provide an upper limit for the performance of models such as caching models.
</nextsent>
<nextsent>adaptation is based upon three concepts.
</nextsent>
<nextsent>first isthe prior, which serves as baseline.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1796">
<title id=" H05-1026.xml">training neural network language models on very large corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>language models play an important role in many applications like character and speech recognition,machine translation and information retrieval.
</prevsent>
<prevsent>several approaches have been developed during the last this work was partially financed by the european commission under the fp6 integrated project tc-star.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
decades like n-gram back-off word models (katz, 1987), class models (brown et al, 1992), <papid> J92-4003 </papid>structured language models (chelba and jelinek, 2000) or maximum entropy language models (rosenfeld, 1996).</citsent>
<aftsection>
<nextsent>to the best of our knowledge word and class n-gramback-off language models are still the dominant approach, at least in applications like large vocabulary continuous speech recognition or statistical machinetranslation.
</nextsent>
<nextsent>in many publications it has been reported that modified kneser-ney smoothing (chen and goodman, 1999) achieves the best results.
</nextsent>
<nextsent>allthe reference back-off language models (lm) described in this paper are build with this technique, using the sri lm toolkit (stolcke, 2002).the field of natural language processing has recently seen some changes by the introduction of new statistical techniques that are motivated by successful approaches from the machine learning community, in particular continuous space lms using neural networks (bengio and ducharme, 2001; bengio et al, 2003; schwenk and gauvain, 2002; schwenk and gauvain, 2004; emami and jelinek, 2004), random forest lms (xu and jelinek, 2004) <papid> W04-3242 </papid>and random cluster lms (emami and jelinek, 2005).</nextsent>
<nextsent>usually new approaches are first verified on small tasks using limited amount of lm training data.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1797">
<title id=" H05-1026.xml">training neural network language models on very large corpora </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to the best of our knowledge word and class n-gramback-off language models are still the dominant approach, at least in applications like large vocabulary continuous speech recognition or statistical machinetranslation.
</prevsent>
<prevsent>in many publications it has been reported that modified kneser-ney smoothing (chen and goodman, 1999) achieves the best results.
</prevsent>
</prevsection>
<citsent citstr=" W04-3242 ">
allthe reference back-off language models (lm) described in this paper are build with this technique, using the sri lm toolkit (stolcke, 2002).the field of natural language processing has recently seen some changes by the introduction of new statistical techniques that are motivated by successful approaches from the machine learning community, in particular continuous space lms using neural networks (bengio and ducharme, 2001; bengio et al, 2003; schwenk and gauvain, 2002; schwenk and gauvain, 2004; emami and jelinek, 2004), random forest lms (xu and jelinek, 2004) <papid> W04-3242 </papid>and random cluster lms (emami and jelinek, 2005).</citsent>
<aftsection>
<nextsent>usually new approaches are first verified on small tasks using limited amount of lm training data.
</nextsent>
<nextsent>for instance, experiments have been performed using the brown corpus (1.1m words), parts of the wall street journal corpus (19m words) or transcriptions of acoustic training data (up to 22m words).
</nextsent>
<nextsent>it ismuch more challenging to compare the new statistical techniques to carefully optimized back-off lm trained on large amounts of data (several hundred millions words).
</nextsent>
<nextsent>training may be difficult and very 201time consuming and the algorithms used with several tens of millions examples may be impracticable for larger amounts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1798">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results on an english german parallel corpus demonstrate the advantages of this approach.
</prevsent>
<prevsent>shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
for instance, information extraction (surdeanu et al, 2003), <papid> P03-1002 </papid>question answering (narayanan and harabagiu, 2004) <papid> C04-1100 </papid>and machine translation (boas, 2002) could stand to benefit from broad coverage semantic processing.</citsent>
<aftsection>
<nextsent>the framenet project (fillmore et al, 2003) has played central role in this endeavour by providing large lexical resource based on semantic roles.
</nextsent>
<nextsent>in framenet, meaning is represented by frames, schematic representations of situations.
</nextsent>
<nextsent>semantic roles are frame-specific, and are called frame elements.
</nextsent>
<nextsent>the database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or fees), lists the possible syntactic realisations of their semantic roles, and provides annotated examples from the british national corpus (burnard, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1799">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>experimental results on an english german parallel corpus demonstrate the advantages of this approach.
</prevsent>
<prevsent>shallow semantic parsing, the task of automatically identifying the semantic roles conveyed by sentential constituents, has recently attracted much attention, partly because of its increasing importance for potential applications.
</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
for instance, information extraction (surdeanu et al, 2003), <papid> P03-1002 </papid>question answering (narayanan and harabagiu, 2004) <papid> C04-1100 </papid>and machine translation (boas, 2002) could stand to benefit from broad coverage semantic processing.</citsent>
<aftsection>
<nextsent>the framenet project (fillmore et al, 2003) has played central role in this endeavour by providing large lexical resource based on semantic roles.
</nextsent>
<nextsent>in framenet, meaning is represented by frames, schematic representations of situations.
</nextsent>
<nextsent>semantic roles are frame-specific, and are called frame elements.
</nextsent>
<nextsent>the database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or fees), lists the possible syntactic realisations of their semantic roles, and provides annotated examples from the british national corpus (burnard, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1800">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic roles are frame-specific, and are called frame elements.
</prevsent>
<prevsent>the database associates frames with lemmas (verbs, nouns, adjectives) that can evoke them (called frame-evoking elements or fees), lists the possible syntactic realisations of their semantic roles, and provides annotated examples from the british national corpus (burnard, 1995).
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
the availability of rich annotations for the surface realisation of semantic roles has triggered interest in semantic parsing and enabled the development of data-driven models (e.g., gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>frame: departing theme the officer left the house.
</nextsent>
<nextsent>the plane leaves at seven.
</nextsent>
<nextsent>his departure was delayed.
</nextsent>
<nextsent>source we departed from new york.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1801">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>(2002) assess the degree of syntactic parallelism in dependency relations between english and chinese.
</prevsent>
<prevsent>their results show that, although assuming direct correspondence is often too restrictive, syntactic projection yields good enough annotations to traina dependency parser.
</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
smith and smith (2004) <papid> W04-3207 </papid>explore syntactic projection further by proposing an english-korean bilingual parser integrated with word translation model.previous work has primarily focused on the projection of morphological and grammatico-syntactic information.</citsent>
<aftsection>
<nextsent>inducing semantic resources from low density languages still poses significant challenge to data-driven methods.
</nextsent>
<nextsent>the challenge is recognised by fung and chen (2004) <papid> C04-1134 </papid>who construct chinese framenet by mapping english framenet entries to concepts listed in hownet2, an on-line ontology for chinese, however without exploiting parallel texts.</nextsent>
<nextsent>the present work extends previous approaches on annotation projection by inducing framenet semantic roles from parallel corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1802">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>smith and smith (2004) <papid> W04-3207 </papid>explore syntactic projection further by proposing an english-korean bilingual parser integrated with word translation model.previous work has primarily focused on the projection of morphological and grammatico-syntactic information.</prevsent>
<prevsent>inducing semantic resources from low density languages still poses significant challenge to data-driven methods.</prevsent>
</prevsection>
<citsent citstr=" C04-1134 ">
the challenge is recognised by fung and chen (2004) <papid> C04-1134 </papid>who construct chinese framenet by mapping english framenet entries to concepts listed in hownet2, an on-line ontology for chinese, however without exploiting parallel texts.</citsent>
<aftsection>
<nextsent>the present work extends previous approaches on annotation projection by inducing framenet semantic roles from parallel corpora.
</nextsent>
<nextsent>analogously to hwa et al (2002), <papid> P02-1050 </papid>we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in non parallel semantic structures.</nextsent>
<nextsent>similarly to fung and chen (2004) <papid> C04-1134 </papid>we automatically induce semantic role annotations for target language.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1804">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the challenge is recognised by fung and chen (2004) <papid> C04-1134 </papid>who construct chinese framenet by mapping english framenet entries to concepts listed in hownet2, an on-line ontology for chinese, however without exploiting parallel texts.</prevsent>
<prevsent>the present work extends previous approaches on annotation projection by inducing framenet semantic roles from parallel corpora.</prevsent>
</prevsection>
<citsent citstr=" P02-1050 ">
analogously to hwa et al (2002), <papid> P02-1050 </papid>we investigate whether there are indeed semantic correspondences between two languages, since there is little hope for projecting meaningful annotations in non parallel semantic structures.</citsent>
<aftsection>
<nextsent>similarly to fung and chen (2004) <papid> C04-1134 </papid>we automatically induce semantic role annotations for target language.</nextsent>
<nextsent>in contrast to them, we resort to parallel corpora as source of semantic equivalence.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1807">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> creation of gold standard corpus.  </section>
<citcontext>
<prevsection>
<prevsent>relying solely on the english framenet database for sampling would yield many sentence pairs which are either inappropriate for the present study (be cause they do not evoke the same frames) or simply problematic for annotation since they are outside the 2see http://www.keenage.com/zhiwang/e_zhiwang.
</prevsent>
<prevsent>html.
</prevsent>
</prevsection>
<citsent citstr=" P03-1068 ">
860 present coverage of the database.for the above reasons, our sample selection procedure was informed by two existing resources,the english framenet and salsa, framenet compatible database for german currently underdevelopment (erk et al, 2003).<papid> P03-1068 </papid></citsent>
<aftsection>
<nextsent>we first used the publicly available giza++ (och and ney, 2003) <papid> J03-1002 </papid>software to induce english-german word alignments.</nextsent>
<nextsent>next, we gathered all german-english sentences in the corpus that had at least one pair of aligned words (we,wg), which were listed in framenet and salsa, respectively, and had at least one frame in common.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1808">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> creation of gold standard corpus.  </section>
<citcontext>
<prevsection>
<prevsent>html.
</prevsent>
<prevsent>860 present coverage of the database.for the above reasons, our sample selection procedure was informed by two existing resources,the english framenet and salsa, framenet compatible database for german currently underdevelopment (erk et al, 2003).<papid> P03-1068 </papid></prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we first used the publicly available giza++ (och and ney, 2003) <papid> J03-1002 </papid>software to induce english-german word alignments.</citsent>
<aftsection>
<nextsent>next, we gathered all german-english sentences in the corpus that had at least one pair of aligned words (we,wg), which were listed in framenet and salsa, respectively, and had at least one frame in common.
</nextsent>
<nextsent>these sentences exemplify 83 frame types, 696 lemma pairs, and 265 unique english and 178 unique german lemmas.
</nextsent>
<nextsent>sentence pairs were grouped into three bands according to their frame frequency (high, medium, low).
</nextsent>
<nextsent>we randomly selected 380 pairs from each band.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1810">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> projection of semantic information.  </section>
<citcontext>
<prevsection>
<prevsent>unfortunately, any attempt to compute match based on categorical data derived from linguistic analyses (e.g., parts of speech,phrase types or grammatical relations), needs to empirically derive cross-linguistic similarities between categories, task which must be repeated for every new language pair, and requires additional data.rather than postulating an ad hoc similarity function, we use word alignments to derive information about semantic roles in the target language.
</prevsent>
<prevsent>our first model family (section 4.2) relies exclusive lyon this knowledge source.
</prevsent>
</prevsection>
<citsent citstr=" H01-1035 ">
although potentially useful as proxy for semantic equivalence, automatically induced alignments are often noisy, thus leading to errors in annotation projection (yarowsky et al., 2001).<papid> H01-1035 </papid></citsent>
<aftsection>
<nextsent>for example, function words commonly diverge across languages and are systematically mis aligned; furthermore, alignments are restricted to single words rather than word combinations.
</nextsent>
<nextsent>this observation motivates second model family with abias towards linguistically meaningful entities (sec tion 4.3).
</nextsent>
<nextsent>such entities can be constituents derived from the output of parser or non-recursive syntactic structures (i.e., chunks).in this paper we compare simple word alignment models against more resource intensive models that utilise constituent-based information and examine whether syntactic knowledge significantly contributes to semantic projection.
</nextsent>
<nextsent>4.2 word-based projection model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1812">
<title id=" H05-1108.xml">cross linguistic projection of role semantic information </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>consider the following (simplified) example for the statement frame (introduced by say) and its semantic role statement (introduced by we): (10) we wir claim behaupten x and und we ? say sagen y the word alignment correctly aligns the german pronoun wir with the first english we and leaves 865 the second occurrence unaligned.
</prevsent>
<prevsent>since there is no corresponding german word for the second we, projection of the speaker role fails.
</prevsent>
</prevsection>
<citsent citstr=" W03-1005 ">
in future work,this problem could be handled with explicit identification of empty categories (see dienes and dubey, 2003).<papid> W03-1005 </papid></citsent>
<aftsection>
<nextsent>in this paper, we argue that parallel corpora show promise in relieving the lexical acquisition bottleneck for low density languages.
</nextsent>
<nextsent>we proposed semantic projection as means of obtaining framenet annotations automatically without additional humaneffort.
</nextsent>
<nextsent>we examined semantic parallelism, prerequisite for accurate projection, and showed that semantic roles can be successfully projected for predicate pairs with matching frame assignments.
</nextsent>
<nextsent>similarly to previous work (hwa et al, 2002), <papid> P02-1050 </papid>we find that some mileage can be gained by assuming direct correspondence between two languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1814">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we also demonstrate that little manual effort can improve the quality ofsense examples, as measured by wsd accuracy.
</prevsent>
<prevsent>the performance of the classifier on wsd also improves as the number of training sense examples increases.
</prevsent>
</prevsection>
<citsent citstr=" W04-0807 ">
the results of the recent senseval-3 competition (mihalcea et al, 2004) <papid> W04-0807 </papid>have shown that supervised wsd methods can yield up to 72.9% accuracy1 on words for which manually sense-tagged data are available.</citsent>
<aftsection>
<nextsent>however, supervised methods suffer from the so-called knowledge acquisition bottleneck: they need large quantities of high quality annotated data 1this figure refers to the highest accuracy achieved in thesenseval-3 english lexical sample task with fine-grained scoring.
</nextsent>
<nextsent>to produce reliable results.
</nextsent>
<nextsent>unfortunately, very few sense-tagged corpora are available and manualsense-tagging is extremely costly and labour intensive.
</nextsent>
<nextsent>one way to tackle this problem is trying to automate the sense-tagging process.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1815">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, wsd approaches that exploit differences between languages have shown greatpromise.
</prevsent>
<prevsent>several trends are taking place simultaneously under this multilingual paradigm.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
a classic one is to acquire sense examples using bilingual parallel texts (gale et al, 1992; resnik and yarowsky, 1997; diab and resnik, 2002; <papid> P02-1033 </papid>ng et al,2003): <papid> P03-1058 </papid>given word-aligned parallel corpus, the different translations in target language serve as the sense tags?</citsent>
<aftsection>
<nextsent>of an ambiguous word in the source language.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>acquired sense examples using english-chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using soft ware.</nextsent>
<nextsent>a manual selection of target translations was then performed, grouping together senses that share the same translation in chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1816">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years, wsd approaches that exploit differences between languages have shown greatpromise.
</prevsent>
<prevsent>several trends are taking place simultaneously under this multilingual paradigm.
</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
a classic one is to acquire sense examples using bilingual parallel texts (gale et al, 1992; resnik and yarowsky, 1997; diab and resnik, 2002; <papid> P02-1033 </papid>ng et al,2003): <papid> P03-1058 </papid>given word-aligned parallel corpus, the different translations in target language serve as the sense tags?</citsent>
<aftsection>
<nextsent>of an ambiguous word in the source language.
</nextsent>
<nextsent>for example, ng et al (2003) <papid> P03-1058 </papid>acquired sense examples using english-chinese parallel corpora, which were manually or automatically aligned at sentence level and then word-aligned using soft ware.</nextsent>
<nextsent>a manual selection of target translations was then performed, grouping together senses that share the same translation in chinese.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1818">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>there sults appear good numerically, but since the sense groups are not in the gold standard, comparison with other senseval-2 results is difficult.
</prevsent>
<prevsent>as discussed by ng et al, there are several problems with relying on bilingual parallel corpora for data collection.
</prevsent>
</prevsection>
<citsent citstr=" P99-1068 ">
first,parallel corpora, especially accurately aligned parallel corpora are rare, although attempts have been made to mine them from the web (resnik, 1999).<papid> P99-1068 </papid></citsent>
<aftsection>
<nextsent>second, it is often not possible to distinguish all senses of word in the source language, by merely relying on parallel corpora, especially when the corpora are relatively small.
</nextsent>
<nextsent>this is common problem for bilingual approaches: useful data for some words cannot be collected because different senses of poly semous words in one language often translate to the same word in the other.
</nextsent>
<nextsent>using parallel corpora can aggravate this problem, because even if word sense in the source language has unique translation in the target language, the translation may not occur in the parallel corpora at all, due to the limited size of this resource.
</nextsent>
<nextsent>to alleviate these problems, researchers seek other bilingual resources such as bilingual dictionaries, together with monolingual resources that can be obtained easily.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1819">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>using parallel corpora can aggravate this problem, because even if word sense in the source language has unique translation in the target language, the translation may not occur in the parallel corpora at all, due to the limited size of this resource.
</prevsent>
<prevsent>to alleviate these problems, researchers seek other bilingual resources such as bilingual dictionaries, together with monolingual resources that can be obtained easily.
</prevsent>
</prevsection>
<citsent citstr=" J94-4003 ">
dagan and itai (1994) <papid> J94-4003 </papid>proposed an approach to wsd using monolingual corpora, abi lingual lexicon and parser for the source lan guage.</citsent>
<aftsection>
<nextsent>one of the problems of this method is that for many languages, accurate parsers do not exist.
</nextsent>
<nextsent>with small amount of classified data and large amount of unclassified data in both the source and the target languages, li and li (2004) <papid> J04-1001 </papid>proposed bilingual bootstrapping.</nextsent>
<nextsent>this repeatedly constructs classifier sin the two languages in parallel and boosts the performance of the classifiers by classifying data ineach of the languages and by exchanging information regarding the classified data between two lan guages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1820">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>dagan and itai (1994) <papid> J94-4003 </papid>proposed an approach to wsd using monolingual corpora, abi lingual lexicon and parser for the source lan guage.</prevsent>
<prevsent>one of the problems of this method is that for many languages, accurate parsers do not exist.</prevsent>
</prevsection>
<citsent citstr=" J04-1001 ">
with small amount of classified data and large amount of unclassified data in both the source and the target languages, li and li (2004) <papid> J04-1001 </papid>proposed bilingual bootstrapping.</citsent>
<aftsection>
<nextsent>this repeatedly constructs classifier sin the two languages in parallel and boosts the performance of the classifiers by classifying data ineach of the languages and by exchanging information regarding the classified data between two languages.
</nextsent>
<nextsent>with certain amount of manual work, they reported promising results, but evaluated on relatively small datasets.
</nextsent>
<nextsent>in previous work, we proposed to use chinese monolingual corpora and chinese-english bilingual dictionaries to acquire sense examples (wang, 2004)<papid> P04-2005 </papid>2.</nextsent>
<nextsent>we evaluated the sense examples using avector space wsd model on small dataset containing words with binary senses, with promisingresults.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1821">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this repeatedly constructs classifier sin the two languages in parallel and boosts the performance of the classifiers by classifying data ineach of the languages and by exchanging information regarding the classified data between two languages.
</prevsent>
<prevsent>with certain amount of manual work, they reported promising results, but evaluated on relatively small datasets.
</prevsent>
</prevsection>
<citsent citstr=" P04-2005 ">
in previous work, we proposed to use chinese monolingual corpora and chinese-english bilingual dictionaries to acquire sense examples (wang, 2004)<papid> P04-2005 </papid>2.</citsent>
<aftsection>
<nextsent>we evaluated the sense examples using avector space wsd model on small dataset containing words with binary senses, with promisingresults.
</nextsent>
<nextsent>this approach does not relyon scarce resources such as aligned parallel corpora or accurate parsers.
</nextsent>
<nextsent>this paper describes further progress based on our proposal: we automatically build larger-scale sense examples and then train nave bayes classifier on them.
</nextsent>
<nextsent>we have evaluated our system on the english lexical sample dataset from senseval-2 and there sults show conclusively that such sense examples can be used successfully in full-scale fine-grained wsd task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1823">
<title id=" H05-1069.xml">word sense disambiguation using sense examples automatically acquired from a second language </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>baseline, since an unsupervised 10accuracies for each word and averages were calculated by us, based on the information on senseval-2 website.
</prevsent>
<prevsent>see: http://www.sle.sharp.co.uk/senseval2/.system does not have such prior knowledge before hand.
</prevsent>
</prevsection>
<citsent citstr=" P04-1036 ">
mccarthy et al (2004) <papid> P04-1036 </papid>argue that this is avery tough baseline for an unsupervised wsd system to beat.</citsent>
<aftsection>
<nextsent>our sys b?
</nextsent>
<nextsent>with multiword detection exceeds it.
</nextsent>
<nextsent>sys b?
</nextsent>
<nextsent>also exceeds the performance of uned (fernandez-amoros et al, 2001), which was the second-best ranked11 unsupervised systems in the senseval-2 competition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1825">
<title id=" H05-1068.xml">optimizing to arbitrary nlp metrics using ensemble selection </title>
<section> tasks.  </section>
<citcontext>
<prevsection>
<prevsent>entities can be people, places, things,etc. the resulting partitioning of noun phrases creates reference chains with one chain per entity.
</prevsent>
<prevsent>we use the same problem formulation as soon etal.
</prevsent>
</prevsection>
<citsent citstr=" P02-1014 ">
(2001) and ng and cardie (2002) <papid> P02-1014 </papid>combination of classification and clustering.</citsent>
<aftsection>
<nextsent>briefly, every noun phrase is paired with all preceding noun phrases, creating multiple pairs.
</nextsent>
<nextsent>for the training data, the pairs are labeled as co referent or not.
</nextsent>
<nextsent>a binary classifier is trained to predict the pair labels.
</nextsent>
<nextsent>during classification, the predicted labels are used to form clusters.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1827">
<title id=" H05-1068.xml">optimizing to arbitrary nlp metrics using ensemble selection </title>
<section> tasks.  </section>
<citcontext>
<prevsection>
<prevsent>during classification, the predicted labels are used to form clusters.
</prevsent>
<prevsent>two noun phrases and share cluster if they are either predicted as co referent by the classifier or if they are transit ively predicted as co referent through one or more other noun phrases.
</prevsent>
</prevsection>
<citsent citstr=" J01-4004 ">
instance selection (soon et al, 2001; <papid> J01-4004 </papid>ng, 2004) is used to increase the percentage of positive instances in the training set.3 we use the learning features described by ng and cardie (2002).<papid> P02-1014 </papid></citsent>
<aftsection>
<nextsent>all learning algorithms are trained with the full set of features.
</nextsent>
<nextsent>additionally, the rule learner, svm, and lr are also trained with hand-selected subset of the features that ng and cardie (2002) <papid> P02-1014 </papid>find to outperform the full feature set.</nextsent>
<nextsent>essentially this is an additional parameter to set for the learning task.special metrics: rather than focusing on performance at the pairwise coreference classification level, performance for this task is typically reported using either the muc metric (vilain et al, 1995) <papid> M95-1005 </papid>or the bcubed metric (bagga and baldwin, 1998).both of these metrics measure the degree that predicted coreference chains agree with an answer key.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1830">
<title id=" H05-1068.xml">optimizing to arbitrary nlp metrics using ensemble selection </title>
<section> tasks.  </section>
<citcontext>
<prevsection>
<prevsent>all learning algorithms are trained with the full set of features.
</prevsent>
<prevsent>additionally, the rule learner, svm, and lr are also trained with hand-selected subset of the features that ng and cardie (2002) <papid> P02-1014 </papid>find to outperform the full feature set.</prevsent>
</prevsection>
<citsent citstr=" M95-1005 ">
essentially this is an additional parameter to set for the learning task.special metrics: rather than focusing on performance at the pairwise coreference classification level, performance for this task is typically reported using either the muc metric (vilain et al, 1995) <papid> M95-1005 </papid>or the bcubed metric (bagga and baldwin, 1998).both of these metrics measure the degree that predicted coreference chains agree with an answer key.</citsent>
<aftsection>
<nextsent>in particular they measure the chain-level precision and recall (and the corresponding f-measure).
</nextsent>
<nextsent>we abbreviate these metrics muc-f1, and b3f1.data set: for our experiments we use the muc 6 corpus, which contains 60 documents annotated with coreference information.
</nextsent>
<nextsent>the training, tuning, and test sets consist of documents 1-20, 21-30, and 3soon-1 instance selection is used for all algorithms; we also use soon-2 (ng, 2004) instance selection for the rule learner.
</nextsent>
<nextsent>542 31-60, respectively.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1831">
<title id=" H05-1068.xml">optimizing to arbitrary nlp metrics using ensemble selection </title>
<section> tasks.  </section>
<citcontext>
<prevsection>
<prevsent>the 400 documents are randomly split to get 320 training, 40 tuning, and 40 testing documents.
</prevsent>
<prevsent>4.3 determining psf hierarchy.
</prevsent>
</prevsection>
<citsent citstr=" C04-1018 ">
the third task is taken from breck and cardie(2004).<papid> C04-1018 </papid></citsent>
<aftsection>
<nextsent>explicit psfs each have source that corresponds to the person or entity expressing the subjectivity.
</nextsent>
<nextsent>in the presence of second-hand reporting,sources are often nested.
</nextsent>
<nextsent>this has the effect of filtering subjectivity through chain of sources.
</nextsent>
<nextsent>given sentences annotated with psf information (i.e. which spans are psfs), the task is to discover the hierarchy among the psfs that corresponds to the nesting of their respective sources.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1834">
<title id=" H05-1002.xml">data driven approaches for information structure identification </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and steedman, 2003) for an overview.
</prevsent>
<prevsent>however, all theories consider at least one of the following two distinctions: (i) topic/focus1 distinction that divides the linguistic meaning of the sentence into parts that link the sentence content ? we use the praguian terminology for this distinction.to the discourse context, and other parts that advance the discourse, i.e., add or modify information; and (ii) background/kontrast2 distinction between parts of the utterance which contribute to distinguishing its actual content from alternatives the context makes available.information structure is an important factor in determining the felicity of sentence in given context.
</prevsent>
</prevsection>
<citsent citstr=" H94-1035 ">
applications in which is is crucial are text to-speech systems, where is helps to improve the quality of the speech output (prevost and steedman, 1994; <papid> H94-1035 </papid>kruijff-korbayova?</citsent>
<aftsection>
<nextsent>et al, 2003; moore et al, 2004), and machine translation, where is improves target word order, especially that of free word order languages (stys and zemke, 1995).
</nextsent>
<nextsent>existing theories, however, state their principles using carefully selected illustrative examples.
</nextsent>
<nextsent>be cause of this, they fail to adequately explain how different linguistic dimensions cooperate to realize information structure.
</nextsent>
<nextsent>in this paper we describe data-driven, machine learning approaches for automatic identification of information structure; we describe what aspects of is we deal with and report results of the performance of our systems and make an error analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1835">
<title id=" H05-1034.xml">a comparative study on language model adaptation techniques using new evaluation metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>count merging and linear interpolation of models are the two map estimation methods investigated in speech recognition experiments (iyer et al, 1997; bacchiani and roark, 2003), with count merging reported to slightly outperform linear interpolation.
</prevsent>
<prevsent>discriminative approaches to lm adaptation, on the other hand, aim at using the adaptation data to directly minimize the errors on the adaptation data made by the background model.
</prevsent>
</prevsection>
<citsent citstr=" N04-4006 ">
these techniques have been applied successfully to the task of language modeling in non-adaptation (roark et al, 265 2004) as well as adaptation (bacchiani et al, 2004) <papid> N04-4006 </papid>scenarios.</citsent>
<aftsection>
<nextsent>in this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models.
</nextsent>
<nextsent>the four models we compare are maximum posteriori (map) method and three discriminative training methods, namely the boosting algorithm (collins, 2000), the average perceptron (collins, 2002) <papid> W02-1001 </papid>and the minimum sample risk method (gao et al, 2005).<papid> H05-1027 </papid></nextsent>
<nextsent>our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (cer): we use metric of distributional similarity to measure the distance between background and adaptation domains, and attempt to correlate it with the cer of each adaptation method.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1836">
<title id=" H05-1034.xml">a comparative study on language model adaptation techniques using new evaluation metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques have been applied successfully to the task of language modeling in non-adaptation (roark et al, 265 2004) as well as adaptation (bacchiani et al, 2004) <papid> N04-4006 </papid>scenarios.</prevsent>
<prevsent>in this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
the four models we compare are maximum posteriori (map) method and three discriminative training methods, namely the boosting algorithm (collins, 2000), the average perceptron (collins, 2002) <papid> W02-1001 </papid>and the minimum sample risk method (gao et al, 2005).<papid> H05-1027 </papid></citsent>
<aftsection>
<nextsent>our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (cer): we use metric of distributional similarity to measure the distance between background and adaptation domains, and attempt to correlate it with the cer of each adaptation method.
</nextsent>
<nextsent>we also propose novel metric for measuring the side effects of adapted models using the notion of backward compatibility, which is very important from software deployment perspective.
</nextsent>
<nextsent>our experiments are conducted in the setting of japanese kana-kanji conversion, as we believe this task is excellently suited for evaluating lms.
</nextsent>
<nextsent>we begin with the description of this task in the following section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1837">
<title id=" H05-1034.xml">a comparative study on language model adaptation techniques using new evaluation metrics </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these techniques have been applied successfully to the task of language modeling in non-adaptation (roark et al, 265 2004) as well as adaptation (bacchiani et al, 2004) <papid> N04-4006 </papid>scenarios.</prevsent>
<prevsent>in this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models.</prevsent>
</prevsection>
<citsent citstr=" H05-1027 ">
the four models we compare are maximum posteriori (map) method and three discriminative training methods, namely the boosting algorithm (collins, 2000), the average perceptron (collins, 2002) <papid> W02-1001 </papid>and the minimum sample risk method (gao et al, 2005).<papid> H05-1027 </papid></citsent>
<aftsection>
<nextsent>our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (cer): we use metric of distributional similarity to measure the distance between background and adaptation domains, and attempt to correlate it with the cer of each adaptation method.
</nextsent>
<nextsent>we also propose novel metric for measuring the side effects of adapted models using the notion of backward compatibility, which is very important from software deployment perspective.
</nextsent>
<nextsent>our experiments are conducted in the setting of japanese kana-kanji conversion, as we believe this task is excellently suited for evaluating lms.
</nextsent>
<nextsent>we begin with the description of this task in the following section.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1838">
<title id=" H05-1034.xml">a comparative study on language model adaptation techniques using new evaluation metrics </title>
<section> language modeling in the task of ime.  </section>
<citcontext>
<prevsection>
<prevsent>this paper studies language modeling in the context of asian language (e.g., chinese or japanese) text input.
</prevsent>
<prevsent>the standard method for doing this is that the users first input the phonetic strings, which are then converted into the appropriate word string by software.
</prevsent>
</prevsection>
<citsent citstr=" W02-1032 ">
the task of automatic conversion has been the subject of language modeling research in the context of pinyin-to-character conversion in chinese (gao et al, 2002<papid> W02-1032 </papid>a) and kana-kanji conversion in japanese (gao et al, 2002<papid> W02-1032 </papid>b).</citsent>
<aftsection>
<nextsent>in this paper, we call the task ime (input method editor), based on the name of the commonly used win dows-based application.
</nextsent>
<nextsent>the performance of ime is typically measured by the character error rate (cer), which is the number of characters wrongly converted from the phonetic string divided by the number of characters in the correct transcript.
</nextsent>
<nextsent>current ime systems exhibit about 5-15% cer on real-world data in wide variety of domains.
</nextsent>
<nextsent>in many ways, ime is similar task to speech recognition.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1850">
<title id=" H05-1034.xml">a comparative study on language model adaptation techniques using new evaluation metrics </title>
<section> discriminative algorithms for lm ad-.  </section>
<citcontext>
<prevsection>
<prevsent>this is repeated until sr stops decreasing.
</prevsent>
<prevsent>regular numerical line search algorithms cannot be applied directly because, as described above, the value of feature parameter versus sr is not smooth and there are many local minima.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
msr thus adopts the method proposed by och (2003).<papid> P03-1021 </papid></citsent>
<aftsection>
<nextsent>let gen(a) be the set of n-best candidate word strings that could be converted from a. by adjusting for selected feature fd, we can find set of intervals for within which particular candidate word string is selected.
</nextsent>
<nextsent>we can compute er(.)
</nextsent>
<nextsent>for the candidate and use it as the er(.)
</nextsent>
<nextsent>value for the corresponding interval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1855">
<title id=" E93-1002.xml">the incremental generation of passive sentences </title>
<section> computational  modeling.  </section>
<citcontext>
<prevsection>
<prevsent>incremental processing (e.g., reithinger 1992) often make extensive use of feedback at the cost of economy and cognitive adequacy.
</prevsent>
<prevsent>ture structures.
</prevsent>
</prevsection>
<citsent citstr=" J90-1004 ">
unlike deduction-based approaches to natural language generation computational linguistics (e.g., shieber et al 1990), <papid> J90-1004 </papid>however, the syn phonics approach involves detailed and transparent process model, with sub-processes being explicitly specified at any point in the overall process.</citsent>
<aftsection>
<nextsent>this property serves to make the model adjustable to empirical results about he course of human language production and open to veri-fication of its claims, namely to aim at the computa-tional modelling of cognitive processes.
</nextsent>
<nextsent>in order to make the above considerations more concrete, we will discuss the roles of the conceptualizer and the formulator in the production of particular linguistic construction some more detail in the remainder of this paper.
</nextsent>
<nextsent>the discussion of the principles guiding the production of passive sentences serves to illustrate to what extent the determinants of this construction can be traced to the feedback-free interplay between the conceptualizer and the formulator and the constraints specific to the involved modules.
</nextsent>
<nextsent>we cannot go into the details of the passive here; rather, we will confine the presentation tosome quite simple cases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1856">
<title id=" E93-1002.xml">the incremental generation of passive sentences </title>
<section> the syn phonics conceptualizer.  </section>
<citcontext>
<prevsection>
<prevsent>we cannot go into the details of the passive here; rather, we will confine the presentation tosome quite simple cases.
</prevsent>
<prevsent>in order to cap-ture the full range of the passive construction across languages, the account presented here needs to be enlarged in parts.
</prevsent>
</prevsection>
<citsent citstr=" C82-1016 ">
the conceptual input into the formulator - in short: cs for  conceptual structure  - is represented in the refo/retn format (habel 1982, <papid> C82-1016 </papid>1986a/b; eschenbach et al. 1989).<papid> E89-1022 </papid></citsent>
<aftsection>
<nextsent>the basic representational units are referen-tial objects (reids), which are stored and processed in net-like structure, areferential net (rein).
</nextsent>
<nextsent>refos are labeled, inter alia, with sortal attributes and property and relation designations.
</nextsent>
<nextsent>the notion of refos comprises the entire range of discourse ntities, such as objects, times, and situations (events, processes, tates).
</nextsent>
<nextsent>the input representation reflects certain aspects of the organization of the information which the conceptn_ali~er livers to the formulator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1857">
<title id=" E93-1002.xml">the incremental generation of passive sentences </title>
<section> the syn phonics conceptualizer.  </section>
<citcontext>
<prevsection>
<prevsent>we cannot go into the details of the passive here; rather, we will confine the presentation tosome quite simple cases.
</prevsent>
<prevsent>in order to cap-ture the full range of the passive construction across languages, the account presented here needs to be enlarged in parts.
</prevsent>
</prevsection>
<citsent citstr=" E89-1022 ">
the conceptual input into the formulator - in short: cs for  conceptual structure  - is represented in the refo/retn format (habel 1982, <papid> C82-1016 </papid>1986a/b; eschenbach et al. 1989).<papid> E89-1022 </papid></citsent>
<aftsection>
<nextsent>the basic representational units are referen-tial objects (reids), which are stored and processed in net-like structure, areferential net (rein).
</nextsent>
<nextsent>refos are labeled, inter alia, with sortal attributes and property and relation designations.
</nextsent>
<nextsent>the notion of refos comprises the entire range of discourse ntities, such as objects, times, and situations (events, processes, tates).
</nextsent>
<nextsent>the input representation reflects certain aspects of the organization of the information which the conceptn_ali~er livers to the formulator.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1858">
<title id=" E95-1013.xml">literal movement grammars </title>
<section> definition and examples.  </section>
<citcontext>
<prevsection>
<prevsent>e.g. for example sentence 1 this means that upon finding the displaced constituent which book, we will not evaluate that constituent, but rather emember during the treatment of the remaining part of the sentence, that this data is still to be fitted into logical place.
</prevsent>
<prevsent>this is not new idea.
</prevsent>
</prevsection>
<citsent citstr=" J81-4003 ">
a number of non- concatenative grammar formalisms has been put for-ward, such as head-wrapping rammars (hg) (pol- lard, 1984), extra position grammars (xg) (pereira, 1981).<papid> J81-4003 </papid></citsent>
<aftsection>
<nextsent>and tree adjoining grammars (tag) (kroch and joshi, 1986).
</nextsent>
<nextsent>a discussion of these formalisms as alternatives to the lmg formalism is given in sec-tion 4.
</nextsent>
<nextsent>91 lessons in parsing by hand in high school (e.g. in english or latin classes) informally illustrate the pur-pose of literal movement grammars: as opposed to the traditional inguistic point of view that there is only one head which dominates phrase, constituents of sentence have several key components.
</nextsent>
<nextsent>a verb phrase for example not only has its finite verb, but also one or more objects.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1864">
<title id=" H05-1032.xml">bayesian learning in text summarization </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>rather, what we are asking is, what summary is popularly favored for xindeed the fact that there could be as many summaries as angles to look at the text from may favor in general how to best set ? requires some experimenting with data and the optimal value may vary from domain to domain.an interesting approach would be to empirically optimize ? using methods suggested in mackay and peto (1994).10incidentally, summarizers, bayesian or not, perform considerably better on g3k3 than on g1k3 or g2k3.
</prevsent>
<prevsent>this happens presumably because large portion of votes concentrate in rather small region of text there, property any classifier should pick up easily.the cf view of summary: the idea of what constitutes good summary may vary from person to person, and may well be influenced by particular interests and concerns of people we elicit data from.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
among some recent work with similar concerns, one notable is the pyramid scheme (nenkova and passonneau, 2004) <papid> N04-1019 </papid>where one does not declare particular human summary absolute reference to compare summaries against, but rather makes every one of multiple human summaries at hand bear on evaluation; rouge (lin and hovy, 2003) <papid> N03-1020 </papid>represents another such effort.</citsent>
<aftsection>
<nextsent>the bayesian sum marist represents yet another, whereby one seeks summary most typical of those created by humans.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1865">
<title id=" H05-1032.xml">bayesian learning in text summarization </title>
<section> concluding remarks.  </section>
<citcontext>
<prevsection>
<prevsent>rather, what we are asking is, what summary is popularly favored for xindeed the fact that there could be as many summaries as angles to look at the text from may favor in general how to best set ? requires some experimenting with data and the optimal value may vary from domain to domain.an interesting approach would be to empirically optimize ? using methods suggested in mackay and peto (1994).10incidentally, summarizers, bayesian or not, perform considerably better on g3k3 than on g1k3 or g2k3.
</prevsent>
<prevsent>this happens presumably because large portion of votes concentrate in rather small region of text there, property any classifier should pick up easily.the cf view of summary: the idea of what constitutes good summary may vary from person to person, and may well be influenced by particular interests and concerns of people we elicit data from.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
among some recent work with similar concerns, one notable is the pyramid scheme (nenkova and passonneau, 2004) <papid> N04-1019 </papid>where one does not declare particular human summary absolute reference to compare summaries against, but rather makes every one of multiple human summaries at hand bear on evaluation; rouge (lin and hovy, 2003) <papid> N03-1020 </papid>represents another such effort.</citsent>
<aftsection>
<nextsent>the bayesian sum marist represents yet another, whereby one seeks summary most typical of those created by humans.
</nextsent>



</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1866">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>we discuss such strapping?
</prevsent>
<prevsent>method sin general, and exhibit particular method for strapping word sense classifiers for ambiguous words.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
our experiments on the canadian hansa rds show that our unsupervised technique is significantly more effective than picking seeds by hand (yarowsky, 1995), <papid> P95-1026 </papid>which in turn is known to rival supervised methods.</citsent>
<aftsection>
<nextsent>some of nlps most interesting problems have to do with unsupervised learning.
</nextsent>
<nextsent>human language learners are ableto discover word senses, grammatical genders, morphological systems, grammars, discourse registers, and so forth.
</nextsent>
<nextsent>one would like to build systems that discover the same linguistic patterns in raw text.
</nextsent>
<nextsent>for that matter, one would also like to discover patterns in bilingual text (for translation), in document collections (for categorization and retrieval), and in other data that fall outside the scope of humans?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1867">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these vectors are detailed abstract representations of the words or documents.
</prevsent>
<prevsent>they can be clustered, or all their bits can be included as potentially relevant features in another task.
</prevsent>
</prevsection>
<citsent citstr=" P05-1001 ">
2ando and zhang (2005) <papid> P05-1001 </papid>independently used this phrase, fora semi-supervised, cross-task learner that differs from our unsupervised, cross-instance learner.</citsent>
<aftsection>
<nextsent>both their work and ours try to transfer knowledge to target problem from many artificial supervised auxiliary problems,?
</nextsent>
<nextsent>which are generated from unlabeled data (e.g., our pseudo word disambiguation problems).
</nextsent>
<nextsent>however, in their structural learning,?
</nextsent>
<nextsent>the target problem is supervised (if inadequately), and the auxiliary problems (super vised instances of different task) are source of useful hidden features for the classifier.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1870">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> bootstrapping.  </section>
<citcontext>
<prevsection>
<prevsent>of web pages.
</prevsent>
<prevsent>strapping may be useful for unsupervised text categorization in general.
</prevsent>
</prevsection>
<citsent citstr=" W03-0404 ">
riloff et al (2003) <papid> W03-0404 </papid>learned lists of subjective nouns in english, seeding their method with 20 high-frequency,strongly subjective words.</citsent>
<aftsection>
<nextsent>this seed set was chosen manually from an automatically generated list of 850 can 4more precisely, they boot strapped two naive bayes classifiers one that looked at page content and the other that looked at links to the page.
</nextsent>
<nextsent>this co-training?
</nextsent>
<nextsent>approach has be come popular.
</nextsent>
<nextsent>it was also used by the cucerzan and yarowsky papers below, which looked at internal?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1871">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> bootstrapping.  </section>
<citcontext>
<prevsection>
<prevsent>classes.
</prevsent>
<prevsent>strapping would guess various different seeds that extended the original document, then try to determine which seeds found cohesive relevant set.?
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
collins and singer (1999) <papid> W99-0613 </papid>boot strapped system for classifying phrases in context.</citsent>
<aftsection>
<nextsent>again, they considered only one instance of this task: classifying english proper names as persons, organizations, or locations.
</nextsent>
<nextsent>their seed consisted of 7 simple rules (that new york, california,and u.s. are locations; that any name containing incorporated is an organization; and that i.b.m. and microsoft are organizations?).
</nextsent>
<nextsent>strapping such classifier would automatically discover named-entity classes in different language, or other phrase classes in english.
</nextsent>
<nextsent>cucerzan and yarowsky (1999) <papid> W99-0612 </papid>built similar system that identified proper names as well as classifying them.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1872">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> bootstrapping.  </section>
<citcontext>
<prevsection>
<prevsent>their seed consisted of 7 simple rules (that new york, california,and u.s. are locations; that any name containing incorporated is an organization; and that i.b.m. and microsoft are organizations?).
</prevsent>
<prevsent>strapping such classifier would automatically discover named-entity classes in different language, or other phrase classes in english.
</prevsent>
</prevsection>
<citsent citstr=" W99-0612 ">
cucerzan and yarowsky (1999) <papid> W99-0612 </papid>built similar system that identified proper names as well as classifying them.</citsent>
<aftsection>
<nextsent>their seed consisted of list of 40 to 300 names.
</nextsent>
<nextsent>large seeds were not necessary for precision but did help recall.
</nextsent>
<nextsent>cucerzan and yarowsky (2003) <papid> N03-1006 </papid>classified masculine vs. feminine nouns.</nextsent>
<nextsent>they experimented with several task instances, namely different indo-european languages.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1873">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> bootstrapping.  </section>
<citcontext>
<prevsection>
<prevsent>their seed consisted of list of 40 to 300 names.
</prevsent>
<prevsent>large seeds were not necessary for precision but did help recall.
</prevsent>
</prevsection>
<citsent citstr=" N03-1006 ">
cucerzan and yarowsky (2003) <papid> N03-1006 </papid>classified masculine vs. feminine nouns.</citsent>
<aftsection>
<nextsent>they experimented with several task instances, namely different indo-european languages.
</nextsent>
<nextsent>in each instance, their seed consisted of up to 30 feminine and 30 masculine words (e.g., girl, princess, father).
</nextsent>
<nextsent>many more papers along these lines could be listed.
</nextsent>
<nextsent>a rather different task is grammar induction, where task instance is corpus of text in some language, and the learned classifier is parser.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1875">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> return cs?..  </section>
<citcontext>
<prevsection>
<prevsent>how can we tell if seed = (x, y) was fertile, without using even small validation set to judge cs?
</prevsent>
<prevsent>there are several types of 5alignment methods in machine translation rely even more heavily on this property.
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
while they begin with small translation lexicon, they are sufficiently robust to the choice of this initial seed (lexicon) that it suffices to construct single seed by crude automatic means (brown et al, 1990; <papid> J90-2002 </papid>melamed, 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>human supervision (or strapping) is unnecessary.
</nextsent>
<nextsent>6this is particularly likely if one favors function words (inparticular determiners and pronouns), which are strong indicators of gender.
</nextsent>
<nextsent>cucerzan and yarowsky used only content words because they could be extracted from bilingual dictionaries.clues to fertility, which may be combined into meta classifier that identifies fertile seeds.judge the result of classification with cs: even without validation set, the result of running cs on the training corpus can be validated in various ways, using independent plausibility criteria that were not considered by the bootstrapping learner.?
</nextsent>
<nextsent>is the classification reasonably balanced?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1876">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> return cs?..  </section>
<citcontext>
<prevsection>
<prevsent>how can we tell if seed = (x, y) was fertile, without using even small validation set to judge cs?
</prevsent>
<prevsent>there are several types of 5alignment methods in machine translation rely even more heavily on this property.
</prevsent>
</prevsection>
<citsent citstr=" P97-1063 ">
while they begin with small translation lexicon, they are sufficiently robust to the choice of this initial seed (lexicon) that it suffices to construct single seed by crude automatic means (brown et al, 1990; <papid> J90-2002 </papid>melamed, 1997).<papid> P97-1063 </papid></citsent>
<aftsection>
<nextsent>human supervision (or strapping) is unnecessary.
</nextsent>
<nextsent>6this is particularly likely if one favors function words (inparticular determiners and pronouns), which are strong indicators of gender.
</nextsent>
<nextsent>cucerzan and yarowsky used only content words because they could be extracted from bilingual dictionaries.clues to fertility, which may be combined into meta classifier that identifies fertile seeds.judge the result of classification with cs: even without validation set, the result of running cs on the training corpus can be validated in various ways, using independent plausibility criteria that were not considered by the bootstrapping learner.?
</nextsent>
<nextsent>is the classification reasonably balanced?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1877">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> return cs?..  </section>
<citcontext>
<prevsection>
<prevsent>(if virtually all examples of the target word are labeled withthe same sense, then cs has not found sense dis tinction.)
</prevsent>
<prevsent>when document contains multiple tokens of the target word, are all examples labeled with the samesense?
</prevsent>
</prevsection>
<citsent citstr=" H92-1045 ">
this property tends to hold for correct classifiers (gale et al, 1992<papid> H92-1045 </papid>a), at least for homonyms.</citsent>
<aftsection>
<nextsent>true word senses usually correlate with docu mentor passage topic.
</nextsent>
<nextsent>thus, choose measure of similarity between documents (e.g., the cosine measure in tf/idf space).
</nextsent>
<nextsent>does the target word tend to have the same sense in document and in its nearby neighbors?
</nextsent>
<nextsent>true word senses may also improve performance on some task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1880">
<title id=" H05-1050.xml">bootstrapping without the boot </title>
<section> return cs?..  </section>
<citcontext>
<prevsection>
<prevsent>our idea is to use them as cheap development data to tune system.
</prevsent>
<prevsent>in our case, they tune few free parameters of h(s), which says what good classifier for this task looks like.
</prevsent>
</prevsection>
<citsent citstr=" N03-2023 ">
pseudo words should be plausible instances of the task (gaus tad, 2001; nakov and hearst, 2003): <papid> N03-2023 </papid>so it is deliberate that banana and wine share syntactic and semantic features, as senses of real ambiguous words often do.</citsent>
<aftsection>
<nextsent>cheap pseudo-supervised?
</nextsent>
<nextsent>data are also available in some other strapping settings.
</nextsent>
<nextsent>for grammar induction, one could construct an artificial probabilistic grammar at random, and generate text from it.
</nextsent>
<nextsent>the task of recovering the grammar from the text then has known answer.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1890">
<title id=" E93-1046.xml">ambiguity resolution in a reductionistic parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we are concerned with grammar-based surface-syntactic analysis of running text.
</prevsent>
<prevsent>morpho-logical and syntactic analysis is here based on the use of tags that express surface-syntactic relations between functional categories such as subject, mod-ifier, main verb etc.; consider the following simple example: pron ~subject see pres @mainverb art qdet bird ~object full stop *the development of engcg was supported by tekes, the finnish technological development center, and part of the work on finite-state syntax has been supported by the academy of finland.
</prevsent>
</prevsection>
<citsent citstr=" C90-3030 ">
in this type of analysis, each word gets mor-phosyntactic analysis i. the present work is closely connected with two parsing formalisms, constraint grammar \[karls- son, 1990; <papid> C90-3030 </papid>karlsson et ai., 1991; voutilainen et ai., 1992; karlsson et ai., 1993\] and finlte-state syn-tax as advocated by \[koskenniemi, 1990; <papid> C90-2040 </papid>tapanai-nen, 1991; koskenniemi et al, 1992\].<papid> C92-1027 </papid></citsent>
<aftsection>
<nextsent>the con-straint grammar parser of english is sequential modular system that assigns shallow surface-true dependency-oriented functional analysis on running text, annotating each word with morphological and syntactic tags.
</nextsent>
<nextsent>the finite-state parser assigns sim-ilar type of analysis, but it operates on all levels of ambiguity 2 in parallel rather than sequentially, en-abling the gram marian to refer to all levels of struc-tural description in single uniform rule component.
</nextsent>
<nextsent>engcg, wide-coverage english constraint grammar and lexicon, was written 1989-1992, and the system is currently available 3.
</nextsent>
<nextsent>the constraint grammar framework was proposed by fred karls-son, and the english constraint grammar was de-veloped by afro voutilainen (lexicon, morphological disambiguation), juha heikkil~i (lexicon) and arto anttila (syntax).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1891">
<title id=" E93-1046.xml">ambiguity resolution in a reductionistic parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we are concerned with grammar-based surface-syntactic analysis of running text.
</prevsent>
<prevsent>morpho-logical and syntactic analysis is here based on the use of tags that express surface-syntactic relations between functional categories such as subject, mod-ifier, main verb etc.; consider the following simple example: pron ~subject see pres @mainverb art qdet bird ~object full stop *the development of engcg was supported by tekes, the finnish technological development center, and part of the work on finite-state syntax has been supported by the academy of finland.
</prevsent>
</prevsection>
<citsent citstr=" C90-2040 ">
in this type of analysis, each word gets mor-phosyntactic analysis i. the present work is closely connected with two parsing formalisms, constraint grammar \[karls- son, 1990; <papid> C90-3030 </papid>karlsson et ai., 1991; voutilainen et ai., 1992; karlsson et ai., 1993\] and finlte-state syn-tax as advocated by \[koskenniemi, 1990; <papid> C90-2040 </papid>tapanai-nen, 1991; koskenniemi et al, 1992\].<papid> C92-1027 </papid></citsent>
<aftsection>
<nextsent>the con-straint grammar parser of english is sequential modular system that assigns shallow surface-true dependency-oriented functional analysis on running text, annotating each word with morphological and syntactic tags.
</nextsent>
<nextsent>the finite-state parser assigns sim-ilar type of analysis, but it operates on all levels of ambiguity 2 in parallel rather than sequentially, en-abling the gram marian to refer to all levels of struc-tural description in single uniform rule component.
</nextsent>
<nextsent>engcg, wide-coverage english constraint grammar and lexicon, was written 1989-1992, and the system is currently available 3.
</nextsent>
<nextsent>the constraint grammar framework was proposed by fred karls-son, and the english constraint grammar was de-veloped by afro voutilainen (lexicon, morphological disambiguation), juha heikkil~i (lexicon) and arto anttila (syntax).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1892">
<title id=" E93-1046.xml">ambiguity resolution in a reductionistic parser </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in this paper we are concerned with grammar-based surface-syntactic analysis of running text.
</prevsent>
<prevsent>morpho-logical and syntactic analysis is here based on the use of tags that express surface-syntactic relations between functional categories such as subject, mod-ifier, main verb etc.; consider the following simple example: pron ~subject see pres @mainverb art qdet bird ~object full stop *the development of engcg was supported by tekes, the finnish technological development center, and part of the work on finite-state syntax has been supported by the academy of finland.
</prevsent>
</prevsection>
<citsent citstr=" C92-1027 ">
in this type of analysis, each word gets mor-phosyntactic analysis i. the present work is closely connected with two parsing formalisms, constraint grammar \[karls- son, 1990; <papid> C90-3030 </papid>karlsson et ai., 1991; voutilainen et ai., 1992; karlsson et ai., 1993\] and finlte-state syn-tax as advocated by \[koskenniemi, 1990; <papid> C90-2040 </papid>tapanai-nen, 1991; koskenniemi et al, 1992\].<papid> C92-1027 </papid></citsent>
<aftsection>
<nextsent>the con-straint grammar parser of english is sequential modular system that assigns shallow surface-true dependency-oriented functional analysis on running text, annotating each word with morphological and syntactic tags.
</nextsent>
<nextsent>the finite-state parser assigns sim-ilar type of analysis, but it operates on all levels of ambiguity 2 in parallel rather than sequentially, en-abling the gram marian to refer to all levels of struc-tural description in single uniform rule component.
</nextsent>
<nextsent>engcg, wide-coverage english constraint grammar and lexicon, was written 1989-1992, and the system is currently available 3.
</nextsent>
<nextsent>the constraint grammar framework was proposed by fred karls-son, and the english constraint grammar was de-veloped by afro voutilainen (lexicon, morphological disambiguation), juha heikkil~i (lexicon) and arto anttila (syntax).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1894">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>finally, we establish that combining the output of context-free and finite state parsers gives much higher results than the previous-best published results, on several common tasks.
</prevsent>
<prevsent>while the efficiency benefit of finite-state models is inarguable, the results presented here show that the corresponding cost in accuracy is higher than previously thought.
</prevsent>
</prevsection>
<citsent citstr=" W95-0107 ">
finite-state parsing (also called chunking or shallowparsing) has typically been motivated as fast first pass for ? or approximation to ? more expensive context-free parsing (abney, 1991; ramshaw and marcus, 1995; <papid> W95-0107 </papid>abney, 1996).</citsent>
<aftsection>
<nextsent>for many very-largescale natural language processing tasks (e.g. open domain question answering from the web), context free parsing may be too expensive, whereas finite state parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text.
</nextsent>
<nextsent>for this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years.
</nextsent>
<nextsent>in addition to the clear efficiency benefit of shallow parsing, li and roth (2001) <papid> W01-0706 </papid>have further claimed both an accuracy and robustness benefit versus context-free parsing.</nextsent>
<nextsent>the output of context free parser, such as that of collins (1997) <papid> P97-1003 </papid>or charniak (2000), <papid> A00-2018 </papid>can be transformed into sequence of shallow constituents for comparison with the output of shallow parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1895">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for many very-largescale natural language processing tasks (e.g. open domain question answering from the web), context free parsing may be too expensive, whereas finite state parsing is many orders of magnitude faster and can also provide very useful syntactic annotations for large amounts of text.
</prevsent>
<prevsent>for this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years.
</prevsent>
</prevsection>
<citsent citstr=" W01-0706 ">
in addition to the clear efficiency benefit of shallow parsing, li and roth (2001) <papid> W01-0706 </papid>have further claimed both an accuracy and robustness benefit versus context-free parsing.</citsent>
<aftsection>
<nextsent>the output of context free parser, such as that of collins (1997) <papid> P97-1003 </papid>or charniak (2000), <papid> A00-2018 </papid>can be transformed into sequence of shallow constituents for comparison with the output of shallow parser.</nextsent>
<nextsent>li and roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known conll 2000 task (sang and buchholz, 2000), outperformed the collins parser incorrectly identifying these constituents in the penn wall street journal (wsj) tree bank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1900">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years.
</prevsent>
<prevsent>in addition to the clear efficiency benefit of shallow parsing, li and roth (2001) <papid> W01-0706 </papid>have further claimed both an accuracy and robustness benefit versus context-free parsing.</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
the output of context free parser, such as that of collins (1997) <papid> P97-1003 </papid>or charniak (2000), <papid> A00-2018 </papid>can be transformed into sequence of shallow constituents for comparison with the output of shallow parser.</citsent>
<aftsection>
<nextsent>li and roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known conll 2000 task (sang and buchholz, 2000), outperformed the collins parser incorrectly identifying these constituents in the penn wall street journal (wsj) tree bank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>they argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining hierarchical analysis over the entire string.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1903">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for this reason, finite-state parsing (hereafter referred to as shallow parsing) has received increasing attention in recent years.
</prevsent>
<prevsent>in addition to the clear efficiency benefit of shallow parsing, li and roth (2001) <papid> W01-0706 </papid>have further claimed both an accuracy and robustness benefit versus context-free parsing.</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the output of context free parser, such as that of collins (1997) <papid> P97-1003 </papid>or charniak (2000), <papid> A00-2018 </papid>can be transformed into sequence of shallow constituents for comparison with the output of shallow parser.</citsent>
<aftsection>
<nextsent>li and roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known conll 2000 task (sang and buchholz, 2000), outperformed the collins parser incorrectly identifying these constituents in the penn wall street journal (wsj) tree bank (marcus et al, 1993).<papid> J93-2004 </papid></nextsent>
<nextsent>they argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining hierarchical analysis over the entire string.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1904">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in addition to the clear efficiency benefit of shallow parsing, li and roth (2001) <papid> W01-0706 </papid>have further claimed both an accuracy and robustness benefit versus context-free parsing.</prevsent>
<prevsent>the output of context free parser, such as that of collins (1997) <papid> P97-1003 </papid>or charniak (2000), <papid> A00-2018 </papid>can be transformed into sequence of shallow constituents for comparison with the output of shallow parser.</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
li and roth demonstrated that their shallow parser, trained to label shallow constituents along the lines of the well-known conll 2000 task (sang and buchholz, 2000), outperformed the collins parser incorrectly identifying these constituents in the penn wall street journal (wsj) tree bank (marcus et al, 1993).<papid> J93-2004 </papid></citsent>
<aftsection>
<nextsent>they argued that their superior performance was due to optimizing directly for the local sequence labeling objective, rather than for obtaining hierarchical analysis over the entire string.
</nextsent>
<nextsent>they further showed that their shallow parser trained on the penn wsj treebank did far better job of annotating out-of-domain sentences (e.g. conversational speech) than the collins parser.this paper re-examines the comparison of shallow parsers with context-free parsers, beginning with critical examination of how their outputs are compared.
</nextsent>
<nextsent>we demonstrate that changes to the conversion routine, which take into account differences between the original treebank trees and the trees output by context-free parsers, eliminate the previously-reported accuracy differences.
</nextsent>
<nextsent>second, we show that convention that is widely accepted for evaluation of context-free parses ? ignoring punctuation when setting the span of constituent ? results in improved shallow parsing performance by certain context-free parsers across variety of shallow parsing tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1921">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>all 98.37 99.72 99.72 truth vp 92.14 98.70 98.70li and roth all 94.64 - (2001) vp 95.28 - collins (1997) <papid> P97-1003 </papid>all 92.16 93.42 94.28 vp 88.15 94.31 94.42 charniak all 93.88 95.15 95.32 (2000) vp 88.92 95.11 95.19 table 1: f-measure shallow bracketing accuracy under three different evaluation scenarios: (a) baseline, used in li and roth (2001), <papid> W01-0706 </papid>with original chun klink script converting treebank trees and context-free parser output; (b) same as (a), except that empty subject nps are inserted into every unary svp produc tion; and (c) same as (b), except that punctuation is ignored for setting constituent span.</prevsent>
<prevsent>results for li and roth are reported from their paper.</prevsent>
</prevsection>
<citsent citstr=" J95-4004 ">
the collins parser is provided with part-of speech tags output by the brill tagger (brill, 1995).<papid> J95-4004 </papid></citsent>
<aftsection>
<nextsent>damentally, constituents are groupings of words.
</nextsent>
<nextsent>interestingly, this convention was not followed in the conll-2000 task (sang and buchholz, 2000),which as we will see has variable effect on context free parsers, presumably depending on the degree to which punctuation is moved in training.
</nextsent>
<nextsent>2.1 evaluation analysis.
</nextsent>
<nextsent>to determine the effects of the conversion routine and different evaluation conventions, we compare the performance of several different models on one of the tasks presented in li and roth (2001).<papid> W01-0706 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1933">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>however, for our purposes, this insertion repair sufficiently corrects the error to perform meaningful comparisons.
</prevsent>
<prevsent>finally, evaluation scenario (c) follows the context free parsing evaluation convention of ignoring punctuation when assigning constituent span.
</prevsent>
</prevsection>
<citsent citstr=" J04-4004 ">
this affects some parsers more than others, depending on how the parser treats punctuation internally; for example, bikel (2004) <papid> J04-4004 </papid>documents that the collins parser raises punctuation nodes within the parsetree.</citsent>
<aftsection>
<nextsent>since ignoring punctuation cannot hurt performance, only improve it, even the smallest of these differences are statistically significant.note that after inserting empty nodes and ignoring punctuation, the accuracy advantage of li and roth over collins is reduced to dead heat.
</nextsent>
<nextsent>of the two parsers we evaluated, the charniak (2000)<papid> A00-2018 </papid>parser gave the best performance, which is consistent with its higher reported performance on the context-free parsing task versus other context-free parsers.</nextsent>
<nextsent>collins (2000) reported reranking model that improved his parser output to roughly the same level of accuracy as charniak (2000)<papid> A00-2018 </papid>, and charniak and johnson (2005) <papid> P05-1022 </papid>report an improvement using reranking over charniak (2000)<papid> A00-2018 </papid>.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1937">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>since ignoring punctuation cannot hurt performance, only improve it, even the smallest of these differences are statistically significant.note that after inserting empty nodes and ignoring punctuation, the accuracy advantage of li and roth over collins is reduced to dead heat.
</prevsent>
<prevsent>of the two parsers we evaluated, the charniak (2000)<papid> A00-2018 </papid>parser gave the best performance, which is consistent with its higher reported performance on the context-free parsing task versus other context-free parsers.</prevsent>
</prevsection>
<citsent citstr=" P05-1022 ">
collins (2000) reported reranking model that improved his parser output to roughly the same level of accuracy as charniak (2000)<papid> A00-2018 </papid>, and charniak and johnson (2005) <papid> P05-1022 </papid>report an improvement using reranking over charniak (2000)<papid> A00-2018 </papid>.</citsent>
<aftsection>
<nextsent>for the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data tobe applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with shallow parser.
</nextsent>
<nextsent>both the bikel (2004) <papid> J04-4004 </papid>imple 789 system np-chunking conll-2000 li &amp; roth task sprep averaged perceptron 94.21 93.54 95.12kudo and matsumoto (2001) <papid> N01-1025 </papid>94.22 93.91 sha and pereira (2003) <papid> N03-1028 </papid>crf 94.38 - voted perceptron 94.09 - zhang et al (2002) - 94.17 li and roth (2001) <papid> W01-0706 </papid>- 93.02 94.64 table 2: baseline results on three shallow parsing tasks: the np-chunking task (ramshaw and marcus, 1995); <papid> W95-0107 </papid>the conll-2000 chunking task (sang and buchholz, 2000); and the li &amp; roth task (li and roth, 2001), <papid> W01-0706 </papid>which is the same as conll-2000 but with more training data and different test section.</nextsent>
<nextsent>the results reported in this table include the best published results on each of these tasks.mentation of the collins parser and the n-best version of the charniak (2000)<papid> A00-2018 </papid> parser, documented in charniak and johnson (2005), <papid> P05-1022 </papid>fit the requirements.since we observed higher accuracy from the charniak parser, from this point forward we report just charniak parser results4.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1943">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>collins (2000) reported reranking model that improved his parser output to roughly the same level of accuracy as charniak (2000)<papid> A00-2018 </papid>, and charniak and johnson (2005) <papid> P05-1022 </papid>report an improvement using reranking over charniak (2000)<papid> A00-2018 </papid>.</prevsent>
<prevsent>for the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data tobe applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with shallow parser.</prevsent>
</prevsection>
<citsent citstr=" N01-1025 ">
both the bikel (2004) <papid> J04-4004 </papid>imple 789 system np-chunking conll-2000 li &amp; roth task sprep averaged perceptron 94.21 93.54 95.12kudo and matsumoto (2001) <papid> N01-1025 </papid>94.22 93.91 sha and pereira (2003) <papid> N03-1028 </papid>crf 94.38 - voted perceptron 94.09 - zhang et al (2002) - 94.17 li and roth (2001) <papid> W01-0706 </papid>- 93.02 94.64 table 2: baseline results on three shallow parsing tasks: the np-chunking task (ramshaw and marcus, 1995); <papid> W95-0107 </papid>the conll-2000 chunking task (sang and buchholz, 2000); and the li &amp; roth task (li and roth, 2001), <papid> W01-0706 </papid>which is the same as conll-2000 but with more training data and different test section.</citsent>
<aftsection>
<nextsent>the results reported in this table include the best published results on each of these tasks.mentation of the collins parser and the n-best version of the charniak (2000)<papid> A00-2018 </papid> parser, documented in charniak and johnson (2005), <papid> P05-1022 </papid>fit the requirements.since we observed higher accuracy from the charniak parser, from this point forward we report just charniak parser results4.</nextsent>
<nextsent>2.2 shallow parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1945">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>collins (2000) reported reranking model that improved his parser output to roughly the same level of accuracy as charniak (2000)<papid> A00-2018 </papid>, and charniak and johnson (2005) <papid> P05-1022 </papid>report an improvement using reranking over charniak (2000)<papid> A00-2018 </papid>.</prevsent>
<prevsent>for the purposes of this paper, we needed an available parser that was (a) trainable on different subsets of the data tobe applied to various tasks; and (b) capable of producing n-best candidates, for potential combination with shallow parser.</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
both the bikel (2004) <papid> J04-4004 </papid>imple 789 system np-chunking conll-2000 li &amp; roth task sprep averaged perceptron 94.21 93.54 95.12kudo and matsumoto (2001) <papid> N01-1025 </papid>94.22 93.91 sha and pereira (2003) <papid> N03-1028 </papid>crf 94.38 - voted perceptron 94.09 - zhang et al (2002) - 94.17 li and roth (2001) <papid> W01-0706 </papid>- 93.02 94.64 table 2: baseline results on three shallow parsing tasks: the np-chunking task (ramshaw and marcus, 1995); <papid> W95-0107 </papid>the conll-2000 chunking task (sang and buchholz, 2000); and the li &amp; roth task (li and roth, 2001), <papid> W01-0706 </papid>which is the same as conll-2000 but with more training data and different test section.</citsent>
<aftsection>
<nextsent>the results reported in this table include the best published results on each of these tasks.mentation of the collins parser and the n-best version of the charniak (2000)<papid> A00-2018 </papid> parser, documented in charniak and johnson (2005), <papid> P05-1022 </papid>fit the requirements.since we observed higher accuracy from the charniak parser, from this point forward we report just charniak parser results4.</nextsent>
<nextsent>2.2 shallow parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J1965">
<title id=" H05-1099.xml">comparing and combining finite state and context free parsers </title>
<section> evaluating heterogeneous parser output.  </section>
<citcontext>
<prevsection>
<prevsent>our shallow parser uses exactly the feature set delineated by sha and pereira, and performs the decoding process using viterbi search with second-order markov assumption as they described.
</prevsent>
<prevsent>these features include unigram and bigram words up to two positions to either side of the current word; unigram, bigram, and trigram part-of-speech (pos) tags up to two positions to either side of the current word; and unigram, bigram, and trigram shallow constituent tags.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
we use the averaged perceptron algorithm, as presented in collins (2002), <papid> W02-1001 </papid>to train the parser.</citsent>
<aftsection>
<nextsent>see (sha and pereira, 2003) <papid> N03-1028 </papid>for more details on this approach.to demonstrate the competitiveness of our baseline shallow parser, which we label the sprep averaged perceptron, table 2 shows results on three different shallow parsing tasks.</nextsent>
<nextsent>the np-chunking 4the parser is available for research purposes atftp://ftp.cs.brown.edu/pub/nlparser/ and can be run in best mode.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2024">
<title id=" E93-1047.xml">type driven semantic interpretation of fstructures </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>this approach works well when the structural discrepancies between form and meaning representations are finitely bounded, but there are some phenomena in natural an- guage, e.g. adverbs in english, where this restriction does not hold.
</prevsent>
<prevsent>in this paper, we describe rule-based type-driven interpreta-tion algorithms which cover cases of such structural misalignment by exploiting new descriptive device, the  restriction op-erator .
</prevsent>
</prevsection>
<citsent citstr=" E93-1024 ">
the algorithms are set up in such way that recursive rules can be derived for the interpretation adjunct sets within codescripiion approach (see \[kaplan and wedekind, 1993\] <papid> E93-1024 </papid>for details).</citsent>
<aftsection>
<nextsent>in \[kaplan and bresnan, 1982\] lexical functional grammar (lfg) was introduced as grammatical formalism that assigns to sentence ntities of two different levels of representation: c-structure rep-resenting information on the structure of the phrases of sentence and an f-structure which represents its underlying predicate-argument structure.
</nextsent>
<nextsent>the struc-tures are set in correspondence by function from the c-structure nodes (constituents) into the sub-structures of the f-structure.
</nextsent>
<nextsent>the f-structure is iden-tified with the smallest structure that satisfies the f-description, description of the f-structure which is built up by instantiation of the annotations of the context-free rules and projected off the c-structure by the correspondence mapping.
</nextsent>
<nextsent>this architecture was then extended by kaplan \[1987\] and halvorsen \[1987\] to structures represent-ing information on other levels of linguistic repre-sentation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2028">
<title id=" E93-1047.xml">type driven semantic interpretation of fstructures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>j*    / ~ . . / john   arrived l.~,.g1 3ohn figure 1 structural correspondences tween c-, f- and a-structure.
</prevsent>
<prevsent>semantic structure ((r-structure) and the structural correspondence tween f- and a-structure for the sentence john arrived are co described by additional 404 annotations of the lexical entry for arrived in (1).
</prevsent>
</prevsection>
<citsent citstr=" E91-1051 ">
(1) arrived: v, (t prted)=  arrive(sudj)  (a rel) ---- arrive (a aaq1) = a(t sunj) within the domain of translation, wedekind \[1988\], and sadler and thompson \[1991\] <papid> E91-1051 </papid>recognized some problems of the correspondence approach which con-cern data of head.switching.</citsent>
<aftsection>
<nextsent>these difficulties also arise in the domain of semantic interpretation.
</nextsent>
<nextsent>in the latter domain we find constructions where the syntactic head (the predicate) does not correspond to the semantic head as, e.g., in adverbially-modified sentences like (2) (2) john arrived late whose f- and a-structure are given in figure 2.
</nextsent>
<nextsent>in f: \]pred  arrive(sub j)  p.el ls ., \[p  john \] arg1 arg\] figure 2 head-switching between fjand a-structure.
</nextsent>
<nextsent>arr wl\] joh.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2034">
<title id=" E93-1047.xml">type driven semantic interpretation of fstructures </title>
<section> a bot tom-up type-dr iven   </section>
<citcontext>
<prevsection>
<prevsent>by the completeness and coherence conditions it can first be guaranteed that the different kinds of nuclei (consisting of predicate and the functions it sub- categorizes for) will get an interpretation of the right type.
</prevsent>
<prevsent>since all free grammatical functions (adj) are homogeneous functors (argument and value are of the same type) and it is clear from the c-structure rules which type of argument they modify (a modifier on s-level is either sentence or predicate modifier, 409 etc.), f-structures with free functions can also be en-sured to be interpreted.
</prevsent>
</prevsection>
<citsent citstr=" C90-2013 ">
on the other hand particular eadings can be ex-cluded by global binding and/or scoping principles, similar to the ones formulated in \[dalrymple et al, 1990\].<papid> C90-2013 </papid></citsent>
<aftsection>
<nextsent>these principles constrain the interpretation of the f-structures and their parts if special con-ditions are satisfied.
</nextsent>
<nextsent>by combining outside-in and inside-out functional uncertainty we can express by the following constraint e.g. that under some condi-tions the substructure (t a) of an f-structure has wide scope over (t b): - * ((fu 0 (t a)) arg + fu)  -?
</nextsent>
<nextsent>o-(t b).
</nextsent>
<nextsent>due to the interpretation function (~r) between typed f-structure and its semantic representation it is also possible to formulate compositionality prin-ciple very similar to the classical one.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2041">
<title id=" E93-1047.xml">type driven semantic interpretation of fstructures </title>
<section> a bot tom-up type-dr iven   </section>
<citcontext>
<prevsection>
<prevsent>in j. bresnan, ed., the mental representation of grammatical relations.
</prevsent>
<prevsent>cambridge, mass.: the mit press, 1982.
</prevsent>
</prevsection>
<citsent citstr=" E89-1037 ">
\[kaplan et al, 1989\] <papid> E89-1037 </papid>kaplan, r., k. netter, j. wedekind, and a. zaenen.</citsent>
<aftsection>
<nextsent>translation by struc-tural correspondences.
</nextsent>
<nextsent>in proceedings of the sth conference of the european chapter of the associ-ation for computational linguistics.
</nextsent>
<nextsent>manchester, 1989.
</nextsent>
<nextsent>\[kaplan and wedekind, 1993\] <papid> E93-1024 </papid>kaplan, r., and j. wedekind.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2047">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>definitions of this kind are often hidden?
</prevsent>
<prevsent>in oblique contexts (e.g., he said that gasohol, mixture of gasoline and ethanol, has been great for his business.?).
</prevsent>
</prevsection>
<citsent citstr=" C04-1199 ">
in recent work, miliaraki and androutsopoulos (2004), <papid> C04-1199 </papid>hereafter m&a;, proposed method we call 1see, for example, wikipedia (http://www.wikipedia.org/).</citsent>
<aftsection>
<nextsent>wordnets glosses are another on-line source of definitions.
</nextsent>
<nextsent>323 defqa, which handles definition questions.
</nextsent>
<nextsent>the method assumes that question pre processor separates definition from other types of questions, andthat in definition questions this module also identifies the term to be defined, called the target term.2the input to defqa is (possibly multi-word) target term, along with the most highly ranked documents that an ir system returned for that term.
</nextsent>
<nextsent>the output is list of 250-character snippets from the documents, at least one of which must contain an acceptable short definition of the target term, much as in the qa track of trec-2000 and trec-2001.3we note that since 2003, trec requires definition questions to be answered by lists of complementary snippets, jointly providing range of information nuggets about the target term (voorhees,2003).<papid> N03-2037 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2048">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>323 defqa, which handles definition questions.
</prevsent>
<prevsent>the method assumes that question pre processor separates definition from other types of questions, andthat in definition questions this module also identifies the term to be defined, called the target term.2the input to defqa is (possibly multi-word) target term, along with the most highly ranked documents that an ir system returned for that term.
</prevsent>
</prevsection>
<citsent citstr=" N03-2037 ">
the output is list of 250-character snippets from the documents, at least one of which must contain an acceptable short definition of the target term, much as in the qa track of trec-2000 and trec-2001.3we note that since 2003, trec requires definition questions to be answered by lists of complementary snippets, jointly providing range of information nuggets about the target term (voorhees,2003).<papid> N03-2037 </papid></citsent>
<aftsection>
<nextsent>in contrast, here we focus on locating single snippet definitions.
</nextsent>
<nextsent>we believe this task is still interesting and of practical use.
</nextsent>
<nextsent>for example, list of single-snippet definitions accompanied by their source urls is good starting point for users of search engines wishing to obtain definitions.
</nextsent>
<nextsent>single snippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2049">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, list of single-snippet definitions accompanied by their source urls is good starting point for users of search engines wishing to obtain definitions.
</prevsent>
<prevsent>single snippet definitions can also be useful in information extraction, where the templates to be filled in often require short entity descriptions.
</prevsent>
</prevsection>
<citsent citstr=" N04-1007 ">
we also note that the post-2003 trec task has encountered evaluation problems, because it is difficult to agree on which nuggets should be included in the multi-snippet definitions (hildebrandt et al , 2004).<papid> N04-1007 </papid></citsent>
<aftsection>
<nextsent>in contrast, our experimental results of section 4 indicate strong inter-assessor agreement for single-snippet answers,suggesting that it is easier to agree upon what constitutes an acceptable single-snippet definition.defqa relies on an svm, which is trained to classify 250-character snippets that have the target termat their centre, hereafter called windows, as accept able definitions or non-definitions.4 to train the svm, collection of training target terms is used; m&a; used the target terms of definition questions from trec-2000 and trec-2001.
</nextsent>
<nextsent>the terms are submitted to an ir system, which returns the most 2alternatively, the user can be asked to specify explicitly the question type and target term via form-based interface.
</nextsent>
<nextsent>3definition questions were not considered in trec-2002.4see, for example, scholkopf and smola (2002) for information on svms.
</nextsent>
<nextsent>following m&a;, we use linear svm, as implemented by wekas smo class (http://www.cs.waikato.ac.nz/ml/weka/).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2050">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> evaluation.  </section>
<citcontext>
<prevsection>
<prevsent>this is lower than the score reported by m&a; (73%), but remarkably high given that in our evaluation the systems were allowed to return only one snippet per question; i.e., the task was much harder than in m&as; experiments.
</prevsent>
<prevsent>defqas answered correctly more than twice as many questions as defqat , despite the fact that its training data contained lot of noise.
</prevsent>
</prevsection>
<citsent citstr=" J04-1005 ">
(single-tailed difference-of-proportions tests show that all the differences of table 1 are statisti 11we follow the notation of di eugenio and glass (2004).<papid> J04-1005 </papid></citsent>
<aftsection>
<nextsent>the ks&c; figures were identical.
</nextsent>
<nextsent>the 2 ? (a) ? 1 figures were 0.80, 0.85, 0.95, 0.95, and 0.89 respectively.
</nextsent>
<nextsent>328 assessor 1 assessor 2 average baser 14.81 (12) 14.81 (12) 14.81 (12) base1 14.81 (12) 12.35 (10) 13.58 (11) defqat 25.93 (21) 25.93 (21) 25.93 (21) defqas 55.56 (45) 60.49 (49) 58.02 (47) table 1: percentage of questions answered correctly cally significant at ? = 0.001.)
</nextsent>
<nextsent>the superiority ofdefqas appears to be mostly due to its automatically acquired patterns.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2051">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>finally, note that the scores of both baselines are very poor, indicating that defqas performs significantly better than picking the first, or random snippet among those returned by the search engine.
</prevsent>
<prevsent>definition questions have recently attracted several qa researchers.
</prevsent>
</prevsection>
<citsent citstr=" C04-1093 ">
many of the proposed approaches,however, relyon manually crafted patterns or heuristics to identify definitions, and do not employ learning algorithms (liu et al , 2003; fujii and ishikawa, 2004; <papid> C04-1093 </papid>hildebrandt et al , 2004; <papid> N04-1007 </papid>xu et al , 2004).</citsent>
<aftsection>
<nextsent>ng et al  (2001) <papid> W01-0509 </papid>use machine learning (c5 with boosting) to classify and rank candidate answers ina general qa system, but they do not treat definition questions in any special way; consequently, their worst results are for what.</nextsent>
<nextsent>questions,that presumably include definition questions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2054">
<title id=" H05-1041.xml">a practically unsupervised learning method to identify single snippet answers to definition questions on the web </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>definition questions have recently attracted several qa researchers.
</prevsent>
<prevsent>many of the proposed approaches,however, relyon manually crafted patterns or heuristics to identify definitions, and do not employ learning algorithms (liu et al , 2003; fujii and ishikawa, 2004; <papid> C04-1093 </papid>hildebrandt et al , 2004; <papid> N04-1007 </papid>xu et al , 2004).</prevsent>
</prevsection>
<citsent citstr=" W01-0509 ">
ng et al  (2001) <papid> W01-0509 </papid>use machine learning (c5 with boosting) to classify and rank candidate answers ina general qa system, but they do not treat definition questions in any special way; consequently, their worst results are for what.</citsent>
<aftsection>
<nextsent>questions,that presumably include definition questions.
</nextsent>
<nextsent>ittycheriah and roukos (2002) employ maximum entropy model to rank candidate answers in general purpose qa system.
</nextsent>
<nextsent>their maximum entropy model uses very rich set of attributes, that includes 8,500 n-gram patterns.
</nextsent>
<nextsent>unlike our work, their n-grams arefive or more words long, they are coupled to two word question prefixes, and, in the case of definition questions, they do not need to be anchored at the target term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2059">
<title id=" H05-1048.xml">detection of entity mentions occuring in english and chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and family?, respectively.
</prevsent>
<prevsent>it is clear that the detection of these mentions is the first crucial step for the extraction of the er model to populate database or an ontology.
</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
extraction of entities and their relationships is usually done in pipe lined system that first identifies entity mentions, next resolves mentions into unique entities (co-reference) and finally finds relations among them (florian et al, 2004; <papid> N04-1001 </papid>kambhatla, 2004).<papid> P04-3022 </papid></citsent>
<aftsection>
<nextsent>in that architecture, the errors in the first stage propagate and reduce the performance of subsequent stages; namely, co-reference re solver, that clusters all different mentions of an entity into unique entity, and relation finder, that links entities according to their relationships.
</nextsent>
<nextsent>in fact, the subtaskof entity mention detection itself is very challeng 379 table 1: categorical structure of entities in ace program entity mention entity mention type sub-type class type role ing subtask since respective expressions can have relatively complex syntactic and categorical (se mantic?)
</nextsent>
<nextsent>structures.
</nextsent>
<nextsent>that is, entity mentions in abody of text can occur in relatively complex embedded constructs with many attributes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2060">
<title id=" H05-1048.xml">detection of entity mentions occuring in english and chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>and family?, respectively.
</prevsent>
<prevsent>it is clear that the detection of these mentions is the first crucial step for the extraction of the er model to populate database or an ontology.
</prevsent>
</prevsection>
<citsent citstr=" P04-3022 ">
extraction of entities and their relationships is usually done in pipe lined system that first identifies entity mentions, next resolves mentions into unique entities (co-reference) and finally finds relations among them (florian et al, 2004; <papid> N04-1001 </papid>kambhatla, 2004).<papid> P04-3022 </papid></citsent>
<aftsection>
<nextsent>in that architecture, the errors in the first stage propagate and reduce the performance of subsequent stages; namely, co-reference re solver, that clusters all different mentions of an entity into unique entity, and relation finder, that links entities according to their relationships.
</nextsent>
<nextsent>in fact, the subtaskof entity mention detection itself is very challeng 379 table 1: categorical structure of entities in ace program entity mention entity mention type sub-type class type role ing subtask since respective expressions can have relatively complex syntactic and categorical (se mantic?)
</nextsent>
<nextsent>structures.
</nextsent>
<nextsent>that is, entity mentions in abody of text can occur in relatively complex embedded constructs with many attributes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2061">
<title id=" H05-1048.xml">detection of entity mentions occuring in english and chinese text </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>additionally, we augment those features using features from external resources (e.g. named entity taggers, gazette ers, wordnet).
</prevsent>
<prevsent>we train number of one-versus-all classifiers (allwein et. al, 2000) using svms (vapnik, 1995; burges, 1998).
</prevsent>
</prevsection>
<citsent citstr=" W00-0730 ">
during testing, classification of each token is performed ina greedy left-to-right manner using finite-size sliding context window centered at the token in focus (kudo and matsu mato, 2000).<papid> W00-0730 </papid></citsent>
<aftsection>
<nextsent>this approach yields large number of classes and large number of overlapping features.
</nextsent>
<nextsent>we useda machine learning framework based on svm classification since large number of classes (in one versus-all set-up) and large number of overlapping features can be easily handled with good generalization properties.
</nextsent>
<nextsent>we argue that data sparsity and computational complexity is not as severe as it might be expected in the other machine learning methods that are based on maximum likelihood parameter estimation.
</nextsent>
<nextsent>in other words, we claim that the large set of classification labels and training data sparseness are not major drawbacks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2062">
<title id=" H05-1126.xml">speech based information retrieval system with clarification dialogue strategy </title>
<section> retrieve from knowledge base (kb)..  </section>
<citcontext>
<prevsection>
<prevsent>the specification is listed in table 1, and there are about 40k documents in total.
</prevsent>
<prevsent>an example of support article is shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" C02-1084 ">
dialog navigator (kiyota et al, 2002) <papid> C02-1084 </papid>has been developed at university of tokyo as retrieval system for this kb.</citsent>
<aftsection>
<nextsent>the system accepts typed-text in put from users and outputs result of the retrieval.
</nextsent>
<nextsent>the system interprets an input sentence by taking syntactic dependency and synonymous expression into consideration for matching it with the kb.
</nextsent>
<nextsent>the target of the matching is the summaries and detail information in the support articles, and the titles ofthe glossary and faq.
</nextsent>
<nextsent>the retrieved result is displayed to the user as the list of documents like web 1004 ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2063">
<title id=" E95-1007.xml">some remarks on the decidability of the generation problem in lfg and patrstyle unification grammars </title>
<section> p re iminar ies   </section>
<citcontext>
<prevsection>
<prevsent>cf.
</prevsent>
<prevsent>the appendix for more details.
</prevsent>
</prevsection>
<citsent citstr=" C88-1009 ">
~the algorithm is adapted from statman 1977 and knuth and bendix 1970 and first applied to feature- description languages by beierle and pletat (1988).<papid> C88-1009 </papid></citsent>
<aftsection>
<nextsent>since m(sm)   m(sp,+l ) fp is defined for sin, the construction terminates with finite set of literals.
</nextsent>
<nextsent>if we set sp = spt ; with = min{i \[ sp, = sin+ ~ } the following lemma can easily be proven by induc-tion on the construction of sp.
</nextsent>
<nextsent>6 7.
</nextsent>
<nextsent>lemma.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2064">
<title id=" E95-1007.xml">some remarks on the decidability of the generation problem in lfg and patrstyle unification grammars </title>
<section> p re iminar ies   </section>
<citcontext>
<prevsection>
<prevsent>by showing (i) be-fore (ii) we get the induction step for subterm ft- of ts, in both cases according to ~p(ft) = ~p(f)(-~p(7-)) = ~,(f)(hc(7-)) = ~p(f)(7-).
</prevsent>
<prevsent>we get .~p(7-) = hc(t) by inductive hypothesis and m(7-) = % since 7- ? hp would imply the existence of 6in order to verify 7(i) cf.
</prevsent>
</prevsection>
<citsent citstr=" E91-1036 ">
e.g. wedekind 1991 <papid> E91-1036 </papid>and 1994.</citsent>
<aftsection>
<nextsent>47 ~  ? o and ft could not be subterm of 7~p according to lemma 7(ii).
</nextsent>
<nextsent>now, if (i) ft ? sub(tep) then ~p(f)(t) is defined and equal to h~(fr) and (ii) if r ? sub(ts,) and .~o(ft) is defined then r ? sub(te~).
</nextsent>
<nextsent>\[\] on the basis of lemma 9 it is now easy to prove: 10.
</nextsent>
<nextsent>lemma.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2065">
<title id=" E99-1002.xml">generating referring expressions with a unification grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>taking account of such factors poses tricky problem for natural language gen-eration (nlg), especially in applications in which efficiency (i.e. fast generation of texts) is prior-ity.
</prevsent>
<prevsent>this paper proposes method that allows effi-cient generation of referring expressions, through unification grammar, at the cost of some ini-tial effort in tailoring the phrase-structure rules to the current knowledge base.
</prevsent>
</prevsection>
<citsent citstr=" P98-2173 ">
the method was invented to meet the needs of applications us-ing  wysiwym editing  (power and scott, 1998), <papid> P98-2173 </papid>which allow an author to control the content of an automatically generated text without prior train-ing in knowledge ngineering.</citsent>
<aftsection>
<nextsent>wysiwym is based \[ 1 oal 1 _f \] procedure r ~ put-on -\[, patch method ~ rest figure 1: network representation of an instruction on the idea of  feedback text , i.e. text, gener-ated by the system, that presents the current con-tent of the knowledge base (however incomplete) along with the set of permitted operations forex- tending or otherwise diting the knowledge; these operations are provided through pop-up menus which open on spans of the feedback text.
</nextsent>
<nextsent>two re-quirements ofwysiwym editing are that feedback texts should be generated fast (even delay of few seconds irritating), and that they should ex-press coreference relations clearly through appro-priate referring expressions; reconciling these two requirements has motivated the work described here.
</nextsent>
<nextsent>the semantic network in figure 1 shows knowl-edge base that might be produced using the icon-oclast 1 system, which generates patient infor-mation leaflets.
</nextsent>
<nextsent>at present his knowledge base defines only the goal and first step of procedure; before generating useful output ext the author would have to add further steps.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2066">
<title id=" E99-1002.xml">generating referring expressions with a unification grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>roughly, pronoun can be used instead of definite description if there is no danger of ambiguity, and if no major structural bound-ary has been passed since the referent was last mentioned.
</prevsent>
<prevsent>we are not concerned here with the details of this issue (hofmann, 1989; walker et al, 1998); in the examples, we have treated the colon in the feedback text as ma-jor structural boundary, so preferring def-inite description in the feedback text and pronoun in the output text.
</prevsent>
</prevsection>
<citsent citstr=" P97-1027 ">
we concentrate here on two contextual features, focus and pr io mentions . the problem of find-ing suitable identifying properties (dale andre- iter, 1995; horacek, 1997) <papid> P97-1027 </papid>will not be addressed here, although as will be shown our approach could incorporate this work.</citsent>
<aftsection>
<nextsent>for any referring expression (e.g.  patch ) one can define two relevant contextual states: first, the context in which the expression may be used; sec-ondly, the context hat results from its use.
</nextsent>
<nextsent>these will be called the  initial  and  final  contexts.
</nextsent>
<nextsent>in the case of  patch , they can be informally de-fined as follows.
</nextsent>
<nextsent>in t ia context : the patch is not in focus, it has not been mentioned before, and no other patch has been mentioned.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2067">
<title id=" E99-1002.xml">generating referring expressions with a unification grammar </title>
<section> incorporating context into the.  </section>
<citcontext>
<prevsection>
<prevsent>in pursuit of efficiency, iconoclast employs top-down generator coupled with unification grammar.
</prevsent>
<prevsent>the grammar adheres trictly to oc- cain razor: features or rules are admitted only if they contribute to generating the desired texts.
</prevsent>
</prevsection>
<citsent citstr=" E95-1025 ">
iconoclast is implemented in profit (erbach, 1995), <papid> E95-1025 </papid>so that feature structures are represented by prolog terms and can be unified efficiently through prolog term unification.</citsent>
<aftsection>
<nextsent>how can linguistic context be fitted into such scheme?
</nextsent>
<nextsent>ideally we would like to incorporate con-text into the phrase-structure rules, so that for example rule introducing pronoun would be applied only if the referent be expressed had value of 1 in the focus vector.
</nextsent>
<nextsent>unfortunately such rule could not be formulated in general terms: both its semantic features and its focus and mention vectors would depend on particular properties of the current knowledge base.
</nextsent>
<nextsent>how- ever, nothing prevents us from constructing  be- spoke  rules, tailored to the current state of the knowledge base, every time that it is updated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2068">
<title id=" E99-1027.xml">an experiment on the upper bound of inter judge agreement the case of tagging </title>
<section> or ientat ion.  </section>
<citcontext>
<prevsection>
<prevsent>arguments concerning the magnitude of this problem have been made especially in relation to tagging, the attempt automatically assign lex- ically and contextually correct morphological de-scriptors (tags) to words.
</prevsent>
<prevsent>a pessimistic view is taken by church (1992) who argues that even af-ter negotiations of the kind described above, no consensus can be reached about the correct anal-ysis of several percent of all word tokens in the text.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
a more mixed view on the matter is taken by marcus et al (1993) <papid> J93-2004 </papid>who on the one hand note that in one experiment moderately trained human text annotators made different analyses even after negotiations in over 3% of all words, and on the other hand argue that an expert can do much bet- ter.</citsent>
<aftsection>
<nextsent>an optimistic view on the matter has been pre-sented by eyes and leech (1993).
</nextsent>
<nextsent>empirical ev-idence for high agreement rate is reported by voutilainen and j~rvinen (1995).
</nextsent>
<nextsent>their results suggest hat at least with one grammatical repre-sentation, namely the engcg tag set (cf.
</nextsent>
<nextsent>karls-son et al, eds., 1995), 100% consistency can be 204 proceedings of eacl  99 reached after negotiations at the level of parts of speech (or morphology in this case).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2069">
<title id=" E95-1034.xml">integrating free word order syntax and information structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in fixed word order languages uch as english, these are indicated largely through intonation and stress rather than word order.
</prevsent>
<prevsent>the context-appropriate use of  free  word or-der is of considerable importance in developing practical applications in natural language gener-ation, machine translation, and machine-assisted translation.
</prevsent>
</prevsection>
<citsent citstr=" W94-0314 ">
i have implemented database query system in prolog, described in (hoffman, 1994), <papid> W94-0314 </papid>which uses multi set ccg to interpret and gen: crate turkish sentences with context-appropriate word orders.</citsent>
<aftsection>
<nextsent>here, concentrate on further devel- *i would like to thank mark steedman, ellen prince, and the support of nsf grant sbr 8920230.
</nextsent>
<nextsent>oping the formalism, especially to handle complex sentences.
</nextsent>
<nextsent>there have been other formalisms that inte-grate information structure into the grammar for  free  word order languages, e.g.
</nextsent>
<nextsent>(sgall et al 1986; engdahl/vallduvi, 1994; steinberger, 1994).<papid> C94-1008 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2070">
<title id=" E95-1034.xml">integrating free word order syntax and information structure </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>oping the formalism, especially to handle complex sentences.
</prevsent>
<prevsent>there have been other formalisms that inte-grate information structure into the grammar for  free  word order languages, e.g.
</prevsent>
</prevsection>
<citsent citstr=" C94-1008 ">
(sgall et al 1986; engdahl/vallduvi, 1994; steinberger, 1994).<papid> C94-1008 </papid></citsent>
<aftsection>
<nextsent>however, believe my approach is the first to tackle complex sentences with embedded infor-mation structures and discontinuous constituents.
</nextsent>
<nextsent>multi set ccg can handle free word order among arguments and adjuncts in all clauses, as well as word order variation across clause boundaries, i.e. long distance scrambling.
</nextsent>
<nextsent>the advantage of using combinatory categorial formalism is that it provides compositional nd flexible sur-face structure, which allows syntactic onstituents to easily correspond with information structure units.
</nextsent>
<nextsent>a novel characteristic of this approach is that the context-appropriate use of word or-der is captured by compositionally building the predicate-argument structure (as) and the infor-mation structure (is) of sentence in parallel.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2071">
<title id=" E95-1034.xml">integrating free word order syntax and information structure </title>
<section> the turkish data.  </section>
<citcontext>
<prevsection>
<prevsent>thus, verb-final anguages uch as korean can be modeled by using this direction feature in verbal categories, e.g. s\]{ffn, ffa).
</prevsent>
<prevsent>multiset-ccg contains small set of rules that combine these categories into larger constituents.
</prevsent>
</prevsection>
<citsent citstr=" P92-1044 ">
the following application rules allow function 3a preliminary version of the syntactic omponent of the grammar was presented in (hoffman, 1992).<papid> P92-1044 </papid></citsent>
<aftsection>
<nextsent>such as verbal category to  combine with one of its arguments to its right ( ) or left ( ).
</nextsent>
<nextsent>we assume that category i0 where there are no arguments left in the multi set rewrites by clean- up rule to just x.
</nextsent>
<nextsent>(6) a. forward app i ca ion ( ): xl(argsuw}) ~ xlargs b. backward app i ca ion ( ): xl(args {\]~}) =~ x\[args using these application rules, verb can ap-ply to its arguments in any order.
</nextsent>
<nextsent>for exam-ple, the following is derivation of transi-tive sentence with the word order object-subject- verb; variables in the semantic interpretations are italicized .4 (7) ahmet  fatma g6rdfi.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2072">
<title id=" H05-1116.xml">multi perspective question answering using the opqa corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much of the relevant research explores sentiment classification, text categorization task in which the goal is to assign toa document either positive (thumbs up?)
</prevsent>
<prevsent>or negative (thumbs down?)
</prevsent>
</prevsection>
<citsent citstr=" W02-1011 ">
polarity (e.g. das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al .</citsent>
<aftsection>
<nextsent>(2003), pang and lee (2004)).<papid> P04-1035 </papid></nextsent>
<nextsent>other research has concentrated on analyzing opinions at, or below,the sentence level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2073">
<title id=" H05-1116.xml">multi perspective question answering using the opqa corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>much of the relevant research explores sentiment classification, text categorization task in which the goal is to assign toa document either positive (thumbs up?)
</prevsent>
<prevsent>or negative (thumbs down?)
</prevsent>
</prevsection>
<citsent citstr=" P02-1053 ">
polarity (e.g. das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al .</citsent>
<aftsection>
<nextsent>(2003), pang and lee (2004)).<papid> P04-1035 </papid></nextsent>
<nextsent>other research has concentrated on analyzing opinions at, or below,the sentence level.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2074">
<title id=" H05-1116.xml">multi perspective question answering using the opqa corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>or negative (thumbs down?)
</prevsent>
<prevsent>polarity (e.g. das and chen (2001), pang et al  (2002), <papid> W02-1011 </papid>turney (2002), <papid> P02-1053 </papid>dave et al .</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
(2003), pang and lee (2004)).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>other research has concentrated on analyzing opinions at, or below,the sentence level.
</nextsent>
<nextsent>recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to reasonable degree of accuracy (e.g. dave et al .
</nextsent>
<nextsent>(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al  (2004), pang and lee (2004), <papid> P04-1035 </papid>wilson et al  (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebe and riloff (2005)).</nextsent>
<nextsent>related work in the area of corpus development includes wiebe et al (2005) opinion annotation scheme to identify subjective expressions ? expressions used to express opinions, emotions, sentiments and other private states in text.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2075">
<title id=" H05-1116.xml">multi perspective question answering using the opqa corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other research has concentrated on analyzing opinions at, or below,the sentence level.
</prevsent>
<prevsent>recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to reasonable degree of accuracy (e.g. dave et al .
</prevsent>
</prevsection>
<citsent citstr=" W03-1014 ">
(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al  (2004), pang and lee (2004), <papid> P04-1035 </papid>wilson et al  (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebe and riloff (2005)).</citsent>
<aftsection>
<nextsent>related work in the area of corpus development includes wiebe et al (2005) opinion annotation scheme to identify subjective expressions ? expressions used to express opinions, emotions, sentiments and other private states in text.
</nextsent>
<nextsent>wiebe et al  have applied the annotation scheme to create the mpqacorpus consisting of 535 documents manually annotated for phrase-level expressions of opinion.
</nextsent>
<nextsent>in addition, the nist-sponsored trec evaluation has begun to develop data focusing on opinions ? the 2003 novelty track features task that requires sys-.
</nextsent>
<nextsent>tems to identify opinion-oriented documents w.r.t. specific issue (voorhees and buckland, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2077">
<title id=" H05-1116.xml">multi perspective question answering using the opqa corpus </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>other research has concentrated on analyzing opinions at, or below,the sentence level.
</prevsent>
<prevsent>recent work, for example, indicates that systems can be trained to recognize opinions, their polarity, their source, and their strength to reasonable degree of accuracy (e.g. dave et al .
</prevsent>
</prevsection>
<citsent citstr=" W03-1017 ">
(2003), riloff and wiebe (2003), <papid> W03-1014 </papid>bethard et al  (2004), pang and lee (2004), <papid> P04-1035 </papid>wilson et al  (2004), yu and hatzivassiloglou (2003), <papid> W03-1017 </papid>wiebe and riloff (2005)).</citsent>
<aftsection>
<nextsent>related work in the area of corpus development includes wiebe et al (2005) opinion annotation scheme to identify subjective expressions ? expressions used to express opinions, emotions, sentiments and other private states in text.
</nextsent>
<nextsent>wiebe et al  have applied the annotation scheme to create the mpqacorpus consisting of 535 documents manually annotated for phrase-level expressions of opinion.
</nextsent>
<nextsent>in addition, the nist-sponsored trec evaluation has begun to develop data focusing on opinions ? the 2003 novelty track features task that requires sys-.
</nextsent>
<nextsent>tems to identify opinion-oriented documents w.r.t. specific issue (voorhees and buckland, 2003).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2083">
<title id=" E99-1048.xml">comparison and classification of dialects </title>
<section> data and method.  </section>
<citcontext>
<prevsection>
<prevsent>l-distance (sl, s2) is the sum of the costs of the cheapest set of operations changing sl to s2.
</prevsent>
<prevsent>s~agirl delete 1 s~agil replace i/0 2 saagl insert 1 saragl sum distance 4 the example above illustrates leven stein distance applied to bostonian and standard american pro-nunciations of saw girl.
</prevsent>
</prevsection>
<citsent citstr=" E95-1009 ">
kessler (1995) <papid> E95-1009 </papid>applied levenshtein distance to irish dialects.</citsent>
<aftsection>
<nextsent>the ex-ample simplifies our procedure for clarity: refine-ments due to feature sensitivity are omitted.
</nextsent>
<nextsent>to obtain the results below, costs are refined based on phonetic feature overlap.
</nextsent>
<nextsent>replacement costs vary depending on the phones involved.
</nextsent>
<nextsent>differ-ent feature systems were tested; the results hown are based on hoppenbrouwers  (spe-like) features (hoppenbrouwers and hoppenbrouwers, 1988).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2084">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>typically, algorithms operate on sentences.
</prevsent>
<prevsent>in the most general setup, one or more source words can generate 0, 1 or more target words.
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
current state of the art machine translation systems (och, 2003) <papid> P03-1021 </papid>use phrasal (n-gram) features extracted automatically from parallel corpora.</citsent>
<aftsection>
<nextsent>these phrases are extracted using word alignment algorithms that are trained on parallel corpora.
</nextsent>
<nextsent>phrases, or phrasal features, represent mapping of source sequences into target sequences which are typically few words long.in this paper, we investigate the feasibility of training alignment algorithms based on supervised alignment data.
</nextsent>
<nextsent>although there is modest cost associated with annotating data, we show that reduction of 40% relative in alignment error (aer) is possible over the giza++ aligner (och and ney, 2003).<papid> J03-1002 </papid></nextsent>
<nextsent>although there are number of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as component in machine translation system.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2085">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these phrases are extracted using word alignment algorithms that are trained on parallel corpora.
</prevsent>
<prevsent>phrases, or phrasal features, represent mapping of source sequences into target sequences which are typically few words long.in this paper, we investigate the feasibility of training alignment algorithms based on supervised alignment data.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
although there is modest cost associated with annotating data, we show that reduction of 40% relative in alignment error (aer) is possible over the giza++ aligner (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>although there are number of other applications for word alignment, for example in creating bilingual dictionaries, the primary application continues to be as component in machine translation system.
</nextsent>
<nextsent>we test our aligner on several machine translation tests and show encouraging improvements.
</nextsent>
<nextsent>most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically.
</nextsent>
<nextsent>the ibm models 1-5 (brown et al, 1993) <papid> J93-2003 </papid>produce word alignments with increasing algorithmic complexity and performance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2086">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>we test our aligner on several machine translation tests and show encouraging improvements.
</prevsent>
<prevsent>most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
the ibm models 1-5 (brown et al, 1993) <papid> J93-2003 </papid>produce word alignments with increasing algorithmic complexity and performance.</citsent>
<aftsection>
<nextsent>these ibm models and more recent refinements (moore, 2004) <papid> P04-1066 </papid>as well as algorithms that bootstrap from these models like the hmm algorithm described in (vogel et al, 1996) <papid> C96-2141 </papid>are unsupervised algorithms.</nextsent>
<nextsent>the relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than decade.(cherry and lin, 2003) <papid> P03-1012 </papid>recently proposed direct alignment formulation and state that it would be straightforward to estimate the parameters givena supervised alignment corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2087">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically.
</prevsent>
<prevsent>the ibm models 1-5 (brown et al, 1993) <papid> J93-2003 </papid>produce word alignments with increasing algorithmic complexity and performance.</prevsent>
</prevsection>
<citsent citstr=" P04-1066 ">
these ibm models and more recent refinements (moore, 2004) <papid> P04-1066 </papid>as well as algorithms that bootstrap from these models like the hmm algorithm described in (vogel et al, 1996) <papid> C96-2141 </papid>are unsupervised algorithms.</citsent>
<aftsection>
<nextsent>the relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than decade.(cherry and lin, 2003) <papid> P03-1012 </papid>recently proposed direct alignment formulation and state that it would be straightforward to estimate the parameters givena supervised alignment corpus.</nextsent>
<nextsent>in this paper, we extend their work and show that with small amount of annotated data, together with modeling strategy and search algorithm yield significant gains in alignment f-measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2088">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>most of the prior work on word alignments has been done on parallel corpora where the alignment at the sentence level is also done automatically.
</prevsent>
<prevsent>the ibm models 1-5 (brown et al, 1993) <papid> J93-2003 </papid>produce word alignments with increasing algorithmic complexity and performance.</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
these ibm models and more recent refinements (moore, 2004) <papid> P04-1066 </papid>as well as algorithms that bootstrap from these models like the hmm algorithm described in (vogel et al, 1996) <papid> C96-2141 </papid>are unsupervised algorithms.</citsent>
<aftsection>
<nextsent>the relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than decade.(cherry and lin, 2003) <papid> P03-1012 </papid>recently proposed direct alignment formulation and state that it would be straightforward to estimate the parameters givena supervised alignment corpus.</nextsent>
<nextsent>in this paper, we extend their work and show that with small amount of annotated data, together with modeling strategy and search algorithm yield significant gains in alignment f-measure.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2089">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the ibm models 1-5 (brown et al, 1993) <papid> J93-2003 </papid>produce word alignments with increasing algorithmic complexity and performance.</prevsent>
<prevsent>these ibm models and more recent refinements (moore, 2004) <papid> P04-1066 </papid>as well as algorithms that bootstrap from these models like the hmm algorithm described in (vogel et al, 1996) <papid> C96-2141 </papid>are unsupervised algorithms.</prevsent>
</prevsection>
<citsent citstr=" P03-1012 ">
the relative success of these automatic techniques together with the human annotation cost has delayed the collection of supervised word-aligned corpora for more than decade.(cherry and lin, 2003) <papid> P03-1012 </papid>recently proposed direct alignment formulation and state that it would be straightforward to estimate the parameters givena supervised alignment corpus.</citsent>
<aftsection>
<nextsent>in this paper, we extend their work and show that with small amount of annotated data, together with modeling strategy and search algorithm yield significant gains in alignment f-measure.
</nextsent>
<nextsent>89 show vany +pal# alvanyp second words wordnet the 2nd 2d pointed +pwvyqal#+ta$arw# wa$art alwvyqpwords segm.
</nextsent>
<nextsent>to aly aly source target papers document indicate point figure 1: alignment example.
</nextsent>
<nextsent>in order to describe the algorithm, we will need to first describe the direct link model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2090">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>in figure 1, an event is single link from an english word to an arabic state and the event space is the sentence pair.
</prevsent>
<prevsent>we use the maximum entropy formulation (e.g.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
(berger et al, 1996)), <papid> J96-1002 </papid>1we are overloading the word state?</citsent>
<aftsection>
<nextsent>to mean arabic word position.
</nextsent>
<nextsent>f = ?(li) = [ ti11 , sk1 ] p(f |h) = 1z(h) exp ? ii(h, f), where z(h) is the normalizing constant, z(h) = ? exp ? ii(h, f).
</nextsent>
<nextsent>and i(h, f) are binary valued feature functions.
</nextsent>
<nextsent>the function ? selects the arabic word at the position being linked or in the case of segmentation features,one of the segment ations of that position.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2092">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> algorithm.  </section>
<citcontext>
<prevsection>
<prevsent>all words that have corpus frequency of 1 are left out of the model and classed into an unknown word class in order to explicitly model connecting unknown words.
</prevsent>
<prevsent>from the training data we obtain 50k lexical features, and applying the arabic segmenter obtain another 17k lexical features of the form ?(english content word, arabic stem).
</prevsent>
</prevsection>
<citsent citstr=" P03-1051 ">
3.2.2 arabic segmentation features an arabic segmenter similar to (lee et al, 2003)<papid> P03-1051 </papid>provides the segmentation features.</citsent>
<aftsection>
<nextsent>a small dictionary is used (with 71 rules) to restrict the set of arabic segments that can align to english stop words, for example that the?
</nextsent>
<nextsent>aligns to al#?
</nextsent>
<nextsent>and that for?, inand to?
</nextsent>
<nextsent>align to b#?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2094">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> experimental data.  </section>
<citcontext>
<prevsection>
<prevsent>92 1k 3k 5k 7k 9k 10.3k # of features 15510 32111 47962 63140 73650 80321 english % oov 15.9 8.2 5.5 4.4 4.05 3.6 arabic % oov 31 19.6 15.6 13.2 10.8 10.3 f-measure 83.2 85.4 86.5 87.4 87.5 87.8 table 2: varying training data size.
</prevsent>
<prevsent>phrase wherever possible but left unaligned if there is no evidence to link the word.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
in order to measure alignment performance, we use the standard aer measure (och and ney, 2000) <papid> P00-1056 </papid>but consider all links as sure.</citsent>
<aftsection>
<nextsent>this measure is then related to the f-measure which can be defined in terms of precision and recall as precision the number of correct word links over the total number of proposed links.
</nextsent>
<nextsent>recall the number of correct word links over the total number of links in the reference.
</nextsent>
<nextsent>and the usual definition of the f-measure, = 2pr(r+ ) and define the alignment error as aer = 1 ? .in this paper, we report our results in terms of measure over aligned links.
</nextsent>
<nextsent>note that links to thenull state (unaligned english words) are not included in the f-measure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2095">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> phrase extraction.  </section>
<citcontext>
<prevsection>
<prevsent>these significance tests indicate that the maxent algorithm presented above is significantly better than either giza++ or hmm.
</prevsent>
<prevsent>figure 2: an alignment showing split link from an arabic word.
</prevsent>
</prevsection>
<citsent citstr=" W99-0604 ">
once an alignment is obtained, phrases which satisfy the inverse projection constraint are extracted(although earlier this constraint was called consistent alignments (och et al, 1999)).<papid> W99-0604 </papid></citsent>
<aftsection>
<nextsent>this constraint enforces that sequence of source words align to sequence of target words as defined by the lowest and highest target index, and when the target words are projected back to the source language through the alignment, the original source sequence is retrieved.
</nextsent>
<nextsent>examination of the hand alignment training data showed that this criteria is often violated for arabic and english.
</nextsent>
<nextsent>prepositional phrases with adjectives often require split?
</nextsent>
<nextsent>for example, the alignment shown in figure 2 has of its relations?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2096">
<title id=" H05-1012.xml">a maximum entropy word aligner for arabic english machine translation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>in future work we explore the use of features with variables to be filled at decode time.
</prevsent>
<prevsent>the experiments in machine translation are carried out on phrase based decoder similar to the one de 94 mt03 mt04 mt05 giza++ 0.454 ? ?
</prevsent>
</prevsection>
<citsent citstr=" J03-1005 ">
hmm 0.459 0.419 0.456 maxent 0.468 0.433 0.451 combined 0.479 0.437 0.465 significance 0.017 0.020 ? table 5: machine translation performance using the nist 2005 bleu scorerscribed in (tillmann and ney, 2003).<papid> J03-1005 </papid></citsent>
<aftsection>
<nextsent>in order to contrast the performance of the extracted features, we compare the translation performance to (a) system built from alignments proposed by an hmm max posterior aligner, and (b) system built from gizaalignments.
</nextsent>
<nextsent>all other parameters of the decoder remain constant and only the feature set is changed for these experiments.
</nextsent>
<nextsent>as training data, we use theun parallel corpus and the ldc news corpora released in 2005.
</nextsent>
<nextsent>comparison should therefore be only made across systems reported here and not to earlier evaluations or other systems.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2097">
<title id=" E93-1040.xml">parsing the wall street journal with the inside outside algorithm </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>furthermore, it is widely believed that high performance an only be achieved by disambiguating lexically sensitive phenomena such as prepositional attachment ambiguity, coordination or subcategoriza- don.
</prevsent>
<prevsent>so far, grammar inference has not been shown to be effective for designing wide coverage parsers.
</prevsent>
</prevsection>
<citsent citstr=" P92-1024 ">
baker (1979) describes training algorithm for sto-chastic ontext-free grammars (scfg) which can be used for grammar reestimation (fujisaki et al 1989, sharrnan et al 1990, black et al 1992, <papid> P92-1024 </papid>briscoe and wae- gner 1992) or grammar inference from scratch (lari and young 1990).</citsent>
<aftsection>
<nextsent>however, the application of scfgs and the original inside-outside algorithm for grammar infer-ence has been inconclusive for two reasons.
</nextsent>
<nextsent>first, each iteration of the algorithm on gr,-unmar with nontermi-nals requires o(n31wl 3)time per t~ning sentence w. sec-ond, the inferred grammar imposes bracketings which do not agree with linguistic judgments of sentence struc-ture.
</nextsent>
<nextsent>pereira nd schabes (1992) extended the inside-out- side algorithm for inferring the parameters ofa stochas-tic context-free grammar to take advantage of constituent bracketing information the training text.
</nextsent>
<nextsent>although they report encouraging experiments (90% bracketing accuracy) on mguage transcriptions in the texas instrument subset of the air travel information system (atis), the small size of the corpus (770 brack-eted sentences containing total of 7812 words), its lin-guistic simplicity, and the computation time required to vain the grammar were reasons to believe that these results may not scale up to larger and more diverse cor-pus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2098">
<title id=" E93-1040.xml">parsing the wall street journal with the inside outside algorithm </title>
<section> training corpus.  </section>
<citcontext>
<prevsection>
<prevsent>then, we describe method for reducing the number of parameters in the inferred gr~unmars.
</prevsent>
<prevsent>finally, we suggest stochastic model for inferring labels on the produced binary br~mching trees.
</prevsent>
</prevsection>
<citsent citstr=" H90-1055 ">
the experiments e texts from the wall street journ~d corpus ,and its partially bracketed version provided by the penn treebank (brill et al, 1990).<papid> H90-1055 </papid></citsent>
<aftsection>
<nextsent>out of 38 600 bracketed sentences (914 000 words), we extracted 34500 sentences (817 000 words) as possible source of training material ,and 4100 sentences (97 000 words) as source for testing.
</nextsent>
<nextsent>we experimented with several subsets (350, 1095, 8000 ,and 34500 sentences) of the available training materi~d. for practiced purposes, the part of the tree bank used for training is preprocessed before being used.
</nextsent>
<nextsent>first, fiat portions of parse trees found in the tree b, mk are turned into right linear binary br~mching structure.
</nextsent>
<nextsent>this enables us to take full adv~mtage of the fact that the extended inside-outside ~dgorithm (as described in pereira nd schabes, 1992) behaves in linear time when the text is fully bracketed.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2099">
<title id=" E93-1040.xml">parsing the wall street journal with the inside outside algorithm </title>
<section> inferring bracketings.  </section>
<citcontext>
<prevsection>
<prevsent>to evaluate the quality of the analyses yielded by the inferred grammars obtained ,after each iteration, we used viterbi-style parser to find the most likely analyses of sentences in several test samples, and compared them with the treebank partial bmcketings of the sentences of those samples.
</prevsent>
<prevsent>for each sample, we counted the percent- 343 age of brackets of the most likely ~malysis that are not  crossing  the partiid bracketing of the same sentences found in the treebank.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
this percentage is called the bracketing accuracy (see pereira and schabes, 1992 <papid> P92-1017 </papid>tor the precise definition of this measure).</citsent>
<aftsection>
<nextsent>we also com-puted the percentage of sentences in each smnple in which no crossing bracket wits found.
</nextsent>
<nextsent>this percentage is called the sentence accuracy.
</nextsent>
<nextsent>figure 2 shows the bracketing and sentence accuracy for the s, une 84 test sentences.
</nextsent>
<nextsent>table 1 shows the bracketing and sentence accuracy for test sentences within various length ranges.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2100">
<title id=" H05-1092.xml">multi way relation classification application to protein protein interactions </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we use graphical models and neural net that were found to achieve high accuracy in the related task of extracting the relation types might hold between the entities treat ment?
</prevsent>
<prevsent>and disease?
</prevsent>
</prevsection>
<citsent citstr=" P04-1055 ">
(rosario and hearst, 2004).<papid> P04-1055 </papid></citsent>
<aftsection>
<nextsent>labeling training and test data is time-consuming and subjective.
</nextsent>
<nextsent>here we report on results using an existing curated database, the hiv-1 human protein interaction database1, to train and test the classification system.
</nextsent>
<nextsent>the accuracies obtained by the classification models proposed are quite high, confirming the validity of the approach.
</nextsent>
<nextsent>we also find support for the hypothesis that the sentences surrounding citations are useful for extraction of key information from technical articles (nakov et al, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2101">
<title id=" H05-1092.xml">multi way relation classification application to protein protein interactions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in the remainder of this paper we discuss related work, describe the dataset, and show the results of the algorithm on documents and sentences.
</prevsent>
<prevsent>there has been little work in general nlp on trying to identify different relations between entities.
</prevsent>
</prevsection>
<citsent citstr=" W02-1010 ">
many papers that claim to be doing relationship recognition in actuality address the task of role extraction:(usually two) entities are identified and the relationship is implied by the co-occurrence of these entities or by some linguistic expression (agichtein and gravano, 2000; zelenko et al, 2002).<papid> W02-1010 </papid></citsent>
<aftsection>
<nextsent>the ace competition2 has relation recognition subtask, but assumes particular type of relation holds between particular entity types (e.g., if the two entities in question are an emp and an org, then an employment relation holds between them; which type of employment relation depends on the type of entity, e.g., staff person vs partner).
</nextsent>
<nextsent>1www.ncbi.nlm.nih.gov/refseq/hivinteractions/index.html 2http://www.itl.nist.gov/iaui/894.01/tests/ace/ 732 in the bionlp literature there have recently been number of attempts to automatically extract protein-protein interactions from pubmed abstracts.
</nextsent>
<nextsent>some approaches simply report that relation exists between two proteins but do not determine which relation holds (bunescu et al, 2005; marcotte et al, 2001; ramani et al, 2005), <papid> W05-1307 </papid>while most others start with list of interaction verbs and label only those sentences that contain these trigger words (blaschkeand valencia, 2002; blaschke et al, 1999; rindflesch et al, 1999; thomas et al, 2000; sekimizu et al., 1998; ahmed et al, 2005; <papid> W05-1308 </papid>phuong et al, 2003; pustejovsky et al, 2002).</nextsent>
<nextsent>however, as marcotte etal.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2102">
<title id=" H05-1092.xml">multi way relation classification application to protein protein interactions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the ace competition2 has relation recognition subtask, but assumes particular type of relation holds between particular entity types (e.g., if the two entities in question are an emp and an org, then an employment relation holds between them; which type of employment relation depends on the type of entity, e.g., staff person vs partner).
</prevsent>
<prevsent>1www.ncbi.nlm.nih.gov/refseq/hivinteractions/index.html 2http://www.itl.nist.gov/iaui/894.01/tests/ace/ 732 in the bionlp literature there have recently been number of attempts to automatically extract protein-protein interactions from pubmed abstracts.
</prevsent>
</prevsection>
<citsent citstr=" W05-1307 ">
some approaches simply report that relation exists between two proteins but do not determine which relation holds (bunescu et al, 2005; marcotte et al, 2001; ramani et al, 2005), <papid> W05-1307 </papid>while most others start with list of interaction verbs and label only those sentences that contain these trigger words (blaschkeand valencia, 2002; blaschke et al, 1999; rindflesch et al, 1999; thomas et al, 2000; sekimizu et al., 1998; ahmed et al, 2005; <papid> W05-1308 </papid>phuong et al, 2003; pustejovsky et al, 2002).</citsent>
<aftsection>
<nextsent>however, as marcotte etal.
</nextsent>
<nextsent>(2001) note, ?... searches for abstracts containing relevant keywords, such as interact*, poorly discriminate true hits from abstracts using the words in alternate senses and miss abstracts using different language to describe the interactions.?
</nextsent>
<nextsent>most of the existing methods also suffer from low recall because they use hand-built specialized templates or patterns (ono et al, 2001; corney et al,2004).
</nextsent>
<nextsent>some systems use link grammars in conjunction with trigger verbs instead of templates (ahmedet al, 2005; <papid> W05-1308 </papid>phuong et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2103">
<title id=" H05-1092.xml">multi way relation classification application to protein protein interactions </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>the ace competition2 has relation recognition subtask, but assumes particular type of relation holds between particular entity types (e.g., if the two entities in question are an emp and an org, then an employment relation holds between them; which type of employment relation depends on the type of entity, e.g., staff person vs partner).
</prevsent>
<prevsent>1www.ncbi.nlm.nih.gov/refseq/hivinteractions/index.html 2http://www.itl.nist.gov/iaui/894.01/tests/ace/ 732 in the bionlp literature there have recently been number of attempts to automatically extract protein-protein interactions from pubmed abstracts.
</prevsent>
</prevsection>
<citsent citstr=" W05-1308 ">
some approaches simply report that relation exists between two proteins but do not determine which relation holds (bunescu et al, 2005; marcotte et al, 2001; ramani et al, 2005), <papid> W05-1307 </papid>while most others start with list of interaction verbs and label only those sentences that contain these trigger words (blaschkeand valencia, 2002; blaschke et al, 1999; rindflesch et al, 1999; thomas et al, 2000; sekimizu et al., 1998; ahmed et al, 2005; <papid> W05-1308 </papid>phuong et al, 2003; pustejovsky et al, 2002).</citsent>
<aftsection>
<nextsent>however, as marcotte etal.
</nextsent>
<nextsent>(2001) note, ?... searches for abstracts containing relevant keywords, such as interact*, poorly discriminate true hits from abstracts using the words in alternate senses and miss abstracts using different language to describe the interactions.?
</nextsent>
<nextsent>most of the existing methods also suffer from low recall because they use hand-built specialized templates or patterns (ono et al, 2001; corney et al,2004).
</nextsent>
<nextsent>some systems use link grammars in conjunction with trigger verbs instead of templates (ahmedet al, 2005; <papid> W05-1308 </papid>phuong et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2106">
<title id=" H05-1094.xml">composition of conditional random fields for transfer learning </title>
<section> linear-chain crfs.  </section>
<citcontext>
<prevsection>
<prevsent>although this maximization cannot be done in closed form, it can be optimized numerically.
</prevsent>
<prevsent>particularly effective are gradient based methods that use approximate second-order information, such as conjugate gradient and limited-memory bfgs (byrd et al, 1994).
</prevsent>
</prevsection>
<citsent citstr=" N03-1028 ">
for more information on current training methods for crfs, see sha and pereira (2003).<papid> N03-1028 </papid></citsent>
<aftsection>
<nextsent>dynamic conditional random fields (sutton et al, 2004) extend linear-chain crfs in the same way that dynamic bayes nets (dean &amp; kanazawa, 1989) extend hmms.
</nextsent>
<nextsent>rather than having single monolithic state variable,dcrfs factorize the state at each time step by an undirected model.
</nextsent>
<nextsent>formally, dcrfs are the class of conditionally-trained undirected models that repeat structure and parameters over sequence.
</nextsent>
<nextsent>if we denote by c(yc,t,xt) the repetition of clique at time step t, then dcrf defines the probability of label sequence given the input as: p(s|x) = ? c(yc,t,xt) z(x) , (5) where as before, the clique templates are parameterized in terms of input features as c(yc,t,xt) = exp { ? kfk(yc,t,xt) } .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2107">
<title id=" H05-1094.xml">composition of conditional random fields for transfer learning </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>set, containing 422 stories from newspaper, newswire,and broadcast news.
</prevsent>
<prevsent>unlike the conll entity recognition dataset, in which only proper names of entities are annotated, the ace data includes annotation both for named entities like united states, and also nominal mentions of entities like the nation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1001 ">
thus, although the input text has similar distribution in the conll ner and ace dataset, the label distributions are very different.current state-of-the-art systems for the ace task (flo rian et al, 2004) <papid> N04-1001 </papid>use the predictions of other named-entityrecognizers as features, that is, they use cascaded transfer.</citsent>
<aftsection>
<nextsent>in this experiment, we test whether the transfer between these datasets can be further improved using joint decoding.
</nextsent>
<nextsent>we train crf entity recognizer on the ace dataset, with the output of named-entity entity recognizer trained on the conll 2003 english dataset.
</nextsent>
<nextsent>the conll recognizer is the same crf as was used in the previous experiment.
</nextsent>
<nextsent>in these results, we use subset of 10% of the ace training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2108">
<title id=" H05-1094.xml">composition of conditional random fields for transfer learning </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>also, carreras and marquez (carreras &amp; ma`rquez, 2004) have obtained increased performance in clause finding by training cascade of perceptrons to minimize single global error function.
</prevsent>
<prevsent>finally, miller et al.
</prevsent>
</prevsection>
<citsent citstr=" A00-2030 ">
(miller et al, 2000) <papid> A00-2030 </papid>have combined entity recognition,parsing, and relation extraction into jointly-trained single statistical parsing model that achieves improved performance on all the subtasks.part of the contribution of the current work is to suggest that joint decoding can be effective even when joint training is not possible because jointly-labeled data is unavailable.</citsent>
<aftsection>
<nextsent>for example, miller et al report that they originally attempted to annotate newswire articles for all of parsing, relations, and named entities, but they stopped because the annotation was simply too expensive.
</nextsent>
<nextsent>instead they hand-labeled relations only, assigning parse trees to the training set using standard statistical parser,which is potentially less flexible than the cascaded training, because the model for main task is trained explicitly to match the noisy subtask predictions, rather than being free to correct them.in the speech community, it is common to compose separately trained weighted finite-state transducers(mohri et al, 2002) for joint decoding.
</nextsent>
<nextsent>our method extends this work to conditional models.
</nextsent>
<nextsent>ordinarily, higher level transducers depend only on the output of the previous transducer: transducer for the lexicon, for example, consumes only phonemes, not the original speechsignal.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2109">
<title id=" H05-1089.xml">using sketches to estimate associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>sampling methods become more and more important with larger and larger collections.
</prevsent>
<prevsent>at web scale, sampling rates as low as 104 may suffice.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
word associations (co-occurrences) have wide range of applications including: speech recognition, optical character recognition and information retrieval (ir) (church and hanks, 1991; dunning, 1993; <papid> J93-1003 </papid>manning and schutze, 1999).</citsent>
<aftsection>
<nextsent>it is easy to compute association scores for small corpus, but more challenging to compute lots of scores for lots of data (e.g. the web), with billions of web pages (d) and millions of word types (v ).
</nextsent>
<nextsent>for small corpus, one could compute pair-wise associations by multiplying the (0/1) term-by-document matrix with its transpose (deerwester et al, 1999).
</nextsent>
<nextsent>but this is probably infeasible at web scale.
</nextsent>
<nextsent>1this work was conducted at microsoft while the first author was an intern.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2111">
<title id=" H05-1089.xml">using sketches to estimate associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>approximations are often good enough.
</prevsent>
<prevsent>we should not have to look at every document to determine that two words are strongly associated.
</prevsent>
</prevsection>
<citsent citstr=" P05-1077 ">
a number of sampling-based randomized algorithms have been implemented at web scale (broder, 1997; chari kar, 2002; ravichandran et al, 2005).<papid> P05-1077 </papid>2 conventional random sample is constructed by selecting ds documents from corpus of doc uments.</citsent>
<aftsection>
<nextsent>the (corpus) sampling rate is dsd . of course, word distributions have long tails.
</nextsent>
<nextsent>there are few high frequency words and many low frequency words.
</nextsent>
<nextsent>it would be convenient if the sampling rate could vary from word to word, unlike conventional sampling where the sampling rate is fixed across the vocabulary.
</nextsent>
<nextsent>in particular, in our experiments, we will impose floor to make sure that the sample contains at least 20 documents for each term.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2114">
<title id=" H05-1089.xml">using sketches to estimate associations </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>as we consider more flexible sampling procedures such as (b), the estimation task becomes more challenging.
</prevsent>
<prevsent>flexible sampling procedures are desirable.
</prevsent>
</prevsection>
<citsent citstr=" W04-3243 ">
many studies focus on rare words (dunning, 1993; <papid> J93-1003 </papid>moore, 2004); <papid> W04-3243 </papid>butterflies are more interesting than moths.the sampling rate can be adjusted on word-by word basis with (b), but not with (a).</citsent>
<aftsection>
<nextsent>the sampling rate determines the trade-off between computational work and statistical accuracy.
</nextsent>
<nextsent>we assume standard inverted index.
</nextsent>
<nextsent>for each word x, there are set of postings, x. contains aset of document ids, one for each document containing x. the size of postings, fx = |x|, corresponds to the margins of the contingency tables in figure 1(a), also known as document frequencies in ir.
</nextsent>
<nextsent>the postings lists are approximated by sketches,skx, first introduced by broder (1997) for removing duplicate web pages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2115">
<title id=" H05-1016.xml">using names and topics for new event detection </title>
<section> previous research.  </section>
<citcontext>
<prevsection>
<prevsent>the tdt group at umass introduced multiple document models for each news story and modified similarity metrics by splitting up stories into only named entities and only terms other than named entities (kumaran and allan, 2004).
</prevsent>
<prevsent>they observed that certain categories of news were better tackled using only named entities, while using only topic terms for the others helped.
</prevsent>
</prevsection>
<citsent citstr=" P03-1030 ">
in approaches similar to named entity tagging, part-of-speech tagging (farahat et al, 2003) <papid> P03-1030 </papid>has also been successfully used to improve ned.papers in the tdt2003 and tdt2004 workshops validated the hypothesis that ensemble single feature classifiers based on majority voting exhibited better performance than single classifiers working with number of features on the ned task (braun and kaneshiro, 2003; braun and kaneshiro, 2004).</citsent>
<aftsection>
<nextsent>examples of features they used are cosine similarity, text tiling output and temporally-weighted tf-idf.probabilistic models for online clustering of documents, with mechanism for handling creation of new clusters have been developed.
</nextsent>
<nextsent>each cluster was assumed to correspond to topic.
</nextsent>
<nextsent>experimental results did not show any improvement over baseline systems (zhang et al, 2005).
</nextsent>
<nextsent>pinning down the character of new stories is toughprocess.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2116">
<title id=" E93-1013.xml">lfg semantics via constraints </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>we take (2) to be fragment of first-order (linear) logic carefully chosen for its computational proper-ties, as discussed below.
</prevsent>
<prevsent>in contrast using the h-calculus to combine fragments of meaning via or-dered applications, we combine fragments of mean-ing through unordered conjunction, and implication.
</prevsent>
</prevsection>
<citsent citstr=" J90-1001 ">
rather than using )~-reduction to simplify mean-ings, we relyon deduction, as advocated by pereira \[1990; <papid> J90-1001 </papid>1991\].</citsent>
<aftsection>
<nextsent>the elements of the f-structure provide an un-ordered set of constraints, expressed in the logic, governing how the semantics can fit together.
</nextsent>
<nextsent>con-straints for combining lexically-provided meanings can be encoded in lexical items, as instructions for combining several arguments into result.
</nextsent>
<nextsent>1 in effect, then, our approach uses first order logicas the  glue  with which semantic representations are assembled.
</nextsent>
<nextsent>once all the constraints are assembled, deduction in the logic is used to infer the mean-ing of the entire structure.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2118">
<title id=" E93-1013.xml">lfg semantics via constraints </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>an f-structure is locally coherent if and only if all the gov-ernable grammatical functions that it contains are gov-erned by local predicate.
</prevsent>
<prevsent>an f-structure is coherent if and only if all its subsidiary f-structures are locally co-herent.
</prevsent>
</prevsection>
<citsent citstr=" E89-1037 ">
\[kaplan and bresnan, 1982, pages 211-212\] 5the related phenomenon of head switching, discussed in connection with machine translation by kaplan et al \[1989\] <papid> E89-1037 </papid>and kaplan and wedekind \[1993\], <papid> E93-1024 </papid>is also amenable to treatment along the lines presented here.</citsent>
<aftsection>
<nextsent>101 bill np (t plied) ----  bill  to = bill obviously adv (t plied) --  obviously  vp.
</nextsent>
<nextsent>(mods t)o = --o (mods t)?
</nextsent>
<nextsent>= obviously(p) kissed (t flied).--  kiss  vx, y. agent((t plied)a, x) ? therlle((t plied)a, y) --o to---- kiss(x, y) hillary np (t plied) =  hillaiiy  to = hillary figure 4: lexical entries for bill, obviously, kissed, ttillary bil : (f2a = bill) hillary : (f3o = hillary) kiss : (vx, y. agent(f1?, x) ? theme(rio, y) --o f4a = kiss(x, y)) obvious ly: (vp.
</nextsent>
<nextsent>f4?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2119">
<title id=" E93-1013.xml">lfg semantics via constraints </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>an f-structure is locally coherent if and only if all the gov-ernable grammatical functions that it contains are gov-erned by local predicate.
</prevsent>
<prevsent>an f-structure is coherent if and only if all its subsidiary f-structures are locally co-herent.
</prevsent>
</prevsection>
<citsent citstr=" E93-1024 ">
\[kaplan and bresnan, 1982, pages 211-212\] 5the related phenomenon of head switching, discussed in connection with machine translation by kaplan et al \[1989\] <papid> E89-1037 </papid>and kaplan and wedekind \[1993\], <papid> E93-1024 </papid>is also amenable to treatment along the lines presented here.</citsent>
<aftsection>
<nextsent>101 bill np (t plied) ----  bill  to = bill obviously adv (t plied) --  obviously  vp.
</nextsent>
<nextsent>(mods t)o = --o (mods t)?
</nextsent>
<nextsent>= obviously(p) kissed (t flied).--  kiss  vx, y. agent((t plied)a, x) ? therlle((t plied)a, y) --o to---- kiss(x, y) hillary np (t plied) =  hillaiiy  to = hillary figure 4: lexical entries for bill, obviously, kissed, ttillary bil : (f2a = bill) hillary : (f3o = hillary) kiss : (vx, y. agent(f1?, x) ? theme(rio, y) --o f4a = kiss(x, y)) obvious ly: (vp.
</nextsent>
<nextsent>f4?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2122">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, these dp procedures are time-consuming in practice.
</prevsent>
<prevsent>in this paper, we present method for speeding up the training with tree kernels.
</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
our target application is node relation labeling, which includes nlp tasks such as semantic role labeling (srl)(gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004).<papid> W04-2416 </papid></citsent>
<aftsection>
<nextsent>for this purpose, we designed kernels on marked labeled ordered trees and derivedo(|t1||t2|) procedures.
</nextsent>
<nextsent>however, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast.
</nextsent>
<nextsent>our speed-up method is based on the observation that very few pairs in the training set have great many common subtrees (we call such pairs malicious pairs) and most pairs have very small number of common subtrees.
</nextsent>
<nextsent>this leads to drastic variance in kernel values, e.g., when (si) = 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2123">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, these dp procedures are time-consuming in practice.
</prevsent>
<prevsent>in this paper, we present method for speeding up the training with tree kernels.
</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
our target application is node relation labeling, which includes nlp tasks such as semantic role labeling (srl)(gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004).<papid> W04-2416 </papid></citsent>
<aftsection>
<nextsent>for this purpose, we designed kernels on marked labeled ordered trees and derivedo(|t1||t2|) procedures.
</nextsent>
<nextsent>however, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast.
</nextsent>
<nextsent>our speed-up method is based on the observation that very few pairs in the training set have great many common subtrees (we call such pairs malicious pairs) and most pairs have very small number of common subtrees.
</nextsent>
<nextsent>this leads to drastic variance in kernel values, e.g., when (si) = 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2124">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, these dp procedures are time-consuming in practice.
</prevsent>
<prevsent>in this paper, we present method for speeding up the training with tree kernels.
</prevsent>
</prevsection>
<citsent citstr=" W04-2416 ">
our target application is node relation labeling, which includes nlp tasks such as semantic role labeling (srl)(gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004; <papid> P04-1043 </papid>hacioglu et al, 2004).<papid> W04-2416 </papid></citsent>
<aftsection>
<nextsent>for this purpose, we designed kernels on marked labeled ordered trees and derivedo(|t1||t2|) procedures.
</nextsent>
<nextsent>however, the lengthy training due to the cost of kernel calculation prevented us from assessing the performance of these kernels and motivated us to make the training practically fast.
</nextsent>
<nextsent>our speed-up method is based on the observation that very few pairs in the training set have great many common subtrees (we call such pairs malicious pairs) and most pairs have very small number of common subtrees.
</nextsent>
<nextsent>this leads to drastic variance in kernel values, e.g., when (si) = 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2125">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> fast training with tree kernels.  </section>
<citcontext>
<prevsection>
<prevsent>,k(ti, tl)}, where is the number of training examples.
</prevsent>
<prevsent>using occurrence pattern op (fi) = {(k,#fi(tk))|#fi(tk) 6= 0} pre 139 algorithm 4.1: calculateks(ti) for each such that #f (ti) 6= 0 do for each (j,#f (tj)) ? op (f ) do ks(j) ? ks(j) +w (f ) ?#f (ti) ?#f (tj) (a) for = 1 to do if (i, j) is malicious then ks(j) ? k(ti, tj) (dp)pared beforehand, we can calculate ks(ti) efficiently (algorithm 4.1).
</prevsent>
</prevsection>
<citsent citstr=" P03-1004 ">
a similar technique was used in (kudo and matsumoto, 2003<papid> P03-1004 </papid>a) to speed up the calculation of inner products.</citsent>
<aftsection>
<nextsent>we can show that the per-pair cost of algorithm4.1 is o(c1q + rmc2|ti||tj |), where is the average number of common feature subtrees in tree pair, rm is the rate of malicious pairs, c1 and c2 arethe constant factors for vector operations and dp operations.
</nextsent>
<nextsent>this cost is independent of the number of training examples.
</nextsent>
<nextsent>we expect from our observations that both and rm are very small and that c1 ? c2.
</nextsent>
<nextsent>4.2 feature subtree enumeration with.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2127">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> fast training with tree kernels.  </section>
<citcontext>
<prevsection>
<prevsent>malicious pair detection to detect malicious pairs and enumerate feature subtrees (and to convert each tree to feature vector),we developed an algorithm based on the freqt algorithm (asai et al, 2002).
</prevsent>
<prevsent>the freqt algorithm can efficiently enumerate subtrees that are included (definition 2.1) in more than pre-specified number of trees in the training examples by generating candidate subtrees using right most expansions (rmes).
</prevsent>
</prevsection>
<citsent citstr=" W04-3239 ">
freqt-based algorithms have recently been used in methods that treat subtrees as features (kudo and matsumoto, 2004; <papid> W04-3239 </papid>kudo and matsumoto, 2003<papid> P03-1004 </papid>b).to develop the algorithm, we made the definition of malicious ness more search-oriented since itis costly to check for malicious ness based on the exact number of common subtrees or the kernel values (i.e., by using the dp procedure for all l2 pairs).whatever definition we use, the correctness is preserved as long as we do not fail to enumerate the subtrees that appear in the pairs we consider non malicious.</citsent>
<aftsection>
<nextsent>first, we consider pairs (i, i) to always be malicious.
</nextsent>
<nextsent>then, we use freqt search that enumerates the subtrees that are included in at least two trees as basis.
</nextsent>
<nextsent>next, we modify freqt so that it stops the search if candidate subtree fi is too large (larger than size d, e.g., 20), and we regard the pairs of the trees where fi appears as malicious because having large subtree in common implies having algorithm 4.2: freqtm(d,r) procedure generatecandidate(fi) for each (j, n) ? occ(fi) do for each (fk, nr) ? rme(fi, tj , n) do ? ? {fk}; occ(fk) ? occ(fk) ?
</nextsent>
<nextsent>(j, nr) if |occ(fk)|/|sup(fi)|   then return ((?, false ))????????????(r) return (({fk|fk ? s, |sup(fk)| ? 2}, true )) procedure search(fi, precheck) if |fi| ? then registermal(fi) return ( false )?(p) (c, suc) ? generatecandidate(fi) if not suc then registermal(fi) return ( false )?(s) for each fk ? do if malicious(fk) then goto next fk ?????-(p2) suc search(fk, precheck) if not suc and |sup(fi)| = |sup(fk)| then return ( false )???????????????(p1) if not pre check and marked(fi) then registersubtree(fi)????????????(f) return ( true ) main m?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2130">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>on the other hand, the freqtm conversion for marked labeled ordered trees might be made faster by exploiting the mark information for pruning.
</prevsent>
<prevsent>although our method is not complete solution in classification setting, it might be in clustering setting (in sense it is training only).
</prevsent>
</prevsection>
<citsent citstr=" P04-1016 ">
however, it is an open question whether unbalanced similarity, which is the key to our speed-up, is ubiquitous in nlp tasks and under what conditions our method scales better than the svms or other kernel-based methods.several studies claim that learning using tree kernels and other convolution kernels tends to overfit and propose selecting or restricting features (cumbyand roth, 2003; suzuki et al, 2004; <papid> P04-1016 </papid>kudo and matsumoto, 2004).<papid> W04-3239 </papid></citsent>
<aftsection>
<nextsent>sometimes, the classification becomes faster as result (suzuki et al, 2004; <papid> P04-1016 </papid>kudo and matsumoto, 2004).<papid> W04-3239 </papid></nextsent>
<nextsent>we do not disagree with these studies.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2139">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>kashima and koyanagi (2002) dealt with this task by inserting the node representing the mark above the node to be tagged and classifying the transformed tree using svms with tree kernels such as klo.
</prevsent>
<prevsent>for the srl task, moschitti (2004) <papid> P04-1043 </papid>applied the tree kernel (kc) to tree fragments that are heuristic ally extracted to reflect the role of interest.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
for relation extraction, culotta and sorensen (2004) <papid> P04-1054 </papid>proposed tree kernel that operates on only the smallest tree fragment including two entities for which relation is assigned.</citsent>
<aftsection>
<nextsent>our kernels on marked labeled ordered trees differ in what subtrees are permitted.although comparisons are needed, we think our kernels are intuitive and general.
</nextsent>
<nextsent>there are many possible structures for which tree kernels can be defined.
</nextsent>
<nextsent>shen et al (2003) <papid> W03-1012 </papid>proposed tree kernel for ltag derivation trees to focus only on linguistically meaningful structures.</nextsent>
<nextsent>culotta and sorensen (2004) <papid> P04-1054 </papid>proposed tree kernel for dependency trees.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2140">
<title id=" H05-1018.xml">speeding uptraining with tree kernels for node relation labeling </title>
<section> discussion.  </section>
<citcontext>
<prevsection>
<prevsent>our kernels on marked labeled ordered trees differ in what subtrees are permitted.although comparisons are needed, we think our kernels are intuitive and general.
</prevsent>
<prevsent>there are many possible structures for which tree kernels can be defined.
</prevsent>
</prevsection>
<citsent citstr=" W03-1012 ">
shen et al (2003) <papid> W03-1012 </papid>proposed tree kernel for ltag derivation trees to focus only on linguistically meaningful structures.</citsent>
<aftsection>
<nextsent>culotta and sorensen (2004) <papid> P04-1054 </papid>proposed tree kernel for dependency trees.</nextsent>
<nextsent>an important future task is to find suit able structures for each task (the srl task in our case).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2142">
<title id=" H05-2005.xml">mindnet an automatically created lexical resource </title>
<section> mindnet.  </section>
<citcontext>
<prevsection>

<prevsent>a mindnet is collection of semantic relations that is automatically extracted from text data using broad coverage parser.
</prevsent>
</prevsection>
<citsent citstr=" P98-2180 ">
previous publications on mindnet (suzuki et al, 2005, richardson et al, 1998, <papid> P98-2180 </papid>vanderwende 1995) have focused on the effort required to build mindnet from the data contained in japanese and english lexicons.</citsent>
<aftsection>
<nextsent>semantic relations the semantic relations that are stored in mindnet are directed, labeled relationships between two words; see table 1: attributive manner source cause means synonym goal part time hypernym possessor typical object location result typical subject table 1: sampling of the semantic relations stored in mindnet these semantic relations are obtained from the logical form analysis of our broad coverage parser nlpwin (heidorn, 2000).
</nextsent>
<nextsent>the logical form is labeled dependency analysis with function words removed.
</nextsent>
<nextsent>we have not completed an evaluation of the quality of the extracted semantic relations.
</nextsent>
<nextsent>anecdotally, however, the quality varies according to the relation type, with hypernym and grammatical relations typical subject and typi calobj being reliable, while relations such as part and purpose are less reliable.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2143">
<title id=" H05-2005.xml">mindnet an automatically created lexical resource </title>
<section> relation to other work.  </section>
<citcontext>
<prevsection>
<prevsent>projects like euro word net address the monolingual aspect of wordnet, but these databases are still labor intensive to create.
</prevsent>
<prevsent>on the other hand, the quality of the information contained in wordnet (fellbaum et al, 1998) is very reliable, exactly because it was manually created.
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
framenet (baker et al, 1998) <papid> P98-1013 </papid>and opencyc are other valuable resources for english, also hand-created, that contain rich set of relations between words and concepts.</citsent>
<aftsection>
<nextsent>their use is still being explored as they have been made available only recently.
</nextsent>
<nextsent>for japanese, there are also concept dictionaries providing semantic relations, similarly hand-created, e.g., edr and nihongo goi-taikei (ntt).
</nextsent>
<nextsent>the demonstration of mindnet will highlight that this resource is automatically created, allowing domain lexical resources to be built quickly, albeit with lesser accuracy.
</nextsent>
<nextsent>we are confident that this is trade-off worth making in many cases, and encourage experimentation in this area.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2144">
<title id=" E99-1005.xml">determinants of adjective noun plausibility </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>adjective-noun combinations can be hard to gen-erate given their collocational status.
</prevsent>
<prevsent>for generator which selects words solely on semantic grounds with- out taking into account lexical constraints, the choice between spotless kitchen and flawless kitchen may look equivalent.
</prevsent>
</prevsection>
<citsent citstr=" P95-1034 ">
current work in natural language gen-eration (knight and hatzivassiloglou, 1995; <papid> P95-1034 </papid>langk- ilde and knight, 1998) has shown that corpus-based knowledge can be used to address lexical choice non- compositionally.</citsent>
<aftsection>
<nextsent>30 proceedings of eacl  99 in the work reported here we acquire plausibility ratings for adjective-noun combinations by eliciting judgements from human subjects, and examine the ex-tent to which different corpus-based models correlate with human intuitions about he  goodness of fit  for range of adjective-noun combinations.
</nextsent>
<nextsent>the research presented in this paper is similar in motivation to resnik (1993) work on selec-tional restrictions.
</nextsent>
<nextsent>resnik evaluated his information- theoretic model of selectional constraints against hu-man plausibility ratings for verb-object ombinations, and showed that, in most cases, his model assigned higher selectional association scores to verb-object combinations which were judged more plausible by human subjects.
</nextsent>
<nextsent>we test five corpus-based models against human plausibility judgements: 1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2145">
<title id=" E99-1005.xml">determinants of adjective noun plausibility </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>coliocational status.
</prevsent>
<prevsent>we employ the log-.
</prevsent>
</prevsection>
<citsent citstr=" J93-1003 ">
likelihood ratio as measure of the collocational status of the adjective-noun pair (dunning, 1993; <papid> J93-1003 </papid>daille, 1996).</citsent>
<aftsection>
<nextsent>if we assume that plausibility dif-ferences between strong tea and powerful tea or guilty verdict and guilty cat reflect differences in collocational status (i.e., appearing together more often than expected by their individual occur-rence frequencies), as opposed to being semantic in nature, then the log-likelihood ratio may also predict adjective-noun plausibility.
</nextsent>
<nextsent>5.
</nextsent>
<nextsent>selectional association.
</nextsent>
<nextsent>finally, we evaluate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2146">
<title id=" E99-1005.xml">determinants of adjective noun plausibility </title>
<section> corpus-based modelling.  </section>
<citcontext>
<prevsection>
<prevsent>the selectional ssociation between class and predicate pi is given in equations (3) and (4).
</prevsent>
<prevsent>more specifically, selectional association represents the contribution of particular semantic lass to the total quantity of information provided by predicate about the semantic lass of its argument, when mea-sured as the relative ntropy between the prior distri- for comparison, the filler items had mean rating of .998.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
2mutual information, though potentially of interest as measure of collocational status, was not tested due to its well-known property of over emphasising the significance of rare events (church and hanks, 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>32 proceedings of eacl  99 pattern example adjective noun educational material adjective specifier noun usual weekly classes adjective noun noun environmental health officers table 1: example of noun-adjective patterns co-occurrence frequency band adjective high medium low hungry animal 1.79 pleasure 1.38 application 0 guilty verdict 3.91 secret 2.56 cat 0 temporary job 4.71 post 2.07 cap .69 naughty girl 2.94 dog 1.6 lunch .69 table 2: example stimuli (with log co-occurrence fr quencies in the bnc) bution of classes p(c) and the posterior distribution p(c pi) of the argument classes for particular pred-icate pi.
</nextsent>
<nextsent>f (adjective, noun) (2) p(noun adjective) = (adjective) (3) a(pi, c) = . e(c pi)  log p(c pi_______~) rli p(c) (4) rli=~-~p(clpi).logp(cplc;i) in the case of adjective-noun combinations, these- lectional association measures the semantic fit of an adjective and each of the semantic lasses of the nouns it co-occurs with.
</nextsent>
<nextsent>we estimated the probabilities p(c pi) and p(c) similarly to resnik (1993) by us-ing relative frequencies from the bnc, together with wordnet (miller et al, 1990) as source of taxo-nomic semantic lass information.
</nextsent>
<nextsent>although these- lectional association is function of the predicate and all semantic classes it potentially selects for, following resnik method for verb-object evaluation, we com-pared human plausibility judgements with the max-imum value for the selectional association for each adjective-noun combination.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2147">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the standard approach to statistical machine translation uses the source-channel model , argmaxtp (t|s) = argmaxtp (t)p (s|t),where (t) is the language model for the target language, and (s|t) is an alignment model from the target language to the source language.
</prevsent>
<prevsent>together they define generative model for the source/targetpair (s, t): first is generated according to the language model (t); then is generated from according to (s|t).2 typically, strong independence assumptions arethen made about the distribution (s|t).
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
forex ample, in the ibm models (brown et al , 1993), <papid> J93-2003 </papid>each word ti independently generates 0, 1, or more 2note that we refer to as the target sentence, even though in the source-channel model, is the source sentence which goes through the channel model (s|t) to produce the observed sentence s.words in the source language.</citsent>
<aftsection>
<nextsent>thus, the words generated by ti are independent of the words generated by tj for each 6= i. this means that correlations between words in the source sentence are not captured by (s|t), and so the context we will use in our word translation models to predict ti givensi is not available to system making these independence assumptions.
</nextsent>
<nextsent>in this type of system, semantic and syntactic relationships between words are only modeled in the target language; most orall of the semantic and syntactic information contained in the source sentence is ignored.
</nextsent>
<nextsent>the language model (t) does introduce some context dependencies, but the standard n-gram model usedin machine translation is too weak to provide reasonable solution to the strong independence assumptions made by the alignment model.
</nextsent>
<nextsent>we define the word translation task as finding, for an individual word in the source language , the correct translation, either word or phrase, in the target language . clearly, there are cases where is part of multi-word phrase that needs to be translated as unit.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2148">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> blank-filling task.  </section>
<citcontext>
<prevsection>
<prevsent>this variation is much higher than the improvement in seman tic?
</prevsent>
<prevsent>accuracy our model is attempting to achieve.moreover, currently available decoders do not provide natural way to incorporate the results of word translation system.
</prevsent>
</prevsection>
<citsent citstr=" P05-1048 ">
for example, carpuat and wu (2005) <papid> P05-1048 </papid>obtain negative results for two methods of incorporating the output of word-sense disambiguation system into machine translation system.</citsent>
<aftsection>
<nextsent>thus, we instead used our word translation model for simplified translation problem.
</nextsent>
<nextsent>we prepared adataset as follows: for each occurrence of an ambiguous words in an english sentence in the first document of the europarl corpus, we tried to determine what the correct translation for that word was in the corresponding french sentence.
</nextsent>
<nextsent>if we found one and exactly one possible translation for that word in the french sentence, we replaced that word with blank?, and linked the english wordto that blank.
</nextsent>
<nextsent>the final result was set of 655 sentences with total of 3018 blanks.for example, the following english-french sentence pair contains the two ambiguous words address and issue and one possible translation for each, examiner and question: ? therefore, the commission should address the issue once and for all.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2150">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> blank-filling task.  </section>
<citcontext>
<prevsection>
<prevsent>776 as the optimized setting.
</prevsent>
<prevsent>in the optimal region, the word-translation model receives twice as much weight as the generative alignment model, indicating that word-translation model is more informative than the generative alignment model.
</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
incorporating the discriminative alignment model into the source channel model also improves performance, but not nearly as much as using the word-translation model.an alternate way to optimize weights over translation features is described in och and ney (2002).<papid> P02-1038 </papid>they consider number of translation features, including the language model and generative and discriminative alignment models.</citsent>
<aftsection>
<nextsent>as we have mentioned, one of the main difficulties in translation is that there are an enormous number of possible translations to consider.
</nextsent>
<nextsent>decoding algorithms must therefore use some kind of search space pruning in order to be efficient.
</nextsent>
<nextsent>a key part of pruning the search space is deciding on the setof words to consider in possible translations (germann et al , 2001).<papid> P01-1030 </papid></nextsent>
<nextsent>one standard method is to consider only target words which have high probability according to the discriminative alignment model.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2151">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> search space pruning.  </section>
<citcontext>
<prevsection>
<prevsent>as we have mentioned, one of the main difficulties in translation is that there are an enormous number of possible translations to consider.
</prevsent>
<prevsent>decoding algorithms must therefore use some kind of search space pruning in order to be efficient.
</prevsent>
</prevsection>
<citsent citstr=" P01-1030 ">
a key part of pruning the search space is deciding on the setof words to consider in possible translations (germann et al , 2001).<papid> P01-1030 </papid></citsent>
<aftsection>
<nextsent>one standard method is to consider only target words which have high probability according to the discriminative alignment model.
</nextsent>
<nextsent>but we have already shown that the word translation model achieves much better performance on word translation than this baseline model; thus, we would expect the word translation model to improve accuracy when used to pick sets of candidate translations.
</nextsent>
<nextsent>given probability distribution over possible translations of word, (b|a, s), there are sever always to choose reduced set of possible translations.
</nextsent>
<nextsent>two commonly used methods are to only consider the top scoring words from this distribution (best-n); and to only consider words such that (b|a, s) is above some fixed threshold (cut-off ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2152">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>instance, to achieve 95% recall (that is, for 95% ofthe ambiguous words, we retain the correct transla tion), we only need candidate sets of average size 4.2 for the cut-off strategy using the word-translationmodel, whereas for the same strategy on the discriminative alignment model we require an average set size of 6.7 words.as the size of the solution space grows exponentially with the size of the candidate sets, the word translation model could potentially greatly reduce the search space while maintaining good accuracy.
</prevsent>
<prevsent>it would be interesting to use similar techniques tolearn null fertility (i.e., when word has no translation in the target sentence t).
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
berger et al  (1996) <papid> J96-1002 </papid>apply maximum entropy methods (equivalent to logistic regression) to, among other tasks, the word-translation task.</citsent>
<aftsection>
<nextsent>however, no quantitative results are presented.
</nextsent>
<nextsent>in this paper we demonstrate that the method can improve performance on large dataset and show how it might be used to improve machine translation.diab and resnik (2002) <papid> P02-1033 </papid>suggest using large bilingual corpora to improve performance on word sense disambiguation.</nextsent>
<nextsent>the main idea is that knowing french word may help determine the meaning of the corresponding english word.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2153">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>berger et al  (1996) <papid> J96-1002 </papid>apply maximum entropy methods (equivalent to logistic regression) to, among other tasks, the word-translation task.</prevsent>
<prevsent>however, no quantitative results are presented.</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
in this paper we demonstrate that the method can improve performance on large dataset and show how it might be used to improve machine translation.diab and resnik (2002) <papid> P02-1033 </papid>suggest using large bilingual corpora to improve performance on word sense disambiguation.</citsent>
<aftsection>
<nextsent>the main idea is that knowing french word may help determine the meaning of the corresponding english word.
</nextsent>
<nextsent>they apply this intuition to the senseval word disambiguation task by running off-the-shelf translators to produce translations which they then use for disambiguation.ng et al  (2003) <papid> P03-1058 </papid>address word sense disambiguation by manually annotating wordnet senses with their translation in the target language (chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the ibm 777models to bilingual corpus.</nextsent>
<nextsent>they achieve comparable results to training on hand-labeled examples.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2154">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>in this paper we demonstrate that the method can improve performance on large dataset and show how it might be used to improve machine translation.diab and resnik (2002) <papid> P02-1033 </papid>suggest using large bilingual corpora to improve performance on word sense disambiguation.</prevsent>
<prevsent>the main idea is that knowing french word may help determine the meaning of the corresponding english word.</prevsent>
</prevsection>
<citsent citstr=" P03-1058 ">
they apply this intuition to the senseval word disambiguation task by running off-the-shelf translators to produce translations which they then use for disambiguation.ng et al  (2003) <papid> P03-1058 </papid>address word sense disambiguation by manually annotating wordnet senses with their translation in the target language (chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the ibm 777models to bilingual corpus.</citsent>
<aftsection>
<nextsent>they achieve comparable results to training on hand-labeled examples.
</nextsent>
<nextsent>koehn and knight (2003) <papid> P03-1040 </papid>focus on the task of noun-phrase translation.</nextsent>
<nextsent>they improve performance on the noun-phrase translation task, and show that they can use this to improve full translations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2155">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>they apply this intuition to the senseval word disambiguation task by running off-the-shelf translators to produce translations which they then use for disambiguation.ng et al  (2003) <papid> P03-1058 </papid>address word sense disambiguation by manually annotating wordnet senses with their translation in the target language (chinese), and then automatically extracting labeled examples for word sense disambiguation by applying the ibm 777models to bilingual corpus.</prevsent>
<prevsent>they achieve comparable results to training on hand-labeled examples.</prevsent>
</prevsection>
<citsent citstr=" P03-1040 ">
koehn and knight (2003) <papid> P03-1040 </papid>focus on the task of noun-phrase translation.</citsent>
<aftsection>
<nextsent>they improve performance on the noun-phrase translation task, and show that they can use this to improve full translations.
</nextsent>
<nextsent>a key difference is that, in predicting noun-phrase translations, they do not consider the context of nouns.
</nextsent>
<nextsent>they present results which indicate that humans can accurately translate noun phrases without looking at the surrounding context.
</nextsent>
<nextsent>however, as we have demonstrated in this paper, context can be very useful for (sub-human-level) machine translator.a similar argument applies to phrase-based translation methods (e.g., koehn et al  (2003)).<papid> N03-1017 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2156">
<title id=" H05-1097.xml">word sense disambiguation for machine translation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>a key difference is that, in predicting noun-phrase translations, they do not consider the context of nouns.
</prevsent>
<prevsent>they present results which indicate that humans can accurately translate noun phrases without looking at the surrounding context.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
however, as we have demonstrated in this paper, context can be very useful for (sub-human-level) machine translator.a similar argument applies to phrase-based translation methods (e.g., koehn et al  (2003)).<papid> N03-1017 </papid></citsent>
<aftsection>
<nextsent>while phrase-based systems do take into account context within phrases, they are not able to use context across phrase boundaries.
</nextsent>
<nextsent>this is especially important when ambiguous words do not occur as part of phrase ? verbs in particular often appear alone.
</nextsent>
<nextsent>in this paper, we focus on the word-translation problem.
</nextsent>
<nextsent>by viewing word-sense disambiguation in the context of larger task, we were able to obtain large amounts of training data and directly evaluate the usefulness of our system for real-world task.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2159">
<title id=" H05-2003.xml">clas summary introducing discussion summarization to online classrooms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these live?
</prevsent>
<prevsent>discussions are now enabling new opportunity, the opportunity to apply and evaluate advanced natural language processing (nlp) technology.
</prevsent>
</prevsection>
<citsent citstr=" P05-1037 ">
recently we designed summarization system for technical chats and emails on the linux kernel(zhou and hovy, 2005).<papid> P05-1037 </papid></citsent>
<aftsection>
<nextsent>it clusters discussions according to sub topic structures on the sub-message level, identifies immediate responding pairs usingmachine-learning methods, and generates subtopicbased mini-summaries for each chat log.
</nextsent>
<nextsent>incorporation of this system into the isi discussion board framework, called clas summary, benefits both distance learning and nlp communities.
</nextsent>
<nextsent>summaries are created periodically and sent to students and teachers via their preferred medium (emails, text messages on mobiles, web, etc).
</nextsent>
<nextsent>this relieves users of the burden of reading through large volume of messages before participating in particular discussion.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2161">
<title id=" E95-1005.xml">the semantics of resource sharing in lexical functional grammar </title>
<section> introduction.  </section>
<citcontext>
<prevsection>


</prevsection>
<citsent citstr=" E93-1013 ">
the resource-based approach to semantic ompo- sition in lexical-functional grammar (lfg) ob-tains the interpretation for phrase via logical deduction, beginning with the interpretations of its parts as premises (dalrymple et al, 1993<papid> E93-1013 </papid>a).</citsent>
<aftsection>
<nextsent>the resource-sensitive system of linear logic is used to compute meanings in accordance with relationships manifest in lfg f-structures.
</nextsent>
<nextsent>the properties of the system ensure that meanings are used exactly once, allowing coherence and com-pleteness conditions on f-structures (kaplan and bresnan, 1982, pages 211-212) to be maintained.
</nextsent>
<nextsent>however.
</nextsent>
<nextsent>there are cases where single con-stituent appears to yield more than one contribu-tion to the meaning of an utterance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2163">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we report on machine learning experiments that show that these distinctions can be learned with high accuracy, and validate our approach using human subjects.
</prevsent>
<prevsent>multi-document summarization has been an active area of research over the past decade (mani and maybury, 1999) and yet, barring few exceptions (daume?
</prevsent>
</prevsection>
<citsent citstr=" J98-3005 ">
iii et al, 2002; radev and mckeown,1998), <papid> J98-3005 </papid>most systems still use shallow features to produce an extractive summary, an age-old technique(luhn, 1958) that has well-known problems.</citsent>
<aftsection>
<nextsent>extractive summaries contain phrases that the reader cannot understand out of context (paice, 1990) and irrelevant phrases that happen to occur in relevant sentence (knight and marcu, 2000; barzilay, 2003).referring expressions in extractive summaries illustrate this problem, as sentences compiled from different documents might contain too little, too much or repeated information about the referent.whether referring expression is appropriate depends on the location of the referent in the hearers mental model of the discourse the referents cognitive status (gundel et al, 1993).
</nextsent>
<nextsent>if, for example, the referent is unknown to the reader at the point of mention in the discourse, the reference should include description, while if the referent was known to the reader, no descriptive details are necessary.determining referents cognitive status, however, implies the need to model the intended audience of the summary.
</nextsent>
<nextsent>can such cognitive status model be inferred automatically for general read ership?
</nextsent>
<nextsent>in this paper, we address this question by performing study with human subjects to confirm that reasonable agreement on the distinctions can be achieved between different humans (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2164">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>241cognitive status distinctions depend on two parameters related to the referenta) whether it already exists in the hearers model of the discourse, and b) its degree of salience.
</prevsent>
<prevsent>the influence of these distinctions on the form of referring expressions hasbeen investigated in the past.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
for example, centering theory (grosz et al, 1995) <papid> J95-2003 </papid>deals predominantly with local salience (local attentional status), and the givenness hierarchy (information status) of prince (1992) focuses on how referent got in the discourse model (e.g. through direct mention in the current discourse, through previous knowledge, or throughinference), leading to distinctions such as discourse old, discourse-new, hearer-old, hearer-new, infer able and containing inferable.</citsent>
<aftsection>
<nextsent>gundel et al (1993) attempt to merge salience and givenness in single hierarchy consisting of six distinctions in cognitive status (in focus, activated, familiar, uniquely identifiable, referential, type-identifiable).
</nextsent>
<nextsent>among the distinctions that have an impact on the form of references in summary are the familiarity of the referent: d. discourse-old vs discourse-new h. hearer-old vs hearer-new and its global salience1: m. major vs minorin general, initial (discourse-new) references to entities are longer and more descriptive, while subsequent (discourse-old) references are shorter and have purely referential function.
</nextsent>
<nextsent>nenkova and mckeown (2003) <papid> N03-2024 </papid>have studied this distinction for references to people in summaries and how it can be used to automatically rewrite summaries to achieve better fluency and readability.</nextsent>
<nextsent>the other two cognitive status distinctions, whether an entity is central to the summary or not(major or minor) and whether the hearer can be assumed to be already familiar with the entity (hearer old vs hearer-new status), have not been previously studied in the context of summarization.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2165">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>gundel et al (1993) attempt to merge salience and givenness in single hierarchy consisting of six distinctions in cognitive status (in focus, activated, familiar, uniquely identifiable, referential, type-identifiable).
</prevsent>
<prevsent>among the distinctions that have an impact on the form of references in summary are the familiarity of the referent: d. discourse-old vs discourse-new h. hearer-old vs hearer-new and its global salience1: m. major vs minorin general, initial (discourse-new) references to entities are longer and more descriptive, while subsequent (discourse-old) references are shorter and have purely referential function.
</prevsent>
</prevsection>
<citsent citstr=" N03-2024 ">
nenkova and mckeown (2003) <papid> N03-2024 </papid>have studied this distinction for references to people in summaries and how it can be used to automatically rewrite summaries to achieve better fluency and readability.</citsent>
<aftsection>
<nextsent>the other two cognitive status distinctions, whether an entity is central to the summary or not(major or minor) and whether the hearer can be assumed to be already familiar with the entity (hearer old vs hearer-new status), have not been previously studied in the context of summarization.
</nextsent>
<nextsent>there isa tradeoff, particularly important for short summary, between what the speaker wants to convey1the notion of global salience is very important to summarization, both during content selection and during generation on initial references to entities.
</nextsent>
<nextsent>on the other hand, in focus or localattentional state are relevant to anaphoric usage during subsequent mentions.
</nextsent>
<nextsent>and how much the listener needs to know.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2166">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>journalistic conventions for many mainstream newspapers dictate that initial mentions to people include minimum description such as their role or title and affiliation.
</prevsent>
<prevsent>however, in human summaries, where there are greater space constraints, the nature of initial references changes.
</prevsent>
</prevsection>
<citsent citstr=" C04-1129 ">
siddharthan et al (2004) <papid> C04-1129 </papid>observed that in duc04 and duc03 data2, news reports contain on average one appositive phrase or relative clause every 3.9 sentences, while the human summaries contain only one per 8.9 sentences on aver age.</citsent>
<aftsection>
<nextsent>in addition to this, we observe from the same data that the average length of first reference to named entity is 4.5 words in the news reports andonly 3.6 words in human summaries.
</nextsent>
<nextsent>these statistics imply that human summarizers do compress references, and thus can save space in the summary for presenting information about the events.
</nextsent>
<nextsent>cognitive status models can inform system when such reference compression is appropriate.
</nextsent>
<nextsent>3 data preparation: the duc corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2167">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>list of features in table1).
</prevsent>
<prevsent>our hypothesis is that features capturing the frequency and syntactic and lexical forms of references are sufficient to infer the desired cognitive model.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
intuitively, pronominal ization indicates that an entity was particularly salient at specific point ofthe discourse, as has been widely discussed in at ten tional status and centering literature (grosz and sidner, 1986; <papid> J86-3001 </papid>gordon et al, 1993).</citsent>
<aftsection>
<nextsent>modified noun phrases (with apposition, relative clauses or premod ification) can also signal different status.
</nextsent>
<nextsent>in addition to the syntactic form features, we used two months worth of news articles collected over the web (and independent of the duc collection we usein our experiments here) to collect unigram and bigram lexical models of first mentions of people.
</nextsent>
<nextsent>the names themselves were removed from the first mention noun phrase and the counts were collected over the premodifiers only.
</nextsent>
<nextsent>one of the lexical features we used is whether persons description contains any of the 20 most frequent description words fromour web corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2169">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>on this data, the human agreement was 91% (    ).
</prevsent>
<prevsent>this is high enough agreement to suggest that the classification of national and international figures as hearer old/new across the educated adult american reader with varied interests and background in current and recent events is well defined task.
</prevsent>
</prevsection>
<citsent citstr=" J98-2001 ">
this is not necessarily true for the full range of cognitive status distinctions; for example poesio and vieira (1998) <papid> J98-2001 </papid>report lower human agreement on more fine-grained classifications of definite descriptions.</citsent>
<aftsection>
<nextsent>5.2 results on the news blaster data.
</nextsent>
<nextsent>we measured how well the models trained on duc data perform with current news labeled using human 4http://newsblaster.cs.columbia.edu 5  (kappa) is measure of inter-annotator agreement over and above what might be expected by pure chance (see carletta (1996) <papid> J96-2004 </papid>for discussion of its use in nlp).</nextsent>
<nextsent> if there is perfect agreement between annotators and ff if the annotators agree only as much as you would expect by chance.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2170">
<title id=" H05-1031.xml">automatically learning cognitive status for multi document summarization of newswire </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this is not necessarily true for the full range of cognitive status distinctions; for example poesio and vieira (1998) <papid> J98-2001 </papid>report lower human agreement on more fine-grained classifications of definite descriptions.</prevsent>
<prevsent>5.2 results on the news blaster data.</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
we measured how well the models trained on duc data perform with current news labeled using human 4http://newsblaster.cs.columbia.edu 5  (kappa) is measure of inter-annotator agreement over and above what might be expected by pure chance (see carletta (1996) <papid> J96-2004 </papid>for discussion of its use in nlp).</citsent>
<aftsection>
<nextsent> if there is perfect agreement between annotators and ff if the annotators agree only as much as you would expect by chance.
</nextsent>
<nextsent>6the human judgments were made within week of the news stories appearing.
</nextsent>
<nextsent>judgment.
</nextsent>
<nextsent>for each person who was mentioned in the automatic summaries for the news blaster data,we compiled one judgment from the 4 human sub jects: an example was labeled as hearer-new if two or more out of the four subjects had marked it as hearer new.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2171">
<title id=" H05-1047.xml">a semantic approach to recognizing textual entailment </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>to obtain the complete set of semantic relations that represents the meaning of the given texts, we introduce new step in our algorithm.
</prevsent>
<prevsent>1after all, the entailment, inference, and equivalence terms originated from logic.
</prevsent>
</prevsection>
<citsent citstr=" C02-1167 ">
set of world knowledge, nlp, and wordnet-basedlexical chain (moldovan and novischi, 2002) <papid> C02-1167 </papid>axioms that assist the logic prover in its search for proofs.</citsent>
<aftsection>
<nextsent>we developed semantic axioms that show how two semantic relations can be combined.
</nextsent>
<nextsent>this will allow the logic prover to combine, whenever possible, semantic instances in order to infer new semantic relationships.
</nextsent>
<nextsent>the instances of relations that participate in semantic combinations can be either provided by the text or annotated between wordnet synsets.
</nextsent>
<nextsent>we also exploit other sources of semantic information from the text.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2172">
<title id=" H05-1047.xml">a semantic approach to recognizing textual entailment </title>
<section> semantic calculus.  </section>
<citcontext>
<prevsection>
<prevsent>the long dash arrows indicate the relations between frame elements.
</prevsent>
<prevsent>3.1 semantic relations.
</prevsent>
</prevsection>
<citsent citstr=" W04-2609 ">
for this study, we adopt revised version of these mantic relation set proposed by (moldovan et al, 2004).<papid> W04-2609 </papid></citsent>
<aftsection>
<nextsent>table 2 enumerates the semantic relations that we consider2.
</nextsent>
<nextsent>2see (moldovan et al, 2004) <papid> W04-2609 </papid>for definitions and examples.</nextsent>
<nextsent>373 possession (pos) make-produce (mak) recipient (rec) theme-patient (thm) kinship (kin) instrument (ins) frequency (frq) result (rsl) property-attribute (pah) location-space (loc) influence (ifl) stimulus (sti) agent (agt) purpose (prp) associated with (oth) extent (ext) temporal (tmp) source-from (src) measure (mea) predicate (prd) depiction (dpc) topic (tpc) synonymy-name (syn) causality (csl) part-whole (pw) manner (mnr) antonymy (ant) justification (jst) hypernymy (isa) means (mns) probability of existence (prb) goal (gol) entail (ent) accompaniment (acc) possibility (psb) belief (blf) cause (cau) experiencer (exp) certainty (crt) meaning (mng) table 2: the set of semantic relations 3.2 combinations of two semantic relations.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2174">
<title id=" H05-1047.xml">a semantic approach to recognizing textual entailment </title>
<section> framenet can help.  </section>
<citcontext>
<prevsection>
<prevsent>john is near the car behind the garage 6?
</prevsent>
<prevsent>loc(john, garage)).
</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
the berkeley framenet project9 (baker et al, 1998) <papid> P98-1013 </papid>is lexicon-building effort based on the theory offrame semantics which defines the meanings of lexical units with respect to larger conceptual structures,called frames.</citsent>
<aftsection>
<nextsent>individual lexical units point to specific frames and establish binding pattern to specific elements within the frame.
</nextsent>
<nextsent>framenet describes the underlying frames for different lexical units and examines sentences related to the frames using the bnc corpus.
</nextsent>
<nextsent>the result is an xml database that 9http://framenet.icsi.berkeley.edu contains set of frames, set of frame elements for each frame, and set of frame annotated sentences.
</nextsent>
<nextsent>4.1 frame-based semantic axioms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2175">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> problem description.  </section>
<citcontext>
<prevsection>
<prevsent>there are instances of pp-attachment, like the one in saw the car in the picture?
</prevsent>
<prevsent>that can be disambiguated only by using contextual discourse information.usually, people dont have much trouble in finding the right way to attach pps.
</prevsent>
</prevsection>
<citsent citstr=" H94-1048 ">
but if one limits the information used for disambiguation of the ppattachment to include only the verb, the noun representing its object, the preposition and the main noun in the pp, the accuracy for human decision degrades from 93.2% to 88.2% (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>on dataset extracted from penn treebank (marcus et 273 al., 1993).</citsent>
<aftsection>
<nextsent>1.2 motivation.
</nextsent>
<nextsent>syntactic parsing is essential for many natural language applications such as machine translation,question answering, information extraction, information retrieval, automatic speech recognition.
</nextsent>
<nextsent>since parsing occurs early in the chain of nlp processing steps it has large impact on the over all system performance.
</nextsent>
<nextsent>our approach to solve the pp-attachment ambiguity is based on support vector machines learner (cortes and vapnik, 1995).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2183">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>since parsing occurs early in the chain of nlp processing steps it has large impact on the over all system performance.
</prevsent>
<prevsent>our approach to solve the pp-attachment ambiguity is based on support vector machines learner (cortes and vapnik, 1995).
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (charniak, 2000), <papid> A00-2018 </papid>trees that will be improved by more accurate pp-attachment decisions.</citsent>
<aftsection>
<nextsent>some of these features were proven efficient for semantic information labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></nextsent>
<nextsent>the feature set al includes unsupervised information obtained from very large corpus (world wide web).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2184">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> approach.  </section>
<citcontext>
<prevsection>
<prevsent>our approach to solve the pp-attachment ambiguity is based on support vector machines learner (cortes and vapnik, 1995).
</prevsent>
<prevsent>the feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (charniak, 2000), <papid> A00-2018 </papid>trees that will be improved by more accurate pp-attachment decisions.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
some of these features were proven efficient for semantic information labeling (gildea and jurafsky, 2002).<papid> J02-3001 </papid></citsent>
<aftsection>
<nextsent>the feature set al includes unsupervised information obtained from very large corpus (world wide web).
</nextsent>
<nextsent>features containing manually annotated semantic information about the verb and about the objects of the verb have also been used.
</nextsent>
<nextsent>we adopted the standard approach to distinguish between verb and noun at tachment; thus the classifier has to choose between two classes: when the prepositional phrase is attached to the verb and when the prepositional phrase is attached to the preceding head noun.
</nextsent>
<nextsent>to be able to extract the required features from dataset instance, one must identify the verb, the phrase identifying the object of the verb that precedes the prepositional phrase in question (np1) which usually is part of the predicate-argument structure of the verb, its head noun, the prepositional phrase (np2), its preposition and its head noun (the second most important word in the pp).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2186">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>we adopted the standard approach to distinguish between verb and noun at tachment; thus the classifier has to choose between two classes: when the prepositional phrase is attached to the verb and when the prepositional phrase is attached to the preceding head noun.
</prevsent>
<prevsent>to be able to extract the required features from dataset instance, one must identify the verb, the phrase identifying the object of the verb that precedes the prepositional phrase in question (np1) which usually is part of the predicate-argument structure of the verb, its head noun, the prepositional phrase (np2), its preposition and its head noun (the second most important word in the pp).
</prevsent>
</prevsection>
<citsent citstr=" W95-0103 ">
we have adopted the notation from (collins and brooks, 1995), <papid> W95-0103 </papid>where is the verb, n1 is the head noun of object phrase, is the preposition and n2 is the head noun of the prepositional phrase.</citsent>
<aftsection>
<nextsent>compared to our datasets, ratnaparkhis dataset (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>contains only the lexical heads v, n1, and n2.</nextsent>
<nextsent>thus, our methodology can not be applied to ratnaparkhis dataset (rrr).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2195">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> data.  </section>
<citcontext>
<prevsection>
<prevsent>compared to our datasets, ratnaparkhis dataset (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>contains only the lexical heads v, n1, and n2.</prevsent>
<prevsent>thus, our methodology can not be applied to ratnaparkhis dataset (rrr).</prevsent>
</prevsection>
<citsent citstr=" P98-1013 ">
in our experiments we used two datasets: ? fn ? extracted from framenet ii 1.1 (baker et al ., 1998) ? <papid> P98-1013 </papid>tb2 ? extracted from penn treebank-ii table 1 presents the datasets1.</citsent>
<aftsection>
<nextsent>the creation of the datasets is described in details in (olteanu, 2004).
</nextsent>
<nextsent>the experiments described in this paper use set of discrete (alphanumeric) and continuous (numeric) features.
</nextsent>
<nextsent>all features are fully deterministic, except the features count-ratio and pp-count that are based on information provided by an external resource - google search engine (http://www.google.
</nextsent>
<nextsent>com).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2216">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" J93-1005 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2241">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" C94-2195 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2246">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W97-0109 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2247">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W97-1016 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2248">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2252">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W99-0628 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2253">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" W99-0606 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2254">
<title id=" H05-1035.xml">ppattachment disambiguation using large context </title>
<section> comparison with previous work.  </section>
<citcontext>
<prevsection>
<prevsent>the same algorithm provides an accuracy onrrr dataset of 84.5% (84.1% without morphological processing).
</prevsent>
<prevsent>the difference inaccuracy between the two datasets is 1.6% (1.7% without morphological processing when using collins and brookss algorithm.
</prevsent>
</prevsection>
<citsent citstr=" P00-1014 ">
the difference inaccuracy between svm model applied to rrr dataset (rrr-basic experiment) andthe same experiment applied to tb2 dataset (tb2 278 description accuracy data extra supervision always noun 55.0 rrr most likely for each 72.19 rrr most likely for each 72.30 tb2 most likely for each 81.73 fn average human, headwords (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>88.2 rrr average human, whole sentence (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>93.2 rrr maximum likelihood-based (hindle and rooth, 1993) <papid> J93-1005 </papid>79.7 ap maximum entropy, words (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr maximum entropy, words &amp; classes (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>81.6 rrr decision trees (ratnaparkhi et al , 1994) <papid> H94-1048 </papid>77.7 rrr transformation-based learning (brill and resnik, 1994) <papid> C94-2195 </papid>81.8 wordnet maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>84.5 rrr maximum-likelihood based (collins and brooks, 1995) <papid> W95-0103 </papid>86.1 tb2 decision trees &amp; wsd (stetina and nagao, 1997) <papid> W97-0109 </papid>88.1 rrr wordnet memory-based learning (zavrel et al , 1997) <papid> W97-1016 </papid>84.4 rrr lex space maximum entropy, unsupervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>81.9 maximum entropy, supervised (ratnaparkhi, 1998) <papid> P98-2177 </papid>83.7 rrr neural nets (alegre et al , 1999) <papid> W99-0628 </papid>86.0 rrr wordnet boosting (abney et al , 1999) <papid> W99-0606 </papid>84.4 rrr semi-probabilistic (pantel and lin, 2000) <papid> P00-1014 </papid>84.31 rrr maximum entropy, ensemble (mclauchlan, 2001) 85.5 rrr lsa svm (vanschoenwinkel and man derick, 2003) 84.8 rrr nearest-neighbor (zhao and lin, 2004) 86.5 rrr dws fn dataset, w/o semantic features (fn-best-no-sem) 91.79 fn pr-www fn dataset, w/ semantic features (fn-best-sem) 92.85 fn pr-www tb2 dataset, best feature set (tb2-best) 93.62 tb2 pr-www table 5: accuracy of pp-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.</citsent>
<aftsection>
<nextsent>also, the baseline ? themost probable pp type for each preposition ? is approximately the same for the two datasets (72.19% on rrr and 72.30% on tb2).one may hypothesize that the majority of the algorithms for pp-attachment disambiguation obtain no more than 4% increase inaccuracy on the tb2 compared to the results on the rrr dataset.
</nextsent>
<nextsent>one important difference between the two datasets is the size ? 20,801 training examples in rrr vs. 54,629 training examples in tb2.
</nextsent>
<nextsent>we plan to implement more algorithms described in literature in order to verify this statement.
</nextsent>
<nextsent>table 5 summarizes the results in pp-attachment ambiguity resolution found in literature along with our best results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2265">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>roukos (1996) and jurafsky et al (1995) use structure-based language models in the context of speech applications.
</prevsent>
<prevsent>grishman (1995) and hobbs et al (1995) use phrasal information in information extraction.
</prevsent>
</prevsection>
<citsent citstr=" P96-1023 ">
alshawi (1996) <papid> P96-1023 </papid>uses dependency information in machine translation system.</citsent>
<aftsection>
<nextsent>the need to impose structure leads to the need to have robust parsers.
</nextsent>
<nextsent>there have been two main robust parsing paradigms: fi-nite state grammar-based approaches (such as abney (1990), grishman (1995), and hobbs et al (1997)) and statistical parsing (such as charniak (1996), magerman (1995), <papid> P95-1037 </papid>and collins (1996)).<papid> P96-1025 </papid></nextsent>
<nextsent>srinivas (1997a) has presented different ap-proach called super tagging that integrates linguis-tically motivated lexical descriptions with the ro-bustness of statistical techniques.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2266">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alshawi (1996) <papid> P96-1023 </papid>uses dependency information in machine translation system.</prevsent>
<prevsent>the need to impose structure leads to the need to have robust parsers.</prevsent>
</prevsection>
<citsent citstr=" P95-1037 ">
there have been two main robust parsing paradigms: fi-nite state grammar-based approaches (such as abney (1990), grishman (1995), and hobbs et al (1997)) and statistical parsing (such as charniak (1996), magerman (1995), <papid> P95-1037 </papid>and collins (1996)).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>srinivas (1997a) has presented different ap-proach called super tagging that integrates linguis-tically motivated lexical descriptions with the ro-bustness of statistical techniques.
</nextsent>
<nextsent>the idea un-derlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (su- pertags) that impose complex constraints in lo-cal context.
</nextsent>
<nextsent>super tag disambiguation is resolved  supported by nsf grants ~sbr-9710411 and ~ger-9354869 by using statistical distributions of super tag co-occurrences collected from corpus of parses.
</nextsent>
<nextsent>it results in representation that is effectively parse (almost parse).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2267">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alshawi (1996) <papid> P96-1023 </papid>uses dependency information in machine translation system.</prevsent>
<prevsent>the need to impose structure leads to the need to have robust parsers.</prevsent>
</prevsection>
<citsent citstr=" P96-1025 ">
there have been two main robust parsing paradigms: fi-nite state grammar-based approaches (such as abney (1990), grishman (1995), and hobbs et al (1997)) and statistical parsing (such as charniak (1996), magerman (1995), <papid> P95-1037 </papid>and collins (1996)).<papid> P96-1025 </papid></citsent>
<aftsection>
<nextsent>srinivas (1997a) has presented different ap-proach called super tagging that integrates linguis-tically motivated lexical descriptions with the ro-bustness of statistical techniques.
</nextsent>
<nextsent>the idea un-derlying the approach is that the computation of linguistic structure can be localized if lexical items are associated with rich descriptions (su- pertags) that impose complex constraints in lo-cal context.
</nextsent>
<nextsent>super tag disambiguation is resolved  supported by nsf grants ~sbr-9710411 and ~ger-9354869 by using statistical distributions of super tag co-occurrences collected from corpus of parses.
</nextsent>
<nextsent>it results in representation that is effectively parse (almost parse).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2268">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> super tagging.  </section>
<citcontext>
<prevsection>
<prevsent>in fact, su- pertag disambiguation may be characterized as providing an almost parse, as shown in the bottom part of figure 1.
</prevsent>
<prevsent>local statistical information, in the form of trigram model based on the distribution of su-pertags in an ltag parsed corpus, can be used to choose the most appropriate super tag for any given word.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
joshi and srinivas (1994) <papid> C94-1024 </papid>define su- per tagging as the process of assigning the best super tag to each word.</citsent>
<aftsection>
<nextsent>srinivas (1997b) and srinivas (1997a) have tested the performance of trigram model, typically used for part-of-speech tagging on super tagging, on restricted domains such as atis and less restricted omains uch as wall street journal (wsj).
</nextsent>
<nextsent>in this work, we explore variety of local techniques in order to improve the performance of supertagging.
</nextsent>
<nextsent>all of the models presented here perform smoothing using good-turing dis-counting technique with katz backoff model.
</nextsent>
<nextsent>with exceptions where noted, our models were trained on one million words of wall street jour-nal data and tested on 48k words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2270">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> class based models.  </section>
<citcontext>
<prevsection>
<prevsent>5 as an alternative, once such reduc-tion in ambiguity has been achieved, partial pars-ing or other techniques could be employed to iden-tify the best single supertag.
</prevsent>
<prevsent>these are the aims of class based models, which assign small set of supertags to each word.
</prevsent>
</prevsection>
<citsent citstr=" J92-4003 ">
it is related to work by brown et al (1992) <papid> J92-4003 </papid>where mutual information is used to cluster words into classes for language modeling.</citsent>
<aftsection>
<nextsent>in our work with class based models, we have considered only trigram based approaches so far.
</nextsent>
<nextsent>4.1 context class mode l. one reason why the trigram model of super tag ging is limited in its accuracy is because it con-siders only small contextual window around the word to be super tagged when making its tagging decision.
</nextsent>
<nextsent>instead of using this limited context to pinpoint the exact super tag, we pos-tulate that it may be used to predict certain 4for example, the n-best model, described below, achieves 98.4% accuracy with on average 4.8 supertags per word.
</nextsent>
<nextsent>5an alternate approach to tag parsing that ef-fectively shares the computation associated with each lexicalized elementary tree (supertag) is described in evans and weir (1998).<papid> P98-1061 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2271">
<title id=" E99-1025.xml">new models for improving super tag disambiguation </title>
<section> class based models.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 context class mode l. one reason why the trigram model of super tag ging is limited in its accuracy is because it con-siders only small contextual window around the word to be super tagged when making its tagging decision.
</prevsent>
<prevsent>instead of using this limited context to pinpoint the exact super tag, we pos-tulate that it may be used to predict certain 4for example, the n-best model, described below, achieves 98.4% accuracy with on average 4.8 supertags per word.
</prevsent>
</prevsection>
<citsent citstr=" P98-1061 ">
5an alternate approach to tag parsing that ef-fectively shares the computation associated with each lexicalized elementary tree (supertag) is described in evans and weir (1998).<papid> P98-1061 </papid></citsent>
<aftsection>
<nextsent>it would be worth comparing both approaches.
</nextsent>
<nextsent>structural characteristics of the correct super tag with much higher accuracy.
</nextsent>
<nextsent>in the context class model, supertags that share the same character-istics are grouped into classes and these classes, rather than individual supertags, are predicted by trigram model.
</nextsent>
<nextsent>this is reminiscent of samuelsson and reich (1999) where some part of speech tags have been compounded so that each word is deterministically in one class.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2272">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since sentences produced by machine translation system are often incorrect but may contain correct parts, method for identifying those correct part sand finding possible errors is desirable.
</prevsent>
<prevsent>for this purpose, each word in the generated target sentence is assigned value expressing the confidence that it is correct.confidence measures have been extensively studied for speech recognition, but are not well known in other areas.
</prevsent>
</prevsection>
<citsent citstr=" C04-1046 ">
only recently have researchers started to investigate confidence measures forma chine translation (blatz et al, 2004; <papid> C04-1046 </papid>gandrabur and foster, 2003; <papid> W03-0413 </papid>quirk, 2004; ueffing et al, 2003).we apply word confidence measures in mt as fol lows: forgiven translation generated by machine translation system, we determine confidence value for each word and compare it to threshold.</citsent>
<aftsection>
<nextsent>all words whose confidence is above this threshold are tagged as correct and all others are tagged as incorrect translations.
</nextsent>
<nextsent>the threshold is optimized on distinct development set beforehand.possible applications for confidence measures include ? post-editing, where words with low confidence could be marked as potential errors, ? improving translation prediction accuracy intrans-type-style interactive machine translation (gandrabur and foster, 2003; <papid> W03-0413 </papid>ueffing and ney, 2005), ? combining output from different machine translation systems: hypotheses with low confidence can be discarded before selecting one of the system translations (akiba et al, 2004), <papid> C04-1047 </papid>orthe word confidence scores can be used for generating new hypotheses from the output of different systems (jayaraman and lavie, 2005), or the sentence confidence value can be employed for re-ranking (blatz et al, 2003).</nextsent>
<nextsent>in this paper, we will present several approaches to word-level confidence estimation and develop anew phrase-based confidence measure which is independent of the machine translation system which 763 generated the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2273">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>since sentences produced by machine translation system are often incorrect but may contain correct parts, method for identifying those correct part sand finding possible errors is desirable.
</prevsent>
<prevsent>for this purpose, each word in the generated target sentence is assigned value expressing the confidence that it is correct.confidence measures have been extensively studied for speech recognition, but are not well known in other areas.
</prevsent>
</prevsection>
<citsent citstr=" W03-0413 ">
only recently have researchers started to investigate confidence measures forma chine translation (blatz et al, 2004; <papid> C04-1046 </papid>gandrabur and foster, 2003; <papid> W03-0413 </papid>quirk, 2004; ueffing et al, 2003).we apply word confidence measures in mt as fol lows: forgiven translation generated by machine translation system, we determine confidence value for each word and compare it to threshold.</citsent>
<aftsection>
<nextsent>all words whose confidence is above this threshold are tagged as correct and all others are tagged as incorrect translations.
</nextsent>
<nextsent>the threshold is optimized on distinct development set beforehand.possible applications for confidence measures include ? post-editing, where words with low confidence could be marked as potential errors, ? improving translation prediction accuracy intrans-type-style interactive machine translation (gandrabur and foster, 2003; <papid> W03-0413 </papid>ueffing and ney, 2005), ? combining output from different machine translation systems: hypotheses with low confidence can be discarded before selecting one of the system translations (akiba et al, 2004), <papid> C04-1047 </papid>orthe word confidence scores can be used for generating new hypotheses from the output of different systems (jayaraman and lavie, 2005), or the sentence confidence value can be employed for re-ranking (blatz et al, 2003).</nextsent>
<nextsent>in this paper, we will present several approaches to word-level confidence estimation and develop anew phrase-based confidence measure which is independent of the machine translation system which 763 generated the translation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2279">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>only recently have researchers started to investigate confidence measures forma chine translation (blatz et al, 2004; <papid> C04-1046 </papid>gandrabur and foster, 2003; <papid> W03-0413 </papid>quirk, 2004; ueffing et al, 2003).we apply word confidence measures in mt as fol lows: forgiven translation generated by machine translation system, we determine confidence value for each word and compare it to threshold.</prevsent>
<prevsent>all words whose confidence is above this threshold are tagged as correct and all others are tagged as incorrect translations.</prevsent>
</prevsection>
<citsent citstr=" C04-1047 ">
the threshold is optimized on distinct development set beforehand.possible applications for confidence measures include ? post-editing, where words with low confidence could be marked as potential errors, ? improving translation prediction accuracy intrans-type-style interactive machine translation (gandrabur and foster, 2003; <papid> W03-0413 </papid>ueffing and ney, 2005), ? combining output from different machine translation systems: hypotheses with low confidence can be discarded before selecting one of the system translations (akiba et al, 2004), <papid> C04-1047 </papid>orthe word confidence scores can be used for generating new hypotheses from the output of different systems (jayaraman and lavie, 2005), or the sentence confidence value can be employed for re-ranking (blatz et al, 2003).</citsent>
<aftsection>
<nextsent>in this paper, we will present several approaches to word-level confidence estimation and develop anew phrase-based confidence measure which is independent of the machine translation system which 763 generated the translation.
</nextsent>
<nextsent>the paper is organized asfollows: in section 2, we will briefly review the statistical approach to machine translation.
</nextsent>
<nextsent>the phrase based translation system, which serves as basis for the new confidence measure, will be presented in section 2.2.
</nextsent>
<nextsent>section 3 will give an overview of related work on confidence estimation for statistical machine translation (smt).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2280">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the translation model is responsible for linking the source string fj1 and the target string ei1, i.e. it captures the semantics of the sentence.
</prevsent>
<prevsent>the target language model captures the well-formedness or the syntax in the target language.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
nowadays, most of the state-of-the-art smt systems are based on bilingual phrases (bertoldi et al, 2004; koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>tillmann, 2003; <papid> W03-1001 </papid>vogel et al, 2004; zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>note that those phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense.
</nextsent>
<nextsent>a more detailed description of phrase-based approach to statistical machine translation will be given in section 2.2.
</nextsent>
<nextsent>2.2 review of phrase-based translation system.
</nextsent>
<nextsent>for the confidence measures which will be introduced in section 5, we use state-of-the-art phrase based approach as described in (zens and ney, 2004).<papid> N04-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2281">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the translation model is responsible for linking the source string fj1 and the target string ei1, i.e. it captures the semantics of the sentence.
</prevsent>
<prevsent>the target language model captures the well-formedness or the syntax in the target language.
</prevsent>
</prevsection>
<citsent citstr=" J04-4002 ">
nowadays, most of the state-of-the-art smt systems are based on bilingual phrases (bertoldi et al, 2004; koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>tillmann, 2003; <papid> W03-1001 </papid>vogel et al, 2004; zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>note that those phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense.
</nextsent>
<nextsent>a more detailed description of phrase-based approach to statistical machine translation will be given in section 2.2.
</nextsent>
<nextsent>2.2 review of phrase-based translation system.
</nextsent>
<nextsent>for the confidence measures which will be introduced in section 5, we use state-of-the-art phrase based approach as described in (zens and ney, 2004).<papid> N04-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2282">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the translation model is responsible for linking the source string fj1 and the target string ei1, i.e. it captures the semantics of the sentence.
</prevsent>
<prevsent>the target language model captures the well-formedness or the syntax in the target language.
</prevsent>
</prevsection>
<citsent citstr=" W03-1001 ">
nowadays, most of the state-of-the-art smt systems are based on bilingual phrases (bertoldi et al, 2004; koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>tillmann, 2003; <papid> W03-1001 </papid>vogel et al, 2004; zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>note that those phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense.
</nextsent>
<nextsent>a more detailed description of phrase-based approach to statistical machine translation will be given in section 2.2.
</nextsent>
<nextsent>2.2 review of phrase-based translation system.
</nextsent>
<nextsent>for the confidence measures which will be introduced in section 5, we use state-of-the-art phrase based approach as described in (zens and ney, 2004).<papid> N04-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2283">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the translation model is responsible for linking the source string fj1 and the target string ei1, i.e. it captures the semantics of the sentence.
</prevsent>
<prevsent>the target language model captures the well-formedness or the syntax in the target language.
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
nowadays, most of the state-of-the-art smt systems are based on bilingual phrases (bertoldi et al, 2004; koehn et al, 2003; <papid> N03-1017 </papid>och and ney, 2004; <papid> J04-4002 </papid>tillmann, 2003; <papid> W03-1001 </papid>vogel et al, 2004; zens and ney, 2004).<papid> N04-1033 </papid></citsent>
<aftsection>
<nextsent>note that those phrases are sequences of words in the two languages and not necessarily phrases in the linguistic sense.
</nextsent>
<nextsent>a more detailed description of phrase-based approach to statistical machine translation will be given in section 2.2.
</nextsent>
<nextsent>2.2 review of phrase-based translation system.
</nextsent>
<nextsent>for the confidence measures which will be introduced in section 5, we use state-of-the-art phrase based approach as described in (zens and ney, 2004).<papid> N04-1033 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2287">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> statistical machine translation.  </section>
<citcontext>
<prevsection>
<prevsent>the search determines the target sentence and segmentation which maximize the objective function.as equation 2 shows, the sub-models are combined via weighted log-linear interpolation.
</prevsent>
<prevsent>the model scaling factors 1, . . .
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
, 5 and the word and phrase penalties are optimized with respect to some evaluation criterion (och, 2003), <papid> P03-1021 </papid>e.g. bleu score.</citsent>
<aftsection>
<nextsent>764
</nextsent>
<nextsent>3.1 related work.
</nextsent>
<nextsent>in this paper, we will present new approach toword-level confidence estimation which makes explicit use of phrase-based translation model.
</nextsent>
<nextsent>most of the word-level confidence measures which have been presented in the literature so far are either based on relatively simple translation models suchas ibm-1 (blatz et al, 2003) or make use of information provided by the smt system such as -best lists or word graphs (blatz et al, 2003; gandrabur and foster, 2003; <papid> W03-0413 </papid>ueffing et al, 2003).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2291">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> ibm-1 based approach.  </section>
<citcontext>
<prevsection>
<prevsent>we modified this confidence measure because we found that the average lexicon probability used there is dominated by the maximum.
</prevsent>
<prevsent>therefore, we determine the maximal translation probability of the target word over the source sentence words: pibm1(e|fj1 ) = maxj=0,...,j p(e|fj) , (9) where f0 is the empty?
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
source word (brown et al,1993).<papid> J93-2003 </papid></citsent>
<aftsection>
<nextsent>the probabilities p(e|fj) are word-based lexicon probabilities.
</nextsent>
<nextsent>investigations on the use of the ibm-1 model for word confidence measures showed promising results (blatz et al, 2003; blatz et al, 2004).<papid> C04-1046 </papid></nextsent>
<nextsent>thus, we apply this method here in order to compare it to the other types of confidence measures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2294">
<title id=" H05-1096.xml">word level confidence estimation for machine translation using phrase based translation models </title>
<section> experiments.  </section>
<citcontext>
<prevsection>
<prevsent>the translation quality in terms of wer, per (position independent word error rate), bleu and nist score is given in tables 2 and 3.
</prevsent>
<prevsent>we see that the best results are obtained on spanish to english translation, followed by french to english and german to english.two more translation systems were used for comparative experiments: one is statistical mt system which is based on finite state architecture (fsa).
</prevsent>
</prevsection>
<citsent citstr=" W05-0831 ">
for description of this system, see (kanthak et al, 2005).<papid> W05-0831 </papid></citsent>
<aftsection>
<nextsent>additionally, we used translations generated by systran2.
</nextsent>
<nextsent>table 3 presents the translation error rates and scores for all systems on the german ? english test corpus.
</nextsent>
<nextsent>these hypotheses were used to investigate whether the phrase-based confidence measures perform well independently of the translation system.
</nextsent>
<nextsent>all three smt systems (at, pbt and fsa) show very similar performance on the german ? english test corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2295">
<title id=" H05-1017.xml">investigating unsupervised learning for text categorization bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>feature-based supervision is referred to as intensional learning (il), as features may often be perceived as describing the in tension of category, such as providing the name or prominent key terms for category in text categorization.
</prevsent>
<prevsent>the il approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
within the machine learning paradigm, il has been incorporated as technique for bootstrapping an extensional learning algorithm, as in (yarowsky, 1995; <papid> P95-1026 </papid>collins and singer, 1999; <papid> W99-0613 </papid>liu et al, 2004).</citsent>
<aftsection>
<nextsent>this way the user does notneed to specify exact classification rules (and feature weights), but rather perform somewhat simpler task of specifying few typical seed features for the category.
</nextsent>
<nextsent>given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (ex tensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an expectation-maximization schema).
</nextsent>
<nextsent>the core part of il bootstrapping is step (i), i.e. the initial unsupervised classification of the unlabeled dataset.this step was often approached by relatively simple methods, which are doomed to obtain mediocre quality.
</nextsent>
<nextsent>even so, it is hoped that the second step of supervised training would be robust enough to the noise in the initial training set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2297">
<title id=" H05-1017.xml">investigating unsupervised learning for text categorization bootstrapping </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>feature-based supervision is referred to as intensional learning (il), as features may often be perceived as describing the in tension of category, such as providing the name or prominent key terms for category in text categorization.
</prevsent>
<prevsent>the il approach reflects on classical rule-based classification methods, where the user is expected to specify exact classification rules that operate in the feature space.
</prevsent>
</prevsection>
<citsent citstr=" W99-0613 ">
within the machine learning paradigm, il has been incorporated as technique for bootstrapping an extensional learning algorithm, as in (yarowsky, 1995; <papid> P95-1026 </papid>collins and singer, 1999; <papid> W99-0613 </papid>liu et al, 2004).</citsent>
<aftsection>
<nextsent>this way the user does notneed to specify exact classification rules (and feature weights), but rather perform somewhat simpler task of specifying few typical seed features for the category.
</nextsent>
<nextsent>given the list of seed features, the bootstrapping scheme consists of (i) preliminary unsupervised categorization of the unlabeled data set based on the seed features, and (ii) training an (ex tensional) supervised classifier using the automatic classification labels of step (i) as the training data (the second step is possibly reiterated, such as by an expectation-maximization schema).
</nextsent>
<nextsent>the core part of il bootstrapping is step (i), i.e. the initial unsupervised classification of the unlabeled dataset.this step was often approached by relatively simple methods, which are doomed to obtain mediocre quality.
</nextsent>
<nextsent>even so, it is hoped that the second step of supervised training would be robust enough to the noise in the initial training set.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2298">
<title id=" H05-1017.xml">investigating unsupervised learning for text categorization bootstrapping </title>
<section> bootstrapping for text categorization.  </section>
<citcontext>
<prevsection>
<prevsent>tn} of unlabeled texts is provided.
</prevsent>
<prevsent>evaluation is performed on separate test corpus of labeled documents, to which standard evaluation metrics can be applied.
</prevsent>
</prevsection>
<citsent citstr=" W99-0908 ">
the approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (mccallum and nigam, 1999; <papid> W99-0908 </papid>ko and seo, 2000; <papid> C00-1066 </papid>liu et al, 2004; ko and seo, 2004).<papid> P04-1033 </papid></citsent>
<aftsection>
<nextsent>several names have been proposed for it ? such as tc by bootstrapping with keywords, unsupervised tc, tc by labelling words ? where the proposed methods fall (mostly) within the il settings described here1.
</nextsent>
<nextsent>it is possible to recognize common structure of these works, based on typical bootstrap schema (yarowsky, 1995; <papid> P95-1026 </papid>collins and singer, 1999): <papid> W99-0613 </papid>step 1: initial unsupervised categorization.</nextsent>
<nextsent>this step was approached by applying some similarity criterion between the initial category seed and each unlabeled document.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2300">
<title id=" H05-1017.xml">investigating unsupervised learning for text categorization bootstrapping </title>
<section> bootstrapping for text categorization.  </section>
<citcontext>
<prevsection>
<prevsent>tn} of unlabeled texts is provided.
</prevsent>
<prevsent>evaluation is performed on separate test corpus of labeled documents, to which standard evaluation metrics can be applied.
</prevsent>
</prevsection>
<citsent citstr=" C00-1066 ">
the approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (mccallum and nigam, 1999; <papid> W99-0908 </papid>ko and seo, 2000; <papid> C00-1066 </papid>liu et al, 2004; ko and seo, 2004).<papid> P04-1033 </papid></citsent>
<aftsection>
<nextsent>several names have been proposed for it ? such as tc by bootstrapping with keywords, unsupervised tc, tc by labelling words ? where the proposed methods fall (mostly) within the il settings described here1.
</nextsent>
<nextsent>it is possible to recognize common structure of these works, based on typical bootstrap schema (yarowsky, 1995; <papid> P95-1026 </papid>collins and singer, 1999): <papid> W99-0613 </papid>step 1: initial unsupervised categorization.</nextsent>
<nextsent>this step was approached by applying some similarity criterion between the initial category seed and each unlabeled document.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2301">
<title id=" H05-1017.xml">investigating unsupervised learning for text categorization bootstrapping </title>
<section> bootstrapping for text categorization.  </section>
<citcontext>
<prevsection>
<prevsent>tn} of unlabeled texts is provided.
</prevsent>
<prevsent>evaluation is performed on separate test corpus of labeled documents, to which standard evaluation metrics can be applied.
</prevsent>
</prevsection>
<citsent citstr=" P04-1033 ">
the approach of categorizing texts based on lists of keywords has been attempted rather rarely in the literature (mccallum and nigam, 1999; <papid> W99-0908 </papid>ko and seo, 2000; <papid> C00-1066 </papid>liu et al, 2004; ko and seo, 2004).<papid> P04-1033 </papid></citsent>
<aftsection>
<nextsent>several names have been proposed for it ? such as tc by bootstrapping with keywords, unsupervised tc, tc by labelling words ? where the proposed methods fall (mostly) within the il settings described here1.
</nextsent>
<nextsent>it is possible to recognize common structure of these works, based on typical bootstrap schema (yarowsky, 1995; <papid> P95-1026 </papid>collins and singer, 1999): <papid> W99-0613 </papid>step 1: initial unsupervised categorization.</nextsent>
<nextsent>this step was approached by applying some similarity criterion between the initial category seed and each unlabeled document.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2325">
<title id=" H05-1072.xml">a cost benefit analysis of hybrid phone manner representations for asr </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>in the past decade, several researchers have started re investigating the use ofsub-phonetic models for lexical representations within automatic speech recognition systems.
</prevsent>
<prevsent>lest history repeat itself, it may be instructive to mine the further past for models of lexical representations in the lexical access literature.
</prevsent>
</prevsection>
<citsent citstr=" P89-1011 ">
in this work, we re-evaluate the model of briscoe(1989), <papid> P89-1011 </papid>in which hybrid strategy of lexical representation between phones and manner classes is promoted.</citsent>
<aftsection>
<nextsent>while many of bris coes assumptions do not match up with current asr processing models, we show that his conclusions are essentially correct, and that reconsidering this structure for asr lexica is an appropriate avenue for future asr research.
</nextsent>
<nextsent>almost every state-of-the-art large vocabulary automatic speech recognition (asr) system requires the sharing of sub-word units in order to achieve the desired vocabulary coverage.
</nextsent>
<nextsent>traditionally, these sub-word units are determined by the phones or phonemes of language (depending on desired detail of representation).
</nextsent>
<nextsent>however, phonetic (orphonemic) representation has its pitfalls (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2327">
<title id=" E99-1036.xml">repair strategies for lexicalized tree grammars </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>cocad ii 78.3 ii 2.o average no of partial results/utter.
</prevsent>
<prevsent>7.1 table 1: global results for the parsing of the gocad corpora utterances ill-formed with with with agrammatical utterances hesitations repetitions self-repairs \[ ellipsis occurrences ii 123 ii 28 22 ii 15 table 2: occurrences of error oral phenomena in the gocad corpora 1.2 exp lo t ing lexicalized tree.
</prevsent>
</prevsection>
<citsent citstr=" P98-1061 ">
grammars the choice of ltg (lexicalized tree grammar), more specifically ltag (lexicalized tree adjo- ing grammar), can be justified by the two main following reasons: first the lexicalization and the extended omain of locality allow to express easily lexical constraints in partial parsing trees (elemen- tary trees), secondly robust bottom-up arsing al-gorithms, stochastic models and efficient precom- pil ation of the grammar (evans and weir, 1998) <papid> P98-1061 </papid>exist for ltg.</citsent>
<aftsection>
<nextsent>when the parsing of an utterance fails, ro-bust bottom-up algorithm gives partial derived and derivation trees.
</nextsent>
<nextsent>with classical chart pars-ing, items are obtained from other items and cor-respond to well-recognized chunk of the utter-ance.
</nextsent>
<nextsent>the chart is an acyclic graph representing all the derivations.
</nextsent>
<nextsent>a partial result corresponds to the maximal expansion of an island, so to an item which is not the origin of any other item.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2329">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these have so far only been applied to written corpora.
</prevsent>
<prevsent>however, the high frequency of abstract object anaphora india- logues means that any attempt to resolve anaphors in spoken language cannot succeed without aking this into account.
</prevsent>
</prevsection>
<citsent citstr=" P98-2241 ">
summarised below are some issues specific to anaphora resolution in spoken dialogues (see also byron and stent (1998) <papid> P98-2241 </papid>who mention some of these problems in their account of the centering model (grosz et al, 1995)).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>center of attention in multi-party discourse.
</nextsent>
<nextsent>in spontaneous speech it is possible that the participants of dialogue may not be focussing on the same entity at given point in the discourse.
</nextsent>
<nextsent>utterances with no discourse entities.
</nextsent>
<nextsent>e.g., uh- huh; yeah; right.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2332">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these have so far only been applied to written corpora.
</prevsent>
<prevsent>however, the high frequency of abstract object anaphora india- logues means that any attempt to resolve anaphors in spoken language cannot succeed without aking this into account.
</prevsent>
</prevsection>
<citsent citstr=" J95-2003 ">
summarised below are some issues specific to anaphora resolution in spoken dialogues (see also byron and stent (1998) <papid> P98-2241 </papid>who mention some of these problems in their account of the centering model (grosz et al, 1995)).<papid> J95-2003 </papid></citsent>
<aftsection>
<nextsent>center of attention in multi-party discourse.
</nextsent>
<nextsent>in spontaneous speech it is possible that the participants of dialogue may not be focussing on the same entity at given point in the discourse.
</nextsent>
<nextsent>utterances with no discourse entities.
</nextsent>
<nextsent>e.g., uh- huh; yeah; right.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2336">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> synchronising units.  </section>
<citcontext>
<prevsection>
<prevsent>our as-sumptions are based on clark (1989) theory of con-tributions (cf.
</prevsent>
<prevsent>also traum (1994)).
</prevsent>
</prevsection>
<citsent citstr=" J97-1002 ">
each dialogue is divided into short, clearly de-fined dialogue acts - initiations and acknowledg-ments - based on the top of the hierarchy given in carletta et al (1997).<papid> J97-1002 </papid></citsent>
<aftsection>
<nextsent>each sentence and each con-joined clause counts as separate i, even if they are part of the same turn.
</nextsent>
<nextsent>a do not convey semantic on- tent but have pragmatic function (e.g., backchannel).
</nextsent>
<nextsent>in addition there are utterances which function as an but also have semantic ontent - these are labelled as a/i. single is paired with an and they jointly form synchronising unit (su).
</nextsent>
<nextsent>in longer turns, each main clause functions as separate unit along with its sub-ordinate clauses.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2337">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> how to resolve discourse deictic.  </section>
<citcontext>
<prevsection>
<prevsent>sui s: so there- the five boxcars of oranges  sil  + that are at- + s-list: \[5 boxcars of oranges\] suj a/i u: +at  sil  +atcoming s-list: \[5 boxcars of oranges, coming\] s: urn - u: okay the orange warehouse  sil  urn + have to + s-list: \[coming, orange warehouse\] suk s: you need + you need to get five  sil  five boxcars of oranges there s-list: \[coming, 5 boxcars of oranges\] u: uh sot no theyre are already waiting for me there (d92a-4.3) figure 1: unacknowledged turns speaker s second turn is an which is not fol-lowed by an a. this means that the entity referred to in that utterance (orange warehouse) is immediately removed from the joint discourse model.
</prevsent>
<prevsent>thus there in the final two turns co-specifies with coming and not the most recent orange warehouse.
</prevsent>
</prevsection>
<citsent citstr=" P98-2204 ">
anaphora we now turn to our method of anaphora reso-lution, which extends the algorithm presented in strube (1998), <papid> P98-2204 </papid>in order to be able to account for discourse deictic anaphora as well as individual anaphora.</citsent>
<aftsection>
<nextsent>4.1 anaphor-anteeedent compatibi i ty.
</nextsent>
<nextsent>as indicated in section 2, information provided by the subcategorisation frame of the anaphor predicate can be used to determine the type of the referent.
</nextsent>
<nextsent>in the algorithm, we make use of the notion of anaphor- antecedent compatibility to distinguish between dis-course deictic and individual reference.
</nextsent>
<nextsent>certain pred-icates (notably verbs of propositional ttitude) require one of their arguments have referent whose mean-ing is correlated with sentences, e.g., is true, assume (referred to as sc-bias verbs in garnsey et al (1997) and elsewhere).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2339">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> how to resolve discourse deictic.  </section>
<citcontext>
<prevsection>
<prevsent>two dialogues (sw2041, sw4877) were used to train the two annota-tors (the authors), and three further dialogues for test-ing (sw2403, sw3117, sw3241).
</prevsent>
<prevsent>the training dia-logues were used for improving the annotation manual and for clarifying the annotation borderline cases.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
after each step the annotations were compared us-ing the ~ statistic as reliability measure for all classifi-cation tasks (carletta, 1996).<papid> J96-2004 </papid></citsent>
<aftsection>
<nextsent>a t~ of 0.68   ~   0.80 allows tentative conclusions while ~   0.80 indicates reliability between the annotators.
</nextsent>
<nextsent>in the following ta-bles, the rows on above the horizontal line show how often particular class was actually marked as such by both annotators.
</nextsent>
<nextsent>in the rows below the line, shows the total number of markables, while gives the num-ber of agreements between the annotations.
</nextsent>
<nextsent>pa is per-cent agreement between the annotators, pe expected agreement by chance.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2340">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> how to resolve discourse deictic.  </section>
<citcontext>
<prevsection>
<prevsent>finally, ~ is computed by the formula pa - pe /1 - pe . dialogue acts.
</prevsent>
<prevsent>first, turns were segmented into di-alogue act units.
</prevsent>
</prevsection>
<citsent citstr=" J97-1005 ">
we turned the segmentation task into classification task by using boundaries between di-alogue acts as one class and non-boundaries the other (see passonneau and litman (1997) <papid> J97-1005 </papid>for simi-lar practice).</citsent>
<aftsection>
<nextsent>in table l, non-bound.
</nextsent>
<nextsent>and bound.
</nextsent>
<nextsent>give the number of non-boundaries and boundaries actu-ally marked by the annotators, is the total number of possible boundary sites, while gives the number of agreements between the annotations.
</nextsent>
<nextsent>sw2403 sw3117 sw3241 non-bound.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2341">
<title id=" E99-1006.xml">resolving discourse deictic anaphora in dialogues </title>
<section> how to resolve discourse deictic.  </section>
<citcontext>
<prevsection>
<prevsent>also, as they do not give preferences on the set of potential candidates, their approaches are not intended as attempts to resolve ab-stract object anaphora.
</prevsent>
<prevsent>concerning anaphora resolution in dialogues, only little research as been carried out in this area to our knowledge.
</prevsent>
</prevsection>
<citsent citstr=" P92-1004 ">
luperfoy (1992) <papid> P92-1004 </papid>does not present cor-pus study, meaning that statistics about he distribution of individual and abstract object anaphora or about the success rate of her approach are not available.</citsent>
<aftsection>
<nextsent>byron and stent (1998) <papid> P98-2241 </papid>present extensions of the cen-tering model (grosz et al, 1995) <papid> J95-2003 </papid>for spoken dialogue and identify several problems with the model.</nextsent>
<nextsent>we have chosen strube (1998) <papid> P98-2204 </papid>model for the resolution of individual anaphora basis because it avoids the problems encountered by byron &amp; stent, who also do not present data on the resolution of pronouns india- logues and do not mention abstract object anaphora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2347">
<title id=" E99-1017.xml">transducers from rewrite rules with back references </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>context sensitive rewrite rules have been widely used in several areas of natural language pro-cessing.
</prevsent>
<prevsent>johnson (1972) has shown that such rewrite rules are equivalent to finite state trans-ducers in the special case that they are not al-lowed to rewrite their own output.
</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
an algo-rithm for compilation into transducers was pro-vided by kaplan and kay (1994).<papid> J94-3001 </papid></citsent>
<aftsection>
<nextsent>improvements and extensions to this algorithm have been pro-vided by karttunen (1995), <papid> P95-1003 </papid>karttunen (1997), karttunen (1996) <papid> P96-1015 </papid>and mohri and sproat (1996).<papid> P96-1031 </papid></nextsent>
<nextsent>in this paper, the algorithm will be ex-tended to provide limited form of back- referencing.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2349">
<title id=" E99-1017.xml">transducers from rewrite rules with back references </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>johnson (1972) has shown that such rewrite rules are equivalent to finite state trans-ducers in the special case that they are not al-lowed to rewrite their own output.
</prevsent>
<prevsent>an algo-rithm for compilation into transducers was pro-vided by kaplan and kay (1994).<papid> J94-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P95-1003 ">
improvements and extensions to this algorithm have been pro-vided by karttunen (1995), <papid> P95-1003 </papid>karttunen (1997), karttunen (1996) <papid> P96-1015 </papid>and mohri and sproat (1996).<papid> P96-1031 </papid></citsent>
<aftsection>
<nextsent>in this paper, the algorithm will be ex-tended to provide limited form of back- referencing.
</nextsent>
<nextsent>back referencing has been im-plicit in previous research, such as in the  batch rules  of kaplan and kay (1994), <papid> J94-3001 </papid>brack-eting transducers for finite-state parsing (kart- tunen, 1996), <papid> P96-1015 </papid>and the  local extension  operation of roche and schabes (1995).<papid> J95-2004 </papid></nextsent>
<nextsent>the explicit use of back referencing leads to more elegant and general solutions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2350">
<title id=" E99-1017.xml">transducers from rewrite rules with back references </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>johnson (1972) has shown that such rewrite rules are equivalent to finite state trans-ducers in the special case that they are not al-lowed to rewrite their own output.
</prevsent>
<prevsent>an algo-rithm for compilation into transducers was pro-vided by kaplan and kay (1994).<papid> J94-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1015 ">
improvements and extensions to this algorithm have been pro-vided by karttunen (1995), <papid> P95-1003 </papid>karttunen (1997), karttunen (1996) <papid> P96-1015 </papid>and mohri and sproat (1996).<papid> P96-1031 </papid></citsent>
<aftsection>
<nextsent>in this paper, the algorithm will be ex-tended to provide limited form of back- referencing.
</nextsent>
<nextsent>back referencing has been im-plicit in previous research, such as in the  batch rules  of kaplan and kay (1994), <papid> J94-3001 </papid>brack-eting transducers for finite-state parsing (kart- tunen, 1996), <papid> P96-1015 </papid>and the  local extension  operation of roche and schabes (1995).<papid> J95-2004 </papid></nextsent>
<nextsent>the explicit use of back referencing leads to more elegant and general solutions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2352">
<title id=" E99-1017.xml">transducers from rewrite rules with back references </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>johnson (1972) has shown that such rewrite rules are equivalent to finite state trans-ducers in the special case that they are not al-lowed to rewrite their own output.
</prevsent>
<prevsent>an algo-rithm for compilation into transducers was pro-vided by kaplan and kay (1994).<papid> J94-3001 </papid></prevsent>
</prevsection>
<citsent citstr=" P96-1031 ">
improvements and extensions to this algorithm have been pro-vided by karttunen (1995), <papid> P95-1003 </papid>karttunen (1997), karttunen (1996) <papid> P96-1015 </papid>and mohri and sproat (1996).<papid> P96-1031 </papid></citsent>
<aftsection>
<nextsent>in this paper, the algorithm will be ex-tended to provide limited form of back- referencing.
</nextsent>
<nextsent>back referencing has been im-plicit in previous research, such as in the  batch rules  of kaplan and kay (1994), <papid> J94-3001 </papid>brack-eting transducers for finite-state parsing (kart- tunen, 1996), <papid> P96-1015 </papid>and the  local extension  operation of roche and schabes (1995).<papid> J95-2004 </papid></nextsent>
<nextsent>the explicit use of back referencing leads to more elegant and general solutions.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2357">
<title id=" E99-1017.xml">transducers from rewrite rules with back references </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>improvements and extensions to this algorithm have been pro-vided by karttunen (1995), <papid> P95-1003 </papid>karttunen (1997), karttunen (1996) <papid> P96-1015 </papid>and mohri and sproat (1996).<papid> P96-1031 </papid></prevsent>
<prevsent>in this paper, the algorithm will be ex-tended to provide limited form of back- referencing.</prevsent>
</prevsection>
<citsent citstr=" J95-2004 ">
back referencing has been im-plicit in previous research, such as in the  batch rules  of kaplan and kay (1994), <papid> J94-3001 </papid>brack-eting transducers for finite-state parsing (kart- tunen, 1996), <papid> P96-1015 </papid>and the  local extension  operation of roche and schabes (1995).<papid> J95-2004 </papid></citsent>
<aftsection>
<nextsent>the explicit use of back referencing leads to more elegant and general solutions.
</nextsent>
<nextsent>back referencing is widely used in editors, script-ing languages and other tools employing regular expressions (friedl, 1997).
</nextsent>
<nextsent>for example, emacs uses the special brackets \ ( and \) to capture strings along with the notation \n to recall the nth such string.
</nextsent>
<nextsent>the expression \ (a* \ )b \ matches strings of the form anba n. unrestricted use of back referencing thus can introduce non-regular languages.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2373">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this results in typical compilation times of about minute, and has allowed rea-sonably full, feature-based description of french inflectional morphology to be de-veloped in about month by linguist new to the system.
</prevsent>
<prevsent>the paradigm of two-level morphology (kosken- niemi, 1983) has become popular for handling word formation phenomena in variety of lan-guages.
</prevsent>
</prevsection>
<citsent citstr=" C90-2064 ">
the original formulation has been ex-tended to allow morpho tactic constraints to be ex-pressed by feature specification (trost, 1990; <papid> C90-2064 </papid>a1- shawi et al 1991) rather than koskenniemi less perspicuous device of continuation classes.</citsent>
<aftsection>
<nextsent>meth-ods for the automatic ompilation of rules from notation convenient for the rule-writer into finite- state automata have also been developed, allowing the efficient analysis and synthesis of word forms.
</nextsent>
<nextsent>the automata may be derived from the rules alone (trost, 1990), <papid> C90-2064 </papid>or involve composition with the lex-icon (karttunen, kaplan and zaenen, 1992).</nextsent>
<nextsent>however, there is often trade-off between run- time efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compi-lation and the size of its output, and the inde-pendence of the morphological nd lexical compo- nents.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2375">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the automata may be derived from the rules alone (trost, 1990), <papid> C90-2064 </papid>or involve composition with the lex-icon (karttunen, kaplan and zaenen, 1992).</prevsent>
<prevsent>however, there is often trade-off between run- time efficiency and factors important for rapid and accurate system development, such as perspicuity of notation, ease of debugging, speed of compi-lation and the size of its output, and the inde-pendence of the morphological nd lexical compo- nents.</prevsent>
</prevsection>
<citsent citstr=" J94-3001 ">
in compilation, one may compose any or all of (a) the two-level rule set, (b) the set of affixes and their allowed combina-tions, and (c) the lexicon; see kaplan and kay (1994 / <papid> J94-3001 </papid>for an exposition of the mathematical basis.</citsent>
<aftsection>
<nextsent>the type of compilation appropriate for rapid development and acceptable run-time performance depends on, at least, the nature of the language being described and the number of base forms in the lexicon; that is, on the position in the three-dimensional space defined by (a), (b) and (c).
</nextsent>
<nextsent>for example, english inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological nmysis can be carried out at com-pile time, producing list of analysed word forms which need only be looked up at run time, or network which can be traversed very simply.
</nextsent>
<nextsent>al-ternatively, there may be no need to provide as powerful mechanism as two-level morphology at all; simpler device such as affix stripping (a1- shawi, 1992, pl l9ff) or merely listing all inflected forms explicitly may be preferable.
</nextsent>
<nextsent>for agglutinative languages such as korean, finnish and turkish (kwon and karttunen, 1994; <papid> C94-2206 </papid>koskenniemi, 1983; oflazer, 1993), <papid> E93-1066 </papid>dimension (b) is very large, so creating an exhaustive word list is out of the question unless the lexicon is trivial.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2376">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, english inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological nmysis can be carried out at com-pile time, producing list of analysed word forms which need only be looked up at run time, or network which can be traversed very simply.
</prevsent>
<prevsent>al-ternatively, there may be no need to provide as powerful mechanism as two-level morphology at all; simpler device such as affix stripping (a1- shawi, 1992, pl l9ff) or merely listing all inflected forms explicitly may be preferable.
</prevsent>
</prevsection>
<citsent citstr=" C94-2206 ">
for agglutinative languages such as korean, finnish and turkish (kwon and karttunen, 1994; <papid> C94-2206 </papid>koskenniemi, 1983; oflazer, 1993), <papid> E93-1066 </papid>dimension (b) is very large, so creating an exhaustive word list is out of the question unless the lexicon is trivial.</citsent>
<aftsection>
<nextsent>compilation to network may still make sense, however, and because these languages tend to ex-hibit few non-eoncatenative morpho phonological phenomena other than vowel harmony, the con-tinuation class mechanism ay suffice to describe the allowed affix sequences at the surface level.
</nextsent>
<nextsent>many european languages are of the inflect- ing type, and occupy still another region of the space of difficulty.
</nextsent>
<nextsent>they are too complex mor-phologically to yield easily to the simpler tech-niques that can work for english.
</nextsent>
<nextsent>the phonologi-cal or orthographic changes involved in affix ation may be quite complex, so dimension (a) can be laige, and feature mechanism ay be needed to handle such varied but interrelated morpho syn 202 tactic phenomena such as umlaut (trost, 1991), case, number, gender, and different morphologi-cal paradigms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2377">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, english inflectional morphology is relatively simple; dimensions (a) and (b) are fairly small, so if (c), the lexicon, is known in advance and is of manageable size, then the entire task of morphological nmysis can be carried out at com-pile time, producing list of analysed word forms which need only be looked up at run time, or network which can be traversed very simply.
</prevsent>
<prevsent>al-ternatively, there may be no need to provide as powerful mechanism as two-level morphology at all; simpler device such as affix stripping (a1- shawi, 1992, pl l9ff) or merely listing all inflected forms explicitly may be preferable.
</prevsent>
</prevsection>
<citsent citstr=" E93-1066 ">
for agglutinative languages such as korean, finnish and turkish (kwon and karttunen, 1994; <papid> C94-2206 </papid>koskenniemi, 1983; oflazer, 1993), <papid> E93-1066 </papid>dimension (b) is very large, so creating an exhaustive word list is out of the question unless the lexicon is trivial.</citsent>
<aftsection>
<nextsent>compilation to network may still make sense, however, and because these languages tend to ex-hibit few non-eoncatenative morpho phonological phenomena other than vowel harmony, the con-tinuation class mechanism ay suffice to describe the allowed affix sequences at the surface level.
</nextsent>
<nextsent>many european languages are of the inflect- ing type, and occupy still another region of the space of difficulty.
</nextsent>
<nextsent>they are too complex mor-phologically to yield easily to the simpler tech-niques that can work for english.
</nextsent>
<nextsent>the phonologi-cal or orthographic changes involved in affix ation may be quite complex, so dimension (a) can be laige, and feature mechanism ay be needed to handle such varied but interrelated morpho syn 202 tactic phenomena such as umlaut (trost, 1991), case, number, gender, and different morphologi-cal paradigms.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2378">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> the description language.  </section>
<citcontext>
<prevsection>
<prevsent>affixes may appear explicitly in pro-duction rules or, like roots, they may be assigned complex feature-valued categories.
</prevsent>
<prevsent>information, including the creation of logical forms, is passed between constituents in rule by the sharing of variables.
</prevsent>
</prevsection>
<citsent citstr=" C94-1022 ">
these feature-augmented production rules are just the same device as those used in the cle syntactico-semantic descriptions, and are much more natural way to express morpho tactic information than finite-state devices uch as con-tinuation classes (see trost and matiasek, 1994, <papid> C94-1022 </papid>for related approach).</citsent>
<aftsection>
<nextsent>the syntactic and semantic production rules for deriving the feminine singular of french adjec-tive by suffix ation with   are given, with some details omitted, in figure 3.
</nextsent>
<nextsent>in this case, nearly mi features are shared between the inflected word and the root, as is the logical form for the word (shown as adj in the dor iv rule).
</nextsent>
<nextsent>the only differ-ing feature is that for gender, shown as the third argument of the agr macro, which itself expands to category.
</nextsent>
<nextsent>irregular forms, either complete words or affix- able stems, are specified by listing the morpho-logical rules and terminal morphemes from which the appropriate analyses may be constructed, for example: irreg(dit, \[-dire,   present_3s   \ ] , \[v_v_affix-only\] ).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2379">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> compilation.  </section>
<citcontext>
<prevsection>
<prevsent>all rules and lexieal entries in the cle are com-piled to form that allows normal prolog unifi-cation to be used for category matching at run time.
</prevsent>
<prevsent>the same compiled forms are used for anal-ysis and generation, but are indexed differently.
</prevsent>
</prevsection>
<citsent citstr=" J88-1004 ">
each feature for major category is assigned unique position in the compiled prolog term, and features for which finite value sets have been spec-ified are compiled into vectors in form that al-lows boolean expressions, involving negation as well as conjunction and disjunction, to be con-joined by unification (see mellish, 1988; <papid> J88-1004 </papid>alshawi, 1992, pp46-48).</citsent>
<aftsection>
<nextsent>the compilation of morphological information is motivated by the nature of the task and of the languages to be handled.
</nextsent>
<nextsent>as discussed in sec-tion 1, we expect the number of affix combina-tions to be limited, but the lexicon is not neces-sarily known in advance.
</nextsent>
<nextsent>morpho phonological interactions may be quite complex, and the purpose of morphological processing is to derive syntactic and semantic analyses from words and vice versa for the purpose of full nlp.
</nextsent>
<nextsent>reasonably quick compilation is required, and run-time speed need only be moderate.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2380">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> compilation.  </section>
<citcontext>
<prevsection>
<prevsent>for example, for english, the sequences *+ed+ly and un+*+ing are among those produced, the asterisk representing theun- specified root.
</prevsent>
<prevsent>then, each sequence, together with any associ-ated restrictions on orthographic features, under- goes analysis by the compiled spelling rules (sec- tion 2.1), with the surface sequence and the root part of the lexical sequence initially uninstanti- ated.
</prevsent>
</prevsection>
<citsent citstr=" C92-3125 ">
rules are applied recursively and non deter ministically, somewhat in the style of abramson (1992), <papid> C92-3125 </papid>taking advantage of prolog unification mechanism to instantiate the part of the surface string corresponding to affixes and to place some spelling constraints on the start and/or end of the surface and/or lexical forms of the root.</citsent>
<aftsection>
<nextsent>this process results in set of spelling pal terns, one for each distinct application of the spelling rules to each affix sequence suggested by the pro-duction rules.
</nextsent>
<nextsent>a spelling pattern consists of par-tially specified surface and lexical root character sequences~ fully specified surface and lexical affix sequences, orthographic feature constraints asso-ciated with the spelling rules and affixes used, and pair of syntactic ategory specifications derived from the production rules used.
</nextsent>
<nextsent>one category is for the root form, and one for the inflected form.
</nextsent>
<nextsent>spelling patterns are indexed according to the surface (for analysis) and lexical (for generation) affix characters they involve.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2381">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> conc lus ions  and further  work.  </section>
<citcontext>
<prevsection>
<prevsent>the rule formalism and compiler described here work well for european languages with reasonably complex orthographic changes but limited range of possible affix combinations.
</prevsent>
<prevsent>development, com-pilation and run-time efficiency are quite accept- able, and the use of rules containing complex feature-augmented categories allows morpho tactic behaviours and non-segmentm spelling constraints to be specified in way that is perspicuous to lin-guists, leading to rapid development of descrip-tions adequate for full nlp.
</prevsent>
</prevsection>
<citsent citstr=" E87-1002 ">
the kinds of non-linear effects common in semitic languages, where vowel and consonant patterns are interpolated in words (kay, 1987; <papid> E87-1002 </papid>kiraz, 1994) <papid> C94-1029 </papid>could be treated efficiently by the mechanisms described here if it proved possible to define representation that allowed the parts of an inflected word corresponding to the root to be separated fairly cleanly from the parts expressing the inflection.</citsent>
<aftsection>
<nextsent>the latter could then be used by modified version of the current system as the basis for efficient lookup of spelling patterns which, as in the current system, would allow possible lexical roots to be calculated.
</nextsent>
<nextsent>agglutinative languages could be handled ef- 208 flciently by the current mechanism if specifica-tions were provided for the affix combinations that were likely to occur at all often in real texts.
</nextsent>
<nextsent>a backup mechanism could then be provided which attempted slower, but more complete, direct ap-plication of the rules for the rarer cases.
</nextsent>
<nextsent>the interaction of morphological nalysis with spelling correction (carter, 1992; <papid> P92-1021 </papid>oflazer, 1994; bowden, 1995) <papid> E95-1044 </papid>is another possibly fruitful area of work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2382">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> conc lus ions  and further  work.  </section>
<citcontext>
<prevsection>
<prevsent>the rule formalism and compiler described here work well for european languages with reasonably complex orthographic changes but limited range of possible affix combinations.
</prevsent>
<prevsent>development, com-pilation and run-time efficiency are quite accept- able, and the use of rules containing complex feature-augmented categories allows morpho tactic behaviours and non-segmentm spelling constraints to be specified in way that is perspicuous to lin-guists, leading to rapid development of descrip-tions adequate for full nlp.
</prevsent>
</prevsection>
<citsent citstr=" C94-1029 ">
the kinds of non-linear effects common in semitic languages, where vowel and consonant patterns are interpolated in words (kay, 1987; <papid> E87-1002 </papid>kiraz, 1994) <papid> C94-1029 </papid>could be treated efficiently by the mechanisms described here if it proved possible to define representation that allowed the parts of an inflected word corresponding to the root to be separated fairly cleanly from the parts expressing the inflection.</citsent>
<aftsection>
<nextsent>the latter could then be used by modified version of the current system as the basis for efficient lookup of spelling patterns which, as in the current system, would allow possible lexical roots to be calculated.
</nextsent>
<nextsent>agglutinative languages could be handled ef- 208 flciently by the current mechanism if specifica-tions were provided for the affix combinations that were likely to occur at all often in real texts.
</nextsent>
<nextsent>a backup mechanism could then be provided which attempted slower, but more complete, direct ap-plication of the rules for the rarer cases.
</nextsent>
<nextsent>the interaction of morphological nalysis with spelling correction (carter, 1992; <papid> P92-1021 </papid>oflazer, 1994; bowden, 1995) <papid> E95-1044 </papid>is another possibly fruitful area of work.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2383">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> conc lus ions  and further  work.  </section>
<citcontext>
<prevsection>
<prevsent>agglutinative languages could be handled ef- 208 flciently by the current mechanism if specifica-tions were provided for the affix combinations that were likely to occur at all often in real texts.
</prevsent>
<prevsent>a backup mechanism could then be provided which attempted slower, but more complete, direct ap-plication of the rules for the rarer cases.
</prevsent>
</prevsection>
<citsent citstr=" P92-1021 ">
the interaction of morphological nalysis with spelling correction (carter, 1992; <papid> P92-1021 </papid>oflazer, 1994; bowden, 1995) <papid> E95-1044 </papid>is another possibly fruitful area of work.</citsent>
<aftsection>
<nextsent>once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affix- stripping, which would be amenable to exactly the technique outlined by carter (1992).<papid> P92-1021 </papid></nextsent>
<nextsent>as in that work, discrimination et of root forms would be required; however, this could be augmented inde-pendently of spelling pattern creation, so that the flexibility resulting from not composing the lexi-con with the spelling rules would not be lost.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2384">
<title id=" E95-1028.xml">rapid development of morphological descriptions for full language processing systems </title>
<section> conc lus ions  and further  work.  </section>
<citcontext>
<prevsection>
<prevsent>agglutinative languages could be handled ef- 208 flciently by the current mechanism if specifica-tions were provided for the affix combinations that were likely to occur at all often in real texts.
</prevsent>
<prevsent>a backup mechanism could then be provided which attempted slower, but more complete, direct ap-plication of the rules for the rarer cases.
</prevsent>
</prevsection>
<citsent citstr=" E95-1044 ">
the interaction of morphological nalysis with spelling correction (carter, 1992; <papid> P92-1021 </papid>oflazer, 1994; bowden, 1995) <papid> E95-1044 </papid>is another possibly fruitful area of work.</citsent>
<aftsection>
<nextsent>once the root spelling patterns and the affix combinations pointing to them have been created, analysis essentially reduces to an instance of affix- stripping, which would be amenable to exactly the technique outlined by carter (1992).<papid> P92-1021 </papid></nextsent>
<nextsent>as in that work, discrimination et of root forms would be required; however, this could be augmented inde-pendently of spelling pattern creation, so that the flexibility resulting from not composing the lexi-con with the spelling rules would not be lost.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2386">
<title id=" E95-1026.xml">a robust and efficient three layered dialogue component for a speechtospeech translation system </title>
<section> layered dialogue processing.  </section>
<citcontext>
<prevsection>
<prevsent>4.1 knowledge-based layers.
</prevsent>
<prevsent>4.1.1 the underlying knowledge source - the dialogue model like previous approaches for modeling task- oriented dialogues we base our ideas on the as-sumption that dialogue can be described by means of limited but open set of speech acts (e.g.
</prevsent>
</prevsection>
<citsent citstr=" E91-1015 ">
(bilange, 1991), (<papid> E91-1015 </papid>mast, 1993)).</citsent>
<aftsection>
<nextsent>as point of departure we take speech acts as proposed by (austin, 1962) and (searle, 1969) and also num-ber of so-called illocutionary acts as employed in model of information-seeking dialogues (sitter and stein, 1992).
</nextsent>
<nextsent>we examined the verb mobil cor-pus of appointment, scheduling dialogues for their occurrence and for the necessity to introduce new speech acts 1 . at present,, our model contains 17 speech acts (see (maier, 1994) for more details on the char-acterization of the various speech acts; the di-alogue model describing admissible sequences of the acts we introduce below are mostly of illocu- tionary nature.
</nextsent>
<nextsent>nevertheless we will refer to them as speech acts throughout this paper.
</nextsent>
<nextsent>189 speech acts is given in fig.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2387">
<title id=" E95-1026.xml">a robust and efficient three layered dialogue component for a speechtospeech translation system </title>
<section> layered dialogue processing.  </section>
<citcontext>
<prevsection>
<prevsent>an important ask of this layer is to signal to the planner when an inconsistency has occurred, i.e. when speech act is not within the standard model so that it can activate repair techniques.
</prevsent>
<prevsent>4.1.3 the dialogue lanner to incorporate constraints in dialogue process-ing and to allow decisions to trigger follow-up actions plan-based approach has been chosen.
</prevsent>
</prevsection>
<citsent citstr=" P88-1020 ">
this approach is adopted from text generation where plan-operators are responsible for choos-ing linguistic means in order to create coherent stretches of text (see, for instance, (moore and paris, 1989) and (hovy, 1988)).<papid> P88-1020 </papid></citsent>
<aftsection>
<nextsent>the application of plan operators depends on the validity of con-straints.
</nextsent>
<nextsent>planning proceeds in top-down fash-ion, i.e. high-level goals are decomposed into sub- goals, each of which has to be achieved individ-ually in order to be fulfilled.
</nextsent>
<nextsent>our top-level goal schedule-meeting (see below) is decomposed into three sub goals each of which is responsible for the treatment of one dialogue segment: the in- 190 troductory phase (greet-introduce-topic), the negotiation phase (negotiate) and the closing phase (finish).
</nextsent>
<nextsent>these goals have to be fulfilled in the specified order.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2389">
<title id=" E93-1032.xml">towards efficient parsing with proof nets </title>
<section> in roduct ion.  </section>
<citcontext>
<prevsection>
<prevsent>in many aspects, the method resembles the well known method of chart-parsing.
</prevsent>
<prevsent>ktnig \[1991, 1992\] was the first to apply chart-parsing tolambek calculus.
</prevsent>
</prevsection>
<citsent citstr=" C92-1024 ">
hepple \[1992\] <papid> C92-1024 </papid>has recently improved this application.</citsent>
<aftsection>
<nextsent>an obvious difference with the method proposed here lies in the fact that, in ours, words are points and intervals between them are edges instead of the contrary in chart-parsing.
</nextsent>
<nextsent>in both cases, computational advantages are found by keeping correct partial analyses after they have been obtained.
</nextsent>
<nextsent>a chart is actually used in both methods.
</nextsent>
<nextsent>2.1 links and nodes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2390">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>we formalize weighted dependency parsing as searching for maximum spanning trees (msts) in directed graphs.
</prevsent>
</prevsection>
<citsent citstr=" C96-1058 ">
using this representation, the parsing algorithm of eisner (1996) <papid> C96-1058 </papid>is sufficient for searching over all projective trees in o(n3) time.</citsent>
<aftsection>
<nextsent>more surprisingly, the representation is extended naturally to non-projective parsing using chu-liu-edmonds (chu andliu, 1965; edmonds, 1967) mst algorithm, yielding an o(n2) parsing algorithm.
</nextsent>
<nextsent>we evaluate these met hodson the prague dependency treebank using online large-margin learning techniques (crammer et al, 2003; mcdonald et al, 2005) <papid> P05-1012 </papid>and show that mst parsing increases efficiency and accuracy for languages with non-projective dependencies.</nextsent>
<nextsent>dependency parsing has seen surge of interest lately for applications such as relation extraction (culotta and sorensen, 2004), <papid> P04-1054 </papid>machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), and lexical resource augmentation (snow et al, 2004).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2391">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> abstract </section>
<citcontext>
<prevsection>
<prevsent>using this representation, the parsing algorithm of eisner (1996) <papid> C96-1058 </papid>is sufficient for searching over all projective trees in o(n3) time.</prevsent>
<prevsent>more surprisingly, the representation is extended naturally to non-projective parsing using chu-liu-edmonds (chu andliu, 1965; edmonds, 1967) mst algorithm, yielding an o(n2) parsing al gorithm.</prevsent>
</prevsection>
<citsent citstr=" P05-1012 ">
we evaluate these met hodson the prague dependency treebank using online large-margin learning techniques (crammer et al, 2003; mcdonald et al, 2005) <papid> P05-1012 </papid>and show that mst parsing increases efficiency and accuracy for languages with non-projective dependencies.</citsent>
<aftsection>
<nextsent>dependency parsing has seen surge of interest lately for applications such as relation extraction (culotta and sorensen, 2004), <papid> P04-1054 </papid>machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), and lexical resource augmentation (snow et al, 2004).</nextsent>
<nextsent>the primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2392">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more surprisingly, the representation is extended naturally to non-projective parsing using chu-liu-edmonds (chu andliu, 1965; edmonds, 1967) mst algorithm, yielding an o(n2) parsing algorithm.
</prevsent>
<prevsent>we evaluate these met hodson the prague dependency treebank using online large-margin learning techniques (crammer et al, 2003; mcdonald et al, 2005) <papid> P05-1012 </papid>and show that mst parsing increases efficiency and accuracy for languages with non-projective dependencies.</prevsent>
</prevsection>
<citsent citstr=" P04-1054 ">
dependency parsing has seen surge of interest lately for applications such as relation extraction (culotta and sorensen, 2004), <papid> P04-1054 </papid>machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), and lexical resource augmentation (snow et al, 2004).</citsent>
<aftsection>
<nextsent>the primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications.
</nextsent>
<nextsent>root john hit the ball with the bat figure 1: an example dependency tree.
</nextsent>
<nextsent>dependency representations, which link words to their arguments, have long history (hudson, 1984).
</nextsent>
<nextsent>figure 1 shows dependency tree for the sentence john hit the ball with the bat.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2393">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>more surprisingly, the representation is extended naturally to non-projective parsing using chu-liu-edmonds (chu andliu, 1965; edmonds, 1967) mst algorithm, yielding an o(n2) parsing algorithm.
</prevsent>
<prevsent>we evaluate these met hodson the prague dependency treebank using online large-margin learning techniques (crammer et al, 2003; mcdonald et al, 2005) <papid> P05-1012 </papid>and show that mst parsing increases efficiency and accuracy for languages with non-projective dependencies.</prevsent>
</prevsection>
<citsent citstr=" P05-1067 ">
dependency parsing has seen surge of interest lately for applications such as relation extraction (culotta and sorensen, 2004), <papid> P04-1054 </papid>machine translation (ding and palmer, 2005), <papid> P05-1067 </papid>synonym generation (shinyama et al, 2002), and lexical resource augmentation (snow et al, 2004).</citsent>
<aftsection>
<nextsent>the primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications.
</nextsent>
<nextsent>root john hit the ball with the bat figure 1: an example dependency tree.
</nextsent>
<nextsent>dependency representations, which link words to their arguments, have long history (hudson, 1984).
</nextsent>
<nextsent>figure 1 shows dependency tree for the sentence john hit the ball with the bat.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2394">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or dummy root symbol as shown in the figure.
</prevsent>
<prevsent>the tree in figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, word and its descendants form contiguous substring of the sentence.in english, projective trees are sufficient to analyze most sentence types.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
in fact, the largest source of english dependency trees is automatically generated from the penn treebank (marcus et al, 1993)<papid> J93-2004 </papid>and is by convention exclusively projective.</citsent>
<aftsection>
<nextsent>however, there are certain examples in which non projective tree is preferable.
</nextsent>
<nextsent>consider the sentence john saw dog yesterday which was yorkshire terrier.
</nextsent>
<nextsent>here the relative clause which was yorkshire terrier and the object it modifies (the dog) are separated by an adverb.
</nextsent>
<nextsent>there is no way to draw the dependency tree for this sentence in the plane withno crossing edges, as illustrated in figure 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2396">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a non-projective example from the czech prague dependency treebank (hajic?
</prevsent>
<prevsent>et al, 2001) is also shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" P99-1065 ">
most previous dependency parsing models have focused on projective trees, including the work of eisner (1996), <papid> C96-1058 </papid>collins et al (1999), <papid> P99-1065 </papid>yamada and matsumoto (2003), nivre and scholz (2004), <papid> C04-1010 </papid>and mcdonald et al (2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>these systems have shown that accurate projective dependency parsers can be automatically learned from parsed data.
</nextsent>
<nextsent>however, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for english.
</nextsent>
<nextsent>in particular, wang and harper (2004) <papid> W04-0307 </papid>describe broad coverage non-projectiveparser for english based on hand-constructed constraint dependency grammar rich in lexical and syntactic information.</nextsent>
<nextsent>nivre and nilsson (2005) <papid> P05-1013 </papid>presented parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2397">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>a non-projective example from the czech prague dependency treebank (hajic?
</prevsent>
<prevsent>et al, 2001) is also shown in figure 2.
</prevsent>
</prevsection>
<citsent citstr=" C04-1010 ">
most previous dependency parsing models have focused on projective trees, including the work of eisner (1996), <papid> C96-1058 </papid>collins et al (1999), <papid> P99-1065 </papid>yamada and matsumoto (2003), nivre and scholz (2004), <papid> C04-1010 </papid>and mcdonald et al (2005).<papid> P05-1012 </papid></citsent>
<aftsection>
<nextsent>these systems have shown that accurate projective dependency parsers can be automatically learned from parsed data.
</nextsent>
<nextsent>however, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for english.
</nextsent>
<nextsent>in particular, wang and harper (2004) <papid> W04-0307 </papid>describe broad coverage non-projectiveparser for english based on hand-constructed constraint dependency grammar rich in lexical and syntactic information.</nextsent>
<nextsent>nivre and nilsson (2005) <papid> P05-1013 </papid>presented parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2401">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these systems have shown that accurate projective dependency parsers can be automatically learned from parsed data.
</prevsent>
<prevsent>however, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for english.
</prevsent>
</prevsection>
<citsent citstr=" W04-0307 ">
in particular, wang and harper (2004) <papid> W04-0307 </papid>describe broad coverage non-projectiveparser for english based on hand-constructed constraint dependency grammar rich in lexical and syntactic information.</citsent>
<aftsection>
<nextsent>nivre and nilsson (2005) <papid> P05-1013 </papid>presented parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser.</nextsent>
<nextsent>they test this system onczech and show improved accuracy relative to projective parser.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2402">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>however, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for english.
</prevsent>
<prevsent>in particular, wang and harper (2004) <papid> W04-0307 </papid>describe broad coverage non-projectiveparser for english based on hand-constructed constraint dependency grammar rich in lexical and syntactic information.</prevsent>
</prevsection>
<citsent citstr=" P05-1013 ">
nivre and nilsson (2005) <papid> P05-1013 </papid>presented parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser.</citsent>
<aftsection>
<nextsent>they test this system onczech and show improved accuracy relative to projective parser.
</nextsent>
<nextsent>our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees.
</nextsent>
<nextsent>the main idea of our method is that dependency parsing can be formalized as the search for maximum spanning tree in directed graph.
</nextsent>
<nextsent>this formalization generalizes standard projective parsing models based on the eisner algorithm (eisner, 1996) <papid> C96-1058 </papid>toyield efficient o(n2) exact parsing methods for non projective languages like czech.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2414">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> online large margin learning.  </section>
<citcontext>
<prevsection>
<prevsent>w = v/(n ? ) figure 4: mira learning algorithm.
</prevsent>
<prevsent>tors after each iteration.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
this averaging effect has been shown to help over fitting (collins, 2002).<papid> W02-1001 </papid></citsent>
<aftsection>
<nextsent>on each update, mira attempts to keep the new weight vector as close as possible to the old weight vector, subject to correctly classifying the instance under consideration with margin given by the loss of the incorrect classifications.
</nextsent>
<nextsent>for dependency trees, the loss of tree is defined to be the number of words with incorrect parents relative to the correct tree.
</nextsent>
<nextsent>this is closely related to the hamming loss that is often used for sequences (taskar et al, 2003).for arbitrary inputs, there are typically exponentially many possible parses and thus exponentially many margin constraints in line 4 of figure 4.
</nextsent>
<nextsent>3.1 single-best mira.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2419">
<title id=" H05-1066.xml">non projective dependency parsing using spanning tree algorithms </title>
<section> online large margin learning.  </section>
<citcontext>
<prevsection>
<prevsent>however, mira aggressively updates to maximize the margin between 527 the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy.
</prevsent>
<prevsent>3.2 factored mira.
</prevsent>
</prevsection>
<citsent citstr=" W04-3201 ">
it is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into polynomial number of local constraints (taskar et al, 2003; taskar et al, 2004).<papid> W04-3201 </papid></citsent>
<aftsection>
<nextsent>for the directed maximum spanning tree problem,we can factor the output by edges to obtain the following constraints: min ? w(i+1) ? w(i) ? ?
</nextsent>
<nextsent>s.t. s(l, j) ? s(k, j) ? 1 ?(l, j) ? yt, (k, j) /?
</nextsent>
<nextsent>yt this states that the weight of the correct incoming edge to the word xj and the weight of all other in coming edges must be separated by margin of 1.
</nextsent>
<nextsent>it is easy to show that when all these constraints are satisfied, the correct spanning tree and all incorrect spanning trees are separated by score at least as large as the number of incorrect incoming edges.this is because the scores for all the correct arcs cancel out, leaving only the scores for the errors causing the difference in overall score.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2437">
<title id=" E99-1013.xml">complementing wordnet with rogets and corpus based thesauri for information retrieval </title>
<section> thesaur i.  </section>
<citcontext>
<prevsection>
<prevsent>in this case, our similarity measure treat all the words in roger as features.
</prevsent>
<prevsent>a word possesses the feature if and belong to the same ro- get category.
</prevsent>
</prevsection>
<citsent citstr=" P98-2127 ">
the similarity between two words is then defined as the dice coefficient of the two feature vectors (lin, 1998).<papid> P98-2127 </papid></citsent>
<aftsection>
<nextsent>sim(wl,w2) = 21r(wl) r(w~)l tn(w,)l + in(w )l where r(w) is the set of words that belong to the same roget category as w. 2.3 corpus-based thesaurus.
</nextsent>
<nextsent>2.3.1 co-occurrence-based thesaurus this method is based on the assumption that pair of words that frequently occur together in the same document are related to the same subject.
</nextsent>
<nextsent>therefore word co-occurrence information can be used to identify semantic relationships between words (schutze and pederson, 1997; schutze and pederson, 1994).
</nextsent>
<nextsent>we use mutual information as tool for computing similarity between words.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2438">
<title id=" E99-1013.xml">complementing wordnet with rogets and corpus based thesauri for information retrieval </title>
<section> thesaur i.  </section>
<citcontext>
<prevsection>
<prevsent>therefore word co-occurrence information can be used to identify semantic relationships between words (schutze and pederson, 1997; schutze and pederson, 1994).
</prevsent>
<prevsent>we use mutual information as tool for computing similarity between words.
</prevsent>
</prevsection>
<citsent citstr=" J90-1003 ">
mu-tual information compares the probability of the co-occurence of words and with the indepen-dent probabilities of occurrence of and (church and hanks, 1990).<papid> J90-1003 </papid></citsent>
<aftsection>
<nextsent>p(a, b) i(a, b) = log p(a)p(b) where the probabilities of p(a) and p(b) are esti-mated by counting the number of occurrences of and in documents and normalizing over the size of vocabulary in the documents.
</nextsent>
<nextsent>the joint probability is estimated by counting the number of times that word co-occurs with and is also normalized over the size of the vocabulary.
</nextsent>
<nextsent>2.3.2 syntact ica ly -based thesaurus in contrast to the previous ection, this method attempts to gather term relations on the ba-sis of linguistic relations and not document co-occurrence statistics.
</nextsent>
<nextsent>words appearing in simi- lax grammatical contexts are assumed to be sim-ilar, and therefore classified into the same class (lin, 1998; <papid> P98-2127 </papid>grefenstette, 1994; grefenstette, 1992; ruge, 1992; hindle, 1990).<papid> P90-1034 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2440">
<title id=" E99-1013.xml">complementing wordnet with rogets and corpus based thesauri for information retrieval </title>
<section> thesaur i.  </section>
<citcontext>
<prevsection>
<prevsent>the joint probability is estimated by counting the number of times that word co-occurs with and is also normalized over the size of the vocabulary.
</prevsent>
<prevsent>2.3.2 syntact ica ly -based thesaurus in contrast to the previous ection, this method attempts to gather term relations on the ba-sis of linguistic relations and not document co-occurrence statistics.
</prevsent>
</prevsection>
<citsent citstr=" P90-1034 ">
words appearing in simi- lax grammatical contexts are assumed to be sim-ilar, and therefore classified into the same class (lin, 1998; <papid> P98-2127 </papid>grefenstette, 1994; grefenstette, 1992; ruge, 1992; hindle, 1990).<papid> P90-1034 </papid></citsent>
<aftsection>
<nextsent>first, all the documents are parsed using the apple pie parser.
</nextsent>
<nextsent>the apple pie parser is natural anguage syntactic analyzer developed by satoshi sekine at new york university (sekine and grishman, 1995).
</nextsent>
<nextsent>the parser is bottom-up probabilistic hart parser which finds the parse tree with the best score by way of the best-first search algorithm.
</nextsent>
<nextsent>its grammar is semi-context sensitive grammar with two non-terminals and was automatically extracted from penn tree bank syntactically tagged corpus developed at the uni-versity of pennsylvania.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2442">
<title id=" H05-1062.xml">robust named entity extraction from large spoken archives </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>most of these conferences have studied the impact of using transcripts generated by an automatic speech recognition (asr) system rather than written texts.
</prevsent>
<prevsent>it appears from these studies that unlike other ie tasks, ner performance is greatly affected by the word error rate (wer) of the transcripts processed.
</prevsent>
</prevsection>
<citsent citstr=" H01-1034 ">
to tackle this problem, different ideas have been pro posed: modeling explicitly the asr errors (palmer and ostendorf, 2001) <papid> H01-1034 </papid>or using the asr system alternate hypotheses found in word lattices (sar aclar and sproat, 2004).</citsent>
<aftsection>
<nextsent>however performance inner decreases dramatically when processing high wer transcripts like the ones that are obtained with unmatched conditions between the asr training model and the data to process.
</nextsent>
<nextsent>this paper investigates this phenomenon in the framework of the ner task of the french rich transcription program of broadcast news ester (gravier et al, 2004).
</nextsent>
<nextsent>several issues are addressed: ? how to jointly optimize the asr and the ner models ? ?
</nextsent>
<nextsent>what is the impact in term of asr and ner performance of temporal mismatch between the corpora used to train and test the model sand how can it be recovered by means of meta data information ? ?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2443">
<title id=" H05-1062.xml">robust named entity extraction from large spoken archives </title>
<section> information extraction from large.  </section>
<citcontext>
<prevsection>
<prevsent>therefore, even if some of these words are missing, numerical information extraction methods can use the remaining salient words and discard the noise generated by asr errors.
</prevsent>
<prevsent>however, this phenomenon is not true for tasks related to the extraction of fine grained entities, like named entities.
</prevsent>
</prevsection>
<citsent citstr=" A00-1044 ">
indeed, several studies have shown that f-measure and wer are strongly correlated : 0.7 points of f-measure lost for each additional 1%of wer according to (miller et al, 2000) <papid> A00-1044 </papid>on the experiments of 1998 nist hub-4 evaluations (przy bocki et al, 1999).</citsent>
<aftsection>
<nextsent>despite the continuous improvement of asr techniques, high wer transcriptions are inevitable in difficult conditions like those found in large spoken archives like in the malach project (ramabhadran et al, 2003).
</nextsent>
<nextsent>moreover, named entities extraction performance is greatly affected by mismatch between training and testing data.
</nextsent>
<nextsent>this is due mainly because proper names, which represent most of the named entity items, are very dynamic category of words, strongly related to the period oftime representing the documents to process.
</nextsent>
<nextsent>therefore this mismatch is inevitable when dealing with archives spreading over long period of time and containing multiple domain information.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2445">
<title id=" H05-1062.xml">robust named entity extraction from large spoken archives </title>
<section> extracting ne from written text vs. asr.  </section>
<citcontext>
<prevsection>
<prevsent>these speech dysfluencies (hesitations, filled pauses, false starts...)
</prevsent>
<prevsent>reduce the quality of the transcript because they are usually not covered by language models (built from textual data) or artificially introduced.one should remove these from the transcript to im prove the quality of the labeling.
</prevsent>
</prevsection>
<citsent citstr=" N04-4010 ">
in order to deal with asr errors two approaches have been proposed: ? modeling explicitly the asr errors, thanks to development corpus and set of confidence measures, in order to detect the possible errors of the 1-best word string hypothesis (with the type of errors) before extracting the nes (palmer and ostendorf, 2001); ? <papid> H01-1034 </papid>exploiting search space bigger than the 1-best hypothesis alone, either by taking into account an n-best list (zhai et al, 2004) <papid> N04-4010 </papid>or the whole word lattice (saraclar and sproat, 2004).</citsent>
<aftsection>
<nextsent>493 the method proposed in this paper is close to this second approach where the whole word lattice out put by the asr system is used in order to increase ner performance from noisy input.
</nextsent>
<nextsent>we will present also in the next section new strategy for adapting ner models to asr transcripts, based on one of the main characteristics of such transcripts: closed vocabulary is used by the asr system.
</nextsent>
<nextsent>to our knowledge this has never been fully exploited byner systems.
</nextsent>
<nextsent>indeed while the key point of ner systems on written text istheir generalization capabilities when processing unknown words, this feature is not relevant for asr transcripts as the system cannot generate words outof the lexicon (there are no unknown words).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2446">
<title id=" H05-1062.xml">robust named entity extraction from large spoken archives </title>
<section> robust named entity extraction.  </section>
<citcontext>
<prevsection>
<prevsent>these two systems are going to be presented in the next sections.
</prevsent>
<prevsent>5.1 text-based ner system: nertext.
</prevsent>
</prevsection>
<citsent citstr=" P02-1022 ">
among all the different methods that have been proposed for ner, one can find rule based models(cunningham et al, 2002), <papid> P02-1022 </papid>maximum entropy models (brothwick et al, 1998), condition nal random fields or probabilistic hmm-based models (bikel et al., 1999).</citsent>
<aftsection>
<nextsent>ling pipe implements an hmm-based model.
</nextsent>
<nextsent>it maximizes the probability of tag sequence ti over 1lingpipe: http://alias-i.com/lingpipe/ word sequence wi.
</nextsent>
<nextsent>a context of two preceding words and one preceding tag is used to approximate this probability.
</nextsent>
<nextsent>generalization is done through simple process: words occurring with low frequency are replaced by feature based categories (capitalized, contains digits, . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2447">
<title id=" H05-1062.xml">robust named entity extraction from large spoken archives </title>
<section> robust named entity extraction.  </section>
<citcontext>
<prevsection>
<prevsent>in order to reduce and control the insertion rate of our ner system, we implemented two level approach: the first level ismade of ne grammars coded as finite state machine (fsm) transducers and the second level is statistical hmm-based tagger.
</prevsent>
<prevsent>5.2.1 ne transducers to each ne category is attached set of regular grammars, extracted from the ester training corpus and generalized thanks to the annotation guidelines and web-gathered word lists.
</prevsent>
</prevsection>
<citsent citstr=" P03-1006 ">
theses grammars are represented by finite state machines (fsms) (thanks to the at&t; grm/fsm toolkit (allauzen et al., 2003)).<papid> P03-1006 </papid></citsent>
<aftsection>
<nextsent>these fsms are transducers that accept word sequences on the input symbols and output ne labels on the output symbols.
</nextsent>
<nextsent>they are all grouped together in single transducer, called tgram, witha filler model that accepts any string of words.
</nextsent>
<nextsent>be cause these fsms are lexicalized with the words of the asr lexicon, one can control the generalization capabilities of the grammars thanks to the occurrence contexts of these words in the training corpus.
</nextsent>
<nextsent>during the ner process, the first step is to compose the fsm representing the ne transducer and the out put of the asr module (either 1-best word string 494 or word lattice, both encoded as an fsm called g).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2449">
<title id=" E93-1027.xml">linguistic knowledge acquisition from parsing failures </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the procedure will be used for adapting or re-using existing linguistic resources for new application domains.
</prevsent>
<prevsent>while quite number of useful grammar formalisms for natural language processing now exist, it still re-mains time-consuming and hard task to develop grammars and dictionaries with comprehensive cov-erage.
</prevsent>
</prevsection>
<citsent citstr=" C92-2085 ">
it is also the case that, though quite few computational grammars and dictionaries with com-prehensive coverage have been used in various ap-plication systems, to re-use them for other applica-tion domains is not always so easy, even if we use the same formalisms and programs uch as parsers, etc. we usually have to revise, add, and delete grammar ules and lexical entries in order to adapt them to the peculiarities of languages ( ublanguages) of new application domains \[sekine et al, 1992; <papid> C92-2085 </papid>tsujii et al, 1992; ananiadou, 1990\].</citsent>
<aftsection>
<nextsent>*also staff member of matsushita electric industrial co.,ltd., tokyo, japan.
</nextsent>
<nextsent>such adaptations of existing linguistic knowledge to new domain are currently performed through rather undisciplined, trial and error processes in-volving much human effort.
</nextsent>
<nextsent>in this paper we show that techniques imilar to those in robust parsing of ill-formed input, together with corpus-based tech-niques, can be used to discover disparities between existing linguistic knowledge and actual language us- age in new domain, and to hypothesize new gram-mar rules or lexical descriptions.
</nextsent>
<nextsent>although our framework appears similar to gram-mar learning from corpora, our current goal is far more modest, i.e. to help linguists revise existing grammars by showing possible defects and hypothe- sizing them through corpus analysis.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2450">
<title id=" E93-1027.xml">linguistic knowledge acquisition from parsing failures </title>
<section> robust parsing and linguistic.  </section>
<citcontext>
<prevsection>
<prevsent>i have bok), etc. as there is usually set of possible hypotheses to complete the analysis, this error detection process becomes non-deterministic.
</prevsent>
<prevsent>furthermore, allowing operations such as deletion and insertion of arbi-trary sequences of words or unrestricted permuta-tion of word sequences, radically expands its search space.
</prevsent>
</prevsection>
<citsent citstr=" P89-1013 ">
the process generates many nonsensical hy-potheses unless we restrict the search space either by heuristies-based cost functions \[mellish, 1989\], <papid> P89-1013 </papid>or 222 type of failures remaining constituents to be collected failure of application of an existing rule unrecognized sequence of characters robust parsing hypotheses of - deletion of necessary words - insertion of unnecessary words - disorder of words relaxation of - feature agreements hypotheses of - spelling errors knowledge acquisition hypotheses of - lack of necessary rules identification of - disagreeing features hypotheses of - e words table 1: types of hypotheses by introducing prior knowledge about regularities of errors in the form of annotated rules \[goeser, 1992\].<papid> C92-1022 </papid></citsent>
<aftsection>
<nextsent>on the other hand, our framework of knowledge acquisition from parsing failures does not assume that the input contains errors, but instead, assumes that linguistic knowledge of the system is incomplete.
</nextsent>
<nextsent>this means that we do not need to, or should not, allow the costly operations of changing input, and therefore the search space explosion encountered by robust parser does not occur.
</nextsent>
<nextsent>for example, when string of characters which is not registered in the dictionary as word appears, robust parser may assume that there are spelling errors and try to identify the errors by changing the character string (deleting characters, adding new characters, etc.) to find the  closest  legitimate word in the dictionary.
</nextsent>
<nextsent>this is because the dictionary is assumed to be complete, e.g. that it contains all lex-ical items that will appear.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2451">
<title id=" E93-1027.xml">linguistic knowledge acquisition from parsing failures </title>
<section> robust parsing and linguistic.  </section>
<citcontext>
<prevsection>
<prevsent>i have bok), etc. as there is usually set of possible hypotheses to complete the analysis, this error detection process becomes non-deterministic.
</prevsent>
<prevsent>furthermore, allowing operations such as deletion and insertion of arbi-trary sequences of words or unrestricted permuta-tion of word sequences, radically expands its search space.
</prevsent>
</prevsection>
<citsent citstr=" C92-1022 ">
the process generates many nonsensical hy-potheses unless we restrict the search space either by heuristies-based cost functions \[mellish, 1989\], <papid> P89-1013 </papid>or 222 type of failures remaining constituents to be collected failure of application of an existing rule unrecognized sequence of characters robust parsing hypotheses of - deletion of necessary words - insertion of unnecessary words - disorder of words relaxation of - feature agreements hypotheses of - spelling errors knowledge acquisition hypotheses of - lack of necessary rules identification of - disagreeing features hypotheses of - e words table 1: types of hypotheses by introducing prior knowledge about regularities of errors in the form of annotated rules \[goeser, 1992\].<papid> C92-1022 </papid></citsent>
<aftsection>
<nextsent>on the other hand, our framework of knowledge acquisition from parsing failures does not assume that the input contains errors, but instead, assumes that linguistic knowledge of the system is incomplete.
</nextsent>
<nextsent>this means that we do not need to, or should not, allow the costly operations of changing input, and therefore the search space explosion encountered by robust parser does not occur.
</nextsent>
<nextsent>for example, when string of characters which is not registered in the dictionary as word appears, robust parser may assume that there are spelling errors and try to identify the errors by changing the character string (deleting characters, adding new characters, etc.) to find the  closest  legitimate word in the dictionary.
</nextsent>
<nextsent>this is because the dictionary is assumed to be complete, e.g. that it contains all lex-ical items that will appear.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2453">
<title id=" H05-1070.xml">using mona for querying linguistic treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in recent years large amounts of electronic texts have become available providing new base for empirical studies in linguistics and offering chance to linguists to compare their theories with large amounts of utterances from the real world?.
</prevsent>
<prevsent>while tagging with morphosyntactic categories has become standard for almost all corpora, more and more of themare nowadays annotated with refined syntactic information.
</prevsent>
</prevsection>
<citsent citstr=" J93-2004 ">
examples are the penn treebank (mar cus et al, 1993) <papid> J93-2004 </papid>for american english annotated at the university of pennsylvania, the french treebank (abeille?</citsent>
<aftsection>
<nextsent>and clement, 1999) developed in paris, the tiger corpus (brants et al, 2002) for german annotated at the universities of saarbrucken and?
</nextsent>
<nextsent>this research was funded by german science foundation grant (dfg sfb441-6).
</nextsent>
<nextsent>stuttgart, and the tubingen treebanks (hinrichs et al., 2000) for japanese, german and english fromthe university of tubingen.
</nextsent>
<nextsent>to make these rich syntactic annotations accessible for linguists, the development of powerful query tools is an obvious need and has become an important task in computational linguistics.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2454">
<title id=" H05-1070.xml">using mona for querying linguistic treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to make these rich syntactic annotations accessible for linguists, the development of powerful query tools is an obvious need and has become an important task in computational linguistics.
</prevsent>
<prevsent>consequently, number of treebank query tools have been developed.
</prevsent>
</prevsection>
<citsent citstr=" E03-1074 ">
probably amongst the most important ones are corpus search (randall, 2000), icecup iii (wallis and nelson, 2000), fsq (kepser, 2003), <papid> E03-1074 </papid>tgrep2 (rohde, 2001), and tiger search (konig and lezius, 2000).<papid> C00-2157 </papid></citsent>
<aftsection>
<nextsent>a common feature of these tools is the relatively low expressive powerof their query languages.
</nextsent>
<nextsent>explicit or implicit references to nodes in tree are mostly interpreted existentially.
</nextsent>
<nextsent>the notable exception is fsq, which employs full first order logicas its query language.
</nextsent>
<nextsent>the importance of the expressive power of the query language is consequence of the sizes of the available treebanks, which can contain several ten thousand trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2455">
<title id=" H05-1070.xml">using mona for querying linguistic treebanks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to make these rich syntactic annotations accessible for linguists, the development of powerful query tools is an obvious need and has become an important task in computational linguistics.
</prevsent>
<prevsent>consequently, number of treebank query tools have been developed.
</prevsent>
</prevsection>
<citsent citstr=" C00-2157 ">
probably amongst the most important ones are corpus search (randall, 2000), icecup iii (wallis and nelson, 2000), fsq (kepser, 2003), <papid> E03-1074 </papid>tgrep2 (rohde, 2001), and tiger search (konig and lezius, 2000).<papid> C00-2157 </papid></citsent>
<aftsection>
<nextsent>a common feature of these tools is the relatively low expressive powerof their query languages.
</nextsent>
<nextsent>explicit or implicit references to nodes in tree are mostly interpreted existentially.
</nextsent>
<nextsent>the notable exception is fsq, which employs full first order logicas its query language.
</nextsent>
<nextsent>the importance of the expressive power of the query language is consequence of the sizes of the available treebanks, which can contain several ten thousand trees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2456">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic roles have become focus of research in computational linguistics during the recent years.
</prevsent>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
</prevsection>
<citsent citstr=" P03-1002 ">
the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</citsent>
<aftsection>
<nextsent>typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</nextsent>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2457">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>semantic roles have become focus of research in computational linguistics during the recent years.
</prevsent>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
</prevsection>
<citsent citstr=" C04-1100 ">
the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</citsent>
<aftsection>
<nextsent>typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</nextsent>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2458">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
<prevsent>the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</prevsent>
</prevsection>
<citsent citstr=" J02-3001 ">
typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</citsent>
<aftsection>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.
</nextsent>
<nextsent>we approach feature engineering not by directly optimizing system performance.
</nextsent>
<nextsent>instead, we proceed by error analysis, like pado and boleda (2004).
</nextsent>
<nextsent>our aim is to form global hypothesis that explains the distribution of errors across classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2459">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
<prevsent>the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</prevsent>
</prevsection>
<citsent citstr=" P04-1043 ">
typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</citsent>
<aftsection>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.
</nextsent>
<nextsent>we approach feature engineering not by directly optimizing system performance.
</nextsent>
<nextsent>instead, we proceed by error analysis, like pado and boleda (2004).
</nextsent>
<nextsent>our aim is to form global hypothesis that explains the distribution of errors across classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2461">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
<prevsent>the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</prevsent>
</prevsection>
<citsent citstr=" N04-1030 ">
typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</citsent>
<aftsection>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.
</nextsent>
<nextsent>we approach feature engineering not by directly optimizing system performance.
</nextsent>
<nextsent>instead, we proceed by error analysis, like pado and boleda (2004).
</nextsent>
<nextsent>our aim is to form global hypothesis that explains the distribution of errors across classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2462">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the driving force behind this interest is the prospect that semantic roles, as shallow meaning representation, can improve many nlp applications, while still being amenable to automatic analysis.
</prevsent>
<prevsent>the benefit of semantic roles has already been demonstrated for number of tasks, among others for machine translation (boas, 2002), information extraction (surdeanu et al , 2003), <papid> P03-1002 </papid>and question answering (narayanan and harabagiu, 2004).<papid> C04-1100 </papid>robust and accurate automatic semantic role assignment, prerequisite for the wide-range use of semantic roles in nlp, has been investigated in number of studies and shared tasks.</prevsent>
</prevsection>
<citsent citstr=" W04-0803 ">
typically, role assignment has been modeled as classification task, with models being estimated from large corpora (gildea and jurafsky, 2002; <papid> J02-3001 </papid>moschitti, 2004;<papid> P04-1043 </papid>xue and palmer, 2004; surdeanu et al , 2003; <papid> P03-1002 </papid>pradhan et al , 2004; <papid> N04-1030 </papid>litkowski, 2004; <papid> W04-0803 </papid>carreras and mrquez, 2005).within this framework, there is number of architectural parameters which lend themselves to optimization: the machine learning framework, the feature set, pre- and postprocessing, each of which hasbeen investigated in the context of semantic role as signment.</citsent>
<aftsection>
<nextsent>the current paper concentrates on feature engineering, since the feature set is pivotal component of any kind of machine learning system, and allows us to incorporate and test linguistic intuitions on the role assignment task.
</nextsent>
<nextsent>we approach feature engineering not by directly optimizing system performance.
</nextsent>
<nextsent>instead, we proceed by error analysis, like pado and boleda (2004).
</nextsent>
<nextsent>our aim is to form global hypothesis that explains the distribution of errors across classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2464">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> experiment 1: variance in role.  </section>
<citcontext>
<prevsection>
<prevsent>we model role assignment as classification task, with parse tree constituents as instances to be classified.
</prevsent>
<prevsent>we repeated the classification with two different learners: the first learner,timbl (daelemans et al , 2003) is an implementation of nearest-neighbor classification algorithms in the memory-based learning paradigm2.
</prevsent>
</prevsection>
<citsent citstr=" W02-2018 ">
the second learner, maloufs probabilistic maximum entropy (maxent) system (malouf, 2002), <papid> W02-2018 </papid>uses the lmvm algorithm to estimate log-linear models.</citsent>
<aftsection>
<nextsent>we did not perform smoothing.
</nextsent>
<nextsent>table 5 shows the features we use.
</nextsent>
<nextsent>here as in the system setup, we keep close to current existing models for semantic role assignment in order to make our results as representative as possible.
</nextsent>
<nextsent>we investigate different feature sets in order to verify our results.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2465">
<title id=" H05-1084.xml">analyzing models for semantic role assignment using conf usability </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>our study suggests that it may be worthwhile to explore the refinement of semantic features as well.
</prevsent>
<prevsent>the most obvious choice is to investigate features related to selectional preferences.
</prevsent>
</prevsection>
<citsent citstr=" W04-0817 ">
possible features include goodness of fit relative to pre-computed preferences (baldewein et al , 2004),<papid> W04-0817 </papid>named entities (pradhan et al , 2004), <papid> N04-1030 </papid>or broad onto logical classes like animate?</citsent>
<aftsection>
<nextsent>or artifact?.
</nextsent>
<nextsent>fol 674 lowing up on this idea, natural continuation of the present study would be to create meta-model that subsumes semantic features.
</nextsent>
<nextsent>such model could use optimal selectional restrictions as predictor.
</nextsent>
<nextsent>the next step would then be to construct combined meta-model that describes the behavior of systems with both syntactic and semantic features.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2467">
<title id=" E91-1050.xml">a language for the statement of binary relations over feature structures </title>
<section> applications of the formalism.  </section>
<citcontext>
<prevsection>
<prevsent>reduction to canon ica form.
</prevsent>
<prevsent>it is often the case that grammar assigns just one of range of logically equivalent represen-tations to sentence; designers of grammars for use in analysis generally take care to ensure that the result of parsing non-ambiguous sen-tence is unique semantic representation, and multiple representations are seen as the hall-mark of (pre-theoretical) ambiguity.
</prevsent>
</prevsection>
<citsent citstr=" C88-2128 ">
in gen-eration, as shieber (1988) <papid> C88-2128 </papid>and appelt (1989) observe, situation may arise in which the representation supplied as input to the process (perhaps by another program) is not itself directly suitable, but is logically equivalent to one that is. the use of distinct grammars for parsing and generation could provide solu-tion to this problem, but it raises others con-nected with management of the resulting sys- tem.</citsent>
<aftsection>
<nextsent>an alternative is to define equivalence classes of representations, and reduce all members of class to the single canonical form which the grammar can map into sen- fence.
</nextsent>
<nextsent>exactly how the classes and reductions are defined will doubtless depend on many fac- tors; we consider here some of the standard logical equivalences exploited in reducing arbitrary expressions of the propositional cal- cuius to disjunctive normal form.
</nextsent>
<nextsent>:t: not-not :li:  * op  ffi not  * val 1 op  ffi not  * val 1 val 1  = :l2:  *  ffi :x:  ffi  :t: not-or :li:  * op  ffi not  * val 1 op  = or  * val 1 val 1  = xi  * val 1 val 2  = x2 :l2:  * op  = and  * val 1 op  ffi not  * val 1 val 1  ffi y1  * val 2 op  = not  * val 2 val 1  = y2 :x: xi  ffi  y1 x2  ffi  y2 the two rules shown above express equivalences which are more familiar as: --,(-,p) ~ and - ,(p q) ~-~ (-,p ^ -,q).
</nextsent>
<nextsent>the - 290 - the mode of application required here is rather different from that described in the preceding section, for context in which  not-not  applies may not exist prior to the application of  not-or .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2468">
<title id=" E91-1050.xml">a language for the statement of binary relations over feature structures </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>individual applica-tions of the rule terminate, nevertheless.
</prevsent>
<prevsent>we have presented what is to our knowledge the first formalization and implementation a type of rule and control regime intended for use in situations where it is desired to produce the effect of transforming one feature structure into another.
</prevsent>
</prevsection>
<citsent citstr=" E89-1025 ">
9 the formalism described above has been implemented as part of issco elu l?, an enhanced patr-ii style (shieber, 1986) unification grammar environment, based on the ud system presented by johnson and rosner (1989).<papid> E89-1025 </papid></citsent>
<aftsection>
<nextsent>elu incorporates parser and genera- tot, and is primarily intended for use as tool for research in machine translation.
</nextsent>
<nextsent>use of transfer rules in translation has not so far brought light instances where the serial rule invocation regime described in section 3.2 proves necessary.
</nextsent>
<nextsent>elu grammars permit the use of typed feature structures (cf.
</nextsent>
<nextsent>johnson and rosner, op.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2469">
<title id=" E91-1050.xml">a language for the statement of binary relations over feature structures </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>elu grammars permit the use of typed feature structures (cf.
</prevsent>
<prevsent>johnson and rosner, op.
</prevsent>
</prevsection>
<citsent citstr=" E89-1024 ">
cit., moens et al, 1989) <papid> E89-1024 </papid>ingram- mars; although the present transfer ule format does not, they are clearly desirable addition, since they would provide means of exerting control over rule interactions.</citsent>
<aftsection>
<nextsent>a third area in which the transfer ule for- realism might be applied concerns the manipu-lation of re-entrant structures.
</nextsent>
<nextsent>while re- entrancy is in general useful property of fss, the complexity entailed by its presence is in some cases unwelcome; the method of genera- : 9 van noord (1990) describes the use of standard unification grammar to successively instantiate single feature structure mbo dying meaning representations for both source and target language xpressions in machine translation application.
</nextsent>
<nextsent>similarly, the transfer ules of zajac (1990) express relation between subparts of sin-gle complex structure.
</nextsent>
<nextsent>such an approach does not appear suitable for the appl/cation discussed in section 3.2 above.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2470">
<title id=" E91-1050.xml">a language for the statement of binary relations over feature structures </title>
<section> conc lus ion.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, the transfer ules of zajac (1990) express relation between subparts of sin-gle complex structure.
</prevsent>
<prevsent>such an approach does not appear suitable for the appl/cation discussed in section 3.2 above.
</prevsent>
</prevsection>
<citsent citstr=" C88-2150 ">
10  environnement linguistique unification  - 291 - tion proposed by wedekind (1988), <papid> C88-2150 </papid>for exam-ple, requires that the lfg-style f-structures which form the input to the generation process be  unfolded  into unordered trees.</citsent>
<aftsection>
<nextsent>this may be done with suitably formulated rule set of the kind introduced here.
</nextsent>
<nextsent>the present rule for-mat is unable to preserve the information that distinct sub-fss in destination fs arise from the duplication of single, re-entrant, sub-fs in the source.
</nextsent>
<nextsent>ways of incorporating this abil-ity into the rule formalism are under considera-tion, one possibility being the addition of an indexing mechanism that would flag sub-fss as originating in re-entrancy.
</nextsent>
<nextsent>a companion paper describes an interpreta-tion of transfer ule sets in terms of partial ordering with respect to the specificity of rules, and discusses linguistic and computational motivations for this view; it also comments in greater detail on the rule interaction problems referred to in fn.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2471">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>correspond-ingly, there has been change from developing rule-based parsers to developing statistical meth-ods for inducing grammatical knowledge from an-notated corpus data.
</prevsent>
<prevsent>the shift has mostly oc-curred because building wide-coverage grammars is time-consuming, error prone, and difficult.
</prevsent>
</prevsection>
<citsent citstr=" C96-1055 ">
the same can be said for crafting the rich lexical rep-resentations that are central component of lin-guistic knowledge, and research in automatic lex-ical acquisition has sought to address this ((dorr and jones, 1996; <papid> C96-1055 </papid>dorr, 1997), among others).</citsent>
<aftsection>
<nextsent>yet there have been few attempts to learn fine- grained lexical classifications from the statisti-cal analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (brent, 1993; <papid> J93-2002 </papid>klavans and chodorow, 1992; <papid> C92-4177 </papid>resnik, 1992)).</nextsent>
<nextsent>in this paper, we propose such an approach for the automatic lassification of verbs into lexical semantic lasses.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2473">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the shift has mostly oc-curred because building wide-coverage grammars is time-consuming, error prone, and difficult.
</prevsent>
<prevsent>the same can be said for crafting the rich lexical rep-resentations that are central component of lin-guistic knowledge, and research in automatic lex-ical acquisition has sought to address this ((dorr and jones, 1996; <papid> C96-1055 </papid>dorr, 1997), among others).</prevsent>
</prevsection>
<citsent citstr=" J93-2002 ">
yet there have been few attempts to learn fine- grained lexical classifications from the statisti-cal analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (brent, 1993; <papid> J93-2002 </papid>klavans and chodorow, 1992; <papid> C92-4177 </papid>resnik, 1992)).</citsent>
<aftsection>
<nextsent>in this paper, we propose such an approach for the automatic lassification of verbs into lexical semantic lasses.
</nextsent>
<nextsent>1 we can express the issues raised by this ap-proach as follows.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>which linguistic distinctions among lexical.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2474">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>the shift has mostly oc-curred because building wide-coverage grammars is time-consuming, error prone, and difficult.
</prevsent>
<prevsent>the same can be said for crafting the rich lexical rep-resentations that are central component of lin-guistic knowledge, and research in automatic lex-ical acquisition has sought to address this ((dorr and jones, 1996; <papid> C96-1055 </papid>dorr, 1997), among others).</prevsent>
</prevsection>
<citsent citstr=" C92-4177 ">
yet there have been few attempts to learn fine- grained lexical classifications from the statisti-cal analysis of distributional data, analogously to the induction of syntactic knowledge (though see, e.g., (brent, 1993; <papid> J93-2002 </papid>klavans and chodorow, 1992; <papid> C92-4177 </papid>resnik, 1992)).</citsent>
<aftsection>
<nextsent>in this paper, we propose such an approach for the automatic lassification of verbs into lexical semantic lasses.
</nextsent>
<nextsent>1 we can express the issues raised by this ap-proach as follows.
</nextsent>
<nextsent>1.
</nextsent>
<nextsent>which linguistic distinctions among lexical.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2475">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>distinguish the verb classes?
</prevsent>
<prevsent>in exploring these questions, we focus on verb classification for several reasons.
</prevsent>
</prevsection>
<citsent citstr=" P98-1112 ">
verbs are very important sources of knowledge in many language engineering tasks, and the relationships among verbs appear to play major role in the orga-nization and use of this knowledge: knowledge about verb classes is crucial for lexical acquisition in support of language generation and machine translation (dorr, 1997), and document classifica-tion (klavans and kan, 1998).<papid> P98-1112 </papid></citsent>
<aftsection>
<nextsent>manual classifica-tion of large numbers of verbs is difficult and resource intensive task (levin, 1993; miller et ah, 1990; dang et ah, 1998).
</nextsent>
<nextsent>to address these issues, we suggest hat one can automatically classify verbs by using statistical approximations to verb dia theses, to train an au-tomatic classifier.
</nextsent>
<nextsent>we use verb dia theses, follow-ing levin and dorr, for two reasons.
</nextsent>
<nextsent>first, verb dia theses are syntactic cues to semantic lasses, ~we are aware that distributional pproach rests on one strong assumption on the nature of the rep-resentations under study: semantic notions and syn-tactic notions are correlated, at least in part.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2478">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> determining  the  features.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, we see that other re-lated features to these usages erve to distinguish the two resolutions of the ambiguity.
</prevsent>
<prevsent>the main verb form is active and main verb part-of-speech (labeled as vbd by automatic pos taggers); by contrast, the reduced relative form is passive and past participle (tagged as vbn).
</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
although these properties are redundant with the intran- sitive/transitive distinction, recent work in ma-chine learning (ratnaparkhi, 1997; <papid> W97-0301 </papid>ratnaparkhi, 1998) <papid> P98-2177 </papid>has shown that using overlapping features can be beneficial for learning in maximum en-tropy framework, and we want to explore it in this setting to test h3 above.</citsent>
<aftsection>
<nextsent>2 in the next section, 2these properties are redundant with the intran- sitive/transitive distinction, as passive implies tran-sitive use, and necessarily entails the use of past participle.
</nextsent>
<nextsent>we performed correlation analysis that 47 proceedings of eacl  99 we describe how we compile the corpus counts for each of the four properties, in order to approxi-mate the distributional information of these alter-nations.
</nextsent>
<nextsent>features we assume that currently available large cor-pora are reasonable approximation to lan-guage (pullum, 1996).
</nextsent>
<nextsent>using combined cor-pus of 65-million words, we measured the rel-ative frequency distributions of the linguistic features (vbd/vbn, active/passive, intr ansi tive/transitive, causative/non-causative) over sample of verbs from the three lexical semantic classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2479">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> determining  the  features.  </section>
<citcontext>
<prevsection>
<prevsent>additionally, we see that other re-lated features to these usages erve to distinguish the two resolutions of the ambiguity.
</prevsent>
<prevsent>the main verb form is active and main verb part-of-speech (labeled as vbd by automatic pos taggers); by contrast, the reduced relative form is passive and past participle (tagged as vbn).
</prevsent>
</prevsection>
<citsent citstr=" P98-2177 ">
although these properties are redundant with the intran- sitive/transitive distinction, recent work in ma-chine learning (ratnaparkhi, 1997; <papid> W97-0301 </papid>ratnaparkhi, 1998) <papid> P98-2177 </papid>has shown that using overlapping features can be beneficial for learning in maximum en-tropy framework, and we want to explore it in this setting to test h3 above.</citsent>
<aftsection>
<nextsent>2 in the next section, 2these properties are redundant with the intran- sitive/transitive distinction, as passive implies tran-sitive use, and necessarily entails the use of past participle.
</nextsent>
<nextsent>we performed correlation analysis that 47 proceedings of eacl  99 we describe how we compile the corpus counts for each of the four properties, in order to approxi-mate the distributional information of these alter-nations.
</nextsent>
<nextsent>features we assume that currently available large cor-pora are reasonable approximation to lan-guage (pullum, 1996).
</nextsent>
<nextsent>using combined cor-pus of 65-million words, we measured the rel-ative frequency distributions of the linguistic features (vbd/vbn, active/passive, intr ansi tive/transitive, causative/non-causative) over sample of verbs from the three lexical semantic classes.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2480">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> determining  the  features.  </section>
<citcontext>
<prevsection>
<prevsent>vbd: part-of-speech tagged corpus was used, hence the counts for vbd/vbn were simply done based on the pos label according to the tagged corpus.
</prevsent>
<prevsent>aus: the causative feature was approximated by the following steps.
</prevsent>
</prevsection>
<citsent citstr=" P97-1003 ">
first, for each verb occur-rence subjects and objects were extracted from parsed corpus (collins 1997).<papid> P97-1003 </papid></citsent>
<aftsection>
<nextsent>then the propor- 3in performing this kind of corpus analysis, one has to take into account he fact that current corpus annotations do not distinguish verb senses.
</nextsent>
<nextsent>however, in these counts, we did not distinguish core sense of the verb from an extended use of the verb.
</nextsent>
<nextsent>so, for instance, the sentence consumer spending jumped 1.7 ~o in february after sharp drop the month be-fore (wsj 1987) is counted as an occurrence of the manner-of-motion verb jump in its in transitive form.
</nextsent>
<nextsent>this kind of extension of meaning does not modify subcategorization distributions (roland and jurafsky, 1998), <papid> P98-2184 </papid>although it might modify the rate of causativ- ity, but this is an unavoidable imitation at the current state of annotation of corpora.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2481">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> determining  the  features.  </section>
<citcontext>
<prevsection>
<prevsent>however, in these counts, we did not distinguish core sense of the verb from an extended use of the verb.
</prevsent>
<prevsent>so, for instance, the sentence consumer spending jumped 1.7 ~o in february after sharp drop the month be-fore (wsj 1987) is counted as an occurrence of the manner-of-motion verb jump in its in transitive form.
</prevsent>
</prevsection>
<citsent citstr=" P98-2184 ">
this kind of extension of meaning does not modify subcategorization distributions (roland and jurafsky, 1998), <papid> P98-2184 </papid>although it might modify the rate of causativ- ity, but this is an unavoidable imitation at the current state of annotation of corpora.</citsent>
<aftsection>
<nextsent>48 proceedings of eacl  99 tion of overlap between the two multi sets of nouns was calculated, meant to capture the property of the causative construction that the subject of the in transitive can occur as the object of the transi-tive.
</nextsent>
<nextsent>we define overlap as the largest multi set of elements belonging to both the subjects and the object multi sets, e.g. {a, a, a, b} {a} = {a, a, a}.
</nextsent>
<nextsent>the proportion is the ratio between the overlap and the sum of the subject and object multisets.
</nextsent>
<nextsent>the verbs in group 1 had been used in an earlier study, in which it was important to minimize noisy data, so they generally underwent greater man-ual intervention the counts.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2483">
<title id=" E99-1007.xml">automatic verb classification using distributions of grammatical features </title>
<section> vbd act intr 45%.  </section>
<citcontext>
<prevsection>
<prevsent>we must now determine which of the distri-butions actually contribute to learning the verb classifications.
</prevsent>
<prevsent>first we describe computational experiments in unsupervised learning, using hi-erarchical clustering, then we turn to supervised classification.
</prevsent>
</prevsection>
<citsent citstr=" P93-1024 ">
other work in automatic lexical semantic lassifi- cation has taken an approach in which clustering over statistical features is used in the automatic formation of classes (pereira et al, 1993; <papid> P93-1024 </papid>pereira et al, 1997; resnik, 1992).</citsent>
<aftsection>
<nextsent>we used the hierar-chical clustering algorithm available in splus5.0, imposing cut point that produced three clus-ters, to correspond to the three verb classes.
</nextsent>
<nextsent>ta-ble 1 shows the accuracy achieved using the four features described above (row 1), and all three- feature subsets of those four features (rows 2- 5).
</nextsent>
<nextsent>note that chance performance in this task (a three-way classification) is 33% correct.
</nextsent>
<nextsent>the highest accuracy in clustering, of 66%-- or half the error rate compared to chance--is ob-tained only by the triple of features in row 5 in the table: vbd, intr., and cans.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2484">
<title id=" H05-1057.xml">matching inconsistently spelled names in automatic speech recognizer output for information retrieval </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>differences in names due to spelling errors, spelling variants and transliteration errors have been dealt with by different kinds of approximate string matching techniques like soundex, phonix, and string edit distance (james c. french, 1997; zobel and dart, 1996).
</prevsent>
<prevsent>the nature of the problem is identical when the domain consists of databases of documents but in order to apply techniques that were developed for names by the database community one would haveto first detect names in the corpus, and then normalize them to some canonical form.
</prevsent>
</prevsection>
<citsent citstr=" W04-2905 ">
this is the approach taken by raghavan and allan (raghavan and allan, 2004) <papid> W04-2905 </papid>who showed that normalizing names using soundex codes resulted in 10% improvement on the tdt3 story link detection task.</citsent>
<aftsection>
<nextsent>they tested their method on newswire stories only.
</nextsent>
<nextsent>their difficulty in applying soundex to the asr documents was that detecting names in asr is too error prone for their methods to be useful (miller et al, 2000).
</nextsent>
<nextsent>spoken document retrieval was track at the trec-6,7 and 8 (voorhees and harman, 1997; voorhees and harman, 1998; voorhees and harman, 1999) conferences.
</nextsent>
<nextsent>at the trec-8 sdr track the conclusion was that asr is not really an issue for ad hoc retrieval.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2485">
<title id=" H05-1057.xml">matching inconsistently spelled names in automatic speech recognizer output for information retrieval </title>
<section> our approaches.  </section>
<citcontext>
<prevsection>
<prevsent>four of our models are trained on parallel text of asr and manual transcripts (or closed caption depending on availability) in order to learn probabilistic model of asr errors.
</prevsent>
<prevsent>the parallel text consists of pairs of sentences: sentences from the asr output and the corresponding manual transcripts.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
this is common technique in machine translation for which the ibm translation models are popular methods (brown et al, 1993).<papid> J93-2003 </papid>as convention, we use upper case letters to denote asr output and lowercase for manual tran scriptions.</citsent>
<aftsection>
<nextsent>given an input of parallel text of asrand manual transcriptions, the model learns probabilistic dictionary.
</nextsent>
<nextsent>the dictionary contains pairsof closed caption and asr words and the probability that the closed caption word is generated from given word in asr.
</nextsent>
<nextsent>thus, the model might learn high probability for p(cat|kate).
</nextsent>
<nextsent>3.1 overview of methods.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2497">
<title id=" H05-1057.xml">matching inconsistently spelled names in automatic speech recognizer output for information retrieval </title>
<section> experimental set up.  </section>
<citcontext>
<prevsection>
<prevsent>mean average precision was used as the measure of evaluation.
</prevsent>
<prevsent>4.4 implementation details.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
we use giza++ (och and ney, 2003) <papid> J03-1002 </papid>to train the machine translation system and the isi rewrite decoder (isi, 2001) to do the actual translations.</citsent>
<aftsection>
<nextsent>the decoder takes as input the models learned by 455 giza++ and sentence from the foreign language.it can output the top translations of the input sentence.
</nextsent>
<nextsent>the rewrite decoder can translate using ibm model-3 or model-4.
</nextsent>
<nextsent>we found model 3 to have lower perplexity and hence chose it for our experiments.
</nextsent>
<nextsent>in order to build the language model (c), we used the cmu language modeling toolkit 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2498">
<title id=" H05-1042.xml">collective content selection for concepttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>next, we present our experimental framework and data.
</prevsent>
<prevsent>we conclude the paper by presenting and discussing our results.
</prevsent>
</prevsection>
<citsent citstr=" P83-1022 ">
the generation literature provides multiple examples of content selection components developed for various domains (kukich, 1983; <papid> P83-1022 </papid>mckeown, 1985;sripada et al, 2001; <papid> W01-0802 </papid>reiter and dale, 2000).</citsent>
<aftsection>
<nextsent>a common theme across different approaches is the emphasis on coherence: related information is selected to produce text that hangs together?
</nextsent>
<nextsent>(mckeown,1985).
</nextsent>
<nextsent>similarly, our method is also guided by coherence constraints.
</nextsent>
<nextsent>in our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing large number of texts from domain-relevant corpus and 332 careful hand-crafting of content selection rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2499">
<title id=" H05-1042.xml">collective content selection for concepttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>next, we present our experimental framework and data.
</prevsent>
<prevsent>we conclude the paper by presenting and discussing our results.
</prevsent>
</prevsection>
<citsent citstr=" W01-0802 ">
the generation literature provides multiple examples of content selection components developed for various domains (kukich, 1983; <papid> P83-1022 </papid>mckeown, 1985;sripada et al, 2001; <papid> W01-0802 </papid>reiter and dale, 2000).</citsent>
<aftsection>
<nextsent>a common theme across different approaches is the emphasis on coherence: related information is selected to produce text that hangs together?
</nextsent>
<nextsent>(mckeown,1985).
</nextsent>
<nextsent>similarly, our method is also guided by coherence constraints.
</nextsent>
<nextsent>in our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing large number of texts from domain-relevant corpus and 332 careful hand-crafting of content selection rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2500">
<title id=" H05-1042.xml">collective content selection for concepttotext generation </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>similarly, our method is also guided by coherence constraints.
</prevsent>
<prevsent>in our case these constraints are derived automatically, while in symbolic generation systems coherence is enforced by analyzing large number of texts from domain-relevant corpus and 332 careful hand-crafting of content selection rules.
</prevsent>
</prevsection>
<citsent citstr=" W03-1016 ">
duboue and mckeown (2003) <papid> W03-1016 </papid>were the first to propose method for learning content selection rules automatically, thus going beyond mere corpusanalysis.</citsent>
<aftsection>
<nextsent>they treat content selection as classification task.
</nextsent>
<nextsent>given collection of texts associated with domain-specific database, their model learns whether database entry should be selected for presentation or not.
</nextsent>
<nextsent>their modeling approach uses an expressive feature space while considering database entries in isolation.
</nextsent>
<nextsent>similarly to duboue and mckeown (2003), <papid> W03-1016 </papid>we view content selection as classification task and learn selection rules from database and its corresponding corpus.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2504">
<title id=" H05-1042.xml">collective content selection for concepttotext generation </title>
<section> the task.  </section>
<citcontext>
<prevsection>
<prevsent>l ? xic+ jc? linkl(xi,x j)the first two elements in this expression capture the penalty for assigning entities to classes against their individual preferences.
</prevsent>
<prevsent>for instance, the penalty for selecting an entry ? c+ will equalind?(x), i.e., xs individual preference of being om mitted.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
the third term captures linking penalty for all pairs of entities (xi,x j) that are connected by link of type l, and are assigned to different classes.this formulation is similar to the energy minimization framework, which is commonly used in image analysis (besag, 1986; boykov et al, 1999) and has been recently applied in natural language processing (pang and lee, 2004).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>the principal advantages of this formulation lie in its computationalproperties.
</nextsent>
<nextsent>despite seeming intractable ? the number of possible subsets to consider for selection is exponential in the number of database entities ? the inference problem has an exact solution.
</nextsent>
<nextsent>provided that the scores ind+(x), ind?(x), and linkl(x,y) are positive, we can find globally optimal label assignment in polynomial time by computing minimal cut partition in an appropriately constructed graph (greig et al, 1989).
</nextsent>
<nextsent>in the following we first discuss how individual preference scores are estimated.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2509">
<title id=" E93-1031.xml">temporal connectives in a discourse context </title>
<section> temporal equivalence and.  </section>
<citcontext>
<prevsection>
<prevsent>first, one might expect hat (2a) and (2b) would be temporally equivalent, both de-scribing el; why, then, does (2b) apparently ead to discourse incoherence?
</prevsent>
<prevsent>secondly, it has been argued that when permits many possible temporal re-lationships between the eventualities denoted by and (cf.
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
moens and steedman 1988); <papid> J88-2003 </papid>it for this reason that (2c) can be interpreted as denoting el; 260 but given this permissiveness, why is (2d) not as ac-ceptable as (2c)?</citsent>
<aftsection>
<nextsent>pre suppositions the basic explanation for the inappropriateness of (2b) and (2d) is actually quite simple.
</nextsent>
<nextsent>sentences con-taining temporal connectives are presuppositional: the temporal clause introduces an eventuality that must be presupposed to have occurred, for the sen-tence as whole to have truth-value (cf.
</nextsent>
<nextsent>hein~imgki 1972; kartunnen 1973).
</nextsent>
<nextsent>if the presupposed eventu-ality is not already in the reader model of the dis-course context, she must add it: process known as accommodation (cf.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2510">
<title id=" E93-1031.xml">temporal connectives in a discourse context </title>
<section> the basic explanation: temporal.  </section>
<citcontext>
<prevsection>
<prevsent>vander sandt and geurts (1991) provide formal mechanism for pre supposition accommodation drt, but they provide only informal heuristics that stipulate how background knowledge might affect he process.
</prevsent>
<prevsent>we extend their ideas, by providing for-mal specification ofthose heuristics.
</prevsent>
</prevsection>
<citsent citstr=" P91-1008 ">
we embed their account of accommodation in dice (discourse and c_ommousense entailment) (lascarides and asher 1991, <papid> P91-1008 </papid>1993; lascarides et al 1992).<papid> P92-1001 </papid></citsent>
<aftsection>
<nextsent>dice permits us to model the interactions between linguistic knowl-edge (lk) and wk which lead to the assignment of discourse coherence relations between propositions introduced by text segments, and temporal-causal relations between the eventualities they denote.
</nextsent>
<nextsent>the primary proposal is that the accommodation of pre- suppositions from temporal subordinate clauses be modelled as discourse attachment, so that accommo-dation is properly constrained by the reader back- ground knowledge.
</nextsent>
<nextsent>let us call this basic idea accom-modation by discourse attachment (ada).
</nextsent>
<nextsent>although we contrasted presuppositional accounts with classical drt approaches, there are clear in-stances where temporal subordinate clause has no special rhetorical role in discourse, but acts in-stead as temporal adverb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2511">
<title id=" E93-1031.xml">temporal connectives in a discourse context </title>
<section> the basic explanation: temporal.  </section>
<citcontext>
<prevsection>
<prevsent>vander sandt and geurts (1991) provide formal mechanism for pre supposition accommodation drt, but they provide only informal heuristics that stipulate how background knowledge might affect he process.
</prevsent>
<prevsent>we extend their ideas, by providing for-mal specification ofthose heuristics.
</prevsent>
</prevsection>
<citsent citstr=" P92-1001 ">
we embed their account of accommodation in dice (discourse and c_ommousense entailment) (lascarides and asher 1991, <papid> P91-1008 </papid>1993; lascarides et al 1992).<papid> P92-1001 </papid></citsent>
<aftsection>
<nextsent>dice permits us to model the interactions between linguistic knowl-edge (lk) and wk which lead to the assignment of discourse coherence relations between propositions introduced by text segments, and temporal-causal relations between the eventualities they denote.
</nextsent>
<nextsent>the primary proposal is that the accommodation of pre- suppositions from temporal subordinate clauses be modelled as discourse attachment, so that accommo-dation is properly constrained by the reader back- ground knowledge.
</nextsent>
<nextsent>let us call this basic idea accom-modation by discourse attachment (ada).
</nextsent>
<nextsent>although we contrasted presuppositional accounts with classical drt approaches, there are clear in-stances where temporal subordinate clause has no special rhetorical role in discourse, but acts in-stead as temporal adverb.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2512">
<title id=" E93-1031.xml">temporal connectives in a discourse context </title>
<section> the basic explanation: temporal.  </section>
<citcontext>
<prevsection>
<prevsent>asher 1993).
</prevsent>
<prevsent>sdrt starts with traditional vltss (cf.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
kamp 1981), but goes on to assume with grosz and sidner (1986) <papid> J86-3001 </papid>that candidate discourses possess hi-erarchical structure, with units linked by discourse relations modelled after those proposed by hobbs (1985) (cf.</citsent>
<aftsection>
<nextsent>also mann and thompson 1987, scha and polanyi 1988).<papid> C88-2120 </papid></nextsent>
<nextsent>the resultant representations are called segmented drss (or sdp.ss).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2513">
<title id=" E93-1031.xml">temporal connectives in a discourse context </title>
<section> the basic explanation: temporal.  </section>
<citcontext>
<prevsection>
<prevsent>sdrt starts with traditional vltss (cf.
</prevsent>
<prevsent>kamp 1981), but goes on to assume with grosz and sidner (1986) <papid> J86-3001 </papid>that candidate discourses possess hi-erarchical structure, with units linked by discourse relations modelled after those proposed by hobbs (1985) (cf.</prevsent>
</prevsection>
<citsent citstr=" C88-2120 ">
also mann and thompson 1987, scha and polanyi 1988).<papid> C88-2120 </papid></citsent>
<aftsection>
<nextsent>the resultant representations are called segmented drss (or sdp.ss).
</nextsent>
<nextsent>here, we use five discourse relations: narration, background, re-sult, explanation and elaboration.
</nextsent>
<nextsent>the latter two are subordinating relations, and the proposition in-troduced by the current sentence can attach only to the previous constituent of the sdrs for the text so far, or constituents it elaborates or explains.
</nextsent>
<nextsent>sdrt defines those parts of an sdrs that are avail-able for attachment with new information via dis-course relation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2515">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>with this approach the quality of the under lying word alignments can have strong influence on phrase-based smt system performance.
</prevsent>
<prevsent>the common practice therefore is to extract phrase pairs from the best attainable word alignments.
</prevsent>
</prevsection>
<citsent citstr=" P00-1056 ">
currently, model-4 alignments (brown and others, 1993) as produced by giza++ (och and ney, 2000) <papid> P00-1056 </papid>are often the best that can be obtained, especially with large bitexts.</citsent>
<aftsection>
<nextsent>despite its modeling power and widespread use, model-4 has shortcomings.
</nextsent>
<nextsent>its formulation is such that maximum likelihood parameter estimation and bitext alignment are implemented by approximate,hill-climbing, methods.
</nextsent>
<nextsent>consequently parameter estimation can be slow, memory intensive, and difficult to parallelize.
</nextsent>
<nextsent>it is also difficult to compute statistics under model-4.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2516">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> hmm word and phrase alignment.  </section>
<citcontext>
<prevsection>
<prevsent>it is not needed during alignment, where sentence lengths are known, and is ignored.
</prevsent>
<prevsent>phrase count (k|m, e) specifies the number of target phrases.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
we use simple, single parameter distribution, with ? = 8.0 throughout (k|m, e) = (k|m, l) ? word-to-phrase alignment alignment is markov process that specifies the lengths of phrases and their alignment with source words (ak1 , hk1 , k1 |k,m, e) = ? k=1 (ak, hk, k|ak1, k1, e) = ? k=1 p(ak|ak1, hk; l) d(hk)n(k; eak )the actual word-to-phrase alignment (ak) is first order markov process, as in hmm-based word-to word alignment (vogel et al, 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>it necessarily depends on the hallucination variable p(aj |aj1, hj ; l) = ? ?
</nextsent>
<nextsent>1 aj = aj1, hj = 0 0 aj 6= aj1, hj = 0 a(aj |aj1; l) hj = 1this formulation allows target phrases to be inserted without disrupting the markov dependencies of phrases aligned to actual source words.the phrase length model n(?; e) gives the probability that word produces phrase with ? words in the target language; n(?; e) is defined for ? = 1, ? ?
</nextsent>
<nextsent>, . the hallucination process is simple i.i.d. process, where d(0) = p0, and d(1) = 1 ? p0.
</nextsent>
<nextsent>word-to-phrase translation the translation of words to phrases is given as (vk1 |ak1 , hk1 , k1 ,k,m, e) = ? k=1 p(vk|eak , hk, k) we introduce the notation vk = vk[1], . . .
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2518">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> hmm word and phrase alignment.  </section>
<citcontext>
<prevsection>
<prevsent>for example, constraining the phrase length component n(?; e) to permit only phrases of one word would give aword-to-word hmm alignment model.
</prevsent>
<prevsent>the extensions introduced are the phrase count, and the phrase length models, and the bigram translation distribution.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
the hallucination process is motivated by the use of null alignments into markov alignment models as done by (och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>the phrase length model is motivated by toutanova et al (2002) <papid> W02-1012 </papid>who introduced stay?</nextsent>
<nextsent>probabilities in hmm alignment as an alternative to word fertility.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2519">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> hmm word and phrase alignment.  </section>
<citcontext>
<prevsection>
<prevsent>the extensions introduced are the phrase count, and the phrase length models, and the bigram translation distribution.
</prevsent>
<prevsent>the hallucination process is motivated by the use of null alignments into markov alignment models as done by (och and ney, 2003).<papid> J03-1002 </papid></prevsent>
</prevsection>
<citsent citstr=" W02-1012 ">
the phrase length model is motivated by toutanova et al (2002) <papid> W02-1012 </papid>who introduced stay?</citsent>
<aftsection>
<nextsent>probabilities in hmm alignment as an alternative to word fertility.
</nextsent>
<nextsent>by comparison, word-to-phrase hmm alignment models contain detailed models of state occupancy, motivated by the ibm fertility model,which are more powerful than single staying parameter.
</nextsent>
<nextsent>in fact, the wtop model is segmental hidden markov model (ostendorf et al, 1996), in which states emit observation sequences.
</nextsent>
<nextsent>comparison with model-4 is less straightforward.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2520">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> hmm word and phrase alignment.  </section>
<citcontext>
<prevsection>
<prevsent>word-to-word alignments are generated by the viterbi algorithm: a?
</prevsent>
<prevsent>= argmaxa (f ,a|e); if eak ? vk , eak is linked to all the words in vk.
</prevsent>
</prevsection>
<citsent citstr=" J96-1002 ">
the bigram translation probability relies on word context, known to be helpful in translation (bergeret al, 1996), <papid> J96-1002 </papid>to improve the identification of target phrases.</citsent>
<aftsection>
<nextsent>as an example, is the chinese word for world trade center?.
</nextsent>
<nextsent>table 1 shows how the likelihood of the correct english phrase is improved with bigram translation probabilities; this example is from the ce, n=4 system of table 2.
</nextsent>
<nextsent>model unigram bigram (world|f) 0.06 0.06 (trade|world, f) 0.06 0.99 (center|trade, f) 0.06 0.99 (world trade center|f, 3) 0.0002 0.0588 table 1: context in bigram phrase translation.
</nextsent>
<nextsent>there are of course much prior work in translation that incorporates phrases.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2521">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> hmm word and phrase alignment.  </section>
<citcontext>
<prevsection>
<prevsent>there are of course much prior work in translation that incorporates phrases.
</prevsent>
<prevsent>sumita et al (2004) develop model of phrase-to-phrase alignment, which while based on hmm alignment process, appears to be deficient.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
marcu and wong (2002) <papid> W02-1018 </papid>propose model to learn lexical correspondences at the phraselevel.</citsent>
<aftsection>
<nextsent>to our knowledge, ours is the first non syntactic model of bitext alignment (as opposed to translation) that links words and phrases.
</nextsent>
<nextsent>we now discuss estimation of the wtop model parameters by the em algorithm.
</nextsent>
<nextsent>since the wtopmodel can be treated as an hmm with very complex state space, it is straightforward to apply baum 171 welch parameter estimation.
</nextsent>
<nextsent>we show the forward recur sion as an example.given sentence pair (el1, fm1 ), the forward probability j(i, ?) is defined as the probability of generating the first target words with the added condition that the target words jj??+1 form phrase aligned to source word ei.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2523">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> phrase pair induction.  </section>
<citcontext>
<prevsection>
<prevsent>by contrast, the 96m english word news+un01-02 is about the largest c-e bitext over which we can train model-4 with our giza++ configuration and computing infrastructure.based on these and other experiments, in this paper we set maximum value of = 4 for fe; inef, we set n=2 and omit the bigram phrase translation probability; ? is set to 8.0.
</prevsent>
<prevsent>we do not claim that this is optimal, however.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
a common approach to phrase-based translation isto extract an inventory of phrase pairs (ppi) from bitext (koehn et al, 2003), <papid> N03-1017 </papid>for example, in the phrase extract algorithm (och, 2002), word alignmentam1 is generated over the bitext, and all word sub sequences ei2i1 and j2 j1 are found that satisfy : am1 : aj ? [i1, i2] iff ? [j1, j2] .</citsent>
<aftsection>
<nextsent>(1) the ppi comprises all such phrase pairs (ei2i1 , j2 j1 ).
</nextsent>
<nextsent>the process can be stated slightly differently.
</nextsent>
<nextsent>first, we define set of alignments : a(i1, i2; j1, j2) = {am1 : aj ? [i1, i2] iff ? [j1, j2]} . if am1 ? a(i1, i2; j1, j2) then (ei2i1 , j2 j1 ) form phrase pair.viewed in this way, there are many possible alignments under which phrases might be paired, and 173 the selection of phrase pairs need not be based on single alignment.
</nextsent>
<nextsent>rather than simply accepting phrase pair (ei2i1 , j2 j1 ) if the unique map alignment satisfies equation 1, we can assign probability to phrases occurring as translation pairs : (f , a(i1, i2; j1, j2 ) | e) = ? : am1 a(i1,i2;j1,j2 ) (f ,a|e)for fixed set of indices i1, i2, j1, j2, the quantity (f , a(i1, i2; j1, j2 ) | e) can be computed efficiently using modified forward algorithm.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2524">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> phrase pair induction.  </section>
<citcontext>
<prevsection>
<prevsent>note that this algorithm is constructed specifically to improve viterbi ppi; it is certainly not the only way to extract phrase pairs under the phrase-to-phrase posterior distribution.once the ppi phrase pairs are set, the phrase translation probabilities are set based on the number of times each phrase pair is extracted from sentence pair, i.e. from relative frequencies.
</prevsent>
<prevsent>for each foreign phrase not in the viterbi ppi : for all pairs (fm1 , el1) and j1, j2 s.t. j2 j1 = : for 1 ? i1 ? i2 ? l, find f(i1, i2) = pfe(a(i1, i2; j1, j2) | el1, fm1 ) b(i1, i2) = pef (a(i1, i2; j1, j2) | el1, fm1 ) g(i1, i2) = ? f(11, i2) b(i1, i2) (i1, i2) = argmax 1i1,i2l g(i1, i2) , and set = ei2i1 add (u, v) to the ppi if any of a, b, or hold : b(i1, i2) ? tg and (i1, i2) ? tg (a) b(i1, i2)   tg and (i1, i2)   tp (b) (i1, i2)   tg and b(i1, i2)   tp (c) ppi augmentation via phrase-posterior induction hmm-based models are often used if posterior distributions are needed.
</prevsent>
</prevsection>
<citsent citstr=" P03-1041 ">
model-1 can also be used in this way (venugopal et al, 2003), <papid> P03-1041 </papid>although it is relatively weak alignment model.</citsent>
<aftsection>
<nextsent>by comparison,finding posterior distributions under model-4 is difficult.
</nextsent>
<nextsent>the word-to-phrase alignment model appears not to suffer this tradeoff: it is good model of word alignment under which statistics such as the phrase to-phrase posterior can be calculated.
</nextsent>
<nextsent>we evaluate the quality of phrase pairs extracted from the bitext through the translation performance of the translation template model (ttm) (kumaret al, 2005), which is phrase-based translation system implemented using weighted finite state transducers.
</nextsent>
<nextsent>performance is measured by bleu (pap ineni and others, 2001).chineseenglish translation we report performance on the nist chinese/english 2002, 2003 and 2004 (news only) mt evaluation sets.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2525">
<title id=" H05-1022.xml">hmm word and phrase alignment for statistical machine translation </title>
<section> translation experiments.  </section>
<citcontext>
<prevsection>
<prevsent>when we translate with viterbi phrase extract ppis taken from wtop alignments create dover all available bitext, we find comparable performance to the model-4 baseline (table 4, line 9).
</prevsent>
<prevsent>using the phrase-posterior augmentation scheme with tp = 0.7 yields further improvement (table 4, line 10).
</prevsent>
</prevsection>
<citsent citstr=" P03-1021 ">
pooling the sets to form two large ce and ae test sets, the ae system improvements are significant at 95% level (och, 2003); <papid> P03-1021 </papid>the ce systems are only equivalent.</citsent>
<aftsection>
<nextsent>we have described word-to-phrase alignment models capable of good quality bitext word alignment.
</nextsent>
<nextsent>in arabic-english and chinese-english translation and alignment they compare well to model-4, even with large bitexts.
</nextsent>
<nextsent>the model architecture was inspired by features of model-4, such as fertility and distortion, but care was taken to ensure that dynamic programming procedures, such as em and viterbi alignment, could still be performed.
</nextsent>
<nextsent>there is practical value in this: training and alignment are easily parallelized.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2526">
<title id=" H05-1115.xml">using random walks for question focused sentence retrieval </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>therefore, there is often more than one correct answer to question.we aim to develop method for sentence retrieval that goes beyond finding sentences that are similar to single query.
</prevsent>
<prevsent>to this end, we propose to use stochastic, graph-based method.
</prevsent>
</prevsection>
<citsent citstr=" P04-1035 ">
recently, graph-based methods have proved useful for number of nlp and ir tasks such as document re-ranking in ad hoc ir (kurland and lee, 2005) and analyzing sentiments in text (pang and lee, 2004).<papid> P04-1035 </papid></citsent>
<aftsection>
<nextsent>in (erkan and radev, 2004), we introduced the lexrank method and successfully applied it to generic, multi-document summarization.
</nextsent>
<nextsent>presently, we introduce topic-sensitive lexrank in creating asentence retrieval system.
</nextsent>
<nextsent>we evaluate its performance against competitive baseline, which considers the similarity between each sentence and the question (using idf-weighed word overlap).
</nextsent>
<nextsent>we demonstrate that lexrank significantly improvesquestion-focused sentence selection over the base line.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2527">
<title id=" H05-1115.xml">using random walks for question focused sentence retrieval </title>
<section> experimental setup.  </section>
<citcontext>
<prevsection>
<prevsent>once we collected the questions for each cluster, two judges independently annotated nine of the training clusters.
</prevsent>
<prevsent>for each sentence and question pair in given cluster, the judges were asked to indicate whether or not the sentence contained complete answer to the question.
</prevsent>
</prevsection>
<citsent citstr=" J96-2004 ">
once an acceptable rate of inter judge agreement was verified on the first nine clusters (kappa (carletta, 1996) <papid> J96-2004 </papid>of 0.68), the remaining 11 clusters were annotated by one judge each.in some cases, the judges did not find any sentences containing the answer forgiven question.</citsent>
<aftsection>
<nextsent>such questions were removed from the corpus.
</nextsent>
<nextsent>the final number of questions annotated for answers over the entire corpus was 341, and the distributions of questions per cluster can be found in table 1.
</nextsent>
<nextsent>4.2 evaluation metrics and methods.
</nextsent>
<nextsent>to evaluate our sentence retrieval mechanism, we produced extract files, which contain list of sentences deemed to be relevant to the question, for the system and from human judgment.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2528">
<title id=" H05-1093.xml">blanc learning evaluation metrics for mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>we compare our metrics performance with traditional and recent automatic evaluation metrics.
</prevsent>
<prevsent>we also describe the parameter conditions under which blanc can emulate them.throughout the remainder of this paper, we distinguish between two components of automatic mt evaluation: the statistics computed on candidate and reference translations and the function used in defining evaluation metrics and generating translation scores.
</prevsent>
</prevsection>
<citsent citstr=" P04-1078 ">
commonly used statistics include bagof-words overlap, edit distance, longest common sub sequence, ngram overlap, and skip-bigram overlap.preferred functions are various combinations of precision and recall (soricut and brill, 2004), <papid> P04-1078 </papid>including 1since existing evaluation metrics (e.g. bleu, rouge) are special cases of our metric family, it is only natural to name it broad learning and adaptation for numeric criteria (blanc) ? white light contains light of all frequencies 740 weighted precision and f-measures (van-rijsbergen, 1979).</citsent>
<aftsection>
<nextsent>blanc implements practical algorithm with learn able parameters for automatic mt evaluation which estimates the reference-candidate translation overlap by computing weighted sum of commonsubsequences (also known as skip-ngrams).
</nextsent>
<nextsent>common skip-ngrams are sequences of words in their sentence order that are found both in the reference and candidate translations.
</nextsent>
<nextsent>by generalizing and separating the overlap statistics from the function used to combine them, and by identifying the latter as learn able component, blanc subsumes the ngram based evaluation metrics as special cases and can better reflect the need of end applications for ade quacy/fluency tradeoffs . 1.1 related work.
</nextsent>
<nextsent>initial work in evaluating translation quality focused on edit distance-based metrics (su et al, 1992; <papid> C92-2067 </papid>akibaet al, 2001).</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2529">
<title id=" H05-1093.xml">blanc learning evaluation metrics for mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>common skip-ngrams are sequences of words in their sentence order that are found both in the reference and candidate translations.
</prevsent>
<prevsent>by generalizing and separating the overlap statistics from the function used to combine them, and by identifying the latter as learn able component, blanc subsumes the ngram based evaluation metrics as special cases and can better reflect the need of end applications for ade quacy/fluency tradeoffs . 1.1 related work.
</prevsent>
</prevsection>
<citsent citstr=" C92-2067 ">
initial work in evaluating translation quality focused on edit distance-based metrics (su et al, 1992; <papid> C92-2067 </papid>akibaet al, 2001).</citsent>
<aftsection>
<nextsent>in the mt context, edit distance (levenshtein, 1965) represents the amount of word insertions, deletions and substitutions necessary to transform candidate translation into reference translation.
</nextsent>
<nextsent>another evaluation metric based on edit distance is the word error rate (niessen et al, 2000) which computes the normalized edit distance.
</nextsent>
<nextsent>bleu is weighted precision evaluation metric introduced by ibm (papineni et al, 2001).
</nextsent>
<nextsent>bleu and its exten sions/variants (e.g. nist (doddington, 2002)) have become de-facto standards in the mt community and are consistently being used for system optimization and tuning.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2530">
<title id=" H05-1093.xml">blanc learning evaluation metrics for mt </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>these methods relyon local features and do not explicitly capture sentence-level features,although implicitly longer n-gram matches are rewarded in bleu.
</prevsent>
<prevsent>the general text matcher (gtm) (turian et al, 2003) is another mt evaluation method that rewards longer ngrams instead of assigning them equal weight.
</prevsent>
</prevsection>
<citsent citstr=" P04-1077 ">
(lin and och, 2004) <papid> P04-1077 </papid>recently proposed set of metrics (rouge) for mt evaluation.</citsent>
<aftsection>
<nextsent>rouge-l is longest common sub sequence (lcs) based automatic evaluation metric for mt. the intuition behind it isthat long common sub sequences reflect large overlap between candidate translation and reference translation.
</nextsent>
<nextsent>rouge-w is also based on lcs, but assigns higher weights to sequences that have fewer gaps.
</nextsent>
<nextsent>however, these metrics still do not distinguish among translations with the same lcs but different number of shorter sized sub sequences, also indicative of overlap.
</nextsent>
<nextsent>rouge-s attempts to correct this problem by combining the precision/recall of skip bigrams of the reference and candidate translations.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2540">
<title id=" H05-1086.xml">a translation model for sentence retrieval </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>the documents for the trec novelty track in 2002 were taken from the trec volumes 4 and 5, and consist of news articles from the financial times, the foreign broadcast information service, and the los angeles times from non-overlapping years.
</prevsent>
<prevsent>in 2003 and 2004, the documents were taken from the aquaint corpus, which is distributed by the linguistic data consortium2 and consists of newswire text in english from the xinhua news service, the new york times, and the associated press from overlapping years.
</prevsent>
</prevsection>
<citsent citstr=" A97-1004 ">
we retrieved the top 1000 documents foreach topic from the trec and aquaint collections, and sentence segmented the documents using mxterminator (reynar and ratnaparkhi, 1997), <papid> A97-1004 </papid>which is freely available sentence boundary detector.</citsent>
<aftsection>
<nextsent>each topic was indexed separately and had an average of 30,000sentences.
</nextsent>
<nextsent>it was impractical to do sentence level relevance assessments for the complete set of 150,000 documents, so we used the relevance assessments provided as part of the novelty task, recognizing that the results are lower bound on performance, because the relevance assessments do not cover the collection.
</nextsent>
<nextsent>the relevance assessments cover 25 known relevant documents for each topic.we evaluated precision at documents because many systems using sentence retrieval emphasize the results at the top of the ranked list, and are less concerned with the overall quality of the list.
</nextsent>
<nextsent>2.1 translation models.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2541">
<title id=" H05-1086.xml">a translation model for sentence retrieval </title>
<section> methodology.  </section>
<citcontext>
<prevsection>
<prevsent>we incorporated machine translation model in two steps: estimation and ranking.
</prevsent>
<prevsent>in the estimation step, the probability that term in the sentence translates?
</prevsent>
</prevsection>
<citsent citstr=" J90-2002 ">
to term in the query is estimated using the implementation of ibm 1http://trec.nist.gov 2http://www.ldc.upenn.edumodel 1 (brown et al , 1990) <papid> J90-2002 </papid>in giza++ (al onaizan et al , 1999) out-of-the-box withoutalteration.</citsent>
<aftsection>
<nextsent>in the ranking step we incorporate the translation probabilities into the query likelihood framework.
</nextsent>
<nextsent>in berger and lafferty (1999), the ibm model 1 is incorporated thus: (qi|s) = ? j=1 (qi|sj)p (sj |s) (1) where (qi|sj) is the probability that term sj in the sentence translates to term qi in the query.
</nextsent>
<nextsent>if the translation probabilities are modified such that (qi|sj) = 1 if qi = sj and 0 otherwise, this is berger and laffertys model 0?, and it is exactly the query-likelihood model (described in section 2.2).a major difference between machine translation and sentence retrieval is that machine translation assumes there is little, if any, overlap inthe vocabularies of the two languages.
</nextsent>
<nextsent>in sentence retrieval we depend heavily on the overlap between the two vocabularies.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2543">
<title id=" H05-1049.xml">robust textual inference via graph matching </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>in machine translation evaluation, we must be able to recognize legitimate translations which structurally differ from our reference translation.
</prevsent>
<prevsent>one sub-task underlying these applications is the ability to recognize semantic entailment; whether one piece of text follows from another.
</prevsent>
</prevsection>
<citsent citstr=" N03-1022 ">
in contrast to recent work which has successfully utilized logic based abductive approaches to inference (moldovanet al, 2003; <papid> N03-1022 </papid>raina et al, 2005b), we adopt graph based representation of sentences, and use graph matching approach to measure the semantic overlap of text.</citsent>
<aftsection>
<nextsent>graph matching techniques have proven to be useful approach for tractable approximate matching in other domains including computer vision.
</nextsent>
<nextsent>in the domain of language, graphs provide natural way to express the dependencies between words and phrases in sentence.
</nextsent>
<nextsent>furthermore, graph matching also has the advantage of providing framework for structural matching of phrases that would be difficult to resolve at the level of individual words.
</nextsent>
<nextsent>we describe our approach in the context of the 2005 recognizing textual entailment (rte) challenge(dagan et al, 2005), but note that our approach easily extends to other related inference tasks.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2544">
<title id=" H05-1049.xml">robust textual inference via graph matching </title>
<section> semantic representation.  </section>
<citcontext>
<prevsection>
<prevsent>perhaps the most common representation of text for assessing content is bag-of-words?
</prevsent>
<prevsent>or bag-of-ngrams?
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
(papineni et al, 2002).<papid> P02-1040 </papid></citsent>
<aftsection>
<nextsent>however, such representations lose syntactic information which can be essential to determining entailment.
</nextsent>
<nextsent>consider question answer system searching for an answer to when was israel established?
</nextsent>
<nextsent>a representation which did not utilize syntax would probably enthusiastically return an answer from (the 2005 rte text): the national institute for psychobiology in israel was established in 1979.in this example, its important to try to match relationships as well as words.
</nextsent>
<nextsent>in particular, any answer to the question should preserve the dependency between israel and established.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2545">
<title id=" H05-1049.xml">robust textual inference via graph matching </title>
<section> semantic representation.  </section>
<citcontext>
<prevsection>
<prevsent>for there mainder of the section we outline how we produce graphs from text, and in the next section we introduce our graph matching model.
</prevsent>
<prevsent>3.2 from text to graphs.
</prevsent>
</prevsection>
<citsent citstr=" P03-1054 ">
starting with raw english text, we use version of the parser described in (klein and manning, 2003), <papid> P03-1054 </papid>to obtain parse tree.</citsent>
<aftsection>
<nextsent>then, we derive dependency tree representation of the sentence using slightly modified version of collins?
</nextsent>
<nextsent>head propagation rules(collins, 1999), which make main verbs not auxiliaries the head of sentences.
</nextsent>
<nextsent>edges in the dependency graph are labeled by set of hand-createdtgrep expressions.
</nextsent>
<nextsent>these labels represent sur face?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2546">
<title id=" H05-1049.xml">robust textual inference via graph matching </title>
<section> semantic representation.  </section>
<citcontext>
<prevsection>
<prevsent>3.
</prevsent>
<prevsent>semantic role labeling: we also augment.
</prevsent>
</prevsection>
<citsent citstr=" P05-1073 ">
the graph representation with probank-style semantic roles via the system described in (toutanova et al, 2005).<papid> P05-1073 </papid></citsent>
<aftsection>
<nextsent>each predicate addsan arc labeled with the appropriate semantic role to the head of the argument phrase.
</nextsent>
<nextsent>this helps to create links between words which share deep semantic relation not evident in the surface syntax.
</nextsent>
<nextsent>additionally, modifying phrases are labeled with their semantic types (e.g., in 1991 is linked by temporal edge in the text graph of figure 2), which should be useful in question answering tasks.
</nextsent>
<nextsent>olution tagger, coref links are added through out the graph.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2547">
<title id=" H05-1049.xml">robust textual inference via graph matching </title>
<section> node and edge substitution models.  </section>
<citcontext>
<prevsection>
<prevsent>in particular, we use the measure described in (resnik, 1995).
</prevsent>
<prevsent>we found it useful to only use similarities above fixed threshold to ensure precision.
</prevsent>
</prevsection>
<citsent citstr=" W04-3205 ">
lsa match: and m(v) are distributionally similar according to freely available latent semantic indexing package,2 or for verbs similar according to verb ocean (chklovski and pantel, 2004).<papid> W04-3205 </papid></citsent>
<aftsection>
<nextsent>pos match: and m(v) have the same part of speech.
</nextsent>
<nextsent>no match: m(v) is null.although the above conditions often produce reasonable matchings between text and hypothesis, we found the recall of these lexical resources to be far from adequate.
</nextsent>
<nextsent>more robust lexical resources would almost certainly boost performance.
</nextsent>
<nextsent>5.2 path substitution cost model.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2551">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both models outperform bidirectional ibm model-4 in terms of word alignment accuracy by 10% absolute on the f-measure.
</prevsent>
<prevsent>using blocks obtained from the models in actual translation systems yields statistically significant improvements in chinese-english smt evaluation.
</prevsent>
</prevsection>
<citsent citstr=" N03-1017 ">
todays statistical machine translation systems relyon high quality phrase translation pairs to acquire state-of-the-art performance, see (koehn et al , 2003; <papid> N03-1017 </papid>zens and ney, 2004; <papid> N04-1033 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>here,phrase pairs, or blocks?
</nextsent>
<nextsent>are obtained automatically from parallel sentence pairs via the underlying word alignments.
</nextsent>
<nextsent>word alignments traditionally are based on ibm models 1-5 (brown et al , 1993) <papid> J93-2003 </papid>or onhmms (vogel et al , 1996).<papid> C96-2141 </papid></nextsent>
<nextsent>automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some languagepairs: for chinese-english, inter-annotator agreement exceeds 90 on f-measure whereas ibm model 4 or hmm accuracy is typically below 80s.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2552">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both models outperform bidirectional ibm model-4 in terms of word alignment accuracy by 10% absolute on the f-measure.
</prevsent>
<prevsent>using blocks obtained from the models in actual translation systems yields statistically significant improvements in chinese-english smt evaluation.
</prevsent>
</prevsection>
<citsent citstr=" N04-1033 ">
todays statistical machine translation systems relyon high quality phrase translation pairs to acquire state-of-the-art performance, see (koehn et al , 2003; <papid> N03-1017 </papid>zens and ney, 2004; <papid> N04-1033 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>here,phrase pairs, or blocks?
</nextsent>
<nextsent>are obtained automatically from parallel sentence pairs via the underlying word alignments.
</nextsent>
<nextsent>word alignments traditionally are based on ibm models 1-5 (brown et al , 1993) <papid> J93-2003 </papid>or onhmms (vogel et al , 1996).<papid> C96-2141 </papid></nextsent>
<nextsent>automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some languagepairs: for chinese-english, inter-annotator agreement exceeds 90 on f-measure whereas ibm model 4 or hmm accuracy is typically below 80s.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2553">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>both models outperform bidirectional ibm model-4 in terms of word alignment accuracy by 10% absolute on the f-measure.
</prevsent>
<prevsent>using blocks obtained from the models in actual translation systems yields statistically significant improvements in chinese-english smt evaluation.
</prevsent>
</prevsection>
<citsent citstr=" J03-1002 ">
todays statistical machine translation systems relyon high quality phrase translation pairs to acquire state-of-the-art performance, see (koehn et al , 2003; <papid> N03-1017 </papid>zens and ney, 2004; <papid> N04-1033 </papid>och and ney, 2003).<papid> J03-1002 </papid></citsent>
<aftsection>
<nextsent>here,phrase pairs, or blocks?
</nextsent>
<nextsent>are obtained automatically from parallel sentence pairs via the underlying word alignments.
</nextsent>
<nextsent>word alignments traditionally are based on ibm models 1-5 (brown et al , 1993) <papid> J93-2003 </papid>or onhmms (vogel et al , 1996).<papid> C96-2141 </papid></nextsent>
<nextsent>automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some languagepairs: for chinese-english, inter-annotator agreement exceeds 90 on f-measure whereas ibm model 4 or hmm accuracy is typically below 80s.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2555">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>here,phrase pairs, or blocks?
</prevsent>
<prevsent>are obtained automatically from parallel sentence pairs via the underlying word alignments.
</prevsent>
</prevsection>
<citsent citstr=" J93-2003 ">
word alignments traditionally are based on ibm models 1-5 (brown et al , 1993) <papid> J93-2003 </papid>or onhmms (vogel et al , 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some languagepairs: for chinese-english, inter-annotator agreement exceeds 90 on f-measure whereas ibm model 4 or hmm accuracy is typically below 80s.
</nextsent>
<nextsent>hmms assume that words close-in-source?
</nextsent>
<nextsent>are aligned towords close-in-target?.
</nextsent>
<nextsent>while this locality assumption is generally sound, hmms do have limitations: the self-transition probability of state (word) is the only control on the duration in the state, the length of the phrase aligned to the word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2556">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>here,phrase pairs, or blocks?
</prevsent>
<prevsent>are obtained automatically from parallel sentence pairs via the underlying word alignments.
</prevsent>
</prevsection>
<citsent citstr=" C96-2141 ">
word alignments traditionally are based on ibm models 1-5 (brown et al , 1993) <papid> J93-2003 </papid>or onhmms (vogel et al , 1996).<papid> C96-2141 </papid></citsent>
<aftsection>
<nextsent>automatic word alignment is challenging in that its accuracy is not yet close to inter-annotator agreement in some languagepairs: for chinese-english, inter-annotator agreement exceeds 90 on f-measure whereas ibm model 4 or hmm accuracy is typically below 80s.
</nextsent>
<nextsent>hmms assume that words close-in-source?
</nextsent>
<nextsent>are aligned towords close-in-target?.
</nextsent>
<nextsent>while this locality assumption is generally sound, hmms do have limitations: the self-transition probability of state (word) is the only control on the duration in the state, the length of the phrase aligned to the word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2557">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the model uses single block to split the sentence pair into two independent regions, it is not clear which block we should select for this purpose.
</prevsent>
<prevsent>therefore, we treat the splitting block as hidden variable.this proposed approach is far simpler than treating the entire sentence as sequence of nonoverlapping phrases (or chunks) and considering such sequential segmentation either explicitly or implicitly.
</prevsent>
</prevsection>
<citsent citstr=" W02-1018 ">
for example, (marcu and wong, 2002) <papid> W02-1018 </papid>for joint phrase based model, (huang et al , 2003) for translation memory system; and (watanabe et al ., 2003) <papid> P03-1039 </papid>for complex model of insertion, deletion and head-word driven chunk reordering.</citsent>
<aftsection>
<nextsent>other approaches including (watanabe et al , 2002) treat extracted phrase-pairs as new parallel data with limited success.
</nextsent>
<nextsent>typically, they share similar architecture of phrase level segmentation, reordering, translation as in (och and ney, 2002; <papid> P02-1038 </papid>koehn and knight, 2002;yamada and knight, 2001).<papid> P01-1067 </papid></nextsent>
<nextsent>the phrase level interaction has to be taken care of for the non-overlapping sequential segmentation in complicated way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2558">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>while the model uses single block to split the sentence pair into two independent regions, it is not clear which block we should select for this purpose.
</prevsent>
<prevsent>therefore, we treat the splitting block as hidden variable.this proposed approach is far simpler than treating the entire sentence as sequence of nonoverlapping phrases (or chunks) and considering such sequential segmentation either explicitly or implicitly.
</prevsent>
</prevsection>
<citsent citstr=" P03-1039 ">
for example, (marcu and wong, 2002) <papid> W02-1018 </papid>for joint phrase based model, (huang et al , 2003) for translation memory system; and (watanabe et al ., 2003) <papid> P03-1039 </papid>for complex model of insertion, deletion and head-word driven chunk reordering.</citsent>
<aftsection>
<nextsent>other approaches including (watanabe et al , 2002) treat extracted phrase-pairs as new parallel data with limited success.
</nextsent>
<nextsent>typically, they share similar architecture of phrase level segmentation, reordering, translation as in (och and ney, 2002; <papid> P02-1038 </papid>koehn and knight, 2002;yamada and knight, 2001).<papid> P01-1067 </papid></nextsent>
<nextsent>the phrase level interaction has to be taken care of for the non-overlapping sequential segmentation in complicated way.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2559">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, (marcu and wong, 2002) <papid> W02-1018 </papid>for joint phrase based model, (huang et al , 2003) for translation memory system; and (watanabe et al ., 2003) <papid> P03-1039 </papid>for complex model of insertion, deletion and head-word driven chunk reordering.</prevsent>
<prevsent>other approaches including (watanabe et al , 2002) treat extracted phrase-pairs as new parallel data with limited success.</prevsent>
</prevsection>
<citsent citstr=" P02-1038 ">
typically, they share similar architecture of phrase level segmentation, reordering, translation as in (och and ney, 2002; <papid> P02-1038 </papid>koehn and knight, 2002;yamada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>the phrase level interaction has to be taken care of for the non-overlapping sequential segmentation in complicated way.
</nextsent>
<nextsent>our models model such interactions in soft way.
</nextsent>
<nextsent>the hidden blocks are allowed to overlap with each other, 177while each block induced two non-overlapping regions, i.e. the model brackets the sentence pair into two independent parts which are generated synchronously.
</nextsent>
<nextsent>in this respect, it resembles bilingual bracketing (wu, 1997), <papid> J97-3002 </papid>but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2560">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>for example, (marcu and wong, 2002) <papid> W02-1018 </papid>for joint phrase based model, (huang et al , 2003) for translation memory system; and (watanabe et al ., 2003) <papid> P03-1039 </papid>for complex model of insertion, deletion and head-word driven chunk reordering.</prevsent>
<prevsent>other approaches including (watanabe et al , 2002) treat extracted phrase-pairs as new parallel data with limited success.</prevsent>
</prevsection>
<citsent citstr=" P01-1067 ">
typically, they share similar architecture of phrase level segmentation, reordering, translation as in (och and ney, 2002; <papid> P02-1038 </papid>koehn and knight, 2002;yamada and knight, 2001).<papid> P01-1067 </papid></citsent>
<aftsection>
<nextsent>the phrase level interaction has to be taken care of for the non-overlapping sequential segmentation in complicated way.
</nextsent>
<nextsent>our models model such interactions in soft way.
</nextsent>
<nextsent>the hidden blocks are allowed to overlap with each other, 177while each block induced two non-overlapping regions, i.e. the model brackets the sentence pair into two independent parts which are generated synchronously.
</nextsent>
<nextsent>in this respect, it resembles bilingual bracketing (wu, 1997), <papid> J97-3002 </papid>but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2561">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our models model such interactions in soft way.
</prevsent>
<prevsent>the hidden blocks are allowed to overlap with each other, 177while each block induced two non-overlapping regions, i.e. the model brackets the sentence pair into two independent parts which are generated synchronously.
</prevsent>
</prevsection>
<citsent citstr=" J97-3002 ">
in this respect, it resembles bilingual bracketing (wu, 1997), <papid> J97-3002 </papid>but our model has more lexical items in the blocks with many-to-many word alignment freedom in both inner and outer parts.</citsent>
<aftsection>
<nextsent>we present our localization constraints using blocks for word alignment in section 2; we detail ourtwo new probabilistic models and their em training algorithms in section 3; our baseline system, maximum-posterior inference for word alignment, is explained in section 4; experimental results of alignments and translations are in section 5; and section 6 contains discussion and conclusions.
</nextsent>
<nextsent>we use the following notation in the remainder of this paper: and denote the english and foreign sentences with sentence lengthes of and , respectively.
</nextsent>
<nextsent>ei is an english word at position in e; fj is foreign word at position in . is the alignment vector with aj mapping the position of the english word eaj to which fj connects.
</nextsent>
<nextsent>therefore, we havethe standard limitation that one foreign word can not be connected to more than one english word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2565">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> inner-outer bracket models.  </section>
<citcontext>
<prevsection>
<prevsent>a few parameters, especially the maximum fertility, are tuned for giza++s optimal performance.
</prevsent>
<prevsent>we collect bidirectional (bi) refined word alignment by growing the intersection of chinese-to-english (ce) alignments and english-to-chinese (ec) alignments with the neighboring unaligned word pairs which appear in the union similar to the final-and?
</prevsent>
</prevsection>
<citsent citstr=" W03-1001 ">
approaches (koehn, 2003; och and ney, 2003; <papid> J03-1002 </papid>tillmann, 2003).<papid> W03-1001 </papid>table 1 summarizes our baseline with different set tings.</citsent>
<aftsection>
<nextsent>table 1 shows that hmm ec-p gives the f-measure(%) func cont both small hmm ec-p 54.69 69.99 64.78 hmm ec-v 31.38 53.56 55.59 hmm ce-p 51.44 69.35 62.69 hmm ce-v 31.43 63.84 55.45 large hmm ec-p 60.08 78.01 71.92 hmm ec-v 32.80 74.10 64.26 hmm ce-p 58.45 79.44 71.84 hmm ce-v 35.41 79.12 68.33 small giza mh-bi 45.63 69.48 60.08giza m4-bi 48.80 73.68 63.75 large giza mh-bi 49.13 76.51 65.67giza m4-bi 52.88 81.76 70.24 - fully-align 2 5.10 15.84 9.28 table 1: baseline: v: viterbi; p: max-posterior 1ldc2002e17 181 best baseline, better than bidirectional refined word alignments from giza m4 and the hmm viterbi aligners.
</nextsent>
<nextsent>5.2 inner-outer bracket models.
</nextsent>
<nextsent>we trained hmm lexicon (f |e) to initialize the inner-outer bracket models.
</nextsent>
<nextsent>afterwards, up to 15?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2569">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> inner-outer bracket models.  </section>
<citcontext>
<prevsection>
<prevsent>we also carried out the translation experiments using the best settings for inner-outer bm-b (i.e. bm-b drop) on the tides chinese-english 2003 test set.we trained our models on 354,252 test-specific sentence pairs drawn from ldc-supplied parallel corpora.
</prevsent>
<prevsent>on this training data, we ran 5 iterations ofem using bm-b to infer word alignments.
</prevsent>
</prevsection>
<citsent citstr=" J03-1005 ">
a monotone decoder similar to (tillmann and ney, 2003)<papid> J03-1005 </papid>with trigram language model3 is set up for trans lations.</citsent>
<aftsection>
<nextsent>we report case sensitive bleu (papineni etal., 2002) <papid> P02-1040 </papid>scorebleuc for all experiments.</nextsent>
<nextsent>the base line system (hmm ) used phrase pairs built from the hmm-ec-p maximum posterior word alignment and the corresponding lexicons.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2570">
<title id=" H05-1023.xml">inner outer bracket models for word alignment using hidden blocks </title>
<section> inner-outer bracket models.  </section>
<citcontext>
<prevsection>
<prevsent>on this training data, we ran 5 iterations ofem using bm-b to infer word alignments.
</prevsent>
<prevsent>a monotone decoder similar to (tillmann and ney, 2003)<papid> J03-1005 </papid>with trigram language model3 is set up for trans lations.</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
we report case sensitive bleu (papineni etal., 2002) <papid> P02-1040 </papid>scorebleuc for all experiments.</citsent>
<aftsection>
<nextsent>the base line system (hmm ) used phrase pairs built from the hmm-ec-p maximum posterior word alignment and the corresponding lexicons.
</nextsent>
<nextsent>the baseline bleuc score is 0.2276 ? 0.015.
</nextsent>
<nextsent>if we use the phrase pairs built from the bracket model instead (but keep the hmm trained lexicons), we get case sensitive bleuc score 0.2526.
</nextsent>
<nextsent>the improvement is statistically significant.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2571">
<title id=" E95-1018.xml">mixing modes of linguistic description in categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although developed separately, these two sources propose formal systems that are in many ways similar, but, interestingly, take pre-cisely opposing views as to what are the  natural relations  between levels.
</prevsent>
<prevsent>this difference of opin-ion has consequences for how the systems may be used as linguistic formalisms (requiring, forex- 2some early examples of muitimodal systems are logics that have coexistence, but without interlink- age, of associative and non-associative lambek calcu-lus (oehrle &amp; zhang 1989; morrill 1990).
</prevsent>
</prevsection>
<citsent citstr=" E93-1034 ">
further ex-amples include systems that combine associative lam- bek calculus with special connectives for discontinuity (e.g. morrill &amp; solias 1993; <papid> E93-1034 </papid>hepple 1994).<papid> C94-2201 </papid></citsent>
<aftsection>
<nextsent>127 ample,  additional apparatus  for handling word order in the second approach), and more crucially for the kind of linguistic accounts they allow to be formulated.
</nextsent>
<nextsent>in this paper, will describe the approach taken in hepple (1993) - - what call the  hybrid  ap-proach, discuss the general inguistic model that it tends to foster and provide some linguistic illus-tration, and discuss possibilities for parsing hybrid systems.
</nextsent>
<nextsent>i will begin with discussion of subs truc tural hierarchy and structural modalities, as it is the behaviour of systems with structural modalit-ies that inspires the hybrid view of how different levels should be related.
</nextsent>
<nextsent>i will address only logics (or levels) having three connectives:  product  connective (a form of con-junction, corresponding to  matter-like addition  of substructures), plus two implicational connect-ives (the left and right  residuals  of the product), notated as ?-l, and for product o. the minimal set of sequent rules for any group 0 of connectives {o,~,.---} is as in .(1), (2): 3 (1) a:v =~ a:v (id) ? ::~ b:b rib:v\] =~ a:a \[cut\] r\[?\] ~ a: ~\[b/v\] (2) (b:v, f) ? =*.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2572">
<title id=" E95-1018.xml">mixing modes of linguistic description in categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>although developed separately, these two sources propose formal systems that are in many ways similar, but, interestingly, take pre-cisely opposing views as to what are the  natural relations  between levels.
</prevsent>
<prevsent>this difference of opin-ion has consequences for how the systems may be used as linguistic formalisms (requiring, forex- 2some early examples of muitimodal systems are logics that have coexistence, but without interlink- age, of associative and non-associative lambek calcu-lus (oehrle &amp; zhang 1989; morrill 1990).
</prevsent>
</prevsection>
<citsent citstr=" C94-2201 ">
further ex-amples include systems that combine associative lam- bek calculus with special connectives for discontinuity (e.g. morrill &amp; solias 1993; <papid> E93-1034 </papid>hepple 1994).<papid> C94-2201 </papid></citsent>
<aftsection>
<nextsent>127 ample,  additional apparatus  for handling word order in the second approach), and more crucially for the kind of linguistic accounts they allow to be formulated.
</nextsent>
<nextsent>in this paper, will describe the approach taken in hepple (1993) - - what call the  hybrid  ap-proach, discuss the general inguistic model that it tends to foster and provide some linguistic illus-tration, and discuss possibilities for parsing hybrid systems.
</nextsent>
<nextsent>i will begin with discussion of subs truc tural hierarchy and structural modalities, as it is the behaviour of systems with structural modalit-ies that inspires the hybrid view of how different levels should be related.
</nextsent>
<nextsent>i will address only logics (or levels) having three connectives:  product  connective (a form of con-junction, corresponding to  matter-like addition  of substructures), plus two implicational connect-ives (the left and right  residuals  of the product), notated as ?-l, and for product o. the minimal set of sequent rules for any group 0 of connectives {o,~,.---} is as in .(1), (2): 3 (1) a:v =~ a:v (id) ? ::~ b:b rib:v\] =~ a:a \[cut\] r\[?\] ~ a: ~\[b/v\] (2) (b:v, f) ? =*.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2573">
<title id=" E95-1018.xml">mixing modes of linguistic description in categorial grammar </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>alternative but equivalent formalisations of the above system are possible.
</prevsent>
<prevsent>hepple (1993), for example, provides natural deduction formalisation.
</prevsent>
</prevsection>
<citsent citstr=" C92-1024 ">
such formal-isation should readily provide the basis for chart based approach to parsing hybrid logic grammars, after the manner of existing chart methods for use with (ksnig 1990; hepple 1992).<papid> C92-1024 </papid></citsent>
<aftsection>
<nextsent>a further promising possibility for efficient parsing of hy-brid system grammars involves proof net reformu-lation, following general scheme for such refor-mulation described in moortgat (1992).
</nextsent>
<nextsent>however, the precise character of either chart or proof net based methods for parsing hybrid system gram-mars is topic requiring further research.
</nextsent>
<nextsent>9 concluding remarks.
</nextsent>
<nextsent>as noted earlier, the approach described here has strong similarities to one developed independ-ently by moortgat &amp; oehrle (1993), although they take precisely opposing view as to what consti-tute the appropriate directions of linkage between levels.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2574">
<title id=" E99-1009.xml">geometry of lexico syntactic interaction </title>
<section> ambiguity and spurious.  </section>
<citcontext>
<prevsection>
<prevsent>it is regarded as problematic computationally because itmeans that in an exhaustive traversal of the proof search space one must either repeat 63 proceedings of eacl  99 sub computations, or else perform book-keeping to avoid so doing.
</prevsent>
<prevsent>the problem is that different \[3rl-long sequent derivations do not necessarily represent different readings, and this is the case because the sequent calculus forces us to choose between sequential isation of inferences ---in the case of (14)/l and kl--- when in fact they are not ordered by dependency and can be performed in parallel.
</prevsent>
</prevsection>
<citsent citstr=" C90-2030 ">
the problem can be resolved by defining stricter normalised proofs which impose unique ordering when alternatives would otherwise be available (k6nig 1990, hepple 1990, <papid> C90-2030 </papid>hendriks 1993).</citsent>
<aftsection>
<nextsent>however, while this removes spurious ambiguity as problem arising from independence of inferences, it signally fails to exploit the fact that such inferences can be parallelised.
</nextsent>
<nextsent>thus we prefer the term  derivational equivalence  to  spurious ambiguity  and interpret he phenomenon not as problem for sequential isation, but as an opportunity for parallelism.
</nextsent>
<nextsent>this opportumty is grasped in pro@nets.
</nextsent>
<nextsent>b. b+ a- i / a\b+ a+ b- \ ii / akb- a- b+ \ / b/a+ b- a+ \ ii / b/a- b+ a+ \ ii / a.b+ a- b- \ / a.b- i- and ii-tinks: two premises, one conclusion
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2575">
<title id=" E99-1004.xml">an object oriented approach to the design of dialogue management functionality </title>
<section> introduction.  </section>
<citcontext>
<prevsection>
<prevsent>24 proceedings of eacl  99 if the system is to conduct dialogue as human interlocutor might, it must use to best advantage whatever information it is given - whether that information was explicitly sought or not - and then be able to ask for information it still requires.
</prevsent>
<prevsent>such self-organizing behaviour, as opposed to simpler state transitions (novick and sutton, 1996), generally has one of number of possible motivations.
</prevsent>
</prevsection>
<citsent citstr=" P96-1009 ">
the system may be plan- based, attempting to identify and understand the ramifications of the problem the user wants to solve (allen et al, 1996).<papid> P96-1009 </papid></citsent>
<aftsection>
<nextsent>alternatively it may attempt prove theorems, questioning the user for the missing facts that it needs to know in order to help him or her complete some complex task (smith and hipp, 1994).
</nextsent>
<nextsent>or the system may attempt to identify or elicit specifically those facts that it needs to complete  request template  for particular transaction.
</nextsent>
<nextsent>it is essentially this last approach that the current prototype has adopted, and to this extent it resembles the speech mania system developed by philips (aust and oerder, 1995), which has already been used successfully to implement speech-based timetable nquiry system for swiss federal railways (aust et al, 1995).
</nextsent>
<nextsent>however, by additionally identifying eneric and specialized functionality, including heuristics that would characterize human expert, it becomes possible to create dialogue management system that can cope with several real-world enquiry domains, or number of complex subtasks, in one and the same adaptable, extensible implementation.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2576">
<title id=" H05-2004.xml">demonstrating an interactive semantic role labeling system </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>our demonstration aims to address this issue.
</prevsent>
<prevsent>we present an interactive system that performs the srl task from raw text in real time.
</prevsent>
</prevsection>
<citsent citstr=" W05-0625 ">
its architecture is based on the top system in the 2005 conll shared task (koomen et al, 2005), <papid> W05-0625 </papid>modified to process raw text using lower level processors but maintaining 6 good real time performance.</citsent>
<aftsection>
<nextsent>our system begins preprocessing raw text by using sentence segmentation tools (available at http://l2r.cs.uiuc.edu/cogcomp/tools.php).
</nextsent>
<nextsent>next,sentences are analyzed by state-of-the-art syntactic parser (charniak, 2000) <papid> A00-2018 </papid>the output of which provides useful information for the main srl module.</nextsent>
<nextsent>the main srl module consists of four stages:pruning, argument identification, argument classification, and inference.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2577">
<title id=" H05-2004.xml">demonstrating an interactive semantic role labeling system </title>
<section> the srl system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>its architecture is based on the top system in the 2005 conll shared task (koomen et al, 2005), <papid> W05-0625 </papid>modified to process raw text using lower level processors but maintaining 6 good real time performance.</prevsent>
<prevsent>our system begins preprocessing raw text by using sentence segmentation tools (available at http://l2r.cs.uiuc.edu/cogcomp/tools.php).</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
next,sentences are analyzed by state-of-the-art syntactic parser (charniak, 2000) <papid> A00-2018 </papid>the output of which provides useful information for the main srl module.</citsent>
<aftsection>
<nextsent>the main srl module consists of four stages:pruning, argument identification, argument classification, and inference.
</nextsent>
<nextsent>the following is the overview of these four stages.
</nextsent>
<nextsent>details of them can be found in (koomen et al, 2005).<papid> W05-0625 </papid>pruning the goal of pruning is to filter out unlikely argument candidates using simple heuristic rules.</nextsent>
<nextsent>only the constituents in the parse tree are considered as argument candidates.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2579">
<title id=" H05-2004.xml">demonstrating an interactive semantic role labeling system </title>
<section> the srl system architecture.  </section>
<citcontext>
<prevsection>
<prevsent>details of them can be found in (koomen et al, 2005).<papid> W05-0625 </papid>pruning the goal of pruning is to filter out unlikely argument candidates using simple heuristic rules.</prevsent>
<prevsent>only the constituents in the parse tree are considered as argument candidates.</prevsent>
</prevsection>
<citsent citstr=" W04-3212 ">
in addition, our system exploits heuristic modified from that introduced by (xue and palmer, 2004) <papid> W04-3212 </papid>to filter out very unlikely constituents.argument identification the argument identification stage uses binary classification to identify whether candidate is an argument or not.</citsent>
<aftsection>
<nextsent>we train and apply the binary classifiers on the constituents supplied by the pruning stage.
</nextsent>
<nextsent>argument classification this stage assigns the final argument labels to the argument candidates supplied from the previous stage.
</nextsent>
<nextsent>a multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage.inference the purpose of this stage is to incorporate some prior linguistic and structural knowledge, such as arguments do not overlap?
</nextsent>
<nextsent>and each verb takes at most one argument of each type.?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2580">
<title id=" E95-1036.xml">splitting the reference time temporal anaphora and quantification in drt </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the rpt is reset during the processing of the discourse.
</prevsent>
<prevsent>note that in  terminal  drs (ready for an embedding test), all the auxiliary rpts  disappear  (do not participate in the embedding).
</prevsent>
</prevsection>
<citsent citstr=" J88-2003 ">
the perfect is analyzed by using the notion of nucleus (moens and steedman, 1988) <papid> J88-2003 </papid>to account for the inner structure of an eventuality.</citsent>
<aftsection>
<nextsent>a nucleus is defined as structure containing preparatory process, culmination and consequent state.
</nextsent>
<nextsent>the categorization of verb phrases into different as- pectual classes can be phrased in terms of which part of the nucleus they refer to.
</nextsent>
<nextsent>the perfect is seen in (kamp and reyle, 1993) as an aspect ual operator.
</nextsent>
<nextsent>the eventualities described by the per-fect of verb refer to the consequent state of its nucleus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2581">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" C04-1080 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2582">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2583">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" P00-1034 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2584">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" P99-1023 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2585">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2587">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" W02-1002 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2588">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>this model remains within the framework of dynamic bayesian networks (dbns) and is conditionally-structured, but resolves the label bias problem inherent in the conditional markov model (cmm).
</prevsent>
<prevsent>a common sequence-labeling task in natural language processing involves assigning part-of speech (pos) tag to each word in the input text.
</prevsent>
</prevsection>
<citsent citstr=" N03-1033 ">
previous authors have used numerous hmm-based models (banko and moore, 2004; <papid> C04-1080 </papid>collins, 2002; <papid> W02-1001 </papid>lee et al, 2000; <papid> P00-1034 </papid>thede and harper, 1999) <papid> P99-1023 </papid>and other types of networks including maximum entropy models (ratnaparkhi, 1996)<papid> W96-0213 </papid>conditional markov models (klein and manning, 2002; <papid> W02-1002 </papid>mccallum etal., 2000), conditional random fields (crf) (laf ferty et al, 2001), and cyclic dependency networks (toutanova et al, 2003).<papid> N03-1033 </papid></citsent>
<aftsection>
<nextsent>all of these models make use of varying amounts of contextual information.in this paper, we present new model which remains within the well understood framework of dynamic bayesian networks (dbns), and we show that it produces state-of-the-art results when applied to the pos-tagging task.
</nextsent>
<nextsent>this new model isconditionally-structured and, through the use of virtual evidence (pearl, 1988; bilmes, 2004), resolves the explaining-away problems (often described as label or observation bias) inherent in the cmm.this paper is organized as follows.
</nextsent>
<nextsent>in section 2 we discuss the differences between hidden markov model (hmm) and the corresponding conditional markov model (cmm).
</nextsent>
<nextsent>in section 3 we describe our observed-child model (ocm), introducing the notion of virtual evidence, and providing aninformation-theoretic foundation for the use of negative training data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2590">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> using virtual evidence.  </section>
<citcontext>
<prevsection>
<prevsent>more flexibly, we can define to be the ratio of the two so that m0 = m1.now we derive method for training the conditional probability table (ci|si1, si) in terms of the pointwise mutual information between the adjacent tags si1 and si.
</prevsent>
<prevsent>we first rewrite the conditional probability (henceforth abbreviated as p) as: = (ci = 1|si1, si) = (ci = 1, si1, si) (si1, si) if the probabilities are maximum likelihood (ml) estimates derived from counts on the training data, we can equivalently write: = n(ci = 1, si1, si)n(si1, si) 2this use of implied negative training data is similar to the neighborhood?
</prevsent>
</prevsection>
<citsent citstr=" P05-1044 ">
concept described in (smith and eisner, 2005) <papid> P05-1044 </papid>461 where n(?)</citsent>
<aftsection>
<nextsent>is the count function.
</nextsent>
<nextsent>expanding the denominator into two terms: = n(ci = 1, si1, si)n(ci = 1, si1, si) + n(ci = 0, si1, si) without any negative training data (labeled with ci = 0), this ratio would always evaluate to 1, and no probabilistic relationship between si1 and si would be learned.from the start, we have implicitly postulated relationship between adjacent tags.
</nextsent>
<nextsent>we now formally state two hypotheses: h1 that there is relationship between adjacent tags which can be described by some joint probability distribution (si1, si), andthe null hypothesis, h0, that there is no such relationship, i.e. si1 and si are independent: ph1 = (si1, si) ph0 = (si1)p (si) now we can express the counts as follows: n(ci = 1, si1, si) = m1 ? (si1, si) n(ci = 0, si1, si) = m0 ? (si1)p (si)where m1 is the total number of tokens in the (positive) training data, and m0 is the total number of tokens in the induced negative training data.
</nextsent>
<nextsent>we substitute m0 with ? m1 for the reasons mentioned earlier, and simplify to obtain: = (si1, si)p (si1, si) + np (si1)p (si) which can be simplified to obtain: = 1 1 + [ (si1,si) (si1)p (si) ] 1 the ratio of probabilities in the denominator is the ratio used in computing the pointwise mutual information between si1 and si.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2591">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> using virtual evidence.  </section>
<citcontext>
<prevsection>
<prevsent>from ? ?
</prevsent>
<prevsent>[0,?)
</prevsent>
</prevsection>
<citsent citstr=" P89-1010 ">
to ? [0, 1).beginning with (church and hanks, 1989), <papid> P89-1010 </papid>numerous authors have used the pointwise mutual information between pairs of words to analyze word co-locations and associations.</citsent>
<aftsection>
<nextsent>this ratio tells us whether si1 and si co-occur more or less often than would be expected by chance alone.
</nextsent>
<nextsent>consider, for example, the tags dt (determiner)and nn (noun), and the four possible ordered tagpairs.
</nextsent>
<nextsent>the probabilities (si) and (si|si1) derived from the training data (see section 4.1), the likelihood ratio score ?, the conditional probability = (ci = 1|si1, si), and the occurrence counts are shown in table 1.
</nextsent>
<nextsent>as expected, the sequence dt-nn (e.g. the surplus) occurs very often and getsa high score, while dt-dt (e.g. this a) and nn dt (e.g. surplus the) occur infrequently and get low scores.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2592">
<title id=" H05-1058.xml">partofspeech tagging using virtual evidence and negative training </title>
<section> experiments and results.  </section>
<citcontext>
<prevsection>
<prevsent>section 4.3 presents comparisons between several simple models using just the tags, the words, and single binary feature for eachword.
</prevsent>
<prevsent>section 4.4 presents results from feature rich second-order observed-child model in which tags are linked in groups of three.
</prevsent>
</prevsection>
<citsent citstr=" N03-2002 ">
all training of language models is done using thesrilm toolkit (stolcke, 2002) with the flm extensions (bilmes and kirchhoff, 2003), <papid> N03-2002 </papid>and the implementation and testing of the various graphical models is carried out with the help of the graphical models toolkit (gmtk) (bilmes and zweig, 2002).</citsent>
<aftsection>
<nextsent>4.1 datasets.
</nextsent>
<nextsent>the data used for these experiments is the wall street journal data from penn treebank iii (mar cus et al, 1994).
</nextsent>
<nextsent>we extracted tagged sentences from the parse trees and divided the data into training (sections 0-18), development (sections 19-21), and test (sections 22-24) sets as in (toutanova et al,2003).<papid> N03-1033 </papid></nextsent>
<nextsent>except for the final results for the feature rich model, all results are on the development set.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2601">
<title id=" E93-1018.xml">a unification based approach to multiple vp ellipsis resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>an assumption shared by many theories of discourse is that discourse structure con-strains anaphora resolution (cf.
</prevsent>
</prevsection>
<citsent citstr=" J86-3001 ">
\[grosz and sidner 1986\] <papid> J86-3001 </papid>for definite nps, \[lascarides and asher 1991\], \[nakhimovsky 1988\] <papid> J88-2004 </papid>for temporal anaphora, \[webber 1990\] for deictic pronouns and \[gardent 1991\], \[prfist and scha 1990\] for vp ellipsis).</citsent>
<aftsection>
<nextsent>the aim of this paper is (i) to show that this as-sumption also applies to multiple vp ellip-sis (vpe), (ii) to argue that other levels of linguistic information (such as syntax and semantics) interact with discourse structure in determining multiple vpe acceptability and (i i)to make these intuitions precise by providing unification-based account of multiple vpe resolution.
</nextsent>
<nextsent>\[klein and stainton-ellis 1989\] convincingly argue that vpe need not resolve to the nearest possible antecedent.
</nextsent>
<nextsent>the most intricate examples they give to support his claim involve what they dubbed mul- iple vpe and can be illustrated by the follow-ing discourses (square brackets urround antecedent vps, 01 indicates vp ellipses and indices represent anaphoric dependencies) 1 : *the work reported here was partially carried out in the lre project 61-062, towards declarative theory of discourse.
</nextsent>
<nextsent>1 although this data often raises suspicion among lin-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2602">
<title id=" E93-1018.xml">a unification based approach to multiple vp ellipsis resolution </title>
<section> abstract </section>
<citcontext>
<prevsection>

<prevsent>an assumption shared by many theories of discourse is that discourse structure con-strains anaphora resolution (cf.
</prevsent>
</prevsection>
<citsent citstr=" J88-2004 ">
\[grosz and sidner 1986\] <papid> J86-3001 </papid>for definite nps, \[lascarides and asher 1991\], \[nakhimovsky 1988\] <papid> J88-2004 </papid>for temporal anaphora, \[webber 1990\] for deictic pronouns and \[gardent 1991\], \[prfist and scha 1990\] for vp ellipsis).</citsent>
<aftsection>
<nextsent>the aim of this paper is (i) to show that this as-sumption also applies to multiple vp ellip-sis (vpe), (ii) to argue that other levels of linguistic information (such as syntax and semantics) interact with discourse structure in determining multiple vpe acceptability and (i i)to make these intuitions precise by providing unification-based account of multiple vpe resolution.
</nextsent>
<nextsent>\[klein and stainton-ellis 1989\] convincingly argue that vpe need not resolve to the nearest possible antecedent.
</nextsent>
<nextsent>the most intricate examples they give to support his claim involve what they dubbed mul- iple vpe and can be illustrated by the follow-ing discourses (square brackets urround antecedent vps, 01 indicates vp ellipses and indices represent anaphoric dependencies) 1 : *the work reported here was partially carried out in the lre project 61-062, towards declarative theory of discourse.
</nextsent>
<nextsent>1 although this data often raises suspicion among lin-.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2603">
<title id=" E93-1018.xml">a unification based approach to multiple vp ellipsis resolution </title>
<section> d is course  grammar  and  vpe   </section>
<citcontext>
<prevsection>
<prevsent>139 adequately.
</prevsent>
<prevsent>in what follows, argue that discourse structure (rather than surface ordering) is one of the main constraint regulating multiple vpe resolution.
</prevsent>
</prevsection>
<citsent citstr=" P84-1085 ">
reso lu ion the discourse grammar used builds on \[polanyi and scha 1984\].<papid> P84-1085 </papid></citsent>
<aftsection>
<nextsent>more specifically, as-sume that discourse is tree structured entity whose well formed ness can be described by unification based discourse grammar.
</nextsent>
<nextsent>under such grammar, discourse constituent is either discourse relation, clause or discourse relation together with one or more discourse constituent(s).
</nextsent>
<nextsent>the grammar as-sociates with each constituent complex category which for the purpose of this paper, will assume to consist of the six main attributes pho, cat, sem, in, out and restr.
</nextsent>
<nextsent>pho, cat, sem unsurprisingly denote the phonology, the category and the seman-tic representation the constituent described by the complex category.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2604">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>natural language applications that use supervised learning methods require annotated training data, but annotated data is scarce for many we thank stephen clark, roger levy, carol nichols,and the three anonymous reviewers for their helpful comments.
</prevsent>
<prevsent>non-english languages.
</prevsent>
</prevsection>
<citsent citstr=" N01-1026 ">
it has been suggested that annotated data for these languages mightbe automatically created by leveraging parallel corpora and high-accuracy english systems (yarowsky and ngai, 2001; <papid> N01-1026 </papid>diab and resnik, 2002).<papid> P02-1033 </papid></citsent>
<aftsection>
<nextsent>the studies are centered around the assumption that linguistic analyses for english(e.g., part-of-speech tags, word sense disambiguation, grammatical dependency relation ships) are also valid analyses in the translation of the english.
</nextsent>
<nextsent>for example, in the english noun phrase the red apples, red modifies ap ples; the same modifier relationship also exists in its french translations les pommes rouges, even though the word orders differ.
</nextsent>
<nextsent>to the extent that the assumption is true, annotated data inthe non-english language can be created by projecting english analyses across word aligned parallel corpus.
</nextsent>
<nextsent>the resulting projected datacan then serve as (albeit noisy) training examples to develop applications in the non-english language.the projection approach faces both theoretical and practical challenge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2605">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>natural language applications that use supervised learning methods require annotated training data, but annotated data is scarce for many we thank stephen clark, roger levy, carol nichols,and the three anonymous reviewers for their helpful comments.
</prevsent>
<prevsent>non-english languages.
</prevsent>
</prevsection>
<citsent citstr=" P02-1033 ">
it has been suggested that annotated data for these languages mightbe automatically created by leveraging parallel corpora and high-accuracy english systems (yarowsky and ngai, 2001; <papid> N01-1026 </papid>diab and resnik, 2002).<papid> P02-1033 </papid></citsent>
<aftsection>
<nextsent>the studies are centered around the assumption that linguistic analyses for english(e.g., part-of-speech tags, word sense disambiguation, grammatical dependency relation ships) are also valid analyses in the translation of the english.
</nextsent>
<nextsent>for example, in the english noun phrase the red apples, red modifies ap ples; the same modifier relationship also exists in its french translations les pommes rouges, even though the word orders differ.
</nextsent>
<nextsent>to the extent that the assumption is true, annotated data inthe non-english language can be created by projecting english analyses across word aligned parallel corpus.
</nextsent>
<nextsent>the resulting projected datacan then serve as (albeit noisy) training examples to develop applications in the non-english language.the projection approach faces both theoretical and practical challenge.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2606">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>to the extent that the assumption is true, annotated data inthe non-english language can be created by projecting english analyses across word aligned parallel corpus.
</prevsent>
<prevsent>the resulting projected datacan then serve as (albeit noisy) training examples to develop applications in the non-english language.the projection approach faces both theoretical and practical challenge.
</prevsent>
</prevsection>
<citsent citstr=" J94-4004 ">
theoretically, it is well-known that two languages often do not express the same meaning in the same way(dorr, 1994).<papid> J94-4004 </papid></citsent>
<aftsection>
<nextsent>practically, the projection framework is sensitive to component errors.
</nextsent>
<nextsent>in particular, poor word alignments significantly degrade the accuracy of the projected annotations.
</nextsent>
<nextsent>previous research on resource projection attempts to address these problems by redistributing the parameter values (yarowsky and ngai, 2001) <papid> N01-1026 </papid>or by applying transformation rules (hwa et al, 851 2002).</nextsent>
<nextsent>their experimental results suggest that while these techniques can overcome some errors, they are not sufficient for projected data that are very noisy.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2609">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>most popular is the family of bootstrapping algorithms, in which model is seeded with small amount of labeled data and iteratively improved as more unlabeled data are folded into the training set,typically, through unsupervised learning.
</prevsent>
<prevsent>an other approach is active learning (cohn et al,1996), in which the model is also iteratively improved but the training examples are chosen by the learning model, and the learning process is supervised.
</prevsent>
</prevsection>
<citsent citstr=" W04-3207 ">
finally, the work that is the closest to ours in spirit is the idea of joint estimation (smith and smith, 2004).<papid> W04-3207 </papid></citsent>
<aftsection>
<nextsent>of the bootstrapping methods, perhaps themost well-known is the expectation maximization (em) algorithm.
</nextsent>
<nextsent>this approach has been explored in the context of many nlp applica tions; one example is text classification (nigam et al, 1999).
</nextsent>
<nextsent>another bootstrapping approach reminiscent of em is self-training.
</nextsent>
<nextsent>yarowsky(1995) <papid> P95-1026 </papid>used this method for word sense disam biguation.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2610">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>this approach has been explored in the context of many nlp applica tions; one example is text classification (nigam et al, 1999).
</prevsent>
<prevsent>another bootstrapping approach reminiscent of em is self-training.
</prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
yarowsky(1995) <papid> P95-1026 </papid>used this method for word sense disam biguation.</citsent>
<aftsection>
<nextsent>in self-training, annotated examples are used as seeds to train an initial classifier with any supervised learning method.
</nextsent>
<nextsent>this initial classifier is then used to automatically annotate data from large pool of unlabeled examples.
</nextsent>
<nextsent>of these newly labeled data, the ones labeled with the highest confidence are used as examples to train new classifier.
</nextsent>
<nextsent>yarowsky showed that repeated application of this process resulted in series of word sense classifiers with improved accuracy and coverage.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2611">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the strong independence requirement on the views is difficult tosatisfy.
</prevsent>
<prevsent>for practical applications, different features sets or models (that are not conditionallyindependent) have been used as an approximation for different views.
</prevsent>
</prevsection>
<citsent citstr=" W03-0407 ">
co-training has been applied to number of nlp applications, including pos-tagging (clark et al, 2003), <papid> W03-0407 </papid>parsing(sarkar, 2001), <papid> N01-1023 </papid>word sense disambiguation (mi halcea, 2004), <papid> W04-2405 </papid>and base noun phrase detection (pierce and cardie, 2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>due to the relaxation of the view independence assumption, most empirical studies suggest marginal improvement.
</nextsent>
<nextsent>the common thread between em, self-training, and co-training is that they all bootstrap off of unannotated data.
</nextsent>
<nextsent>in this work, we explore an alternative to pure?
</nextsent>
<nextsent>unannotated data; our data have been automatically annotated with projected labels from english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2612">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the strong independence requirement on the views is difficult tosatisfy.
</prevsent>
<prevsent>for practical applications, different features sets or models (that are not conditionallyindependent) have been used as an approximation for different views.
</prevsent>
</prevsection>
<citsent citstr=" N01-1023 ">
co-training has been applied to number of nlp applications, including pos-tagging (clark et al, 2003), <papid> W03-0407 </papid>parsing(sarkar, 2001), <papid> N01-1023 </papid>word sense disambiguation (mi halcea, 2004), <papid> W04-2405 </papid>and base noun phrase detection (pierce and cardie, 2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>due to the relaxation of the view independence assumption, most empirical studies suggest marginal improvement.
</nextsent>
<nextsent>the common thread between em, self-training, and co-training is that they all bootstrap off of unannotated data.
</nextsent>
<nextsent>in this work, we explore an alternative to pure?
</nextsent>
<nextsent>unannotated data; our data have been automatically annotated with projected labels from english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2613">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the strong independence requirement on the views is difficult tosatisfy.
</prevsent>
<prevsent>for practical applications, different features sets or models (that are not conditionallyindependent) have been used as an approximation for different views.
</prevsent>
</prevsection>
<citsent citstr=" W04-2405 ">
co-training has been applied to number of nlp applications, including pos-tagging (clark et al, 2003), <papid> W03-0407 </papid>parsing(sarkar, 2001), <papid> N01-1023 </papid>word sense disambiguation (mi halcea, 2004), <papid> W04-2405 </papid>and base noun phrase detection (pierce and cardie, 2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>due to the relaxation of the view independence assumption, most empirical studies suggest marginal improvement.
</nextsent>
<nextsent>the common thread between em, self-training, and co-training is that they all bootstrap off of unannotated data.
</nextsent>
<nextsent>in this work, we explore an alternative to pure?
</nextsent>
<nextsent>unannotated data; our data have been automatically annotated with projected labels from english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2614">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>the strong independence requirement on the views is difficult tosatisfy.
</prevsent>
<prevsent>for practical applications, different features sets or models (that are not conditionallyindependent) have been used as an approximation for different views.
</prevsent>
</prevsection>
<citsent citstr=" W01-0501 ">
co-training has been applied to number of nlp applications, including pos-tagging (clark et al, 2003), <papid> W03-0407 </papid>parsing(sarkar, 2001), <papid> N01-1023 </papid>word sense disambiguation (mi halcea, 2004), <papid> W04-2405 </papid>and base noun phrase detection (pierce and cardie, 2001).<papid> W01-0501 </papid></citsent>
<aftsection>
<nextsent>due to the relaxation of the view independence assumption, most empirical studies suggest marginal improvement.
</nextsent>
<nextsent>the common thread between em, self-training, and co-training is that they all bootstrap off of unannotated data.
</nextsent>
<nextsent>in this work, we explore an alternative to pure?
</nextsent>
<nextsent>unannotated data; our data have been automatically annotated with projected labels from english.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2615">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>unannotated data; our data have been automatically annotated with projected labels from english.
</prevsent>
<prevsent>although the projected labels are error-prone, they provide uswith more information than automatically predicted labels used in bootstrapping methods.with somewhat different goal in mind, active learning addresses the problem of choosing the most informative data for annotators to label so that the model would achieve the greatestimprovement.
</prevsent>
</prevsection>
<citsent citstr=" P96-1042 ">
active learning also has been applied to many nlp applications, including pos tagging (engelson and dagan, 1996) <papid> P96-1042 </papid>and pars 852ing (baldridge and osborne, 2003).<papid> W03-0403 </papid></citsent>
<aftsection>
<nextsent>the draw back of an active learning approach is that it assumes that staff of annotators is waiting on call, ready to label the examples chosen by the system at every iteration.
</nextsent>
<nextsent>in practice, it is more likely that one could only afford to staff annotators for limited period of time.
</nextsent>
<nextsent>although active learning is not focus in this paper, we owe some ideas to active learning in choosing small initial set of training examples; we discuss these ideas in section 3.2.more recently, smith and smith (2004) <papid> W04-3207 </papid>proposed to merge an english parser, word alignment model, and korean pcfg parser trained from small number of korean parse trees under unified loglinear model.</nextsent>
<nextsent>their results suggest that joint model produces somewhat more accurate korean parses than pcfg korean parser trained on small amount of annotated korean parse trees alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2616">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> background.  </section>
<citcontext>
<prevsection>
<prevsent>unannotated data; our data have been automatically annotated with projected labels from english.
</prevsent>
<prevsent>although the projected labels are error-prone, they provide uswith more information than automatically predicted labels used in bootstrapping methods.with somewhat different goal in mind, active learning addresses the problem of choosing the most informative data for annotators to label so that the model would achieve the greatestimprovement.
</prevsent>
</prevsection>
<citsent citstr=" W03-0403 ">
active learning also has been applied to many nlp applications, including pos tagging (engelson and dagan, 1996) <papid> P96-1042 </papid>and pars 852ing (baldridge and osborne, 2003).<papid> W03-0403 </papid></citsent>
<aftsection>
<nextsent>the draw back of an active learning approach is that it assumes that staff of annotators is waiting on call, ready to label the examples chosen by the system at every iteration.
</nextsent>
<nextsent>in practice, it is more likely that one could only afford to staff annotators for limited period of time.
</nextsent>
<nextsent>although active learning is not focus in this paper, we owe some ideas to active learning in choosing small initial set of training examples; we discuss these ideas in section 3.2.more recently, smith and smith (2004) <papid> W04-3207 </papid>proposed to merge an english parser, word alignment model, and korean pcfg parser trained from small number of korean parse trees under unified loglinear model.</nextsent>
<nextsent>their results suggest that joint model produces somewhat more accurate korean parses than pcfg korean parser trained on small amount of annotated korean parse trees alone.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2619">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>one method of acquiring large corpus of automatically pos tagged chinese data is by projection (yarowsky and ngai, 2001).<papid> N01-1026 </papid></prevsent>
<prevsent>this approach requires sentence-aligned english chinese corpus, high-quality english tagger, and method of aligning english and chinese words that share the same meaning.</prevsent>
</prevsection>
<citsent citstr=" W96-0213 ">
given the parallel corpus, we tagged the english words with publicly available maximum entropy tagger (ratnaparkhi, 1996), <papid> W96-0213 </papid>and we used an implementation of the ibm translation model (al onaizan et al, 1999) to align the words.</citsent>
<aftsection>
<nextsent>the chinese words in the parallel corpus would then receive the same pos tags as the english wordsto which they are aligned.
</nextsent>
<nextsent>next, the basic projection algorithm is modified to accommodate two complicating factors.
</nextsent>
<nextsent>first, word alignments are not always one-to-one.
</nextsent>
<nextsent>to compensate, we assign default tag to unaligned chinese words; in the case of one-chinese-to-many english, the chinese word would receive the tag of the final english word.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2621">
<title id=" H05-1107.xml">a backoff model for bootstrapping resources for non english languages </title>
<section> our approach.  </section>
<citcontext>
<prevsection>
<prevsent>the input is and and the output is sselcoverage of unknown words (mwc).
</prevsent>
<prevsent>this algorithm is described in figure 1, 3.3 basic pos tagging model.
</prevsent>
</prevsection>
<citsent citstr=" J93-2006 ">
it is well known that pos tagger can be trained with an hmm (weischedel et al, 1993).<papid> J93-2006 </papid>given trained model, the most likely tag sequence t?</citsent>
<aftsection>
<nextsent>= {t1, t2, . . .
</nextsent>
<nextsent>tn} is computed for the input word sentence: w?
</nextsent>
<nextsent>= {w1, w2, . . .
</nextsent>
<nextsent>wn}: t?
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2626">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>since most of these analyses are not perceived as plausible by human language user, there is need for distinguishing the plausible parse(s) of an input string from the implausible ones.
</prevsent>
<prevsent>in stochastic language processing, it is assumed that the most plausible parse of an input string is its most probable parse.
</prevsent>
</prevsection>
<citsent citstr=" C92-2065 ">
most instantiations of this idea estimate the probability of parse by assigning application probabilities to context free rewrite roles (jelinek, 1990), or by assigning combination probabilities to elementary structures (resnik, 1992; <papid> C92-2065 </papid>schabes, 1992).<papid> C92-2066 </papid></citsent>
<aftsection>
<nextsent>there is some agreement ow that context free rewrite rules are not adequate for estimating the probability of parse, since they cannot capture syntactie/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context.
</nextsent>
<nextsent>in stochastic tree-adjoining grammar (schabes, 1992), <papid> C92-2066 </papid>this lack of context-sensitivity overcome by assigning probabilities to larger structural units.</nextsent>
<nextsent>however, it is not always evident which structures should be considered as elementary structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2627">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>since most of these analyses are not perceived as plausible by human language user, there is need for distinguishing the plausible parse(s) of an input string from the implausible ones.
</prevsent>
<prevsent>in stochastic language processing, it is assumed that the most plausible parse of an input string is its most probable parse.
</prevsent>
</prevsection>
<citsent citstr=" C92-2066 ">
most instantiations of this idea estimate the probability of parse by assigning application probabilities to context free rewrite roles (jelinek, 1990), or by assigning combination probabilities to elementary structures (resnik, 1992; <papid> C92-2065 </papid>schabes, 1992).<papid> C92-2066 </papid></citsent>
<aftsection>
<nextsent>there is some agreement ow that context free rewrite rules are not adequate for estimating the probability of parse, since they cannot capture syntactie/lexical context, and hence cannot describe how the probability of syntactic structures or lexical items depends on that context.
</nextsent>
<nextsent>in stochastic tree-adjoining grammar (schabes, 1992), <papid> C92-2066 </papid>this lack of context-sensitivity overcome by assigning probabilities to larger structural units.</nextsent>
<nextsent>however, it is not always evident which structures should be considered as elementary structures.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2632">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>however, it is not always evident which structures should be considered as elementary structures.
</prevsent>
<prevsent>in (schabes, 1992) <papid> C92-2066 </papid>it is proposed to infer stochastic tag from large training corpus using an inside-outside-like rative algorithm.</prevsent>
</prevsection>
<citsent citstr=" C92-3126 ">
data oriented parsing fdop) (scha, 1990; bod, 1992<papid> C92-3126 </papid>a), distinguishes itself from other statistical approaches in that it omits the step of inferring grammar from corpus.</citsent>
<aftsection>
<nextsent>instead, an annotated corpus is directly used as stochastic grammar.
</nextsent>
<nextsent>an input string is parsed by combining subtrees from the corpus.
</nextsent>
<nextsent>in this view, every subtree can be considered as an elementary structure.
</nextsent>
<nextsent>as consequence, one parse tree can usually be generated by several derivations that involve different subtrees.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2636">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> motivation </section>
<citcontext>
<prevsection>
<prevsent>we will show ,hat conventional parsing techniques can be applied to dop, but that this becomes very inefficient, since the number of derivations of parse grows exponentially with the length of the input suing.
</prevsent>
<prevsent>however, we will show that dop can be parsed in polynomial time by using monte carlo techniques.
</prevsent>
</prevsection>
<citsent citstr=" P92-1017 ">
an important advantage of using corpus for probability calculation, is that no tr0jning of parameters is needed, as is the case for other stochastic grammars (jelinek et al, 1990; pereira and schabes, 1992; <papid> P92-1017 </papid>schabes, 1992).<papid> C92-2066 </papid></citsent>
<aftsection>
<nextsent>secondly, since we take into account all derivations of parse, no relationship that might possibly be of statistical interest is ignored.
</nextsent>
<nextsent>38
</nextsent>
<nextsent>as might be clear by now, ix)p-model is characterized by corpus of tree structures, together with set of operations that combine subtrees from the corpus into new trees.
</nextsent>
<nextsent>in this section we explain more precisely what we mean by subtree, operations etc., in order to arrive at definitions of parse and the probability of parse with respect a corpus.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2652">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> monte carlo parsing.  </section>
<citcontext>
<prevsection>
<prevsent>with respect to the theory of computation, monte carlo parser is probabilistic algorithm which belongs to the class of bounded error probabilistic polynomial time (bpp) algorithms.
</prevsent>
<prevsent>bpp-problems are characterized by the following: it may take exponential time to solve them exactly, but there exists an estimation algorithm with probability of error that becomes arbitrarily small in polynomial time.
</prevsent>
</prevsection>
<citsent citstr=" H90-1021 ">
experiments on the atis corpus for our experiments we used part-of-speech sequences of spoken-language transcriptions from the air travel information system (atis) corpus (hemphill et al, 1990), <papid> H90-1021 </papid>with the labeled-bracketings of those sequences in the penn treebank (marcus, 1991).<papid> H91-1104 </papid></citsent>
<aftsection>
<nextsent>the 750 41 labeled-bracketings were divided at random into dop-corpus of 675 trees and test set of 75 part-of- speech sequences.
</nextsent>
<nextsent>the following tree is an example from the dop-corpns, where for reasons of readability the lexical items are added to the part-of-speech tags.
</nextsent>
<nextsent>( (s (np *) fvp (vb show) (np (pp me)) (np (np (pdt all)) (dt the) (jj nonstop) (nns flights) (pp (pp on from) (np (np dallas))) (pp (to to) (np (np denver)))) (adjp (jj early) (pp (in in) (np (dt the) (nn morning)))))) .) as measure for pars/n# accuracy we took the percentage of the test sentences for which the maximum probability parse derived by the monte carlo parser (for sample size n) is identical to the treebankparse.
</nextsent>
<nextsent>it is one of the most essential features of the dop approach, that arbitrarily large subtrees are taken into consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2653">
<title id=" E93-1006.xml">using an annotated corpus as a stochastic grammar </title>
<section> monte carlo parsing.  </section>
<citcontext>
<prevsection>
<prevsent>with respect to the theory of computation, monte carlo parser is probabilistic algorithm which belongs to the class of bounded error probabilistic polynomial time (bpp) algorithms.
</prevsent>
<prevsent>bpp-problems are characterized by the following: it may take exponential time to solve them exactly, but there exists an estimation algorithm with probability of error that becomes arbitrarily small in polynomial time.
</prevsent>
</prevsection>
<citsent citstr=" H91-1104 ">
experiments on the atis corpus for our experiments we used part-of-speech sequences of spoken-language transcriptions from the air travel information system (atis) corpus (hemphill et al, 1990), <papid> H90-1021 </papid>with the labeled-bracketings of those sequences in the penn treebank (marcus, 1991).<papid> H91-1104 </papid></citsent>
<aftsection>
<nextsent>the 750 41 labeled-bracketings were divided at random into dop-corpus of 675 trees and test set of 75 part-of- speech sequences.
</nextsent>
<nextsent>the following tree is an example from the dop-corpns, where for reasons of readability the lexical items are added to the part-of-speech tags.
</nextsent>
<nextsent>( (s (np *) fvp (vb show) (np (pp me)) (np (np (pdt all)) (dt the) (jj nonstop) (nns flights) (pp (pp on from) (np (np dallas))) (pp (to to) (np (np denver)))) (adjp (jj early) (pp (in in) (np (dt the) (nn morning)))))) .) as measure for pars/n# accuracy we took the percentage of the test sentences for which the maximum probability parse derived by the monte carlo parser (for sample size n) is identical to the treebankparse.
</nextsent>
<nextsent>it is one of the most essential features of the dop approach, that arbitrarily large subtrees are taken into consideration.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2657">
<title id=" H05-1005.xml">improving multilingual summarization using redundancy in the input to correct mt errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>webegin by tackling the problem of generating references to people in english summaries of arabic texts(  2).
</prevsent>
<prevsent>this special case involves large amounts of redundancy and allows for relatively deep english language modeling, resulting in good error correction.
</prevsent>
</prevsection>
<citsent citstr=" N04-1019 ">
we extend our approach to arbitrary nps in  3.the evaluation emphasis in multi-document summarization has been on evaluating content (not read ability), using manual (nenkova and passonneau, 2004) <papid> N04-1019 </papid>as well as automatic (lin and hovy, 2003) <papid> N03-1020 </papid>methods.</citsent>
<aftsection>
<nextsent>we evaluate readability of the generated noun phrases by computing precision, recall and fmeasure of the generated version compared to multiple human models of the same reference, computing these metrics on n-grams.
</nextsent>
<nextsent>our results show that our system performs significantly better on precision over two baselines (most frequent initial reference and randomly chosen initial reference).
</nextsent>
<nextsent>precision isthe most important of these measures as it is important to have correct reference, even if we dont retain all of the words used in the human models.
</nextsent>
<nextsent>2.1 data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2658">
<title id=" H05-1005.xml">improving multilingual summarization using redundancy in the input to correct mt errors </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>webegin by tackling the problem of generating references to people in english summaries of arabic texts(  2).
</prevsent>
<prevsent>this special case involves large amounts of redundancy and allows for relatively deep english language modeling, resulting in good error correction.
</prevsent>
</prevsection>
<citsent citstr=" N03-1020 ">
we extend our approach to arbitrary nps in  3.the evaluation emphasis in multi-document summarization has been on evaluating content (not read ability), using manual (nenkova and passonneau, 2004) <papid> N04-1019 </papid>as well as automatic (lin and hovy, 2003) <papid> N03-1020 </papid>methods.</citsent>
<aftsection>
<nextsent>we evaluate readability of the generated noun phrases by computing precision, recall and fmeasure of the generated version compared to multiple human models of the same reference, computing these metrics on n-grams.
</nextsent>
<nextsent>our results show that our system performs significantly better on precision over two baselines (most frequent initial reference and randomly chosen initial reference).
</nextsent>
<nextsent>precision isthe most important of these measures as it is important to have correct reference, even if we dont retain all of the words used in the human models.
</nextsent>
<nextsent>2.1 data.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2659">
<title id=" H05-1005.xml">improving multilingual summarization using redundancy in the input to correct mt errors </title>
<section> references to people.  </section>
<citcontext>
<prevsection>
<prevsent>for four grams, p, and are zero, as there is four gram in the models, but none in the generated np.we used 6 document sets from duc04 for development purposes and present the average p, and for the remaining 18 sets in table 1.
</prevsent>
<prevsent>there were 210 generated references in the 18 testing sets.
</prevsent>
</prevsection>
<citsent citstr=" P02-1040 ">
the table also shows the popular bleu (papineni et al, 2002)<papid> P02-1040 </papid>and nist2 mt metrics.</citsent>
<aftsection>
<nextsent>we also provide two baselines - most frequent initial reference to the person in the input (base1) and randomly selected initial reference to the person (base2).
</nextsent>
<nextsent>as table 1 shows, base1 performs better than random selection.
</nextsent>
<nextsent>this 2http://www.nist.gov/speech/tests/mt/resources/scoring.htm 36 unigrams j? j? j? generated 0.847*@ 0.786 0.799*@ base1 0.753* 0.805 0.746* base2 0.681 0.767 0.688 bigrams j? j? j? generated 0.684*@ 0.591 0.615* base1 0.598* 0.612 0.562* base2 0.492 0.550 0.475 trigrams j? j? j? generated 0.514*@ 0.417 0.443* base1 0.424* 0.432 0.393* base2 0.338 0.359 0.315 four grams j? j? j? generated 0.411*@ 0.336 0.351* base1 0.320 0.360* 0.302 base2 0.252 0.280 0.235 @ significantly better than base1 * significantly better than base2 (significance tested using unpaired t-test at 95% confidence) mt metrics generated base1 base2 bleu 0.898 0.499 0.400 nist 8.802 6.423 5.658 table 1: evaluation of generated reference is intuitive as it also uses redundancy to correct errors, at the level of phrases rather than words.
</nextsent>
<nextsent>the generation module outperforms both baselines, particularly on precision - which for unigrams gives an indication of the correctness of lexical choice, andfor higher ngrams gives an indication of gram mati cality.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2660">
<title id=" H05-1005.xml">improving multilingual summarization using redundancy in the input to correct mt errors </title>
<section> arbitrary noun phrases.  </section>
<citcontext>
<prevsection>
<prevsent>thus, similar noun phrases in sentences within cluster are likely to refer to the same entities.
</prevsent>
<prevsent>we do noun phrase coreference by identifying lexically similar noun phrases within cluster.
</prevsent>
</prevsection>
<citsent citstr=" W99-0625 ">
we use simfinder(hatzivassiloglou et al, 1999) <papid> W99-0625 </papid>for sentence clustering and the f-measure for word overlap to compare noun phrases.</citsent>
<aftsection>
<nextsent>we set threshold for deciding coreference by experimenting on the 6 development sets (cf.
</nextsent>
<nextsent> 2.6)?
</nextsent>
<nextsent>the most accurate coreference occurred with threshold of f=0.6 and constraint that thetwo noun phrases must have at least 2 words in common that were neither determiners nor prepositions.
</nextsent>
<nextsent>for the reference to theun special commission in figure 2, we obtained the following choices from alignments and coreference across translations and documents within sentence cluster:1.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2661">
<title id=" H05-1046.xml">disambiguating toponyms in news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another learner we tried, the smo support vector machine from weka (witten and frank 2005), was marginally better, showing 81.0 predictive accuracy training and testing on mac dev+mac-ml (ten-fold cross-validation, k=20) and 78.5 predictive accuracy training on mac dev+mac-ml and testing on hac (k=20).
</prevsent>
<prevsent>ripper rules are of course more transparent: example rules learned from mac-dev are shown in table 7, along with their coverage of feature vectors and accuracy on the test set hac.
</prevsent>
</prevsection>
<citsent citstr=" W03-0103 ">
work related to toponym tagging has included harvesting of gazette ers from the web (uryupina 2003), <papid> W03-0103 </papid>hand-coded rules to place name disambiguation, e.g., (li et al 2003) <papid> W03-0106 </papid>zong et al 2005), and machine learning approaches to the problem, e.g., (smith and mann 2003).<papid> W03-0107 </papid></citsent>
<aftsection>
<nextsent>there has of course been large amount of work on the more general problem of word-sense disambiguation, e.g., (yarowsky 1995) (<papid> P95-1026 </papid>kilgarriff and edmonds 2002).</nextsent>
<nextsent>we discuss the most relevant work here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2662">
<title id=" H05-1046.xml">disambiguating toponyms in news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another learner we tried, the smo support vector machine from weka (witten and frank 2005), was marginally better, showing 81.0 predictive accuracy training and testing on mac dev+mac-ml (ten-fold cross-validation, k=20) and 78.5 predictive accuracy training on mac dev+mac-ml and testing on hac (k=20).
</prevsent>
<prevsent>ripper rules are of course more transparent: example rules learned from mac-dev are shown in table 7, along with their coverage of feature vectors and accuracy on the test set hac.
</prevsent>
</prevsection>
<citsent citstr=" W03-0106 ">
work related to toponym tagging has included harvesting of gazette ers from the web (uryupina 2003), <papid> W03-0103 </papid>hand-coded rules to place name disambiguation, e.g., (li et al 2003) <papid> W03-0106 </papid>zong et al 2005), and machine learning approaches to the problem, e.g., (smith and mann 2003).<papid> W03-0107 </papid></citsent>
<aftsection>
<nextsent>there has of course been large amount of work on the more general problem of word-sense disambiguation, e.g., (yarowsky 1995) (<papid> P95-1026 </papid>kilgarriff and edmonds 2002).</nextsent>
<nextsent>we discuss the most relevant work here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2664">
<title id=" H05-1046.xml">disambiguating toponyms in news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>another learner we tried, the smo support vector machine from weka (witten and frank 2005), was marginally better, showing 81.0 predictive accuracy training and testing on mac dev+mac-ml (ten-fold cross-validation, k=20) and 78.5 predictive accuracy training on mac dev+mac-ml and testing on hac (k=20).
</prevsent>
<prevsent>ripper rules are of course more transparent: example rules learned from mac-dev are shown in table 7, along with their coverage of feature vectors and accuracy on the test set hac.
</prevsent>
</prevsection>
<citsent citstr=" W03-0107 ">
work related to toponym tagging has included harvesting of gazette ers from the web (uryupina 2003), <papid> W03-0103 </papid>hand-coded rules to place name disambiguation, e.g., (li et al 2003) <papid> W03-0106 </papid>zong et al 2005), and machine learning approaches to the problem, e.g., (smith and mann 2003).<papid> W03-0107 </papid></citsent>
<aftsection>
<nextsent>there has of course been large amount of work on the more general problem of word-sense disambiguation, e.g., (yarowsky 1995) (<papid> P95-1026 </papid>kilgarriff and edmonds 2002).</nextsent>
<nextsent>we discuss the most relevant work here.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2666">
<title id=" H05-1046.xml">disambiguating toponyms in news </title>
<section> related work.  </section>
<citcontext>
<prevsection>
<prevsent>ripper rules are of course more transparent: example rules learned from mac-dev are shown in table 7, along with their coverage of feature vectors and accuracy on the test set hac.
</prevsent>
<prevsent>work related to toponym tagging has included harvesting of gazette ers from the web (uryupina 2003), <papid> W03-0103 </papid>hand-coded rules to place name disambiguation, e.g., (li et al 2003) <papid> W03-0106 </papid>zong et al 2005), and machine learning approaches to the problem, e.g., (smith and mann 2003).<papid> W03-0107 </papid></prevsent>
</prevsection>
<citsent citstr=" P95-1026 ">
there has of course been large amount of work on the more general problem of word-sense disambiguation, e.g., (yarowsky 1995) (<papid> P95-1026 </papid>kilgarriff and edmonds 2002).</citsent>
<aftsection>
<nextsent>we discuss the most relevant work here.
</nextsent>
<nextsent>while (uryupina 2003) <papid> W03-0103 </papid>uses machine learning to induce gazette ers from the internet, we merely download and merge information from two popular web gazetteers.</nextsent>
<nextsent>(li et al 2003) <papid> W03-0106 </papid>use statistical approach to tag place names as location class.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2670">
<title id=" H05-1046.xml">disambiguating toponyms in news </title>
<section> conclusion.  </section>
<citcontext>
<prevsection>
<prevsent>it has developed toponym disambiguator that, when trained on entirely machine annotated corpora that avail of easily available internet gazette ers, disambiguates toponyms in human-annotated corpus at 78.5% accuracy.
</prevsent>
<prevsent>our current project includes integrating our disambiguator with other gazette ers and with geo visualization system.
</prevsent>
</prevsection>
<citsent citstr=" W95-0101 ">
we will also study the effect of other window sizes and the combination of this unsupervised approach with minimally-supervised approaches such as (brill 1995) (<papid> W95-0101 </papid>smith and mann 2003).<papid> W03-0107 </papid></citsent>
<aftsection>
<nextsent>to help mitigate against data sparseness, we will cluster terms based on stemming and semantic similarity.
</nextsent>
<nextsent>the resources and tools developed here may be obtained freely by contacting the authors.
</nextsent>


</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2673">
<title id=" H89-1045.xml">chart parsing of stochastic spoken language models </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>spoken language systems, however, deal with probabilistic symbols and both the grammar formalism and the parsing algorithm must accommodate hese (hemphill and picone, 1989).
</prevsent>
<prevsent>we have developed chart parsing algorithm that allows on-line parsing and correctly operates with probabilities.
</prevsent>
</prevsection>
<citsent citstr=" P83-1021 ">
basically, it is similar to earley algorithm (earley, 1970), augmented with unification (pereira and warren, 1983) <papid> P83-1021 </papid>and probability (paeseler, 1987), but with delayed commitment approach to scoring (aho and peterson, 1972).</citsent>
<aftsection>
<nextsent>this algorithm operates from left to right in combined bottom-up and top-down fashion, providing terminal hypotheses at each timeframe to lower levels and accepting completed hypotheses that began at some time in the past.
</nextsent>
<nextsent>the algorithm has not yet been fully implemented for ugs, but the following section explores the ramifications of this type of approach.
</nextsent>
<nextsent>251 best + beam in 1 best figure effect of chart parsing on pruning.
</nextsent>
<nextsent>probabilistic chart parsing in this section, we describe the chart parser as applied to stochastic regular grammars.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2674">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> introduction </section>
<citcontext>
<prevsection>
<prevsent>vijay-shanker and joshi (1985) introduced the first tag parser in cyk-like algorithm.
</prevsent>
<prevsent>because of the adjoining operation, the time complexity of ltag parsing is as large as  , compared with   of cfg parsing, where  is the length of the sentence to be parsed.
</prevsent>
</prevsection>
<citsent citstr=" E91-1006 ">
many ltag parsers were proposed, such as the head-driven earley style parser (lavelli and satta, 1991) <papid> E91-1006 </papid>and the head-corner parser (van noord, 1994).</citsent>
<aftsection>
<nextsent>the high time complexity prevents ltag parsing from real-time applications.
</nextsent>
<nextsent>in this paper, we work on ltag-spinal (shen and joshi, 2005), an interesting subset of ltag, which preserves almost all of the strong generative power of ltag, and it is both weakly and strongly more powerful than cfg 1.
</nextsent>
<nextsent>we will present statistical incremental parsing for ltag-spinal.
</nextsent>
<nextsent>as far as we know, this parser is the first comprehensive attempt of efficient statistical parsing with formal grammar with prov ably stronger generative power than cfg, supporting the full adjoining operation, dynamic predicate coordination, as well as non-projective dependencies 2.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2675">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>we use for attach, for adjoin, and for conjoin.
</prevsent>
<prevsent>1further formal results are described in (shen and joshi, 2005).
</prevsent>
</prevsection>
<citsent citstr=" P02-1035 ">
there is also some relationship of ltag-spinal to th espinal form context-free tree grammar, as in (fujiyoshi and kasai, 2000) 2in (riezler et al, 2002), <papid> P02-1035 </papid>the maxent model was used to rerank the k-best parses generated by rule-based lfg parser.</citsent>
<aftsection>
<nextsent>811 jj vp jj vp to cc vp *vbznndt xp xp wdt prp interesting new andseemswhichparsera meto t t a c vp xpxp xpxp figure 1: an example in ltag-spinal.
</nextsent>
<nextsent>a=adjoin, t=attach, c=conjoin.
</nextsent>
<nextsent>attachment in ltag-spinal is similar to sister adjunction (chiang, 2000) <papid> P00-1058 </papid>in tree insertion grammar (tig) (schabes and waters, 1995).<papid> J95-4002 </papid></nextsent>
<nextsent>it represents combination of substitution and sister adjunction.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2676">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>811 jj vp jj vp to cc vp *vbznndt xp xp wdt prp interesting new andseemswhichparsera meto t t a c vp xpxp xpxp figure 1: an example in ltag-spinal.
</prevsent>
<prevsent>a=adjoin, t=attach, c=conjoin.
</prevsent>
</prevsection>
<citsent citstr=" P00-1058 ">
attachment in ltag-spinal is similar to sister adjunction (chiang, 2000) <papid> P00-1058 </papid>in tree insertion grammar (tig) (schabes and waters, 1995).<papid> J95-4002 </papid></citsent>
<aftsection>
<nextsent>it represents combination of substitution and sister adjunction.
</nextsent>
<nextsent>the attachment operation is designed to encode the ambiguity of an argument and an adjunct.
</nextsent>
<nextsent>adjunction inserts part of the spine and the foot node of an auxiliary tree into to the spine of another tree.
</nextsent>
<nextsent>the adjunction operation can effectively do wrapping, which distinguishes itself from sister adjunction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2677">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>811 jj vp jj vp to cc vp *vbznndt xp xp wdt prp interesting new andseemswhichparsera meto t t a c vp xpxp xpxp figure 1: an example in ltag-spinal.
</prevsent>
<prevsent>a=adjoin, t=attach, c=conjoin.
</prevsent>
</prevsection>
<citsent citstr=" J95-4002 ">
attachment in ltag-spinal is similar to sister adjunction (chiang, 2000) <papid> P00-1058 </papid>in tree insertion grammar (tig) (schabes and waters, 1995).<papid> J95-4002 </papid></citsent>
<aftsection>
<nextsent>it represents combination of substitution and sister adjunction.
</nextsent>
<nextsent>the attachment operation is designed to encode the ambiguity of an argument and an adjunct.
</nextsent>
<nextsent>adjunction inserts part of the spine and the foot node of an auxiliary tree into to the spine of another tree.
</nextsent>
<nextsent>the adjunction operation can effectively do wrapping, which distinguishes itself from sister adjunction.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2678">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>it is not difficult to see that adjunction only happens on the spine of tree.
</prevsent>
<prevsent>this property will be exploited in the incremental parser.
</prevsent>
</prevsection>
<citsent citstr=" C96-2103 ">
conjunction is similar to what was originally proposed in (sarkar and joshi, 1996).<papid> C96-2103 </papid></citsent>
<aftsection>
<nextsent>however, inltag-spinal, the conjunction operation is much easier to handle, since we only conjoin spinal elementary trees and we do not need to enumerate contraction sets for conjunction.
</nextsent>
<nextsent>in our formalization, conjunction can be treated as special adjunction, how ever, this is beyond the scope of this paper.
</nextsent>
<nextsent>we use the ltag-spinal treebank described in (shen and joshi, 2005), which was extracted from the penn treebank (ptb) (marcus et al, 1994) with propbank (palmer et al, 2005) <papid> J05-1004 </papid>annotations.</nextsent>
<nextsent>2.1 relation to traditional ltag.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2679">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>however, inltag-spinal, the conjunction operation is much easier to handle, since we only conjoin spinal elementary trees and we do not need to enumerate contraction sets for conjunction.
</prevsent>
<prevsent>in our formalization, conjunction can be treated as special adjunction, how ever, this is beyond the scope of this paper.
</prevsent>
</prevsection>
<citsent citstr=" J05-1004 ">
we use the ltag-spinal treebank described in (shen and joshi, 2005), which was extracted from the penn treebank (ptb) (marcus et al, 1994) with propbank (palmer et al, 2005) <papid> J05-1004 </papid>annotations.</citsent>
<aftsection>
<nextsent>2.1 relation to traditional ltag.
</nextsent>
<nextsent>ltag-spinal preserves most of the strong generative power of ltag.
</nextsent>
<nextsent>it can be shown that ltagspinal with adjoining restrictions (joshi and schabes, 1997) has stronger generative capacity as compared to cfg.
</nextsent>
<nextsent>for example, there exists an ltag spinal grammar that generates  fffiffifl!
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2680">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>a spinal template plus the root nodes of the subtrees attached to this template can be viewed as traditional ltag elementary tree.
</prevsent>
<prevsent>more specifically, it encodes setof possible elementary trees if we distinguish substitution from sister adjunction.
</prevsent>
</prevsection>
<citsent citstr=" W02-1507 ">
thus, the ltag spinal parsing model to be proposed in section 3 canbe viewed as parser at the meta-grammar (candito, 1998; kinyon and prolo, 2002) <papid> W02-1507 </papid>level for traditional ltag.</citsent>
<aftsection>
<nextsent>derivation tree construction and full size elementary tree filtering are processed in parallel.
</nextsent>
<nextsent>researches in statistical cfg parsing (ratna parkhi, 1997; <papid> W97-0301 </papid>collins, 1999) and psycho linguistics 812(shieber and johnson, 1993) showed that this strategy is desirable for nlp.</nextsent>
<nextsent>furthermore, the way that we split traditional ltag elementary tree along the spine is similar to the method with which evans and weir (1997) compiled the xtag english grammar into finite state automata.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2681">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> ltag-spinal and the treebank.  </section>
<citcontext>
<prevsection>
<prevsent>thus, the ltag spinal parsing model to be proposed in section 3 canbe viewed as parser at the meta-grammar (candito, 1998; kinyon and prolo, 2002) <papid> W02-1507 </papid>level for traditional ltag.</prevsent>
<prevsent>derivation tree construction and full size elementary tree filtering are processed in parallel.</prevsent>
</prevsection>
<citsent citstr=" W97-0301 ">
researches in statistical cfg parsing (ratna parkhi, 1997; <papid> W97-0301 </papid>collins, 1999) and psycho linguistics 812(shieber and johnson, 1993) showed that this strategy is desirable for nlp.</citsent>
<aftsection>
<nextsent>furthermore, the way that we split traditional ltag elementary tree along the spine is similar to the method with which evans and weir (1997) compiled the xtag english grammar into finite state automata.
</nextsent>
<nextsent>in their work, this method was designed to employ shared structure in rule-basedparser.
</nextsent>
<nextsent>but here we extend this technique to statistical ltag parsing.
</nextsent>
<nextsent>2.2 relation to propbank.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2684">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> incremental parsing.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, there is also strong connection between incremental parsing and psycho linguistics, and this connection is also observed in the ltag formalism (ferreira, 2000; sturt and lombardo, 2005).
</prevsent>
<prevsent>in recent years, there have been many interesting works on incremental or semi-incremental parsing.by semi-incremental we mean the parsers that allow several rounds of left to right scans instead of one.
</prevsent>
</prevsection>
<citsent citstr=" J01-2004 ">
both left-corner strategy (ratnaparkhi, 1997; <papid> W97-0301 </papid>roark, 2001; <papid> J01-2004 </papid>prolo, 2003; henderson, 2003; collin sand roark, 2004) <papid> P04-1015 </papid>and head-corner strategy (hen derson, 2000; yamada and matsumoto, 2003) were employed in incremental parsing.</citsent>
<aftsection>
<nextsent>the head-corner approach is more natural to the ltag formalism (evans and weir, 1997).
</nextsent>
<nextsent>in our approach, we use stack of derivation tree lets to represent the partial parsing result.
</nextsent>
<nextsent>furthermore, the ltag formalism allows us to handle non-projectivity dependencies,which cannot be generated by cfg or dependency parser.
</nextsent>
<nextsent>in fact, the idea of incremental parsing with ltag is closely related to the work on super tagging (joshi and srinivas, 1994).<papid> C94-1024 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2685">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> incremental parsing.  </section>
<citcontext>
<prevsection>
<prevsent>furthermore, there is also strong connection between incremental parsing and psycho linguistics, and this connection is also observed in the ltag formalism (ferreira, 2000; sturt and lombardo, 2005).
</prevsent>
<prevsent>in recent years, there have been many interesting works on incremental or semi-incremental parsing.by semi-incremental we mean the parsers that allow several rounds of left to right scans instead of one.
</prevsent>
</prevsection>
<citsent citstr=" P04-1015 ">
both left-corner strategy (ratnaparkhi, 1997; <papid> W97-0301 </papid>roark, 2001; <papid> J01-2004 </papid>prolo, 2003; henderson, 2003; collin sand roark, 2004) <papid> P04-1015 </papid>and head-corner strategy (hen derson, 2000; yamada and matsumoto, 2003) were employed in incremental parsing.</citsent>
<aftsection>
<nextsent>the head-corner approach is more natural to the ltag formalism (evans and weir, 1997).
</nextsent>
<nextsent>in our approach, we use stack of derivation tree lets to represent the partial parsing result.
</nextsent>
<nextsent>furthermore, the ltag formalism allows us to handle non-projectivity dependencies,which cannot be generated by cfg or dependency parser.
</nextsent>
<nextsent>in fact, the idea of incremental parsing with ltag is closely related to the work on super tagging (joshi and srinivas, 1994).<papid> C94-1024 </papid></nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2686">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> incremental parsing.  </section>
<citcontext>
<prevsection>
<prevsent>in our approach, we use stack of derivation tree lets to represent the partial parsing result.
</prevsent>
<prevsent>furthermore, the ltag formalism allows us to handle non-projectivity dependencies,which cannot be generated by cfg or dependency parser.
</prevsent>
</prevsection>
<citsent citstr=" C94-1024 ">
in fact, the idea of incremental parsing with ltag is closely related to the work on super tagging (joshi and srinivas, 1994).<papid> C94-1024 </papid></citsent>
<aftsection>
<nextsent>a supertager first assigns the correct ltag elementary tree to each word.
</nextsent>
<nextsent>thena lightweight dependency analyzer (lda) (srini vas, 1997) composes the whole derivation tree with these elementary trees.
</nextsent>
<nextsent>we use incremental parsing to incorporate supertager and lda dynamically.
</nextsent>
<nextsent>the model of incremental ltag parsing is also similar to structured language modeling (slm) in(chelba and jelinek, 2000).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2689">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> incremental parsing.  </section>
<citcontext>
<prevsection>
<prevsent>many machine learning algorithms have been successfully applied to parsing, incremental parsing, or shallow parsing (ratnaparkhi, 1997; <papid> W97-0301 </papid>punyakanok and roth, 2001; lafferty et al, 2001; taskar et al,2003), which can be applied to our incremental parsing algorithm.</prevsent>
<prevsent>814 jj vp jj vp to prp cc vp *vbznn wdtdt xp xp interesting new andseemswhichparsera meto 1 2 4 5 6 11 16 17 gg g 14 g t t 9 8 7 10 12 3 15 13 vp xpxp xp xp figure 2: incremental parsing with eager model.</prevsent>
</prevsection>
<citsent citstr=" W02-1001 ">
a=adjoin, t=attach, c=conjoin, g=generate in this paper, we use the perceptron-like algorithm proposed in (collins, 2002) <papid> W02-1001 </papid>which does not suffer from the label bias problem, and is fast in train ing.</citsent>
<aftsection>
<nextsent>we also employ the voted perceptron algorithm (freund and schapire, 1999) and the early update technique as in (collins and roark, 2004).<papid> P04-1015 </papid></nextsent>
<nextsent>3.5 features.</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2691">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>table 2 shows that the eager model is more thantwo times faster than the flex model, as we expected.
</prevsent>
<prevsent>the time spent on k-best parsing is proportional to the beam width.
</prevsent>
</prevsection>
<citsent citstr=" A00-2018 ">
the parser proposed in this paper is an incremental parser, so the accuracy on dependency is lower than that for chart parsers, for example like those reported in (collins, 1999; charniak, 2000).<papid> A00-2018 </papid></citsent>
<aftsection>
<nextsent>5 however, it should be noted that the dependencies computed by our parser are deeper than those calculated by parsers working directly on ptb.
</nextsent>
<nextsent>this is due to the treatment of adjunction and coordination.
</nextsent>
<nextsent>on the other hand, the ltag-spinal treebank used in this paper shows high degree of compatibility with the propbank, as shown in (shen and joshi, 2005), so the ltag derivations given by the parser are very useful for predicate-argument recognition.
</nextsent>
<nextsent>we plan to improve the parsing performance by reranking and extend our work to semantic parsing (mooney, 2004).
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
<cited id="J2692">
<title id=" H05-1102.xml">incremental ltag parsing </title>
<section> discussion and future work.  </section>
<citcontext>
<prevsection>
<prevsent>we plan to improve the parsing performance by reranking and extend our work to semantic parsing (mooney, 2004).
</prevsent>
<prevsent>another interesting topic is whether this parser can be applied to languages which have various long-distance scrambling, as in german.
</prevsent>
</prevsection>
<citsent citstr=" E91-1005 ">
it appears that by carefully modifying the definition of visible spines, we can represent scrambling structures,which at present can only be represented by multicomponent tag (becker et al, 1991).<papid> E91-1005 </papid></citsent>
<aftsection>
<nextsent>in this paper, we present an efficient incremental parser for ltag-spinal, variant of ltag whichis both linguistically and psycholinguistically motivated.
</nextsent>
<nextsent>as far as we know, the statistical incremental parser proposed in this paper is the first comprehensive attempt of efficient statistical parsing witha formal grammar with prov ably stronger generative power than cfg, supporting the adjoining operation, dynamic predicate coordination, as well as non-projective dependencies.
</nextsent>
<nextsent>we have trained and tested our parser on the ltag-spinal treebank, extracted from the penn treebank with propbank annotation, using gold standard pos tags as part of the input, the parser achieves an f-score of 89.3% for syntactic dependency on section 23 of ptb.
</nextsent>
<nextsent>because of the treatment of adjunction and predicate coordination,these dependencies, which are defined on ltagspinal derivation trees, are deeper than the dependencies extracted from ptb alone with head rules.
</nextsent>
</aftsection>
</citcontext>
<tag> </tag>
</cited>
</paper>